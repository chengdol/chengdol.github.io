<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Ansible Quick Test</title>
    <url>/2021/03/27/ansible-quick-test/</url>
    <content><![CDATA[
When I was working at IBM, I applied a dedicated cluster for Ansible learning.
After I left, I decide to use Vagrant to create local cluster for the same
purpose.

> NOTE: I have also created a docker sponsored Ansible testing environment,
please see [here](https://github.com/chengdol/InfraTree/tree/master/docker-ansible)

Please check
[Vagrant Ansible testing cluster repo](https://github.com/chengdol/InfraTree/tree/master/vagrant-ansible). Follow the README to set up and play with ansible. The problems I had at the
time of creating the repo:
1. how to establish the SSH connection to Vagrant VM.
2. the `sed` insert has subtle difference in Mac.

# Ansible Install
[Ansible Install guide](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html)
- Control node requirements:
Starting with `ansible-core 2.11`, the project will only be packaged for Python
`3.8` and newer. 

If you are using Ansible to manage machines in a cloud, consider using a machine
inside that cloud as your control node. In most cases Ansible will perform
better from a machine on the cloud than from a machine on the open Internet.

Managed node requirements:
Although you do not need a daemon on your managed nodes, you do need a way for
Ansible to communicate with them. For most managed nodes, Ansible makes a
connection over SSH and transfers modules using SFTP. For any machine or device
that can run Python, you also need Python 2 (version 2.6 or later) or Python 3
(version 3.5 or later).

If install on Linux using yum (I use pip install in virtualenv in the demo, see repo README):
```bash
# search ansible package
# ansible.noarch
# ansible-python3.noarch
# SSH-based configuration management, deployment, and task execution system
yum search ansible
# python2
sudo yum install -y -q ansible
# python3
sudo yum install -y -q ansible-python3
```

# Ansible Inventory
[How to build your inventory](https://docs.ansible.com/ansible/2.10/user_guide/intro_inventory.html#intro-inventory), for `inventory` file, 主要涉及一些ssh connection的设置.
[Position the target hosts from group](https://docs.ansible.com/ansible/latest/user_guide/intro_patterns.html#using-group-position-in-patterns)


# Ansible Config
[Ansible Configuration Settings](https://docs.ansible.com/ansible/latest/reference_appendices/config.html)
for `ansible.cfg` file.

- [ansible callback plugin](https://www.jeffgeerling.com/blog/2018/use-ansibles-yaml-callback-plugin-better-cli-experience): use `stdout_callback = debug` is fine in most cases.


# Ansible Yaml Format
[Yaml syntax](https://docs.ansible.com/ansible/latest/reference_appendices/YAMLSyntax.html),
Especially the difference between `>` and `|` for multi-line coding:

Spanning multiple lines using a `|` will include the newlines and any trailing
spaces. Using a `>` will fold newlines to spaces; In either case the indentation
will be ignored.
```yaml
include_newlines: |
            exactly as you see
            will appear these three
            lines of poetry

fold_newlines: >
            this is really a
            single line of text
            despite appearances
```

# Ansible Run
[Ad-hoc command](https://docs.ansible.com/ansible/latest/user_guide/intro_adhoc.html)
example:
```bash
# -v: verbose, display output
# can specify single machine
ansible -v -i vagrant_ansible_inventory.ini worker1 -m ping
# all
ansible -v -i vagrant_ansible_inventory.ini all -m shell -a 'echo $(whoami)'
```

[Playbook](https://docs.ansible.com/ansible/2.10/user_guide/playbooks_intro.html)
[Role](https://docs.ansible.com/ansible/2.10/user_guide/playbooks_reuse_roles.html), check role directory structure and how to [use role](https://docs.ansible.com/ansible/2.10/user_guide/playbooks_reuse_roles.html#using-roles).
```bash
# -e|--extra-vars: pass extra variables
# -b: become
# -v: verbose
ansible-playbook [-b] -v -i vagrant_ansible_inventory.ini setup.yml \
-e '{"version":"1.10.5","other_variable":"foo"}' # json format

ansible-playbook [-b] -v -i vagrant_ansible_inventory.ini setup.yml \
-e "foo=23" \
-e "bar=hello"
```

]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title>Ansible Galaxy Modules</title>
    <url>/2020/12/16/ansible-galaxy/</url>
    <content><![CDATA[
最近在做项目的时候，发现用到了Ansible Galaxy上的模块，这里记录一下。

[Web page](https://galaxy.ansible.com/)
- [ansible java role](https://galaxy.ansible.com/lean_delivery/java)，对于安装不同的Java 版本非常方便。]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title>Ansible Module Reference</title>
    <url>/2019/03/22/ansible-module-refer/</url>
    <content><![CDATA[
There are some common Ansible modules that use frequently, more details please
refer to Ansible online document.

How to organize the ansible files structure, see this best
[practice](https://docs.ansible.com/ansible/2.8/user_guide/playbooks_best_practices.html).

# play strategy
There are several play
[strategies](https://docs.ansible.com/ansible/latest/user_guide/playbooks_strategies.html#selecting-a-strategy) in ansible:
```yaml
- hosts: all
  strategy: free
  tasks:
  # ...
```
The default is linear strategy with 5 forks parallelism.

# gather_facts
https://docs.ansible.com/ansible/latest/collections/ansible/builtin/gather_facts_module.html
Variables related to remote systems are called facts.

This module is automatically called by playbooks to gather useful variables
about remote hosts that can be used in playbooks.
```yaml
- hosts: all
  # disable gather facts if necessary
  gather_facts: no
  tasks:
  ...
```

For example, I can use `ansible_memtotal_mb` and `ansible_processor_vcpus`
(processor number in /proc/cpuinfo) to config other daemons, they are both
[facts](https://docs.ansible.com/ansible/latest/user_guide/playbooks_vars_facts.html#ansible-facts)
from remote machine:
```yaml
jvm_heap_size: "{{ ansible_memtotal_mb // 2 | int }}m"
```

Here is an case about using ansible internal variable to get machine memory size:
https://www.redhat.com/sysadmin/configuration-verification-ansible
Then we can use it for example to calculate and set heap size used for ES.

# magic variables
https://docs.ansible.com/ansible/latest/user_guide/playbooks_vars_facts.html#information-about-ansible-magic-variables

Variables related to Ansible are called magic variables. For example: `hostvars`
,`groups` and `inventory_hostname`.

# add_host
Dynamically
[create host and group](https://docs.ansible.com/ansible/latest/collections/ansible/builtin/add_host_module.html) just like inventory file for later play use.
```yaml
- name: Add host to group 'just_created' with variable foo=42
  add_host:
    hostname: '{{ ip_from_ec2 }}'
    groups: just_created
    # can have inventory parameters
    ansible_host: '{{ inventory_hostname }}'
    ansible_port: '{{ new_port }}'
    ansible_ssh_private_key_file: "{{ ssh_priv_key }}"
    ansible_ssh_common_args: "-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
    foo: 42
  no_log: True
```

# Delegation
https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_delegation.html
Delegate tasks to different machine or group, for example localhost, etc.

There is also a shorthand syntax that you can use on a per-task basis:
`local_action`, the same as `delegate_to: 127.0.0.1`:
```yaml
---
- hosts: webservers
  serial: 5

  tasks:
    - name: Take out of load balancer pool
      ansible.builtin.command: /usr/bin/take_out_of_pool {{ inventory_hostname }}
      delegate_to: 127.0.0.1

    - name: Recursively copy files from management server to target
      local_action: >
        ansible.builtin.command
        rsync -a /path/to/files {{ inventory_hostname }}:/path/to/target/
```

# serial
[This](https://docs.ansible.com/ansible/latest/user_guide/playbooks_strategies.html#setting-the-batch-size-with-serial) keyword is useful when doing rolling
upgrade or patching.

You can also set [maximum failure percentage](https://docs.ansible.com/ansible/latest/user_guide/playbooks_error_handling.html#setting-a-maximum-failure-percentage) to abort
play during the batch execution:
```yaml
---
- name: test play
  hosts: webservers
  serial: 4
  # if 2 out of 4 fails, abort execution
  max_fail_percentage: 49
```

# run_once
https://docs.ansible.com/ansible/latest/user_guide/playbooks_strategies.html#running-on-a-single-machine-with-run-once

Run task only on the first host in the batch, for example, setting on ES cluster
master. Here provides other options like `delegate_to` and
`when: inventory_hostname == webservers[0]`.

# retry block
By the time Ansible does not
[support retry block](https://github.com/ansible/ansible/issues/46203), but
there is another workaround to implement this useful feature, for instance, to
make group of tasks atomic, see this [post](https://dev.to/nodeselector/retrying-groups-of-tightly-coupled-tasks-in-ansible-579d):

```yml
- name: Group of tasks that are tightly coupled
  block:
  - name: Increment the retry count
    set_fact:
      retry_count: "{{ 1 if retry_count is undefined else retry_count | int + 1 }}"
  # start block tasks
  - name: Some task that might fail
    ...
  - name: Some task that might fail
    ...

  rescue:
    - fail:
        msg: Maximum retries of grouped tasks reached
      when: retry_count | int == 3

    - debug:
        msg: "Task Group failed, let's give it another shot"

    - include_task: coupled_task_group.yml
      vars:
        foo: 110
```

# K8s management
挺有意思，看看这篇[文章](https://blog.51cto.com/99cloud/2336420?source=dra)
和Helm做了一下比较:

Use Ansible on k8s management:
- [Ansible Virtualization and Containerization Guides](https://docs.ansible.com/ansible/latest/scenario_guides/virt_guides.html)
- [Ansible k8s module](https://docs.ansible.com/ansible/latest/collections/community/kubernetes/k8s_module.html)

# uri
- [rui module](https://docs.ansible.com/ansible/latest/collections/ansible/builtin/uri_module.html)
- [uri json response check](https://stackoverflow.com/questions/40235550/how-to-inspect-a-json-response-from-ansible-uri-call)

```yaml
# PUT method and get response to result
- name: disable shard allocation
  uri:
    url: http://localhost:9200/_cluster/settings?format=json
    method: PUT
    status_code: 200
    body_format: json
    return_content: yes
    headers:
      Content-Type: application/json
    body: "{\"persistent\": {\"cluster.routing.allocation.enable\": \"primaries\"}}"
    # if request response is slow
    timeout: 900
  register: result

- name: display response body
  debug:
    var: result.json

# if reponse is an array of dicts
- name: Print certain element
  debug:
    var: result.json[0].xxx.yyy
```

# pause
https://docs.ansible.com/ansible/latest/modules/pause_module.html

Pauses playbook execution for a set amount of time, or until a prompt is
acknowledged. All parameters are optional. The default behavior is to pause with
a prompt.

The pause module integrates into async/parallelized playbooks without any
special considerations (see Rolling Updates). When using pauses with the serial
playbook parameter (as in rolling updates) you are only prompted once for the
current group of hosts.

Useful when debug certain task to see the execution result:
```yaml
# just pause 
- pause:

- name: Pause for 5 seconds
  ansible.builtin.pause:
    seconds: 5

# a helpful reminder of what to look out for post-update.
- pause:
    prompt: "Make sure org.foo.FooOverload exception is not present"

# pause to get some sensitive input.
- pause:
    prompt: "Enter a secret"
    echo: no
```

# skip
Sometimes I need to skip tasks on some machines with prompt confirmation, how to
do this?

It seems there is no `skip` module in ansible, but we have workaroud, see this
[issue](https://unix.stackexchange.com/questions/424151/skip-some-task-with-prompt-in-ansible).

You can also apply [`tag`](https://docs.ansible.com/ansible/latest/user_guide/playbooks_tags.html)
and [condition](https://docs.ansible.com/ansible/latest/user_guide/playbooks_conditionals.html#conditionals).

# debug
https://docs.ansible.com/ansible/latest/modules/debug_module.html
This module prints statements during execution and can be useful for debugging
variables or expressions without necessarily halting the playbook.

Useful for debugging together with the `when:` directive.
```yaml
## Example that prints return information from the previous task
- shell: /usr/bin/uptime
  register: result

## var option already runs in Jinja2 context and has an implicit {{ }} wrapping
- debug:
    var: result
    ## this verbosity is associated with `-vv` parameter
    verbosity: 2

## prints the loopback address and gateway for each host
- debug:
    msg: System {{ inventory_hostname }} has gateway {{ ansible_default_ipv4.gateway }}
  when: ansible_default_ipv4.gateway is defined
```

# fail
https://docs.ansible.com/ansible/latest/modules/fail_module.html
This module fails the progress with a custom message.
It can be useful for bailing out when a certain condition is met using when.

More error handling see:
https://docs.ansible.com/ansible/latest/user_guide/playbooks_error_handling.html

```yaml
- name: check async task status
  ignore_errors: true
  ## check async task status
  async_status:
    jid: "{{ sleepTask.ansible_job_id }}"
  register: job_result
  until: job_result.finished
  when: "inventory_hostname == groups.master[0]"
  retries: 5
  delay: 1

## other things to do

## fail the process if the retry failed
- fail:
    msg: The time limit hit, the cluster may not be in ready status!
  ## it depends what output is in the register variable
  when: job_result.failed == true
```

# copy
https://docs.ansible.com/ansible/latest/modules/copy_module.html
The copy module copies a file from the local or remote machine to a location on
the remote machine (depends on the condition). 和template类似, 如果task下面有files
文件夹, 在不指定src路径的时候, eg: `src: xxx.txt`, 会从files文件夹里copy.

Use the `fetch` module to copy files from remote locations to the local box.

If you need variable interpolation in copied files, use the `template` module.
Using a variable in the content field will result in unpredictable output.
```yaml
# {{ baseDir }}/registry-certs/tls.crt is in control machine
- name: Copy secure docker registry ssl/tls certs to all worker nodes
  any_errors_fatal: true
  copy:
    src: "{{ baseDir }}/registry-certs/tls.crt"
    dest: "/etc/docker/certs.d/{{ service name }}:5000/tls.crt"
    owner: root
    group: root
    mode: '0644'

# remote_src: yes means copy is happening on remote machine, the src is
# also in that remote machine.
# remote_src supports recursive copying as of version 2.8
- name: Copy a "sudoers" file on the remote machine for editing
  copy:
    src: /etc/sudoers
    dest: /etc/sudoers.edit
    remote_src: yes

# With delegate_to, it copies local ssh_key.path file(from control machine) to
# that fileserver_ip.
- name: Copy SSH key to file server for SCP use
  no_log: "{{ task_no_log }}"
  copy:
    src: "{{ ssh_key.path }}"
    dest: "/tmp/ns1.pk"
    owner: root
    group: root
    mode: '0400'
  delegate_to: "{{ fileserver_ip }}"
```

> NOTE that `mode: '0400'` must use 4 digits, such as `0644` instead of `644`,
otherwise you write sticky bit. See
[ansible copy make sticky bit](https://askubuntu.com/questions/976168/difference-between-three-and-four-digit-file-permissions).

# fetch
https://docs.ansible.com/ansible/latest/collections/ansible/builtin/fetch_module.html
This module works like copy, but in reverse. It is used for fetching files from
remote machines and storing them locally in a file tree, organized by hostname.

Files that already exist at dest will be overwritten if they are different than
the src.
```yaml
# fetched file is marked by the remote hostname 
- name: Store file into /tmp/fetched/<src machine hostname>/tmp/somefile
  fetch:
    src: /tmp/somefile
    dest: /tmp/fetched

# If dest ends with '/', it will use the basename of the source file, similar to
# the copy module.
# This can be useful if working with a single host, or if retrieving files that
# are uniquely named per host.
# If using multiple hosts with the same filename, the file will be overwritten
# for each host.
- name: Specifying a destination path
  fetch:
    src: /tmp/uniquefile
    dest: /tmp/special/
    flat: yes
```

# template
https://docs.ansible.com/ansible/latest/modules/template_module.html
这个module在统一设置配置文件时很常用, 或者可以用来初始化 script template 中的参数, 然后传递
到各个host去运行.

Templates are processed by the `Jinja2` templating language.
Documentation on the template formatting can be found in the
[Template Designer Documentation](https://jinja.palletsprojects.com/en/2.10.x/templates/).

Usually we have the ansible role structure:
```ini
roles/
  install.components/
      defaults/
        main.yml
      tasks/
        main.yml
      templates/
        example.conf.j2
        example.sh.j2
      files/
        bar.txt
        foo.sh
```
When template works it picks source file from role's `templates/` folder.
If the template file contains jinja2 placeholder, it will be interpolated.

```yaml
# Copy from control machine to target nodes
- name: Template a file to /etc/files.conf
  template:
    src: /mytemplates/foo.j2
    dest: /etc/file.conf
    owner: bin
    group: wheel
    mode: '0644'
```

Besides the jinja2 built-in filters:
https://jinja.palletsprojects.com/en/latest/templates/#list-of-builtin-filters
There are Ansible supplies:
https://ansible-docs.readthedocs.io/zh/stable-2.0/rst/playbooks_filters.html#jinja2-filters

Some useful cases:
1. remove quotes: using `regex_replace("\"", "")`, or using slice cut
`xxx[1:-2]`

# shell
https://docs.ansible.com/ansible/latest/modules/shell_module.html

It is almost exactly like the `command` module but runs the command through a
shell (`/bin/sh`) on the remote node.

```yaml
# the symbol | is a Yaml formater
- name: Copy IIS docker images to specified worker nodes
  any_errors_fatal: true
  shell: |
    /tmp/copyIISDockers.sh {{ iisDockers }} {{ imageTag }}
    if [[ $? -eq 0 ]]; then
      touch /tmp/copyIISImageToWorker.done
    fi
  when: "inventory_hostname == groups.master[0]"
  args:
    ## A filename, when it already exists, this step will not be run.
    creates: /tmp/copyIISImageToWorker.done
    ## A filename, when it does not exist, this step will not be run.
    removes: /tmp/preTaskOk.done
    ## disable task warning
    warn: no
    ## change the shell
    executable: /bin/bash

- name: Change the working directory to somedir/ before executing the command.
  shell: somescript.sh >> somelog.txt
  args:
    chdir: somedir/
```

# command
https://docs.ansible.com/ansible/latest/modules/command_module.html
The `command` module takes the command name followed by a list of
space-delimited arguments. The given command will be executed on all selected
nodes.

The command(s) will **not** be processed through the `shell`, so variables like
`$HOME` and operations like "<", ">", "|", ";" and "&" will not work. Use the
shell module if you need these features.

```yaml
- name: return motd to registered var
  command: cat /etc/motd
  register: mymotd

# 'args' is a task keyword, passed at the same level as the module
- name: Run command if /path/to/database does not exist (with 'args' keyword).
  command: /usr/bin/make_database.sh db_user db_name
  args:
    creates: /path/to/database

# 'cmd' is module parameter
- name: Run command if /path/to/database does not exist (with 'cmd' parameter).
  command:
    cmd: /usr/bin/make_database.sh db_user db_name
    creates: /path/to/database
```

# service
https://docs.ansible.com/ansible/latest/modules/service_module.html
Controls services on remote hosts. Supported init systems include BSD init,
OpenRC, SysV, Solaris SMF, systemd, upstart.

```yaml
- name: Start service httpd, if not started
  service:
    name: httpd
    state: started
```

# sysctl
https://docs.ansible.com/ansible/2.9/modules/sysctl_module.html
Managing entries in `sysctl.conf` file:
```yaml
- sysctl:
    name: net.core.rmem_max
    value: "{{ foo | int }}"
    state: present
    reload: yes
```

# systemd
https://docs.ansible.com/ansible/latest/modules/systemd_module.html
More dedicated then `service` module, controls systemd services on remote hosts.

```yaml
- name: Enable and start docker
  any_errors_fatal: true
  systemd:
    name: docker
    enabled: yes
    state: started
```

# file
Set attributes of files, symlinks or directories.
Alternatively, remove files, symlinks or directories.

```yaml
- name: Create a directory if it does not exist
  file:
    path: /etc/some_directory
    state: directory
    mode: '0755'

- name: Touch a file, using symbolic modes to set the permissions (equivalent to 0644)
  file:
    path: /etc/foo.conf
    state: touch
    mode: u=rw,g=r,o=r

- name: Remove file (delete file)
  file:
    path: /etc/foo.txt
    state: absent

- name: Recursively remove directory
  file:
    path: /etc/foo
    state: absent
```

# replace
https://docs.ansible.com/ansible/latest/collections/ansible/builtin/replace_module.html

```yaml
# Remember to escape regex character
- name: update elasticsearch jvm deprecated options
  replace:
    path: /etc/elasticsearch/jvm.options
    regexp: "{{ update_item.old }}"
    replace: "{{ update_item.new }}"
  loop:
    - { old: ^-XX:\+UseConcMarkSweepGC, new: 8-13:-XX:+UseConcMarkSweepGC }
    - { old: ^-XX:CMSInitiatingOccupancyFraction=75, new: 8-13:-XX:CMSInitiatingOccupancyFraction=75 }
    - { old: ^-XX:\+UseCMSInitiatingOccupancyOnly, new: 8-13:-XX:+UseCMSInitiatingOccupancyOnly }
  loop_control:
    loop_var: update_item
```

# lineinfile
https://docs.ansible.com/ansible/latest/modules/lineinfile_module.html

If you use `sed` in command module, you will get warning, you can disable the
warning by add `warn: false` or use `lineinfile` module.

This module ensures a particular line is in a file, or replace an existing line
using a back-referenced regular expression.

This is primarily useful when you want to change a single line in a file only.

See the `replace` module if you want to change multiple, similar lines or check
`blockinfile` if you want to insert/update/remove a block of lines in a file.
For other cases, see the copy or template modules.

```yaml
# remove
- name: remove old node role syntax in elasticsearch.yml file
  lineinfile:
    path: /etc/elasticsearch/elasticsearch.yml
    regexp: "{{ remove_item }}"
    state: absent
  loop:
    - ^node\.data
    - ^node\.master
    - ^node\.roles
  loop_control:
    loop_var: remove_item

# insert
- name: Update G1GC config for jdk 14+ in jvm options file
  lineinfile:
    path: /etc/elasticsearch/jvm.options
    insertafter: "^# To use G1GC uncomment the lines below."
    line: "{{ add_item }}"
  loop:
    - 14-:-XX:+UseG1GC
    - 14-:-XX:G1ReservePercent=25
    - 14-:-XX:InitiatingHeapOccupancyPercent=30
  loop_control:
    loop_var: add_itema
```

> NOTE that if run multiple times, only the first time take effect! it will not
remove or insert duplicates.

# mount
https://docs.ansible.com/ansible/latest/modules/mount_module.html
This module controls active and configured mount points in `/etc/fstab`

For `/etc/exports`(nfs server side config), no dedicated module for it.

```yaml
- name: Edit /etc/fstab file to mount share directory
  any_errors_fatal: true
  mount:
    path: "/mnt"
    src: "{{ dfsFileServer }}:{{ dfsDataDir }}"
    fstype: nfs
    opts: "defaults,timeo=10,retrans=3,rsize=1048576,wsize=1048576"
    state: mounted

- name: Edit /etc/fstab file to unmout share directory
  any_errors_fatal: true
  mount:
    path: "/mnt"
    fstype: nfs
    opts: "defaults,timeo=10,retrans=3,rsize=1048576,wsize=1048576"
    state: absent
```

# asynchronous
https://docs.ansible.com/ansible/latest/user_guide/playbooks_async.html
By default task in playbook blocks, this may not always be desirable, or you may
be running operations that take longer than the SSH timeout.

This module can be use to create progress bar for long time task.

> Notice that async task can only be accessed in the same playbook.

```yaml
---
- name: async task in background
  any_errors_fatal: true
  shell: |
    echo "start first task"
    sleep 20
    touch /tmp/sleep.done
  ## when poll > 0, task still blocks
  ## when poll = 0, task run in background
  poll: 0
  ## explicitly sets the timeout 1000 seconds
  async: 1000 
  ## register for later check
  register: sleepTask
  when: "inventory_hostname == groups.master[0]"
  args:
    creates: /tmp/sleep.done

- name: check async task status
  any_errors_fatal: true
  ## check async task status
  async_status:
    jid: "{{ sleepTask.ansible_job_id }}"
  register: job_result
  until: job_result.finished
  when: "inventory_hostname == groups.master[0]"
  retries: 30
  delay: 3
```

# ini_file
```yaml
- name: "Add override config file for xxx service"
  ini_file:
    path: /etc/systemd/system/xxx.service.d/override.conf
    section: Service
    option: MemoryLimit
    value: "800M"
    no_extra_spaces: yes
    mode: 0644
```

# synchronize
The rsync command wrapper.

# loop
https://docs.ansible.com/ansible/latest/user_guide/playbooks_loops.html
需要注意的是在有嵌套循环时，要rename default `item`, for example, below loop is a
inner loop, rename the loop var to `inner_item`, otherwise you get warning
message:
```yaml
# main.yml
- include_tasks: inner.yml
  loop:
    - 1
    - 2
    - 3
  loop_control:
    loop_var: outer_item

# inner.yml
- name: Add several users
  debug:
    msg: "{{ inner_item.name }} and {{ inner_item.groups }}"
  loop:
    - { name: 'testuser1', groups: 'wheel' }
    - { name: 'testuser2', groups: 'root' }
  loop_control:
    loop_var: inner_item
```

# variables
- inventory_hostname
- groups
- jinjia2 template ```{{ }}```

If the vars expresson is long, can modify it into multipe lines:
```yaml
# using > instead of |, > will ignore the line return ane assemble the
# multi-line into one line.
  result: >
    "{{ ((groups['data'] | length) == 0
    and (groups['data_hot'] | length) == 0
    and (groups['data_warm'] | length) == 0)
    | ternary('it is OK', 'bad!')
    }}"
```

# conditional
- when clause

```yaml
## 不是每个module都支持creates的
args:
  creates: xxx
```]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title>Ansible Tower</title>
    <url>/2022/01/29/ansible-tower/</url>
    <content><![CDATA[
Watched Pluralsight `<<Managing Ansible with Red Hat Ansible Tower>>`

This is a brief introduction for Tower, to see details please check official documents.

Need to know:
1. Create project to set the runtime environment(Python virtual env), playbook directory.
2. Add template, associated with project, set verbosity, concurrent job, prompt, etc.
3. Launch job from template, may be provide extra variables in prompt.
4. Check job status and log from job dashboard.

Step 1,2,3 could be done by running playbook on Tower.

# Introduction
Tower is a kind of control node that also provides a central web UI, authentation and API for Ansible. The new version of Tower is called as `Ansible Automation Platform`.

> Tower installation needs license.

Red Hat Ansible Tower official web site:
https://access.redhat.com/products/ansible-tower-red-hat

I use Tower version `3.7.4`:
https://docs.ansible.com/ansible-tower/3.7.4/html/quickinstall/index.html

Need to apply subscription in order to login the Tower web UI, get trial free license from there:
https://docs.ansible.com/ansible-tower/3.7.4/html/installandreference/updates_support.html#trial-evaluation

Tower install package download:
https://releases.ansible.com/ansible-tower/setup/
For example, I am using bundled(self-contained) installer [`ansible-tower-setup-bundle-3.7.4-1.tar.gz`](https://releases.ansible.com/ansible-tower/setup-bundle/), can be used without netwrork connection.

The installation may fail due to lack of necessary packages, just install it, for example:
```bash
sudo yum install -y rsync
```

For Tower single node installation, extract the tar.gz and edit the `inventory` file(Tower is installed through Ansible as well) to fill passwords:
```ini
admin_password='admin'
pg_password='admin'
rabbitmq_password='admin'
```
Then install by running:
```bash
sudo ./setup.sh
```

The playbook location: `/var/lib/awx/projects`, you can put playbooks and ansible.cfg and others info in a `tar.gz` package and place it under this path (should not need to manually manage these directories).

Tower REST API:
```bash
# check api version
curl -XGET -k https://localhost/api/
```

There are 4 main components for Tower:
- Nginx: provide web server for UI and API
- PostgreSQL: internal relational database server
- supervisord: process control system that manages the application: running jobs, etc
- rabbitmq-server: AMQP message broker supporting signalling by application components
- memcached: local caching service

These services communicate with each other using normal network protocols:
- Nginx: 80/tcp,443/tcp
- PostgreSQL: 5432/tcp
- Rabbitmq-server: beam listens on 5672/tcp, 15672/tcp, 25672/tcp

In the single machine installation, only need to expose 80/tcp and 443/tcp.

There are some wrapper systemctl commands for Tower:
```bash
ansible-tower-service status
ansible-tower-service start
ansible-tower-service stop
ansible-tower-service restart
```


# Dashboard
To have a overview of Tower dashboard and setup:
https://www.youtube.com/watch?v=ToXoDdUOzj8

1. create a project, SCM TYPE set to `Manual` which means you will put your playbook folder in the `/var/lib/awx/<any folder>/my-playbook` directory. Set `ANSIBLE ENVIRONMENT` to a virtual python env folder.
2. create inventory.
3. create templates, set the PROJECT, PLAYBOOK path, JOB TYPE, INVENTORY, ENABLE CURRENT JOBS, etc
4. launch the template job w/o extra vars from console or from Tower API.


# Manual Quick Debug
Sometimes I would like to run playbook in CLI, that's easy to do:
1. upload playbook in one of the Tower VM path `/var/lib/awx/projects/my-playbook`.
2. source the python venv, for example the venv is put in `/var/lib/awx/venv`.
3. run playbook from **inside** the `my-playbook` directory, otherwise you may encounter strange issue(if you check the process launched by Tower, it runs this way), for example
```bash
source /var/lib/awx/venv/my-venv/bin/activate
cd /var/lib/awx/projects/my-playbook

# no inventory means run on localhost
ansible-playbook playbook_v1.yml \
-e @var.json \
-e "endpoint=http://example.com/xs73s93jsdfsf" \
-vvv
```

# Search Job Log
It is useful to accurately locate the job specific task logs, in the job log search bar, it can do target and fuzzy search:
```bash
task:"<task name>"
```
Other search bars have similar syntax.]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title>Ansible Role</title>
    <url>/2023/01/16/ansible-role/</url>
    <content><![CDATA[
To reuse role in a playbook, there are
[3 options](https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_reuse_roles.html#using-roles):

- `roles`: at play level, statically, executed before `tasks`.
- `include_role`: at task level, dynamically, parsed and executed at where it is
defined.
- `import_role`: at task level, statically, parsed before the calling play
execution and if no syntax error executed at where it is defined.

Regarding `statically` and `dynamically` above, there is
[explanation](https://www.ansiblejunky.com/blog/ansible-101-include-vs-import/).

Example snippet for role reusing:
```yaml
---
- hosts: all
  gather_facts: no
  any_errors_fatal: true
  become: true
  roles:
    # The same as import_role module.
    - <role1 name>
    - <role2 name>

  tasks:
    # Dynamiaclly load, parse and run role in playbook where it is defined.
    - name: Include role
      include_role:
        name: <role name>

    # Statically load role, the same as 'roles' clause that will parse the
    # imported role before playbook runs, it will fail the playbook if there is
    # any syntax error.
    - name: Import role
      import_role:
        name: <role name>
```
]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title>Prompt Engineering</title>
    <url>/2023/05/22/ai-llm/</url>
    <content><![CDATA[
At the time of writing, there is a time-limited free course about ChatGPT prompt
engineering for developers, shared by [deeplearning.ai](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/).

## Introduction

Two types of LLMs:

* Base LLM: predicts next word, based on test training data, not able to answer
user questions.

* Instruction Tuned LLM: tries to follow instruction, fine tune on instructions
and good attempt at following those instructions. `RLHF`(refinement learning
with human feedback).

The course is focusing on the second type.

For how to write openai code, the Python library can be found here:
https://platform.openai.com/docs/libraries

The boilerplate:
```py
import openai
import os

from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file for openai api key

openai.api_key  = os.getenv('YOUR_OPENAI_API_KEY')

def perform_task(prompt, model="gpt-3.5-turbo", temperature=0): 
    messages = [{"role": "user", "content": prompt}]
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=temperature, 
    )
    return response.choices[0].message["content"]
```

Here is the openai chat completion guide:
https://platform.openai.com/docs/guides/chat

## Guidelines

Prompting principles and tactics:

* Use clear and specific instructions.
  * Use delimiters to avoid prompt injection.
    * Such as: `"""`, `---` or `<tag>`, `<>`, etc.

    ```py
    text = f"""
    You should ```express what you want``` a model to do by \ 
    providing instructions that are as clear and \ 
    specific as you can possibly make them. \ 
    This will guide the model towards the desired output, \ 
    and reduce the chances of receiving irrelevant \ 
    or incorrect responses. Don't confuse writing a \ 
    clear prompt with writing a short prompt. \ 
    In many cases, longer prompts provide more clarity \ 
    and context for the model, which can lead to \ 
    more detailed and relevant outputs.
    """
    prompt = f"""
    Summarize the text delimited by single backtick \ 
    into a single sentence.
    `{text}`
    """

    response = perform_task(prompt)
    print(response)
    ```

  * Ask for structed output.
    * JSON, YAML, etc, so the they can be used later.

    ```py
    prompt = f"""
    Generate a list of three made-up book titles along \ 
    with their authors and genres. 
    Provide them in JSON format with the following keys: 
    book_id, title, author, genre.
    """
    response = perform_task(prompt)
    print(response)
    ```

  * Ask the model to check whether conditions are satisfied.
    * For example, does the input contain a sequence of instructions?
    
    ```py
    text_1 = f"""
    Making a cup of tea is easy! First, you need to get some \ 
    water boiling. While that's happening, \ 
    grab a cup and put a tea bag in it. Once the water is \ 
    hot enough, just pour it over the tea bag. \ 
    Let it sit for a bit so the tea can steep. After a \ 
    few minutes, take out the tea bag. If you \ 
    like, you can add some sugar or milk to taste. \ 
    And that's it! You've got yourself a delicious \ 
    cup of tea to enjoy.
    """
    prompt = f"""
    You will be provided with text delimited by triple quotes. 
    If it contains a sequence of instructions, \ 
    re-write those instructions in the following format:

    Step 1 - ...
    Step 2 - …
    …
    Step N - …

    If the text does not contain a sequence of instructions, \ 
    then simply write \"No steps provided.\"

    \"\"\"{text_1}\"\"\"
    """
    response = perform_task(prompt)
    print("Completion for Text 1:")
    print(response)
    ```

  * "Few-shot" prompting.
    * Give successful example of completing tasks then ask model to perform the
    task.

    ```py
    prompt = f"""
    Your task is to answer in a consistent style.

    <child>: Teach me about patience.

    <grandparent>: The river that carves the deepest \ 
    valley flows from a modest spring; the \ 
    grandest symphony originates from a single note; \ 
    the most intricate tapestry begins with a solitary thread.

    <child>: Teach me about resilience.
    """
    response = perform_task(prompt)
    print(response)
    ```

* Give the model time to "think".
  * Specify the steps required to complete a task.

  ```py
  text = f"""
  In a charming village, siblings Jack and Jill set out on \ 
  a quest to fetch water from a hilltop \ 
  well. As they climbed, singing joyfully, misfortune \ 
  struck—Jack tripped on a stone and tumbled \ 
  down the hill, with Jill following suit. \ 
  Though slightly battered, the pair returned home to \ 
  comforting embraces. Despite the mishap, \ 
  their adventurous spirits remained undimmed, and they \ 
  continued exploring with delight.
  """
  # example 1
  prompt_1 = f"""
  Perform the following actions: 
  1 - Summarize the following text delimited by single \
  backtick with 1 sentence.
  2 - Translate the summary into French.
  3 - List each name in the French summary.
  4 - Output a json object that contains the following \
  keys: french_summary, num_names.

  Separate your answers with line breaks.

  Text:
  `{text}`
  """
  response = perform_task(prompt_1)
  print("Completion for prompt 1:")
  print(response)
  ```

  * Instruct the model to work out its own solution before rushing to a
  conclusion.

  ```py
  prompt = f"""
  Your task is to determine if the student's solution \
  is correct or not.
  To solve the problem do the following:
  - First, work out your own solution to the problem. 
  - Then compare your solution to the student's solution \ 
  and evaluate if the student's solution is correct or not. 
  Don't decide if the student's solution is correct until 
  you have done the problem yourself.

  Use the following format:
  Question:
  ``
  question here
  ``
  Student's solution:
  ``
  student's solution here
  ``
  Actual solution:
  ``
  steps to work out the solution and your solution here
  ``
  Is the student's solution the same as actual solution \
  just calculated:
  ``
  yes or no
  ``
  Student grade:
  ``
  correct or incorrect
  ``

  Question:
  ``
  I'm building a solar power installation and I need help \
  working out the financials. 
  - Land costs $100 / square foot
  - I can buy solar panels for $250 / square foot
  - I negotiated a contract for maintenance that will cost \
  me a flat $100k per year, and an additional $10 / square \
  foot
  What is the total cost for the first year of operations \
  as a function of the number of square feet.
  ``
  Student's solution:
  ``
  Let x be the size of the installation in square feet.
  Costs:
  1. Land cost: 100x
  2. Solar panel cost: 250x
  3. Maintenance cost: 100,000 + 100x
  Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000
  ``
  Actual solution:
  """
  response = perform_task(prompt)
  print(response)
  ```

> NOTE: it is possible that the answer sounds plausible but are not true.

Reduce the hallucinations: Ask the LLM first find relevant information, then
answer the question based on the relevant information.

## Iterative

Iterative prompt development steps:

* Try something.
* Analyze where the result does not give what you want.
* Clarify instructions, give more time to think.
* Refine prompt with a batch of examples.

For example, below examples show how to evolve the prompt to get desired outcome
for a e-commerce website:

```py
fact_sheet_chair = """
OVERVIEW
- Part of a beautiful family of mid-century inspired office furniture, 
including filing cabinets, desks, bookcases, meeting tables, and more.
- Several options of shell color and base finishes.
- Available with plastic back and front upholstery (SWC-100) 
or full upholstery (SWC-110) in 10 fabric and 6 leather options.
- Base finish options are: stainless steel, matte black, 
gloss white, or chrome.
- Chair is available with or without armrests.
- Suitable for home or business settings.
- Qualified for contract use.

CONSTRUCTION
- 5-wheel plastic coated aluminum base.
- Pneumatic chair adjust for easy raise/lower action.

DIMENSIONS
- WIDTH 53 CM | 20.87”
- DEPTH 51 CM | 20.08”
- HEIGHT 80 CM | 31.50”
- SEAT HEIGHT 44 CM | 17.32”
- SEAT DEPTH 41 CM | 16.14”

OPTIONS
- Soft or hard-floor caster options.
- Two choices of seat foam densities: 
 medium (1.8 lb/ft3) or high (2.8 lb/ft3)
- Armless or 8 position PU armrests 

MATERIALS
SHELL BASE GLIDER
- Cast Aluminum with modified nylon PA6/PA66 coating.
- Shell thickness: 10 mm.
SEAT
- HD36 foam

COUNTRY OF ORIGIN
- Italy
"""
```

Original prompt:
```py
prompt = f"""
Your task is to help a marketing team create a 
description for a retail website of a product based 
on a technical fact sheet.

Write a product description based on the information 
provided in the technical specifications delimited by 
single backtick.

Technical specifications: `{fact_sheet_chair}`
"""
response = perform_task(prompt)
print(response)
```

If the output is too long, you can limit the length explicitly:
```py
prompt = f"""
Your task is to help a marketing team create a 
description for a retail website of a product based 
on a technical fact sheet.

Write a product description based on the information 
provided in the technical specifications delimited by 
single backtick.

Use at most 50 words.

Technical specifications: `{fact_sheet_chair}`
"""
response = perform_task(prompt)
print(response)
```

After many rounds of improvements, the last prompt gets specified details and
table/HTML format:
```py
prompt = f"""
Your task is to help a marketing team create a 
description for a retail website of a product based 
on a technical fact sheet.

Write a product description based on the information 
provided in the technical specifications delimited by 
single backtick.

The description is intended for furniture retailers, 
so should be technical in nature and focus on the 
materials the product is constructed from.

At the end of the description, include every 7-character 
Product ID in the technical specification.

After the description, include a table that gives the 
product's dimensions. The table should have two columns.
In the first column include the name of the dimension. 
In the second column include the measurements in inches only.

Give the table the title 'Product Dimensions'.

Format everything as HTML that can be used in a website. 
Place the description in a <div> element.

Technical specifications: `{fact_sheet_chair}`
"""

response = perform_task(prompt)
print(response)
```

## Summarizing

For example, the text to be summarized:
```py
prod_review = """
Got this panda plush toy for my daughter's birthday, \
who loves it and takes it everywhere. It's soft and \ 
super cute, and its face has a friendly look. It's \ 
a bit small for what I paid though. I think there \ 
might be other options that are bigger for the \ 
same price. It arrived a day earlier than expected, \ 
so I got to play with it myself before I gave it \ 
to her.
"""
```

You can ask LLM like this:
```py
prompt = f"""
Your task is to generate a short summary of a product \
review from an ecommerce site. 

Summarize the review below, delimited by single 
backtick, in at most 30 words. 

Review: `{prod_review}`
"""

response = perform_task(prompt)
print(response)
```

Focusing on shipping and delivery details:

```py
prompt = f"""
Your task is to generate a short summary of a product \
review from an ecommerce site to give feedback to the \
Shipping deparmtment. 

Summarize the review below, delimited by single 
backtick, in at most 30 words, and focusing on any aspects \
that mention shipping and delivery of the product. 

Review: `{prod_review}`
"""

response = perform_task(prompt)
print(response)
```

Focusing on price and value:

```py
prompt = f"""
Your task is to generate a short summary of a product \
review from an ecommerce site to give feedback to the \
pricing deparmtment, responsible for determining the \
price of the product.  

Summarize the review below, delimited by single 
backtick, in at most 30 words, and focusing on any aspects \
that are relevant to the price and perceived value. 

Review: `{prod_review}`
"""

response = perform_task(prompt)
print(response)
```

If the summaries are not relevant to focus, you can adjust the word by using
`extract` instead of `summarize`:

```py
prompt = f"""
Your task is to extract relevant information from \ 
a product review from an ecommerce site to give \
feedback to the Shipping department. 

From the review below, delimited by single quote \
extract the information relevant to shipping and \ 
delivery. Limit to 30 words. 

Review: `{prod_review}`
"""

response = perform_task(prompt)
print(response)
```

To summarize multiple payloads:

```py
# reviews contains multiple texts
for i in range(len(reviews)):
    prompt = f"""
    Your task is to generate a short summary of a product \ 
    review from an ecommerce site. 

    Summarize the review below, delimited by triple \
    quotes in at most 20 words. 

    Review: '''{reviews[i]}'''
    """

    response = perform_task(prompt)
    print(i, response, "\n")
```

## Inferring

For example, extract sentiment(positive or negative), LLMs are pretty good at
extracting these.

For example giving the customer review:

```py
lamp_review = """
Needed a nice lamp for my bedroom, and this one had \
additional storage and not too high of a price point. \
Got it fast.  The string to our lamp broke during the \
transit and the company happily sent over a new one. \
Came within a few days as well. It was easy to put \
together.  I had a missing part, so I contacted their \
support and they very quickly got me the missing piece! \
Lumina seems to me to be a great company that cares \
about their customers and products!!
"""
```

You can do multiple tasks at once:
```py
prompt = f"""
Identify the following items from the review text: 
- Sentiment (positive or negative)
- Is the reviewer expressing anger? (true or false)
- Item purchased by reviewer
- Company that made the item

The review is delimited with triple backticks. \
Format your response as a JSON object with \
"Sentiment", "Anger", "Item" and "Brand" as the keys.
If the information isn't present, use "unknown" \
as the value.
Make your response as short as possible.
Format the Anger value as a boolean.

Review text: '''{lamp_review}'''
"""
response = perform_task(prompt)
print(response)
```

You can also infer topics:

```py
prompt = f"""
Determine five topics that are being discussed in the \
following text, which is delimited by triple backticks.

Make each item one or two words long. 

Format your response as a list of items separated by commas.

Text sample: '''{story}'''
"""
response = perform_task(prompt)
print(response)
```

And you can have a list of keywords and check if the text to be verified is in
one of them:

```py
# topic_list contains a list of key words to be identified.
prompt = f"""
Determine whether each item in the following list of \
topics is a topic in the text below, which
is delimited with triple backticks.

Give your answer as list with 0 or 1 for each topic.\

List of topics: {", ".join(topic_list)}

Text sample: '''{story}'''
"""
response = perform_task(prompt)
print(response)
```

You can output as JSON format to further process.

## Transforming

Such as language translation, spelling and grammar checking, tone adjustment,
and format conversion.

For example, a universal translator:
```py
user_messages = [
  # System performance is slower than normal
  "La performance du système est plus lente que d'habitude.",
  # My monitor has pixels that are not lighting
  "Mi monitor tiene píxeles que no se iluminan.",
  # My mouse is not working
  "Il mio mouse non funziona",
  # My keyboard has a broken control key
  "Mój klawisz Ctrl jest zepsuty",
  # My screen is flashing
  "我的屏幕在闪烁"
] 

for issue in user_messages:
    prompt = f"Tell me what language this is: ```{issue}```"
    lang = perform_task(prompt)
    print(f"Original message ({lang}): {issue}")

    prompt = f"""
    Translate the following  text to English \
    and Korean: '''{issue}'''
    """
    response = perform_task(prompt)
    print(response, "\n")
```

Tone transformation, for informal to formal:

```py
prompt = f"""
Translate the following from slang to a business letter: 
'Dude, This is Joe, check out this spec on this standing lamp.'
"""
response = perform_task(prompt)
print(response)
```

Spelling and grammar check, to signal to the LLM that you want it to proofread
your text, you instruct the model to 'proofread' or 'proofread and correct'.

```py
text = [ 
  "The girl with the black and white puppies have a ball.",  # The girl has a ball.
  "Yolanda has her notebook.", # ok
  "Its going to be a long day. Does the car need it’s oil changed?",  # Homonyms
  "Their goes my freedom. There going to bring they’re suitcases.",  # Homonyms
  "Your going to need you’re notebook.",  # Homonyms
  "That medicine effects my ability to sleep. Have you heard of the butterfly affect?", # Homonyms
  "This phrase is to cherck chatGPT for speling abilitty"  # spelling
]
for t in text:
    prompt = f"""Proofread and correct the following text
    and rewrite the corrected version. If you don't find
    and errors, just say "No errors found". Don't use 
    any punctuation around the text:
    ```{t}```"""
    response = perform_task(prompt)
    print(response)
```

Another example, proofread and correct long text:
```py
text = f"""
Got this for my daughter for her birthday cuz she keeps taking \
mine from my room.  Yes, adults also like pandas too.  She takes \
it everywhere with her, and it's super soft and cute.  One of the \
ears is a bit lower than the other, and I don't think that was \
designed to be asymmetrical. It's a bit small for what I paid for it \
though. I think there might be other options that are bigger for \
the same price.  It arrived a day earlier than expected, so I got \
to play with it myself before I gave it to my daughter.
"""
prompt = f"proofread and correct this review: ```{text}```"
response = perform_task(prompt)
print(response)

from redlines import Redlines

diff = Redlines(text,response)
# This helps highlights the differences
display(Markdown(diff.output_markdown))

prompt = f"""
proofread and correct this review. Make it more compelling. 
Ensure it follows APA style guide and targets an advanced reader. 
Output in markdown format.
Text: '''{text}'''
"""
response = perform_task(prompt)
display(Markdown(response))
```

## Expanding

Expand short text to long, for example, generate customer service emails that
are tailored to each customer's review.

```py
# Assume the 'review' and 'sentiment' are ready to use.
prompt = f"""
You are a customer service AI assistant.
Your task is to send an email reply to a valued customer.
Given the customer email delimited by ```, \
Generate a reply to thank the customer for their review.
If the sentiment is positive or neutral, thank them for \
their review.
If the sentiment is negative, apologize and suggest that \
they can reach out to customer service. 
Make sure to use specific details from the review.
Write in a concise and professional tone.
Sign the email as `AI customer agent`.
Customer review: '''{review}'''
Review sentiment: {sentiment}
"""
response = perform_task(prompt, temperature=0.7)
print(response)
```

About the temperature parameter, uses `0` if you require reliability and
predictability, use high value if requires variety.

## Charbot

Utilizing the chat format to have extended conversations with chatbots
personalized or specialized for specific tasks or behaviors.

Basically, you need to provide the `complete` context for each conversation, for
example, by appending the messages(from user and assistant) to a list, and
dumping the list to LLM for response.

```py
# The 'system' role is used to provide overall context and details.
# The 'user' role is used to carry user input.
# The 'assistant' role is to specify/record LLM response.
messages =  [  
{'role':'system', 'content':'You are an assistant that speaks like Shakespeare.'},    
{'role':'user', 'content':'tell me a joke'},   
{'role':'assistant', 'content':'Why did the chicken cross the road'},   
{'role':'user', 'content':'I don\'t know'}  ]

response = perform_chatbot_task(messages, temperature=0)
print(response)
```

And you can summarize the chat session, for example:

```py
messages =  context.copy()
messages.append(
{'role':'system', 'content':'create a json summary of the previous food order.\
Itemize the price for each item the fields should be 1) pizza, include size 2) \
list of toppings 3) list of drinks, include size   4) list of sides include \
size  5)total price '},    
)

response = get_completion_from_messages(messages, temperature=0)
print(response)
```
]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>ai</tag>
      </tags>
  </entry>
  <entry>
    <title>Engineer Survival Guide</title>
    <url>/2023/12/02/book-engineer-survival-guide/</url>
    <content><![CDATA[
## Imposter syndrome

Be aware we all feel like imposter syndrome sometimes, and don't let it get to
you. It is fun not knowing what to do and figuring it out as opposed to fearing
it, the more you fear it, the more control it has over you.

## The importance of data to convince others

Engineers are data-driven people. Always back up your ideas with data, the more
you provide, the easier you convince others.

## How often should you interview

1. meet amazing people.
2. know your market value.
3. get backup options: turn down the offer, ask match the offer, jump to new
opportunity, negotiate.
4. opportunity to find sth you love.
5. stay sharp about domain knowledge.
6. learn what you know well and what you do not.

## Learn to say no and brutal prioritization

* You are assigned work based on your experience and reputation.
* Opportunity cost: if **comes at a cost of missing your deadline**.

Always calculate the opportunity cost, prioritize brutally, and learn to say no
to people with an explanation of why it is a no.

## Never say no

* say no but never use actual word "no" directly
* **I would love to help on this, but to set expectation, here are the tasks I**
**need to do for this week that are higher priority.**

Keep a good blacklog and always use it to your advantage to tell people why you
cannot prioritize what they are asking instead of saying no, stay away from
saying no as much as possible.

## Finding a mentor

* Pick a design decision and study it as much as possible, discuss on **blind spot**.

Always look out for mentors, the more the better. But pick them carefully to make
sure their **goals are aligned with yours**.

## Prototype fast, first working prototype always wins

Always turn your ideas into working prototypes as fast as possible. Everyone has
great ideas, but a working prototype is **game-changer**.

## Visibility is everything

* which one do you think deserves promotion?
* which one do you think should be let go if company is downsizing?
* for repeated questions, write a playbook and answer for others reference.

Always find ways to increase your visibility. If no one knows you did sth, no
one will believe you have done anything important.

## Let people fail, help them fall slowly

* You need to remind yourself of how you felt as a junior with a micromanaging
monster.

Don't be a micromanager. Set a direction and let people do their best while
preparaing a safety net in case they fall.

## The tiebreaker - reaching consensus

* The problem starts when the discussion is about sth non-quantifiable and
people's options come from their intuition.
* The majority wins will not help you get global optimum, but if things go wrong,
nobody is to be blamed and everyone is responsible, the failure will still make
team move forward and make better decision next time.

Always have an odd number of people in **decision-making** so you can have a
tiebreaker, remove egos from the picture, the majority wins.

Or ask the authority to decide and clarify when you cannot **speak in power**.

## The importance of allies in design decisions

* A better approach is composing the design with different options without too
many details and sending it around for a small group of people to get feedback
as early as possible and keep the design an **evolving collaborative** document.
* Doing **iterative design** help you pick the solution that has the agreement.

1. Use design discussion to utilize other people's brain as much as possible.
2. Make sure all the decisions and the alternatives are documented so everyone
can follow.
3. Make the design an **evolving design** instead of an end-to-end design.
4. Have a small group of engineers and get them to you allies before the final
design discussion.

##  The power of meeting summary emails/notes

* give them chance to correct any misunderstandings, brings everyone to consensus.
* keep track of all decisions made and people's positions.

Get yourself in the habit of taking notes and sharing them with stakeholders
after each meeting.

## Align and Align

* The secret of success of any company is to figure out **long-term vision**, the
short-term direction.
* join the **all-hands meetings** in your company to get a better understanding of
the leadership's long-term goals, keep asking questions, have meetings with the
skip-manager to learn from them.
* Be a regular user of your product; **keep an eye** on what other projects are going
around.
* The more alignment, the higher impact of your projects, the higher chance to
get promoted.

Make sure both you and team are aligned with management, division and company,
this should be a regular exercise for you.

## The power of giving credit

Giving credit to the owner either by directly telling them or spreading in meeting
or emial, the more credit you give, the better.

## Sharing the responsibility

For oncall specifically:
After heavy on-call week, the on-call engineers try their best to mitigate and
prioritize things, and drive the sync up to distribute(visible to team) the
remaining tasks to the team(for example owner who introduced them) and make sure
they don't have too much during the sprint.

Pay attention to **patterns** and come up with process to solve problems instead
of getting frustrated.

## Taking responsibility & ownership

Leaders step up, take responsibility for all the risks that carries, and try
their best to avoid it **ever** happening again.

Pick an area, start acting like an owner, take responsibility and shield people.

## How to disarm assholes

1. Try your best to change your perspective and **see things from their side**.
2. Change the way you communiate with them. If in-person is a problem, utilize
messages/emails
3. Take a break, let your emotions **calm down**, and **look at the conflict objectively**.
4. Start documenting conflicts and ask someone to give you advice.
5. Pick projects that do not involve working with them.
6. Ignore them at all cost.
7. Talk to manager.
8. Change the team.
9. Change the company.

## Adaptability

Be **positive** to the changes around you, **calm down** and think about the
challenges and opportunities introduced rather than being fearful, keep good
mindset.

## The important of tools and patterns

Always consider whether there is any tool that can help your task better, faster
and with fewer mistakes, keep learning tool consistently in your career.

## Open source internally

Even for company internal project, treat and evolve it as the open source style,
this can remove yourself as a single dependency from the project in the long-term.

The general workflow for open source project:
1. code sth useful
2. doc how it works and API usage
3. doc how to contribute
4. upload the code to a repo
5. preferably add doc for each component, the more the better.
6. add unit test, the more tested the better.
7. preferably functional tests.
8. keep an excellent backlog to prioritize and track tasks.
9. advertise the project and get more engineers invested.
10. drive the long-term vision and assign task to appropriate engineers.
11. stay on top of code reviews to keep code quality high.
12. keep repeating maintaining, docing, adding tests, prioritizing, etc.

## How to get more headcount

1. Switch project unimportant as fast as you can.
2. Increase your project visibility, **advertise the project and yourself better**.
3. Maintain a good **backlog** and make it visible.
4. Make sure other team requests are visible, do not fix things before discussing
them during sprint.
5. Seek feedback and improve yourself.
6. Find another team with management that has faith in you.

## Side projects and knowing your surroundings

Tracking other projects will be confused at beginning but it takes less and less
time later. Networking is important to achieve that, pls see below section.

Benefits:
1. Become the **go-to person** for others, by helping others know what is going on,
you become more senior.
2. Become the **bridge between teams**, start impacting on projects by commenting
and knowing priorities than teammates.
3. Chance to help other teams to make right choices in their design.

Most of the engineers in big company are working on existing project by doing
incremental changes, it is lucky and not common to start sth from scratch and
watching it grow.

You can working on small things with surrounding projects, get insider knowledge
of how system is being designed and long-term vision for it, this can also help
you find critical-priority project and truely care about.

## The importance of networking

Started building relationships before you need anything:
1. Eating lunch with coworkers.
2. Attend the happy hours, events.
3. Monthly/biweekly meeting, ask pick up a topic.

After networking, you will not feel you are approaching someone for gain but
approaching a friend for help, people love help friends.

## Changing teams, risks, timing, making the switch

Familiarize with performance deadlines. The best time to switch team is after
your results are finalized, especially right after a promotion, if possible.

## How fast to respond email/chat

1. The one who always replies answer fast in chat/email is the **go-to person**.
2. Find the "sweet spot" to decide how often to check the notifications: are you
good at context switch?
3. The ack can be:
  * ASAP: ack, acked, checking
  * will respond in a minute, have meeting and will get back to this in half
  hour, will anwer by EOD
  * **how urgent is it**? can I look at it later, get back to you later today.
4. tag people if you are not the right owner, if not very sure, say: **pls take a look and tag the right person**
5. create post for repeated questions and share.

## 1 on 1 with leadership

Status update only is not useful to you, to have right perspective for these
meetings, make best use of them, they are here to support you:

1. happy with your career?
2. burning out?
3. plan the career deveoplment.
4. plan the long-term goal.
5. help you not fail.

For project:
1. **set boundaries** with another team?
2. need someone to drive the discussion, tiebreaker?

You need to get good at **reading people**, take notes in things you see, this will
help you recognize patterns, keep noting the things they do amazingly and the
things the are not doing that well.

More important in one-on-one:
1. career developlment, what you want to learn or get involved?
2. long-term vision, what your team is trying to achieve? great chance to know
the surrounding projects, **what others are working on**?
3. getting high quality feedback, make sure the feedback is proactive and
**actionable**, keep follow-up questions, ask to be more concrete and how to improve?
Put a plan to move forward.
4. ask for recommendation when need help, who to access, etc.
5. ask for mentors, manager has larger network than you.
6. talk about other people who need help, make sure to pick your words wisely so
don't like you are complaining about someone and focus on what kind of help you
manager can bring.
7. not to be stranger to higher management, ask for meeting with them, they
have long-term goal and details. Stay away from yes/no question, instead:
    * how do you see my project aligning with the long-term goal?
    * how do you envision my project evolving in time?
    * what the best outcome you think we can get?

## Ack then think

1. Never let yourself be quiet when you have sth important to say, no matter how
early or late, it has to come out.
2. You can give up safely, say: "Never mind?"

Stop thinking too much and start acting. Start with simplest step to force
yourself. At the very least, start making sounds like "Umm" or send out the
meeting invite, and then you can figure out the rest later.

## Do not be scared to take the time off

If you are sick or have a family situation going on, or have vacation saved up,
use it! You should put you 100% at your job, if cannot, take the PTO until you
can.

## Biggest regret

How to answer interview questions:
1. **what is your biggest regert?**
2. **what is the big risks you have taken?**

If you cannot answer these questions, that means you are playing too safe. Work
on things you will be proud to mention in the future, you can work on 2 safe
projects and jump to a risk one, and repeat.

## Perfectionism the biggest enemy

Focus on MVP(here it means most valuable product), focus on **selling point** and
make sure that is working and nothing more, don't spend on too many details, too
much perfecting!

Don't let the sneaky ego ruin your life.

## Finding an owner area

1. Establish authority and job security, knowing how to add new features and make
sure things do not get broken is super valuable.
2. Who is in charge of what when join a new team, what is the dependency structure
like? Any old system that everyone scared to touch and avoids working on?
3. After gaining ownership, you can slowly start replacing pieces with better
designs.
4. New and high impact projects will have many people trying to stay as an owner.

Lookout any system without clear owners and try to take them over, or start a
new system.

## Do not redesign a working system

1. No working code is bad code! System/requirement keeps changing, the "perfect"
design will be **short-lived**.
2. Your new system is likely not very different from the old one, when crashing
**hacky solution**(inelegant solution to a particular problem) could be added.
3. **weigh** the pros and cons of redesigning a system.

Do not touch a working system thinking you can make it better. Every project
starts super clean and nicely design. Then life happens, and each hacky code
you see is actually fixing a business logic for sth important.

## The important of culture

1. Company culture, the division adds their own culture on top if it and team
further adds sth on their own, for example, how much do you feel encouraged to
break things?
2. Different team/project can have differet **pace**, think about that:
  (1) fast prototype and implementation, breaking and fix fast?
  (2) rethink and do many **sanity check** before submit?
3. adding sth of your own to the culture.

Pick the company that aligns with your cultural expectations.

## Ambiguity

1. Dealing with unknowns
2. Chasing after dozens of people, asking right questions and bringing everyone
to a consensus, this is the hardest part.
3. requirements are constantly changing.
4. senior people work more on **influencing people** to the right choice.
5. A half working system is better than a not working one.

Keep fighting the ambiguty. Start an evolving design doc, collect requirements:
**must-haves vs nice-to-haves** and use cases, prioritize, find solutions that
fit, do **comparative analysis**(pros and cons), setting deadline, bring
tiebreaker, sharing meeting summary, prototype fast, repeat until ambiguity
disappears.

## Hold that question

**Hold your questions for a second**, build your process, and do basic searches
before asking questions. If you cannot find answers, mention your process and
ask for help.

## Put it on my tasks, unblock yourself

1. what if you keep ending up stuck in a discussion you don't believe is worth
the time?
2. never shut someone up in discussion and push back by saying "No".

Make sure people feel valued when they share suggestions by writing them down
to notes, design documents, or simply by creating tasks. If you cannot get into
the task soon, it is nice gesture to reach out to the person and explain why
you cannot prioritize it right now.

## Do you have sometime, ask my calendar

Get better at using calendars, they are like have an executive assistant without
actually paying for one. Try incorporating them into your personal life as well.

## Setting focus time on calendar

Utilize the calendar tool to set up starting, ending business hours, lunch block
and focus time, so people know when not to schedule meetings or ping you directly.

## Make people think it was their idea

1. People let their egos get involved in the decision-making process.
2. Ask yourself how to reach the idea? past learning, data, experience?
3. direct people by asking questions.
4. If no agreement, **step back** and find the **missing pieces**.

Do not tell people what your solution is. Give them the data that help you reach
the idea and ask leading questions until they reach the same solution.

## Being more active on interviewing others

Answer common questions from candidates:
1. what are you working on?
2. how much you like it?
3. how much chance you get for self-development?
4. what your day-to-day life is like?

Be more involved in the interview process.

## What to expect from a brand-new manager

The new manager career has to learn a lot, pick up new perspectives, give up an
old habits, and let go of control. 

Do your best to help them with every challenge they face, otherwise, change your
team as quick as possible.

## Collecting actionable feedback

1. write down the things that need to be improved in each cycle, document how
you had done to improve through the cycle and show the progress at the end.
2. get "constructive" feedback, **actionable** from your colleagues, managers.
3. ask feedback sooner rather than later, anonymized comments, also ask feedback
on product and process.

Always think of what you can improve about yourself, share it with peers and work
on it. Ask for actionable feedback throughout the cycle, find an useful tools to
**anonymize**/ə'nɔnimaiz/(hide identity) them if needed to make your peers feel
comfortable about their honest opinions.

## Dogfooding

Just list private preivew.

1. It is the process of using or testing your own services before going public.

Keep using your service as much as possible, and plan dogfooding sessions. Make
them organized, assign some parts to everyone involved, grant incentives.

## The importance of knowing terms, office jargon

Acronym /'ækrənɪm/: OOO, LGTM, AFAIK, LMAO.

Every time you hear an acronym or term you don't know, loop them up and note
them down until you memorize them. Terms are super helpful to communication
and design discussion, never underrestimate their value.

## Knowing popular frameworks and how they work

1. This is essential to senior track.
2. search **google trends** or **github trends** to see which projects are
trending.
3. Know cloud provider's new product and try it.
4. Take a note of this project in case later you need it, run small prototype.

This can prepare yourself for a great design discussion and have an idea of
what are the **available solution out there** and how they solved similar
problems you are facing

## Never keep all your eggs in the same bucket

1. Implement and **landing** project, then start onboarding other engineers and
focus more on leading them and **offloading** your time to set the direction.
2. When you feel the project is not too challenging anymore or not taking all
of your time, it is a sign to look for other projectsm, and so on.

Solve the hard challenges, put things in order, onboard more engineers, and
focus on becoming a great leader who is hard to replace.

## The importance of positive surroundings

We are the average of the closet friends we have. Make sure you pick the close
friend wisely.

## Being the happy coworker

Find ways to love what you are working on or find projects you will love working
on. If you can achieve being happy during the day, your teammates will love
working with you.

## Working at a start-up

Start-up concerns:
1. move fast, insane paces, good fit?
2. shared office spaces.
3. layoffs due to financial difficulty.
4. tech stack, more open-sources and build from scratch.
5. long work hours, less WLB.
6. income, stock compensation may not be fulfilled before IPO.

Practice to be good at **context-switch and multitasking**.
What stage the start-up is in, and the risk is different.

Start-up can teach you a lot, before decision making connect with engineers to
get some insider info on what they like and dislike.

## Office politics

1. the long-term vision is driven by the leadership, project distributions are
influenced by the leadership.
2. remember that management tends to change regularly; positions get **realigned**
all the time.

Build connections with the leadership but use it more for influencing long-term
vision and not to bully people. Always prefer other conflict resolution tactics,
but if you no other choices, having strong connections is always better than not
having them.

## How to help your manager work for you

1. working on your expectation document improvements.
2. ask for feedback actionable:
  * what areas would you improve on?
  * do you need to be better at communication?
  * should you increase your visibility?
  * is you code quality not up to the standards?
  * do you need to work on higher impact project?
3. help on priority tasks from other teams and talk to others, share the task
details, your insight and how valuable you think.
4. tag your manager in all your design review for visibility, even if they do
not have time to fully review or comment.
5. use your manager as central people database.
6. start writing your GRAD/performance doc early and keep asking review and
feedback.
7. ask for more interview opportunities.
8. ask for the promotion template.
9. ask to find your mentors.
10. share your long-term vision to them, how the project you working on can set
the team at a privileged position in the future and get feedback.
11. have your manager resolve conflicts for you.

## The importance of intelligence

Be good at networking. Build your intelligence netowrk, remember trust is a
two-way thing.

## Comparative analysis

1. putting things in pros and cons list/table and comparing them, **highlight**
the difference for reviewer.

Using comparative analysis to visualize the pros and cons and make a choice to
fit all requirements easily.

## Power of A/B testing

People might suggest things like bias, seasonality or random shifts in user
actions, but if you have strong metrics to prove your contribution, they cannot
say much about it.

Learn how A/B testing works and uses it on your projects so you get data to
prove exactly how you have helped the critical metrics. Data is hard to argue
against.

## Logs, events, metrics and dashboards

Always write self-explanatory log messages and add as many events as possible;
create good dashboards to make sure your system is functioning correctly.

Dashboard with meaningful data is a easy way to convince people, learn dashboard
build tool and SQL.

## Debuggers and crash investigation

1. breakpoint, step in, variable value display, auto crash detection, edit code
and exection on the fly, change vars value, etc
2. there could be some tunnel extension to help you debug on live server
remotely.

Use debuggers on your code smartly and help you fix issues.

## Remember it is not your company

Always remember it is not your company. Use all benefits, enjoy your time there,
focus on learning the most and being more productive.

## Playing for the long game

Focus on the **long game**, not **short-term gain** in your career and life.

]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>Better Small Talk</title>
    <url>/2023/12/23/book-better-small-talk/</url>
    <content><![CDATA[
Objective: 
1. become better at casual conversation/small talk/short interaction.
2. build social confidence
3. keep smiling in facial expression

# Chapter 1
## The small talk mindset

Prepare for the conversation beforehand, to get ready **psychologically**, 
otherwise your thoughts are unorganized, lack of focus and awareness.

Simply like:
```
Hello there!
How's your day going? Great to hear, bye!
```

Do you eat lunch **solo**? Set beside a coworker and start the conversation by
asking recent company/team events.

Daily short interactions with people:
- cashier at checkout counter
- shuttle driver
- waiter
- valet/ˈvælɪt/
- doormen

They need to be courteous/ˈkɜːrtiəs/ to you, so don't be fear of crashing and
burning.

Make it a goal to initiate and create a 10 seconds interaction with a stranger
each day, and especially on the way to functions, events and parties, this will
warm you up for conversation and build the habit of being interested in people.

## A childlike exercise

**Read out loud physically**, to help your speaking muscles and get them in
shape.

You can have short excerpt to stretch your emotion, express them with high and
low points. Pay attention to the tonality, the rate of speech, for big point you
need to **slow the pace**.

## Your conversation resume

Developing and constantly updating your conversation resume can save you from
awkward silence and make it nearly effortless to connect to others.

When someone ask you a question, you don't need to answer it literally and
instead can redirect them to sth else you've prepared on your resume.

Practice the resume before you go to **socailly intense situations**.

## Converstaional stages

Getting to know someone happens in a sequential manner, we cannot skip steps if
we want to go deepers:
1. small talk
2. fact disclosure (share details of your life)
3. opinion disclosure (common ground to share viewpoint and opinions)
4. emotion disclosure (be cautious be the intimacy level)

# Chapter 2

For new people, first remain reserved so you can calibrate your interactions,
read your new acquaintance, and determine how familiar or relaxed you can be.

You are making impression on everyone you meet, send a signal of comfort and
familiarity

## Set the tone

Remeber not to be so literal and serious, be playful, relaxing attitude is right.

## Make the first move

Don't think you are interrupting people, come up with a compelling reason to
interrupt people or walk up to a stranger, that would override your fear of
judgment.

1. Ask people for objective/subjective info.

Switch the perspective to "I don't feel condifent or comfortable" more than an
"I don't know what to say". It does not matter that the people you are asking
know the answer, it is just a way to begin a dialogue, it does not even matter
you don't know the answer.

- do you know what time the speech begin?
- do you know where the closest starbucks is?

2. comment on sth in the environment, context, or specific situation.

3. comment on a commonality you both share.
- why do you know here?
- how do you know jack?

## Find similarity

Find similarity by:
1. mirroring the behavior: gesture, tone, words, etc
2. share a heathy amount of info, enrich the details.

## Manufacture connection

For people that are less talktive, not good at opening up themselves, you can
use elicitation/i,lisi'teiʃən/ tech to ask question indirectly and guide their
response.

1. Recognition: your sweater is nice!
2. Complaining, people love mutual dislike or they defend your point
3. Correction, people love to be right and correct you
4. Naive, but not mean to act stupid

Or:
1. ask a question that your think may be answered, act as if they answered it
and react to that hypothetical answer.

# Chapter 3 How to be captivating

How to use storytelling in your everyday and small talk: talk about the past in
a way that makes people pay attention.

Instead of give one-word answer, get into habit of framing your answers as a
story with a high point.

Bring people's curiosity, you can have compelling anecdotes/ˈænɪkdoʊt/ at hand
in response to very common and widespread questions.

## 1:1:1 method for mini story

1. has one action.
2. can be summed up in one sentence.
3. evokes one primary emotion in the listener.

# Chapter 4 Keep it flowing and smooth

Keep small talk anything but small.

How can you think more quickly on your feet? This requires practice: by simply
taking a word and naming a few words or concepts that it reminds you of, to train
your brain's quick response.

Acronyms to help assit you in never running out of things to say on the fly:
* HPM: history(your personal experience), philosophy(your opinion on the topic),
Metaphor(what the topic makes you think of)
* SBR: specific(more detail on the topcic), board(broader context of topic),
related(related topics)
* EDR: emotion(emotion the topic evokes in the other person), detail(on topic),
restatements(restarting the topic to prompt greater elaboration)

# Chapter 5 Go Deeper, Be Better

let people guard down and build an actual relationship:
1. positive compliment on people's conscious choice that reflect their thinking
process.
2. Listening, having a listening mindset, if they said sth, there is a reason.
3. better questions: ask open-ended questions, follow-up questions.

Get comfortable with dead air and utilize it.

# Chapter 6 Looking Inwards

Looking inward to yourself, are you someone who is easy to make small talk with?

If not, take action and build yourself, find passions and hobbies, proactively
learn about what you are interested in.
]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>Design Data-Intensive Applications</title>
    <url>/2020/05/21/book-design-dataintensive-app/</url>
    <content><![CDATA[
最近的一次阅读是在2023年1月，因为这几年工作中用到了不少书中提到的工具，有一些体会，所以理解起来
比较容易，对相关的概念和知识补充也印象深刻。这本书不太建议没有业界工作经验的人阅读，否则啃起来比
较累效果还不好。

# Part I
主要介绍了Data System Foundation.

## Reliable, Scalable and Maintainable
各自的定义，分类和要点，从这3个方面考虑去构建适合需求的系统。比如Relability，有hardware,
software and human faults; Scalability，用什么去describe workload and performance,
如何处理越来越多的workload; Mainyainability，要多多考虑operations team (run smoothly)
,new engineer (easy to pick up)的感受，以及对未来可能的需求改动留有余地。

## Data Models and Query Language
主要讲了传统的relational DB 和 NoSQL中的document DB (suitable for one to many
relation), 如何随着需求的改变不断进化，还提到了Graph-like Data Model (many to many
relation). 讲到了Declarative language 对比 Imperative的好处，比如SQL，系统可以在内部优
化而不影响Query语句本身。

## Storage and Retrieval
数据库的底层实现，从Hash Indexes，到SSTables (String Sorted Table)，再到LSM-Trees
(Log Structure Merge Tree), 最后到B-Trees (一种在disk上保持sorted structure的结构).

Advantages and downsides of LSM-Trees. 特别是read/write的performance, LSM-Tree的
write operation一般来说比B-Trees效率高，但这里没有一个普世法则，需要通过实际测试才能知道哪种
模式适合你。

In-memory database也提到了，相比于non-inmemory，它不需要encode data in the strucutre
to store in disk, 并且可以实现disk上难以实现的index 结构.

最后提到了data warehouse, 这和传统的处理transaction的数据库独立开来了，优化于适合
analytics. column-oriented storage经常在data warehouse中使用，但不全是。要注意的是
Cassandra有column families的概念，但它并不是属于column-oriented storage.

## Encoding and Evolution
Everything changes and nothing stands still.

A changes to app's feature also requires a change to data that it stores, 这就遇
到2个主要的问题: backward compatibility and forward compatibility. backward is
commonly to see, forward means old code can read data that was written by new
code.

Here encoding has nothing to do with encryption.
The translation from in-memory presentation to a byte sequence is called
encoding (also known as serialization or marshalling), the reverse is called
decoding (parsing, deserialization, unmarshalling).

介绍了一些Language-specific formats，比如Java, Python自带的encoding module or
package, but they have deep problems，比如和语言耦合太强，perofrmance太低很verbose.

JSON,XML,CSV很常用，有一些问题但remain popular and good for many purposes.

Binary encoding可以节约一些空间，但它没有schema，所以需要将object field name encode
into data.

Schema binary encoding, save a lot compare to JSON/XML/CSV: Thrift (from
facebook) and Protocol Buffers (from google), both were made open source. They
are more or less similar: define schema (field can be required or optional),
then code with field tags, good for schema evolution.

Apache Avro, a sub-project from Hadoop, 对于处理Hadoop的大数据很有效, no field tag.
有writer's schema and reader's schema来解决encode, decode的问题，对于schema
evolution, 只要保证双方schema compitable就可以。

# Part 2]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>book</tag>
        <tag>system design</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes 网络权威指南</title>
    <url>/2020/02/03/book-k8s-networking-guide/</url>
    <content><![CDATA[
网络在云计算中又起着至关重要的作用。大概浏览了一下目录和内容总结，很符合我的需求。类似的英文版书籍我一直在寻找，但还真没见到过。很赞同作者在前言中的一句话：工程师不能只当使用者，还要理解底层的实现。其实很多新技术都是原有技术的再封装和创新应用，真正理解了本质的东西对快速学习非常有帮助。




]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>book</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker Advanced</title>
    <url>/2019/06/22/book-docker-concept/</url>
    <content><![CDATA[
最近看了一本书，书名是`<<Docker进阶与实战>>`， 这里并不是讲一些基础入门，而是在已经掌握和应用的基础上，告诉你背后的原理和技术细节。平时留意到的很多现象都在这里得到了解释，还是很值得记一下要点的。


## Chapter 3 镜像
主要介绍 Docker Image，其实就是启动容器的只读模板，是容器启动所需的rootfs。

Image 表示方法：
```
dockerhub-web-address/namespace/repository:tag
```
* namespace: 用于划分用户或组织，有时并没有用到
* repository: 类似于Git仓库，一个仓库有很多镜像
* tag: 区分同一镜像不同版本

Layer这个东西类似于git commit。Image ID是最上层layer的ID。

Docker开源了镜像存储部分的代码，也就是docker registry, 在接触Docker的开始阶段我一直没明白`registry`与`repository`的区别，这2个词长得有点像哦，新手稍不注意就用混了，中文意思也有点类似，一个是档案室，一个是仓库，都可以放东西。

一般来说，docker registry需要与nginx去添加基本鉴权功能，才是一个合格的secure私有镜像库，但有时我并没有这么做。

已经下载到本地的镜像默认是存储在`/var/lib/docker`路径下的。
```
cd /var/lib/docker/image/devicemapper
ls -ltr

total 4
drwx------ 4 root root   37 May  8 15:21 imagedb
drwx------ 5 root root   45 May  8 15:22 layerdb
drwx------ 4 root root   58 May  8 15:29 distribution
-rw------- 1 root root 1269 Jun 17 09:21 repositories.json
```

### 使用Docker镜像
`dangling` image doesn't have name and tag (present as `<none>`), `docker commit` sometimes can generate dangling image, you can use filter to show dangling:
```
docker images --filter "dangling=true"
```
只显示image ID:
```
docker images -q
```

Remove all dangling images:
```bash
# if no use {} in xargs
# the input arguments will be placed at end: docker rmi xxx
docker images --filter "dangling=true" -q | xargs docker rmi
```

There is a tool [`dockviz`](https://github.com/justone/dockviz) can do image analysis job. 可以图形化的展示image的层次。

`docker load`用于被`docker save`导出的镜像，还有一个`docker import`用于导入包含根文件系统的归档，并将之变为镜像, `docker import`常用来制作Docker baseimage。
```bash
docker save -o busybox.tar busybox
docker load -i busybox.tar
```

`docker commit`用于增量生成镜像，效率较低，一般用于前期测试（比如当时non-root开发），最终确定步骤后，可以用`docker build`。

### 镜像的组织结构
可以用这2个命令去窥探一下镜像结构和元数据
```
docker history
docker inspect
```

### 镜像扩展知识
Docker引入了联合挂载`Union mount`技术，使镜像分层成为可能：
发展路径:
```
unionfs -> aufs -> overlayfs 
```

`写时复制 (copy-on-write)` 是Docker image强大的一个重要原因，操作系统中也广泛用到了，比如`fork`.当父进程fork子进程时，并没有真正分配内存给子进程，而是共享，当2者之一修改共享内存时，触发缺页异常才导致真正的内存分配。这样加速了子进程的创建，也减少了内存消耗。

联合文件系统是实现写时复制的基础，Ubuntu自带`aufs`, Red Hat和Suse采用`deivcemapper`方案（在`/var/lib/docker/image`下就是这个东西），作为Docker的存储驱动，它们的存储结构和性能都有显著差异，要根据实际情况选用。

## Chapter 4 仓库进阶
对Docker registry的API访问，传输对象主要包括镜像layer的块数据(`blob`)和表单(`manifest`)。layer数据以二进制方式存放于registry中。主要讲了一下用API进行pull, push的过程，步骤。其实都划分了很多步。

List and Dlete Image 可以参考我这篇博客[`<<Docker Registry API>>`](https://chengdol.github.io/2019/06/10/docker-registry-api/)。

鉴权机制，这里使用的是Docker Engine, Registry和Auth Server协作完成。Auth Server由Registry开发者部署搭建，Registry完全信任Auth Server.

```js
+---------------+               +------------------------+
|               +-------+       |                        |
|  Registry     |       |       | Authorization Service  |
|               +<--+   |       |                        |
+-+--+----------+   |   |       +---------------+--+-----+
  ^  |              |   |                       ^  |
  |  |            5 |   | 6                     |  |
  |  |              |   |                       |  |
1 |  | 2            |   |                     3 |  | 4
  |  |          +---+---v---------+             |  |
  |  +--------->+                 +-------------+  |
  |             |  Docker Daemon  |                |
  +-------------+                 +<---------------+
                +--------+--------+
                         ^
                         |
           +-------------+----------------+
           |      Docker Client           |
           |    $docker pull busybox      |
           |                              |
           +------------------------------+
```
1. Docker Engine试图赋予HTTP请求一个鉴权的token，如果没有，Daemon会试图fetch/refresh一个新的token。
2. 如果请求没有做过认证且不含token则Registry会返回401 Unauthorized状态
3. 用户带着Registry返回的信息以及证书去访问Auth Server申请token （**这里具体怎么操作？证书在哪里？**）
4. Auth Server后端账户记录着用户名和密码，获取用户请求后，会将鉴权信息的token返回给用户
5. 用户带着token再次访问Regisrty, HEADER中包含:
   ```
   Authorization: Bearer <token content>
   ```
6. Registry检验token，如通过则开始工作

### 构建私有仓库
In general, we can simply run a private docker registry:
```
docker run -d \
  --hostname localhost \
  --name registry-v2 \
  -v /opt/data:/var/lib/registry \
  -p 5000:5000 \
  registry:2.0
```
这里将本地目录`/opt/data`挂载到容器镜像存储目录`/var/lib/registry`，方便查看和管理镜像数据，在DataStage Installer中也是如此。这时的registry是不安全的，能访问本机5000端口的人都可以上传和下载镜像。

需要为其加上HTTPS反向代理，这里以Nginx来实现。然后代理服务器会接受HTTPS请求，然后将请求转发给内部网络上的registry服务器，并将registry访问结果返回给用户。

可以参考我的这篇blog关于如何搭建[`Secure Docker Registry`]().

## Chapter 5 Docker网络














]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>book</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes 进阶实战</title>
    <url>/2019/09/15/book-k8s-adv-practical/</url>
    <content><![CDATA[
> 06/12/2020， 打算下半年把K8s CKA/CKAD 证书考了，要review一下这本书。

这是最近几周看的书（中文），年初从国内带过来的。

最近的项目涉及到很多微服务架构设计的问题，和组里大佬讨论的时候发现自己有的地方理解得不太正确甚至一张白纸，给不出一个完善的设计方案和想法，赶紧更新和归纳一下自己之前学到的知识点。

其实接触Kubernetes也快一年了，但一直是对之前项目的维护和更新操作，这次难得机会要集成一个新组件到已有的集群里，从头开始设计这些功能组件的各种配置，结构，生命周期，存储卷，依赖等。这本书算是一个由点到面的总结，提供了不少的帮助。

最近逐渐体会到一个好的，完善的，充分考虑的顶层设计是多么重要，否则每次设计改动对应到底层可能耗时耗力，甚至积重难返，不得不相互妥协。

这本书不打算做细致的笔记，但会记录一些以前没意识到或没见过的概念和工具。之后打算更细致的看一看`<<Kubernetes in Action>>`。我看最近`O'REILLY`出版了或即将出版很多关于Kubernetes的新书，其中比较吸引我的是`pattern`，`best practices`, `Operator`以及一些`cloud native devops`相关的知识点，当然还包括一些重要系统组件和插件，比如`SSL/TLS`, `CoreDNS`和`etcd`等。看来2019剩下的日子是真的会很忙了。

# 笔记
按照计划在2019年10月之前把这本书看完了第一遍，在书中做了不少标记和勾画，接下来快速记录一下其中的我比较注意的地方：

---------
Kubernetes通过其附加组件之一的CoreDNS为系统内置了服务注册和服务发现功能，为每个Service配置了DNS名称，允许客户端通过此名称发出访问，并且Service通过kube-proxy creates iptabls or ipvs内建了负载均衡机制。Service本质上讲是一个四层代理服务，也可将外部流量引入集群内部。

网络存储系统，诸如：NFS, GlusterFS, Ceph, Cinder...目前项目里面其实用的是本地存储，只不过额外设置了NFS将本地存储连接了起来，并没有直接只用K8s中的NFS配置。

API Server是整个集群的网关。

etcd是由CoreOS基于raft协议开发的分布式键值存储，可用于服务发现，共享配置以及一致性保障，它是独立的服务组件，并不隶属于K8s。etcd中键值发生变化时会通知API Server，并通过watch API向客户端输出，实现高效协同。

K8s supports container runtime: Docker, cri-o, RKT, Fraki
cri-o use gRPC(Remote procedure call)

K8s Dashborad
Prometheus and it's add-ons
Ingress controller: 应用层负载均衡机制: Nginx, HAProxy...

Pod网络由网络插件实现(for example: fannel, calico, weave...), Service网络由K8s指定。

--------


]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>book</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>The Linux Command Line 2nd</title>
    <url>/2019/08/15/book-linux-command-line/</url>
    <content><![CDATA[
距离第一次看这本书的第一版已经快2年了，当时还完全是个newbie. 这是看第二遍，第二版，重新整理一下笔记，巩固一下不太常用的内容。
--- 2021/02/06 - 2021/03/13.

# Part 1
When we speak of the command line, we are really referring to the shell. The shell is a program that takes keyboard commands and passes them to the operating system to carry out.

If the last character of the prompt is a `hash mark(#)` rather than a `$`, the terminal session has superuser privileges.

Terminal emulator，想想为啥叫emulator, 因为它是从GUI中提供的，并不是系统boot后直接进入terminal的形式。

Unix-like systems such as Linux always have a single file system tree, regardless of how many drives or storage devices are attached to the computer. Starts from `/`.

Linux has no concept of a `file extension` like some other operating systems. Although Unix-like operating systems don’t use file extensions to determine the contents/purpose of files, many application programs do.

Some options and arguments unknown to me or not common:
```bash
# cd to home of vagrant user
cd ~vagrant
# list multiple directories
ls ~ /usr /opt

# A: list all but . and ..
# F: append / for directories
ls -AF

# 3: number of existing hard link to this file data part
drwxr-xr-x 3 root root  24 Jun  6  2020 mnt

# in less page hit 'h': display help screen
less file

# re-initialize the terminal, deeper than clear
# 很少用到
reset

# -u: When copying files from one directory to another
# only copy files that either don’t exist or are newer
# than the existing corresponding files in the destination directory. 
cp -u|--update

# Normally, copies take on the default attributes of the user performing the copy.
# a: preserve the original permission and user/group
# for example, root user cp oridinary user file
cp -a
```

Remember that a file will always have at least one hard link because the file’s name is created by a link. When we create hard links, we are actually creating additional name parts that all refer to the same data part.

Directory 是不能有hard link的，一个文件夹的hardlink 部分数字一般是2+n, 2代表文件夹内部`.`, `..`, n 代表subdirectories数量。

The `type` command is a shell builtin that displays the kind of command the shell will execute, command can be categorized as:
- executable binary
- script
- function
- alias

`which` is for locating executables only.
`apropos` is the same as `man -k`:
```bash
apropos partition
man -k partition
```

Modern version of redirection:
```bash
# &> is equal to 2>&1
ls -l /bin/usr &> ls-output.txt
# append
echo "123" &>> ls-output.txt
```

Interesting accident below, The lesson here is that the redirection operator silently creates or overwrites files, so you need to treat it with a lot of respect.
```bash
cd /usr/bin
# overwrite less program under /usr/bin
# instead use ls | less
ls > less
```

Path expansion, 我有blog专门记录了shell globings, pattern match, expansion:
```bash
# list hidden files without . and ..
echo .[!.]*

# arithmetic
# 注意区别(( ))是针对integer的compound command, used in condition such as if, while, for, etc.
echo $((3 + 4))
# can use single parentheses inside without $ prefix
echo "$(((5**2) * 3))"
```
Remember, parameter expansion "$USER", arithmetic expansion, and command substitution still take place within double quotes. If we need to suppress all expansions, we use single quotes.

Command history quick rerun:
```bash
# execute last command
!!
# execute command at 123 line in hsitory
!123

# be cautious
# last command starts with "string"
!string
# last command contains "string"
!?string
```

Mentioned a new command `script`, used to record the terminal typing as a hardcopy into a file, for example for students learning. The history is controlled by env vars:
```bash
# ignore duplicates command
export HISTCONTROL=ignoredups
export HISTSIZE=1000
```

Load average in `top` command: refers to the number of processes that are waiting to run; that is, the number of processes that are in a runnable state and are sharing the CPU. Three values are shown, each for a different period of time. The first is the average for the last 60 seconds, the next the previous 5 minutes, and finally the previous 15 minutes. Values less than 1.0 indicate that the machine is not busy (还和有多少cores有关，多核则可以大于1).

Other commands for `top` useful are `?/h` for help, `f` for column and sort selection, `m` check memory utilization, sorted by memory can type `shift + m`.

About signal (这部分解释得比较好):
- HUP: also used by many daemon programs to cause a `reinitialization`. This means that when a daemon is sent this signal, it will reload its configuration file. The Apache web server is an example of a daemon that uses the HUP signal in this way.
- INT: Interrupt. This performs the same function as `ctrl-c` sent from the terminal. It will usually terminate a program.
- TERM: Terminate. This is the default signal sent by the kill command. If a program is still “alive” enough to receive signals, it will terminate.
- STOP: Stop. This signal causes a process to pause without terminating. Like the KILL signal, it is not sent to the target process, and thus it cannot be ignored.
- TSTP: Terminal stop. This is the signal sent by the terminal when `ctrl-z` is pressed. Unlike the STOP signal, the TSTP signal is received by the program, but the program may choose to ignore it.
- CONT: Continue. This will restore a process after a STOP or TSTP signal. This signal is sent by the bg and fg commands. For example, use `ctrl-z` with a fg and then run bg to make it run in background.


# Part 2
The `set` command will show both the shell and environment variables as well as any defined shell functions, while `printenv` will display only the environment variables. 

Environment variables:
`PS1`: Stands for `prompt string 1`. This defines the contents of the shell prompt. Also prompt can be customized by shell function and scripts. 还有`PS2`, `PS3`, etc. see this [article](https://www.thegeekstuff.com/2008/09/bash-shell-take-control-of-ps1-ps2-ps3-ps4-and-prompt_command/).


# Part 3
High- and Low-Level Package Tools
```
Distributions              | Low-level tools       |  High-level tools
------------------------------------------------------------------------------
Debian-style               | dpkg                  |  apt-get, apt, aptitude
------------------------------------------------------------------------------
Fedora,                    | rpm                   |  yum, dnf
Red Hat Enterprise Linux,  |                       |
CentOS                     |                       |
```
Low-level tools that handle tasks such as installing and removing package files.
High-level tools that perform metadata searching and dependency resolution.

主要了解安装，更新，删除，查找这几个操作即可，low/high level都有相关的选项.

Linux storage device names convention:
`/dev/fd*` Floppy disk drives
`/dev/hd*` IDE (PATA) disks on older systems
`/dev/lp*` printer
`/dev/sd*` SCSI disks. On modern Linux systems, the kernel treats all disk-like device as SCSI disks
`/dev/sr*` Optical drives (CD/DVD readers and burners)

`/var/log/messages` vs `/var/log/syslog`, `/var/log/messages` is the syslog on non-Debian/non-Ubuntu systems such as RHEL or CentOS systems usually. `/var/log/messages` instead aims at storing valuable, non-debug and non-critical messages. This log should be considered the "general system activity" log.
`/var/log/syslog` in turn logs everything, except auth related messages.
```bash
# a great way to watch what the system is doing in near real-time.
tail -f /var/log/messages
```

The last digit in `/etc/fstab` file is used to specify integrity checking privilege when system boots by `fsck` (file system check), `0` means not routinely checked:
```
/dev/sdb /data xfs defaults 0 0
```
`fsck` can also repair corrupt file systems with varying degrees of success, depending on the amount of damage. On Unix-like file systems, recovered portions of files are placed in the `lost+found` directory, located in the root of each file system.
```bash
# unmount /dev/sdb1 first
sudo fsck /dev/sdb1
```

`dd` (data definition) can be used to copy block of data:
```bash
# backup sdb to sdc
dd if=/dev/sdb of=/dev/sdc
# backup to ordinary file
dd if=/dev/sdb of=/tmp/sdb.bk
```

When it comes to networking, there is probably nothing that cannot be done with Linux. Linux is used to build all sorts of networking systems and appliances, including firewalls, routers, name servers, network-attached storage (NAS) boxes, and on and on.

The `locate` database is created by another program named `updatedb`(you can run it manually or by system cronjob).
`find` command can have logic operators:
```bash
# -and/-a
# -or/-o
# -not/!
# escape () as it has special meaning to shell
# object can only be a file or directory, so use -or
find ~ \( -type f -not -perm 0600 \) -or \( -type d -not -perm 0700 \)

# -print, always test first
find ~ -type f -name '*.bak'[-print]
# the same as
find ~ -type f -and -name '*.bak' -and -print
# order matters!!! just like if condition and group, from left to right, short execution
find ~ -print -and -type f -and -name '*.bak'
# -delete action
find ~ -type f -name '*.bak' -delete

# Here, command is the name of a command, {} is a symbolic 
# representation of the current pathname, and the semicolon 
# is a required delimiter indicating the end of the command
# {} and ; need to quote as special meaning to shell
find ~ -type f -name 'foo*' -exec ls -l '{}' ';'
# -ok: interactive with user
find ~ -type f -name 'foo*' -ok ls -l '{}' ';'

# more powerful and flexible
find /tmp -mindepth 1 -maxdepth 1 -type d -mmin +50 -exec ls -d {} \;
```
It is really useful to do range search with timestamp: [atime, ctime, and mtime](https://unix.stackexchange.com/questions/558124/understanding-find-with-atime-ctime-and-mtime).


`xargs` is more efficient then regular `-exec {} ;` because it executes in one time (when setting properly), 关于这2个效率的比较: [FIND -EXEC VS. FIND | XARGS](https://www.everythingcli.org/find-exec-vs-find-xargs/). 不同options是不同的效率！！
```bash
# -r: when no input for xargs, don't run it
# 如果不用-r, 则xargs总会执行，输出不符合的结果
find ~ -type f -name 'foo*' -print | xargs -r ls -l
# same as -exec with +
find ~ -type f -name 'foo*' -exec ls -l '{}' '+'

# practices
find ~ \( -type f -not -perm 0600 -exec chmod 0600 '{}' ';' \) -or \( -type d -not -perm 0700 -exec chmod 0700 '{}' ';' \)
# with tar
# r: append mode to tar
find ~ -name 'file-A' -exec tar rf old.tar '{}' '+'

# -: stdin or stdout as needed
# --files-from=- 表示从upper pipeline得到文件列表
# cf - 表示tar的东西又输出到下一个pipeline了
find playground -name 'file-A' | tar cf - --files-from=- | gzip > playground.tgz
# --files-from=- abbr is equal to -T -
find playground -name 'file-A' | tar cf target.tar -T -

# tar and transfer remote dir to local and untar
ssh remote-sys 'tar cf - Documents' | tar xf -
```
If the filename `-` is specified, it is taken to mean standard input or output, as needed. (By the way, this convention of using `-` to represent standard input/output is used by a number of other programs, too). 


下面介绍一些text processing commands，其实就是git 使用的部分功能.
commonly use and known: `cat`, `sort`, `uniq`, `cut`, `paste`, `join`, `comm`, `diff`, `patch`, `tr`, `sed`, `aspell`. 

`sort`: Many uses of sort involve the processing of tabular data
```bash
# --key/-k 1,1: starts from field 1 and end at field 1
# -key=2n: field 2 sort as numeric
sort --key=1,1 --key=2n distros.txt

# sort on specific part of a field
# Fedora    10   11/25/2008
# Ubuntu    8.10   10/30/2008
sort -k 3.7nbr -k 3.1nbr -k 3.4nbr distros.txt

# sort on shell field
sort -t ':' -k 7 /etc/passwd | head
```
`cut`: cut is best used to extract text from files that are produced by other programs, rather than text directly typed by humans. 
```bash
# Fedora    10   11/25/2008
# Ubuntu    8.10   10/30/2008
# extrace year number
# -c: character range
# -f: field index, start from 1
cut -f 3 file | cut -c 7-10
# can convert tab to spaces
# tab 是会对齐field的
expand file | cut -c 23-
```

`paste`: it adds one or more columns of text to a file in argument order
```bash
# assume file1 file2 file3 are columned format
paste file1 file2 file3
```

The `join` program it joins data from multiple files based on a `shared` key field. The same as DB join operation:
```bash
# assume file1 and file2 has shared field
join file1 file2
```

`comm`: Compare Two Sorted Files Line by Line
`diff`: Compare Files Line by Line
```bash
# column 1 is in file1
# column 2 is in file2
# column 3 is in both file1 and file2
# -n (-12): remove 1 and 2 column in output
comm -12 file1.txt file2.txt
```
`patch`: Apply a diff to an Original, It accepts output from `diff` and is generally used to convert older version files into newer versions. git其实push的也是diff的部分，然后patch 到target branch中.
```bash
# run in the same dir level
diff -Naur old_file new_file > diff_file
# diff_file has enough information for patch
patch < diff_file
```
where old_file and new_file are either single file or directory containing files. The `r` option supports recursion of a directory tree.

`tr`: think of this as a sort of character-based search-and-replace operation.
```bash
echo "lowercase letters" | tr a-z A-Z
echo "lowercase letters" | tr [:lower:] A-Z
# delete
tr -d '\r' <dos_file> unix_file
# squeeze
echo "aaabbbccc" | tr -s ab
```

`nl`: number Lines, add header, body, footer
`fold`: wrap each line to a specified length
```bash
# -w: width 50 character
# -s: bread at speace not in the middle of word
fold -w 50 -s test.file
```
`fmt`: simple text formetter, it fills and joins lines in text while preserving blank lines and indentation.
```bash
# -w: width 50 characters
# -c: operate in crown margin mode, better format than fold
fmt -cw 50 test.file
```

`printf`: not used for pipelines, used mostly in scripts where it is employed to format tabular data, similar to C printf:
```bash
printf "I formatted '%s' as a string.\n" foo
# the same placeholder as C program
# %[flags][width][.precision]placeholder
printf "%d, %f, %o, %s, %x, %X\n" 380 380 380 380 380 380
```

`make` command to compile C/C++ program.
Some compilers translate high level instructions into assembly language and then use an assembler to perform the final stage of translation into machine language. A process often used in conjunction with compiling is called linking. A program called a linker is used to form the connections between the output of the compiler and the libraries that the compiled program requires. The final result of this process is the executable program file, ready for use.

Shell scripts that do not require compiling. They are executed directly. These are written in what are known as scripting or interpreted languages. These languages have grown in popularity in recent years and include Perl, Python, PHP, Ruby, and many others.

we need `Makefile` (usually generated by configure script) to instruct `make` command to compile source code:
```bash
# usually this file is to collect system configuration and check
# required libraries
./configure
# compile and build app
# will choose Makefile in the same directory automatically
make
# install app
# install is also a build target from running make command
sudo make install
```
`install` will install the final product in a system directory for use. Usually, this directory is `/usr/local/bin`, the traditional location for locally built software. However, this directory is not normally writable by ordinary users, so we must become the superuser to perform the installation.


# Part 4
Part 4 primarily talks about scripting, I place the notes in shell and scripting blogs.

Positional arguments, `$0` will always contain the first item appearing on the command line(exactly what it is). When parameters size is large, use `shift` to access:
```bash
count=1
# $2 keep moving to $1
while [[ $# -gt 0 ]]; do
  echo "Argument $count = $1"
  count=$((count + 1))
  shift
done
```
Just as positional parameters are used to pass arguments to shell scripts, they can also be used to pass arguments to shell functions.

Here document(`<<[-]`) and here string(`<<<`), `<<-` is related to stripe leading tab, depends on your use case (usually `<<` is fine).

If the optional `in` words portion of the `for` command is omitted, for defaults to processing the positional parameters:
```bash
# set positional parameters
set a b c
# without in keywork, for loops the positional parameters
for i; do
  echo "$i"
done
```
Modern for loop mimics C program:
```bash
# i is treated as integer, no need $i prefix in (( ))
for (( i=0; i<5; i=i+1 )); do
  echo $i
done
```

Arithmetic evaluation:
```bash
# the same as (( )) for integer compution
echo $(( a + 2 ))
echo $(( 5 % 2 ))
# number bases, output is converted to 10 based value
# $1 is a 16 based value
echo $((0x$1))
echo $((0xff))
# 2#: 2 based
echo $((2#11111111))

# can use C style shortcut, also used in for loop (( ))
declare -i a=0
$((a+=1))
$((a-=1))
$((a/=1))
$((a*=1))
$((a++))
$((a--))
$((++a))
$((--a))

# bit operations are the same as C
for ((i=0;i<8;++i)); do echo $((1<<i)); done
# logic operation: < > <= >= == != && || !
echo $((a<1?++a:--a))
```
类似C中的用法，同时做赋值和判断:
```bash
foo=
# foo is assigned 5 and valued as true
# notice that here = is not ==, although in [ ], = is the same as ==
if (( foo = 5 )); then echo "It is true."; fi
```

The `bc` program reads a file written in its own C-like language and executes it. A `bc` script may be a separate file, or it may be read from standard input.
```bash
# start and quiet bc, use stdin to input
bc -q
# use here string
bc <<< "2+2"
```

Group command does not create subshell:
```bash
# note the space after { and before }
# each cmd must has ;
{ cmd1; cmd2; [cmd3; ...] }

# { } returns the total output
{ ls -l; echo "Listing of foo.txt"; cat foo.txt; } > output.txt
```
Therefore, in most cases, unless a script requires a subshell, group commands are preferable to sub­ shells. Group commands are both faster and require less memory.

Process substitution, it feeds the output of a process (or processes) into the stdin of another process.
这里的例子主要是和read 结合使用:
```bash
# rediect output from process substitution to read
read < <(echo "foo")
# or using here string
read <<< "foo"
echo $REPLY
```
Process substitution allows us to treat the output of a subshell as an ordinary file for purposes of redirection. 可以看做一个文件.
```bash
# output is /dev/fd/63
echo <(echo "foo")
```
Process substitution is often used with loops containing `read`.

Other example of process substitution:
```bash
grep word /usr/share/dict/linux.words | wc
# can be modified as
wc <(grep word /usr/share/dict/linux.words)
```

In most Unix­like systems, it is possible to create a special type of file called a `named pipe`. Named pipes are used to create a connection `between two processes` and can be used just like other types of files. Named pipes behave like files but actually form first­in first­out (FIFO) buffers.
```bash
# type is p
# prw-r--r--. 1 root root 0 Mar 13 23:11 pipe1
mkfifo pipe1

# in terminal 1
ls -l > pipe1

# in terminal 2
cat < pipe1
```
The input will block if no receiving part.]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>book</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Infrastructure as Code</title>
    <url>/2021/01/02/book-infra-as-code/</url>
    <content><![CDATA[
刚看完第一版，第二版就出来了, hmmm... -_-|||

This book doesn’t offer instructions in using specific scripting languages or tools. There are code examples from specific tools, but these are intended to illustrate concepts and approaches, rather than to provide instruction.

最开始介绍了作者之前的一些经历 starts from team Vmware virtual server farm，从这些经历中，慢慢领悟和学习到了IaC的必要性。puppet and chef 看来是很久之前的config automation tool了. 后来过渡到cloud，从其他IT Ops team 中学到很多new ideas, eye-opener: "The key idea of our new approach was that every server could be automatically rebuilt from scratch, and our configuration tooling would run continuously, not ad hoc. Every server added into our new infrastructure would fall under this approach. If automation broke on some edge case, we would either change the automation to handle it, or else fix the design of the service so it was no longer an edge case."

虚拟机和容器相辅相成。
Virtualization was one step, allowing you to add and remove VMs to scale your capacity to your load on a timescale of minutes. Containers take this to the next level, allowing you to scale your capacity up and down on a timescale of seconds.

后来我想到了一个问题，如果把容器运行在虚拟机之上，不又多了一层overhead吗？有没有优化。比如GKE runs on GCE VM, any optimization on VM image for k8s? 是的，用的是[container-optimized OS](https://cloud.google.com/container-optimized-os/docs/concepts/features-and-benefits?hl=en).

这里还有一些文章，讲了k8s运行环境的比较:
[Where to Install Kubernetes? Bare-Metal vs. VMs. vs. Cloud](https://platform9.com/blog/where-to-install-kubernetes-bare-metal-vs-vms-vs-cloud/).
[Running Containers on Bare Metal vs. VMs: Performance and Benefits](https://www.stratoscale.com/blog/data-center/running-containers-on-bare-metal/)


# Part I Fundations
## Chapter 1
`Infrastructure as code` is an approach to infrastructure automation based on practices from software development. It emphasizes consistent, repeatable routines for provisioning and changing systems and their configuration. Changes are made to definitions and then rolled out to systems through unattended processes(指不需要人参与) that include thorough validation.

The phrase `dynamic infrastructure` to refer to the ability to create and destroy servers programmatically.

`Challenges` with dynamic infrastructure, the previous one can cause the next:
- Server Sprawl: servers growing faster then ability can control.
- Configuration drift: inconsistency across the servers, such as manual ad-hoc fixes, config.
- Snowflake Server: can’t be replicated.
- Fragile Infrastructure: snowflake server problem expands.
- Automation Fear: lack of confidence.
- Erosion: infrastructure decays over time, such as components upgrade, patches, disk fill up, hardware failure.

An operations team should be able to confidently and quickly rebuild any server in their infrastructure.

`Principles` of Infrastruction as Code to mitigate above challenges:
- Systems can be easily reproduced.
- Systems are disposable.
- Systems are consistent.
- Processes are repeatable.
- Design is always changing.

Effective infrastructure teams have a strong `scripting culture`. If a task can be scripted, script it. If a task is hard to script, drill down and see if there’s a technique or tool that can help, or whether the problem the task is addressing can be handled in a different way.

General `practices` of infrastructure as Code:
- Use definition files: to specify infra elements and config.
- Self-documented systems and processes: doc may leave gaps over time.
- Version all things.
- Continuously test systems and processes, how? see Chapter 11.
- Small changes rather than batches.
- Keep services available continuously, see chapter 14.
- Antifragility, beyond robust: When something goes wrong, the priority is not simply to fix it, but to improve the ability of the system to cope with similar incidents in the future.


## Chapter 2
`Dynamic Infrastructure Platform`: is a system that provides computing resources, particularly servers, storage, and networking, in a way that they can be programmatically allocated and managed.

主要讲了构造dynamic infra platform 的要求，比如programmable, on-demand, self-service。需要提供用户什么功能，比如compute, storage(block, storage storage, networked filesystem), network, anth, etc。

这里有个概念要澄清一下: private cloud vs bare-metal cloud. 之前认为意义相同，但并不是，bare-metal cloud is running an OS directly on server hardware rather than in a VM. There are many reasons why running directly on hardware may be the best choice for a given application or service. Virtualization adds performance overhead, because it inserts extra software layers between the application and the hardware resources it uses. Processes on one VM can impact the performance of other VMs running on the same host. 常用的tool for managing bare-metal: Cobbler, Foreman, etc.

An IT professional, the deeper and stronger your understanding of how the system works down the stack and into the hardware, the more proficient you’ll be at getting the most from it.

并不是说new instance就一定是well performance的，虚拟化也有很多不确定因素:
For example, the Netflix team knew that a percentage of AWS instances, when provisioned, will perform much worse than the average instance, whether because of hardware issues or simply because they happen to be sharing hardware with someone else’s poorly behaving systems. So they wrote their provisioning scripts to immediately test the performance of each new instance. If it doesn’t meet their standards, the script destroys the instance and tries again with a new instance.

Software and infrastructure should be architected, designed, and implemented with an understanding of the true architecture of the hardware, networking, storage, and the dynamic infrastructure platform.


## Chapter 3
`Infrasturcture Definition Tools`: This chapter has discussed the types of tools to manage high-level infrastructure according to the principles and practices of infrastructure as code.


## Chapter 4
`Server Configuration Tools`
主要讲了provisioning tools, such as chef, puppet and ansible, salt. Tools for packing server templates, such as Packer. Tools for running commands on server.

Many server configuration tool vendors provide their own `configuration registry` to manage configuration definitions, for example, Chef Server, PuppetDB, and `Ansible Tower`.

In many cases, new servers can be built using off-the-shelf `server template` images. Packaging common elements onto a template makes it `faster` to provision new servers. Some teams take this further by creating server templates for particular roles such as web servers and application servers. Chapter 7 discusses trade-offs and patterns around baking server elements into templates versus adding them when creating servers (这个是当时正在做的新项目)。

[Unikernel Server Templates](https://en.wikipedia.org/wiki/Unikernel). an OS image that is custom-compiled with the application it will run. The image only includes the parts of the OS kernel needed for the application, so is small and fast. This image is run directly as a VM or container (see later in this chapter) but has a single address space.

It’s important for an infrastructure team to build up and continuously improve their skills with scripting. Learn new languages, learn better techniques, learn new libraries and frameworks

`Server change management` models:
- Ad hoc change, lead to config drift, snowflake server and other evils.
- Configuration synchronization, may cause config drift on left parts
- Immutable infra, completely replacing, requires good templates management.

Containerized services follows something similar to immutable infra, replace old container completely when apply changes. A container uses operating system features to isolate the processes, networking, and filesystem of the container, so it appears to be its own, self-contained server environment.

There is actually some dependency between the host and container. In particular, container instances use the Linux kernel of the host system, so a given image could potentially behave differently, or even fail, when run on different versions of the kernel.

A host server runs virtual machines using a hypervisor, Container instances share the operating system kernel of their host system, so they can’t run a different OS. Container has less overhead than a hardware virtual machine. Container image can be much smaller than a VM image, because it doesn’t need to include the entire OS. It can start up in seconds, as it doesn’t need to boot a kernel from scratch. And it consumes fewer system resources, because it doesn’t need to run its own kernel. So a given host can run more container processes than full VMs.

[Container security](https://docs.docker.com/engine/security/), While containers isolate processes running on a host from one another, this isolation is not impossible to break. Different container implementations have different strengths and weaknesses. When using containers, a team should be sure to fully understand how the technology works, and where its vulnerabilities may lie.

Teams should ensure the provenance of each image used within the infrastructure is well known, trusted, and can be verified and traced. (当时ICP4D image RedHat 也专门去扫描检测了)。


## Chapter 5
`General Indrastructure Services`.
The purpose of this chapter isn’t to list or explain these services and tools. Instead, it is intended to explain how they should work in the context of a dynamic infrastructure managed as code. 

The services and tools addressed are `monitoring`, `service discovery`, `distributed process management`, and `software deployment`. (这是几个主要的在infra 完成构建后的其他主要服务)

`Monitor`: alerting, metrics and logging.
Monitoring information comes in two types: state and events. State is concerned with the current situation, whereas an event records actions or changes.
- Alerting: Tell Me When Something Is Wrong
- Metrics: Collect and Analyze Data
- Log Aggregation and Analysis

`Service Discovery`: Applications and services running in an infrastructure often need to know how to find other applications and services. 

`Distributed Process Management`: VMs or containers. K8s, Nomad, Openshift.

`Software Deployment`: Many have a series of environments for testing stages, including things like operational acceptance testing (`OAT`), `QA` (for humans to carry out exploratory testing), system integration testing (`SIT`), user acceptance testing (`UAT`), staging, preproduction, and performance.


# Part II Patterns
## Chapter 6
`Patterns for Provisioning Servers`.

Provisioning is not only done for a new server. Sometimes an existing server is re- provisioned, changing its role from one to another.

Server's lifecycle:
1. package a server template.
2. create a new server
3. update a server
4. replace a server
5. delete a server

Zero-downtime replacement ensures that a new server is completely built and tested while the existing server is still running so it can be `hot-swapped` into service once ready.

Advocates of immutable servers view making a change to the configuration of a production server as bad practice, no better than modifying the source code of software directly on a production server.

6. recover from failure, outage, maintenance
7. resize server pool, add/remove instances
8. reconfig hardware resources, for example, add CPU, RAM, mount new disks, etc.

`Server roles`:
Another pattern is to have a `role-inheritance hierarchy`(我们确实也是这么做的). The base role would have the software and configuration common to all servers, such as a monitoring agent, common user accounts, and common configuration like DNS and NTP server settings. Other roles would add more things on top of this, possibly at several levels.

It can still be useful to have servers with multiple roles even with the role inheritance pattern. For example, although production deployments may have separate web, app, and db servers, for development and some test cases, it can be pragmatic to combine these onto a single server.

Cloned server (similar to save container to image) suffers, because they have `runtime data` from the original server, which is not reproducible and it accumulate changes or data.

Bootstrapping new servers:
- push bootstrapping: Ansible, Chef, Puppet
- pull bootstrapping: cloud-init

`Smoke test` every new server instance:
- Is the server running and accessible?
- Is the monitoring agent running?
- Has the server appeared in DNS, monitoring, and other network services?
- Are all of the necessary services (web, app, database, etc.) running?
- Are required user accounts in place?
- Are there any ports open that shouldn’t be?
- Are any user accounts enabled that shouldn’t be?

Smoke tests could be integrated with monitoring systems. Most of the checks that would go into a smoke test would work great as routine monitoring checks, so the smoke test could just verify that the new server appears in the monitoring system, and that all of its checks are green.


## Chapter 7
`Patterns for Managing Server Templates` 需要重点关注.

这也是我们采取的前后2种方法，new generation采用第二种。也可以两者结合，把经常变化的部分放在creation time provisioning.
One end of the spectrum is minimizing what’s on the template and doing most of the provisioning work when a new server is created.

Keeping templates minimal makes sense when there is a lot of variation in what may be installed on a server. For example, if people create servers by self-service, choosing from a large menu of configuration options, it makes sense to provision dynamically when the server is created. Otherwise, the library of prebuilt templates would need to be huge to include all of the variations that a user might select.

At the other end of the provisioning spectrum is putting nearly everything into the server template.

Doing all of the significant provisioning in the template, and disal‐ lowing changes to anything other than runtime data after a server is created, is the key idea of `immutable servers`. 

`Process to build template`
An alternative to booting the origin image is to mount the origin disk image in another server and apply changes to its `filesystem`. This tends to be much faster, but the customization process may be more complicated. 

Netflix’s Aminator tool builds AWS AMIs by mounting the origin image as a disk volume. The company’s blog post on [Aminator](https://netflixtechblog.com/ami-creation-with-aminator-98d627ca37b0) describes the process quite well. Packer offers the [amazon-chroot builder](https://www.packer.io/docs/builders/amazon-chroot.html) to support this approach.

It could make sense to have server templates tuned for different pur‐ poses. Database server nodes could be built from one template that has been tuned for high-performance file access, while web servers may be tuned for network I/O throughput.(我们并没有考虑这么多)


## Chapter 8
`Patterns for Updating and Changing Servers` 需要重点关注。
An effective change management process ensures that any new change is rolled out to all relevant existing servers and applied to newly created servers.

`Continuous Configuration Synchronization`, for example, google gcloud resource configuration has a central of truth repo(主要是针对API, role权限), the configuration process sync every one hour or so to elminiate config drift.

Any areas not explicitly managed by configuration definitions may be changed outside the tooling, which leaves them vulnerable to configuration drift.

`Immutable Servers`, the practice is normally combined with keeping the lifespan of servers `short`, as with the Phoenix. So servers are rebuilt as frequently as every day, leaving little opportunity for unmanaged changes. Another approach to this issue is to set those parts of a server’s filesystems that should not change at runtime as read-only.

Using the term “immutable” to describe this pattern can be misleading. “Immutable” means that a thing can’t be changed, so a truly immutable server would be useless. As soon as a server boots, its runtime state changes—processes run, entries are written to logfiles, and application data is added, updated, and removed.It’s more useful to think of the term “immutable” as applying to the server’s configu‐ ration, rather than to the server as a whole. 

Depending on the design of the configuration tool, a pull-based system may be more scalable than a push-based system. A push system needs the master to open connections to the systems it manages, which can become a bottleneck with infrastructures that scale to thousands of servers. Setting up clusters or pools of agents can help a push model scale. But a pull model can be designed to scale with fewer resources, and with less complexity.


## Chapter 9
`Patterns for Defining Infrastructure`
This chapter will look at how to provision and configure larger groups of infrastructure elements.

`Stack`: A stack is a collection of infrastructure elements that are defined as a unit.

Use parameterized environment definitions, for example terraform brings up a stack with a single definition file for different environments.

提到了Consul configuration registry, 里面存储了不同stack的资源，比如run time IP address，可以供stack之间相互引用，这样decouple了stack，于是可以各自为政.
```ini
# AWS, get vip_ip from consul
resource "consul_keys" "app_server" { 
  key {
  name = "vip_ip"
  path = "myapp/${var.environment}/appserver/vip_ip" 
  }
}
```

It’s better to ensure that infrastructure is provisioned and updated by running tools from centrally managed systems, such as an `orchestration agent`. An orchestration agent is a server that is used to execute tools for provisioning and updating infrastructure. These are often controlled by a CI or CD server, as part of a change management pipeline. 处于安全，一致性，依赖的原因，确实应该如此.


# Part III Practice
## Chapter 10
`Software Engineering Practices for Infrastructure`
Assume everything you deliver will need to change as the system evolves.

The true measure of the quality of a system, and its code, is how quickly and safely changes are made to it.

gitlab-ci上组里就是这么做的:
Although a CI tool can be used to run tests automatically on commits made to each separate branch, the integrated changes are only tested together when the branches are merged. Some teams find that this works well for them, generally by keeping branches very short-lived.
这里总结得很好，commit changes to short-lived branch and then merge to trunk, do both CI on before and after the merge for branch and trunk.

这个CI/CD也解释得很好:
`CI Continuous integration` addresses work done on a single codebase. `CD Continuous delivery` expands the scope of this continuous integration to the entire system, with all of its components.

The idea behinds CD is to ensure that all of the deployable components, systems, and infrastructure are continuously validated to ensure that they are production ready. It is used to address the problems of the "integration phase."

One misconception about CD is that it means every change committed is applied to production immediately after passing automated tests. The point of CD is not to apply every change to production immediately, but to ensure that every change is ready to go to production.

`Code Quality`
The key to a well-engineered system is simplicity. Build only what you need, then it becomes easier to make sure what you have built is correct. Reorganize code when doing so clearly adds value.

`Technical debt` is a metaphor for problems in a system that have been left unfixed. 最好不要积累technical debts，发现的时候就去修复.

An optional feature that is no longer used, or whose development has been stopped, is technical debt. It should be pruned ruthlessly. Even if you decide later on that you need that code, it should be in the history of the VCS. If, in the future, you want to go back and dust it off, you’ve got it in the history in version control.


## Chapter 11
`Testing Infrastructure Changes`，需要关注.
The pyramid puts tests with a broader scope toward the top, and those with a narrow scope at the bottom. The lower tiers validate smaller, individual things such as defini‐ tion files and scripts. The middle tiers test some of the lower-level elements together —for example, by creating a running server. The highest tiers test working systems together—for example, a service with multiple servers and their surrounding infrastructure.

There are more tests at the lower levels of the pyramid and fewer at the top. Because the lower-level tests are smaller and more focused, they run very quickly. The higher- level tests tend to be more involved, taking longer to set up and then run, so they run slower.

In order for CI and CD to be practical, the full test suite should run every time someone commits a change. The committer should be able to see the results of the test for their individual change in a matter of minutes. Slow test suites make this difficult to do, which often leads teams to decide to run the test suite periodically—every few hours, or even nightly.

If running tests on every commit is too slow to be practical, the sol‐ ution is not to run the tests less often, but instead to fix the situa‐ tion so the test suite runs more quickly. This usually involves re- balancing the test suite, reducing the number of long-running tests and increasing the coverage of tests at the lower levels.

This in turn may require rearchitecting the system being tested to be more modular and loosely coupled, so that individual compo‐ nents can be tested more quickly.

其实test cases也不太好决定, 还是要根据实际需求去选择测试什么部分，经常变动或容易broken的组件，及时的更新。
Practice:
- Test at the Lowest Level Possible
- Only Implement the Layers You Need
- Prune the Test Suite Often
- Continuously Review Testing Effectiveness

Whenever there is a major issue in production or even in testing, consider running a
[`blameless post-mortem`](https://codeascraft.com/2012/05/22/blameless-postmortems/). 谷歌内部也提倡这个习惯。

`Low-level testing`
对于ansible playbook, packer json file 之类的文件检查，有几个步骤:
- syntax check, ansible and others 自带有parser
- static code analysis: linting, Static analysis can be used to check for common errors and bad habits which, while syntactically correct, can lead to bugs, security holes, performance issues, or just code that is difficult to understand.
- unit testing, ansible has dedicate module for this, also puppet and chef.

`Mid-level testing`
For example, starts building template via Packer and Ansible, the validation process would be to create a server instance using the new template, and then run some tests against it.

Tools to test server configuration: [Serverspec](https://serverspec.org/), 目前对于packer instance 都是自己去检查的, for example:
```js
describe service('login_service') do
  it { should be_running }
end

describe host('dbserver') do
  it { should be_reachable.with( :port => 5432 ) }
end

// 
describe 'install and configure web server' do
  let(:chef_run) { ChefSpec::SoloRunner.converge(nginx_configuration_recipe) }

  it 'installs nginx' do
    expect(chef_run).to install_package('nginx')
  end 
end

describe 'home page is working' do 
  let(:chef_run) {
    ChefSpec::SoloRunner.converge(nginx_configuration_recipe,
                                  home_page_deployment_recipe)
  }

  it 'loads correctly' do
    response = Net::HTTP.new('localhost',80).get('/') 
    expect(response.body).to include('Welcome to the home page')
  end 
end
```

Automatically tests that remotely logging into a server can be challenging to implement securely. These tests either need a hardcoded password, or else an SSH key or similar mechanism that authorizes unattended logins.

One approach to mitigate this is to have tests execute on the test server and push their results to a central server. This could be combined with monitoring, so that servers can self-test and trigger an alert if they fail.

Another approach is to generate one-off authentication credentials when launching a server to test.

`High-level testing`
The higher levels of the test suite involve testing that multiple elements of the infra‐ structure work correctly when integrated together.

`Testing Operational Quality` 这部分也很重要，但是应该在QA的范围.
People managing projects to develop and deploy software have a bucket of requirements they call `non-functional requirements`, or `NFRs`; these are also sometimes referred to as cross-functional requirements (`CFRs`). Performance, availability, and security tend to be swept into this bucket.

Operational testing can take place at multiple tiers of the testing pyramid, although the results at the top tiers are the most important. 

`关于testing and monitoring 的关系:`
Testing is aimed at detecting problems when making changes, before they are applied to production systems. Monitoring is aimed at detecting problems in running systems.

In order to effectively test a component, it must be isolated from any dependencies during the test. A solution to this is to use a `stub server` instead of the application server. It’s important for the stub server to be simple to maintain and use. It only needs to return responses specific to the tests you write.

Mocks, fakes, and stubs are all types of `test doubles`. A test double replaces a dependency needed by a component or service being tested, to simplify testing.

`QA` tester means: quality analyst/assurance.

`story`: a small piece of work (Jira 中的分类), 可能是这个意思.


## Chapter 12
`Change Management Pipelines for Infrastructure`
This chapter explains how to implement continuous delivery for infrastructure by building a change management pipeline. 讲了如何设计，集成，测试CD pipeline.

A change management pipeline could be described as the automated manifestation of your infrastructure change management process. 就理解成CD pipeline.

Guidelines for Designing Pipelines:
- Ensure Consistency Across Stages, e.g:  server operating system versions and configuration should be the same across environments. Make sure that the essential characteristics are the same.
- Get Immediate Feedback for Every Change
- Run Automated Stages Before Manual Stages
- Get Production-Like Sooner Rather Than Later

My colleague Chris Bird described this as `DevOops`; the ability to automatically configure many machines at once gives us the ability to automatically break many machines at once. 也就是说利害是hand by hand的。

这里有recap了一下一个CI/CD的流程:
1. local development stage, make code and test on local virtualization, then commit to VSC.
2. build stage, syntax checking, unit tests, test doubles, publish reports, packaging and upload code/template image, etc.

如果不是用的immutable server的模式，则你会需要一个configuration master (chef server, puppet master or `ansibel tower`)去配置环境，所以在CI pipeline的最后，会打包上传一个configuration artifact 供这些config master 使用去配置running server，或者是masterless configuration, running server 会自动从一个file server 下载.

如果使用的是immutable server模式，则内容都在image template中配置好了，比如使用packer，不在需要configuration master or masterless.

3. automated test stage, refer to test pyramid
4. manual validation stage
5. apply to live, any significant risk or uncertainty at this stage should be modeled and addressed in upstream stages.

还要注意的是，并不是每个commit 都会走所有的流程，可能commit 1/2/3 走到一个stage，然后合起来进入下一个stage, the earlier stages of the pipeline will run more often than the later stages. not every change, even ones that pass testing and demo, are necessarily deployed immediately to production.

`Pipeline for complex system`:
fan-in pattern: The fan-in pattern is a common one, useful for building a system that is composed of multiple components. Each component starts out with its own pipeline to build and test it in isolation. Then the component pipelines are joined so that the components are tested together. A system with multiple layers of components may have multiple joins. 这个流程图就如同fan-in的扇形.

`Contract tests` are automated tests that check whether a provider interface behaves as consumers expect. This is a much smaller set of tests than full functional tests, purely focused on the API that the service has committed to provide to its consumers.


## Chapter 13
`Workflow for the Infrastructure Team` 这章描述用语很好.
An infrastructure engineer can no longer just log onto a server to make a change. Instead, they make changes to the tools and definitions, and then allow the change management pipeline to roll the changes out to the server.

A `sandbox` is an environment where a team member can try out changes before com‐ mitting them into the pipeline. It may be run on a local workstation, using virtualiza‐ tion, or could be run on the virtualization platform.

`Autonomic Automation Workflow`
Using local sandbox for testing:
A `sandbox` is an environment where a team member can try out changes before com‐ mitting them into the pipeline. It may be run on a local workstation, using virtualiza‐ tion, or could be run on the virtualization platform.

Keeping the whole change/commit cycle short needs some habits around how to structure the changes so they don’t break production even when the whole task isn’t finished. Feature toggles and similar techniques mentioned in Chapter 12 can help.

## Chapter 14
`Continuity with Dynamic Infrastructure`
This chapter is concerned with the operational quality of production infrastructure.
Many IT service providers use availability as a key performance metric or `SLA`(service level agreement). This is a percentage, often expressed as a number of nines: “five nines availability” means that the system is available 99.999% of the time.

`Service continuity`
Keeping services available to end users in the face of problems and changes

A pitfall of using `dynamic pools` to automatically replace failed servers is that it can mask a problem. If an application has a bug that causes it to crash frequently, it may take a while for people to notice. So it is important to implement metrics and alerting on the pool’s activity. The team should be sent critical alerts when the frequency of server failures exceeds a threshold.

Software that has been designed and implemented with the assumption that servers and other infrastructure elements are routinely added and removed is sometimes referred to as `cloud native`. Cloud-native software handles constantly changing and shifting infrastructure seamlessly.

The team at Heroku published a list of guidelines for applications to work well in the context of a dynamic infrastructure, called the [12-factor application](https://12factor.net/).

Some characteristics of `non-cloud-native` software that require lift and shift migrations:
- Stateful sessions
- Storing data on the local filesystem
- Slow-running startup routines
- Static configuration of infrastructure parameters

`Zero-Downtime Changes`
Many changes require taking elements of the infrastructure offline, or completely replacing them. Examples include upgrading an OS kernel, reconfiguring a network, or deploying a new version of application software. However, it’s often possible to carry out these changes without interrupting service.
- Blue-Green Replacement
- Phoenix Replacement
- Canary Replacement
- [dark launching](http://agiletesting.blogspot.com/2009/07/dark-launching-and-other-lessons-from.html)

Routing Traffic for Zero-Downtime Replacements. Zero-downtime change patterns involve fine-grained control to switch usage between system components.

Zero-Downtime Changes with Data. The problem comes when the new version of the component involves a change to data formats so that it’s not possible to have both versions share the same data stor‐ age without issues. An effective way to approach data for zero-downtime deployments is to decouple data format changes from software releases.

`Data continuity`
Keeping data available and consistent on infrastructure that isn’t. 
There are many techniques that can be applied to this problem. A few include:
- Replicating data redundantly
- Regenerating data
- Delegating data persistence
- Backing up to persistent storage

`Disaster recovery`
Coping well when the worst happens

Iron-age IT organizations usually optimize for mean time between failures (MTBF), whereas cloud-age organizations optimize for mean time to recover (MTTR).

`Security`
Keeping bad actors at bay
- Reliable Updates as a Defense
- Provenance of Packages
- Automated Hardening

Common vulnerabilities list from [CVE](https://cve.mitre.org/index.html).

`Hardening` refers to configuring a system to make it more secure than it would be out of the box. Typical activities include:
- Configuring security policies (e.g., firewall rules, SSH key use, password policies, sudoers files, etc.).
- Removing all but the most essential user accounts, services, software packages, and so on.
- Auditing user accounts, system settings, and checking installed software against known vulnerabilities.

Frameworks and scripts for hardening system, see [here](https://github.com/dev-sec). It is essential that the members of the team review and understand the changes made by externally created hardening scripts before applying them to their own infrastructure.


## Chapter 15
`Organizing for Infrastructure as Code`
This final chapter takes a look at implementing it from an organizational point of view.

The organizaitional principles that enable this include:
- A continuous approach to the design, implementation, and improvement of services
- Empowering teams to continuously deliver and improve their services
- Ensuring high levels of quality and compliance while delivering rapidly and continuously

A `kanban board` is a powerful tool to make the value stream visible. This is a variation of an agile story wall, set up to mirror the value stream map for work.

A `retrospective` is a session that can be held regularly, or after major events like the completion of a project. Everyone involved in the process gathers together to discuss what is working well, and what is not working well, and then decide on changes that could be made to processes and systems in order to get better outcomes.

`Post-mortems` are typically conducted after an incident or some sort of major prob‐ lem. The goal is to understand the root causes of the issue, and decide on actions to reduce the change of similar issues happening.]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>The Manager&#39;s Path</title>
    <url>/2021/05/19/book-manager-path/</url>
    <content><![CDATA[
2021年的5月10号我开始带实习了。谷歌内部对intern host的要求挺多的，除了参加前期培训，还需要根据timeline设计实习项目的milestone 以及安排其他1-1s等，我和另外一个co-host一起合作指导。由于是第一次带实习，这对我来说确实是一个挑战，特别是在英文环境中，准备趁此机会好好的巩固一下之前学到的softskills, 以及学习新的management 技能。此外，在hosts check in weekly meeting 中我从其他 hosts, co-hosts身上也学到很多东西，主要方面是他们的表达，对intern的指导方式等等。

这本书是在一次connect program 中一个senior 告诉我的。
`<<The Manager's Path: A Guide for Tech Leaders Navigating Growth and Change>>`

# Management 101
What to expect from a manager?
- help navigate difficult situations
- figure out what need to learn
- enable you to have focus, what is important to focus on

1-1s meeting with your manager for:
1. good work relationship. The bedrock of strong teams is human connection, which leads to trust
2. not a status report, plan first with checklist for what to discuss privately
3. feedback, not just performance review, used to write the self-review
4. Behavioral feedback, especailly when new to workspace
5. presentation feedback, review content and suggest changes
6. design document feedback, areas to improve
7. feedback and discuss on `career ladder`, what to focus on to get promoted. Assign `stretch project` to help you grow and learn new things
(自己补充一下)
8. set next short/long term goal, 
9. what I can do to make difference and aligned with expectation
10. prioritize things to focus
11. tasks that have interaction and collaboration with other teammates

when become more senior, the feedback changes somewhat from personal feedback to team/strategy related inputs, you should drive your 1-1s and bring topics to discuss.

Sometimes delivering feedback quickly is more vaulable than waiting for a convenient time to say someting.
Praising in public, reinforces what positive behavior looks like.

Find conference to attend and class to take, book to read, `point you to an expert` in company who can help you learn something. (My responsibility)

promotion packet: set of materials that the committee will review.

Create and build strong network of peers, from schoolmates to office mates/teammates, socialize with all sorts of people. Has a track record of people who have left and to found new companies

Suffer career uncertainty at various stages of your career, need to figure out what you want to go next.


# Mentoring
Mentoring is important for junior team members, for example be mentor of new hires/intern.
Take time to discuss project, sit at the whiteboard, go through the code together.

How to build effective mentoring relationships?

## For Intern
1. welcome email, onboarding, office or remote, meeting people, learning the system.
2. ensure intern does not feel lost and overwhelmed by the volume of new information.
3. prepare yourself for intern project, design milestones, roadmap, tasks, toolkits.
4. walk through the breakdown(milestone or task) with the intern, listen and answer questions, can draw in whiteboard, etc.
(自己补充的)
5. live demo, chart to explain the project big picture.

Listening is one of core skills of a quality manager.

People are not good at saying precisely what they mean in a way that others can exactly understand. Listening goes beyond hearing the words the mentee is saying.

If you don't understand the question mentee asked, repeat the question in a different way and let him correct you.

Mentee is probably nervious, trying to please you, trying hard not to look stupid and may not ask questions even he does not understand,

In the first several weeks, learn the frequency that you need to check in with him to provide right adjustments. once a week regardless.

## For New Hire
Onboarding, build your and new hire's network, relationship goes on for a lot longer.

Unspoken (不言而喻，潜规则) rules are foreign to newcomer, for example:
1. don't take long vacation in business critical weeks
2. approximately how long you are expected to struggle with something before asking for help

`Onboarding document`(we should have this and new hire should edit it up-to-date) should consistently evolve to meet the changes of the workspace itself.

You may be an introvert, or someone who does not find socializing easy, but conscious effort and practice in getting to know new people and helping them succeed will pay off.

## For Technical/Career
Help to be more productive, to get up to speed faster.
Tell mentee what you expect from him, prepared meeting agenda to ask questions.

It's OK to say no to mentoring. (reason can be current workload, vacation plan, other commitments)
Mentee, think about what you want to get out of the relationship and comre prepared to the sessions.

## Good/Bad Manager
Alpha geek (很有能力但自视甚高，人际能力差), make absolutely terrible managers, very harmful to collaboration and deeply undermine.

## Key Takeaways
As a mentor, focus on the three actions:
- be courious and open-minded
  You may find areas you thought understood but cannot explain clearly to new people, see the world through fresh eyes
- listen and speak their language
  mentoring forces you to hone your communication skills
- make connections
  the career ultimately succeeds or fails on the strength of your network.


# Tech Lead
It is a leadership position, even when it's not a complete management position. It is a set of responsibilities, more than just a good engineer: a good communicator, write clear documents, good presentations, explain what is going on to other people, good at prioritizing, push work forward.

From `<<Talking with Tech Leads>>`: A leader, responsible for a development team, who spends at least 30% of their time writing code with the team.

A new major technical skill: project management.

Tech lead trick: the willingness to step away from the code and figure out how to balance your technical commitments with the work the whole team needs.

It is natural for humans to perfer activities they have mastered, learning new thing sometimes feel quite uncomfortable (through trial and error).

一定要做好balance, scheduling，特别是不要无计划地卷入各种会议，否则coding会被持续地打断，无法集中注意力. 或者是提前把coding tasks break down成几个部分，逐个完成。

## Tech Lead 101
Taking a wide view of work and keep project moving. 分别扮演不同的角色:

System architect and Business analyst:
Have a giid sense of the overall architecture of the systems and a solid understanding of how to design complex software. Understand business requirements and translate them into software.

Project planner:
Break work down into rough deliverables, gather input from the experts on your team, start identifying priorities.

Software developer, expert in your code base:
Writing code, communicate challenges (for tech lead, not too much coding)

Spending time outside of code.

How to be a great tech lead:
1. understand the architecture, get a sense for it, visualize it, know where the data lives, how it flows between systems, how it reflects the products it is supporting.
2. be a team player, looking at tricky, boring, annoying ateas of technical need and unstick thoes areas. Stretch yourself.
3. lead technical decisions, what decisions must be made by you and which should be delegated to others.
4. communication skills.


## Project Management
Break down project, complex end goal down into smaller pieces, identify which pieces can be down in parallel and which must be done in sequence.

可能遇到bumps, bugs, unexpected delays and the things that get missed, could not perfectly predicated.

1. break down the work
Using spreadsheet, start with the biggest pieces, breaking down into smaller and then even smaller ones, ask for help from person who knows the parts. Then ordering the work.

2. push through the details and the unknowns

3. run the project and adjust the plan as you go
How far the project has came and how far it is from completion

4. use the insights gained in the planning process to manage requirements change

5. revisit the details as you get close to completion
Decicde where the line for "good enough" is, make launch plan, rollback plan.

仍然需要增强技术能力，达到一定程度后，再学习更多的management skills 以及走上管理岗位.


# Managing People
Figure out your management style.

1. how do you like to be praised, in public or private?
2. what is the preferred methods of communication for serious feedback? In writing so have time to digest or less formal verbal feedback?
3. what are you excited about to work here?
4. how do I know when you are in bad mood or annoyed, Are there any things that I should be aware of?
5. are there any manager behaviors that you know you hate?
6. any clear career goals that I should know about to help you achieve them?
 
Ohter example [First 1-1s questions](https://larahogan.me/blog/first-one-on-one-questions/).

Have onboarding document and get new hire involved to update it, add comments, etc.

Create a 30/60/90 day plan, include basic goals, getting up to speed on the code, committing a bug fix or performing a release. This can help to catch mishire.

Disciss expectations:
1. how often 1:1s and meets?
2. how often to review the work, weekly summary via email/chat?
3. how long he should work along trying to solve a problem?
4. what point he should ask for help?



























]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>Python for Devops</title>
    <url>/2019/10/01/book-python-for-devops/</url>
    <content><![CDATA[Good description for daily use:

One time I was in the ocean and a wave crashed on top of me and took my breath away as it pulled me deeper into the ocean. Just as I started to recover my breath, another wave dropped on top of me and extracted much of my remaining energy and pulled me even deeper into the ocean. Just as I started to recover, yet another wave crashed down on top of me. The more I would fight the waves and the ocean, the more energy I drained. I seriously wondered if I would die at that moment. I couldn’t breath, my body ached and I was terrified I was going to drown.

Being close to death helped me focus on the only thing that could save me, which was conserving my energy and using the waves not fighting them.

# Install and Configure
Usually python2 is pre-installed, need to install `python3`, can refer this blog
[Install Python 3.7 on centos 7](https://www.osradar.com/install-python-3-7-on-centos-7-and-fedora-27-28/)
This installation will not disturb original `python2` pre-installed (it is the dependency of some other packages).

[install pip3](https://pip.pypa.io/en/stable/installing/)
```bash
curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
## install pip3
python3 get-pip.py
## or install pip
python get-pip.py
```
then you can use `pip3` to install other packages, otherwise `pip` is still use `python2`.

Make [python command execute python3](https://askubuntu.com/questions/320996/how-to-make-python-program-command-execute-python-3), you can use alias.

[Installing packages using pip and virtual environments](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)


# Chapter 1 Introduction
Install `ipython` or `ipython3`, powerful mixed interactive shell:
https://ipython.org/index.html#
```bash
pip install ipython
## or
pip3 install ipython
```
then run it as
```bash
ipython
## or
ipython3
```

Python variables use dynamic typing. In practice, this means reassigned to values of different types or classes

## Built-in Functions
- print
- range
Use spaces instead of tabs to indent.

## Functions
functions can be object, can put it in the list.
```py
>>> functions = [double, triple]
>>> for function in functions:
...     print(function(3))
```

Wrapping Functions with Decorators, there are some other python command line build tools. [`click` package](https://click.palletsprojects.com):
```
pip install click
```

```py
'''A example of using the click package to develop a command line tool'''

import click

@click.command()
@click.argument('name')
def hello(name):
    '''Say hello to name'''    
    print(f"Hello {name}")

if __name__ == '__main__':
    hello()
```

call it:
```bash
$ python simple_cli.py Sue
Hello Sue
```

lambda function, just like `java` use lambda to create comparator for priority queue.
```py
sorted(items, key=lambda item: item[1])
```

## RE package
The `re` module uses `\` to delineate special character for matching, for example `\.`, `\n`, etc. To avoid confusion with regular string escape sequences, `raw` strings are recommended in defining regular expressions. Raw strings are prepended with a `r` before the first quotation mark.

Similar as `grep`:
```py
import re

re.search(r'Rostam', cc_list)
re.search(r'Chr[a-z][a-z]', cc_list)
re.search(r'[A-Za-z]{6}', cc_list)
re.search(r'[A-Za-z]+@[a-z]+\.[a-z]+', cc_list)
```

## Lazy Evaluation
This will have footprint in memory, generate values as needed, will not take much memory.

Create generator, using `()` instead of `[]`(list comprehension)
```py
gen_o_nums = (x for x in range(100))
```

## More IPYTHON features
Using IPython To Run Unix Shell Commands, add `!` before command, but sometime does not need (default setting):
```bash
In [2]: !ls -l
```
can assign to a variable:
```bash
In [6]: res = !df -h | head -n 7

## list format
In [7]: res
Out[7]:
['Filesystem                 Size  Used Avail Use% Mounted on',
 '/dev/mapper/rhel-root      241G   73G  169G  31% /',
 'devtmpfs                   3.9G     0  3.9G   0% /dev',
 'tmpfs                      3.9G     0  3.9G   0% /dev/shm',
 'tmpfs                      3.9G  403M  3.5G  11% /run',
 'tmpfs                      3.9G     0  3.9G   0% /sys/fs/cgroup',
 '/dev/vda1                 1014M  208M  807M  21% /boot']

In [8]: res.grep("dev")
Out[8]:
['/dev/mapper/rhel-root      241G   73G  169G  31% /',
 'devtmpfs                   3.9G     0  3.9G   0% /dev',
 'tmpfs                      3.9G     0  3.9G   0% /dev/shm',
 '/dev/vda1                 1014M  208M  807M  21% /boot']
```

magic commands:
```bash
## enter bash 
In [13]: %%bash
## write into a file
In [14]: %%writefile test.sh
```

[Make IPython import shell alias](https://www.quora.com/How-can-I-make-IPython-import-the-aliases-defined-in-my-bash-profile-upon-startup)
```py
#!/usr/bin/env python
import re
import os.path
c = get_config()
with open(os.path.expanduser('~/.bashrc')) as bashrc:
    for line in bashrc:
        if not line.startswith('alias'):
            continue
        parts = re.match(r'^alias (\w+)=([\'"]?)(.+)\2$', line.strip())
        if not parts:
            continue
        source, _, target = parts.groups()
        c.AliasManager.user_aliases.append((source, target))
```
Drop this code in `~/.ipython/profile_default/ipython_config.py`, just like `.bashrc` and `.vimrc`, the configuration file for ipython.

How to import shell functions in `.bashrc`? Or wirte python function instead.


# Chapter 2 Automating Text and Files
In the DevOps world, you are continually parsing, searching, and changing the text in files, whether it’s searching application logs or propagating configuration files.

read regular file:
```py
## don't need to close explicitly
with open("/root/DS/tmp.txt", "r") as handler:
  data = handler.read()

## one char
data[0]
## file size
len(data)
```
or use
```py
## this will parse lines by `\n`
data = readlines()
## i-th line
data[i]
```

Different operating systems use different escaped characters to represent line-endings. Unix systems use `\n` and Windows systems use `\r\n`. Python converts these to `\n` when you open a file as text. If you are opening a binary file, such as a jpeg image, you are likely to corrupt the data by this conversion if you open it as text. You can, however, read binary files by appending a `b` to mode:
```py
file_path = 'bookofdreamsghos00lang.pdf'
with open(file_path, 'rb') as open_file:
  btext = open_file.read()
```

write file:
```py
content='''export a=123
export b=456
'''
with open("/root/DS/.envrc", "w") as handler:
  handler.write(content)
```
The `open` function creates the file if it does not already exist and overwrites if it does. if want to append, use `a` mode instead of `w`. For binary file use `bw` or `ba`.

## JSON
```py
import json
with open('xx.json', 'r') as handler:
  data = json.load(handler)

json.load() is used to load file
## Deserialize fp (a .read()-supporting text file or binary file containing a JSON document) to a Python object using this conversion table.

json.loads() is used to load other object
##Deserialize s (a str, bytes or bytearray instance containing a JSON document) to a Python object using this conversion table.

pprint
Pretty printing has been turned ON
## then print data is good
## or
print(json.dumps(data, indent=2))

## update
data["workerNodeHosts"][0]["name"] = "localhost"
## write file
with open('xx.json', 'w') as handler:
  json.dump(data, handler, indent=2)

## the same as load() and loads()
json.dump() is for file
json.dumps() is for general object
```

actually you can use data pretty printer:
```py
import pprint

pprint.pprint(data)
```

## YAML
The most commonly used library for parsing YAML files in Python is `PyYAML`. It is not in the Python Standard Library, but you can install it using pip:
```py
pip install pyyaml

import yaml
## read
with open("xx.yml", "r") as handler: 
  data = yaml.safe_load(handler) 

## python convert data as a dict, so you can edit it

print(yaml.dump(data, indent=2))
## write
with open("xx.yml", "w") as handler:
  yaml.dump(data, handler, indent=2)
```

## XML
Historically, many web systems used XML to transport data. One use is for `RSS` feeds. `RSS (Really Simple Syndication)` feeds are used to track and notify users of updates to websites. These feeds have been used to track the publication of articles from various sources. RSS uses XML formatted pages. Python offers the xml library for dealing with XML documents. It maps the XML documents hierarchical structure to a tree-like data structure.
```py
import xml.etree.ElementTree as ET
tree = ET.parse('/tmp/test.xml')
root = tree.getroot()
for child in root:
  print(child.tag, child.attrib)
```

## CSV
data stored as comma-separated values.
```py
In [16]: import csv
In [17]: file_path = '/tmp/user.csv'

In [18]: with open(file_path, newline='') as handler:
    ...:     off_reader = csv.reader(handler, delimiter=',')
    ...:     for _ in range(5):
    ...:         print(next(off_reader))
    ..
```

`pandas` packages is mainstay to do data science work.
Pandas has many more methods for analyzing and manipulating table like data, and there are many books on its use. You should be aware that it is available if you need to do data analysis.

## Search Text
One widely used format is the `Common Log Format (CLF)`. A variety of log analysis tools can understand this format:
```
<IP Address> <Client Id> <User Id> <Time> <Request> <Status> <Size>
127.0.0.1 - swills [13/Nov/2019:14:43:30 -0800] "GET /assets/234 HTTP
```

Just give some examples:
```py
line = '127.0.0.1 - swills [13/Nov/2019:14:43:30 -0800] "GET /assets/234 HTTP/1.0" 200 2326'
## use name groups
r = r'(?P<IP>\d+\.\d+\.\d+\.\d+) - (?P<User>\w+) \[(?P<Time>\d\d/\w{3}/\d{4}:\d{2}:\d{2}:\d{2} [-+]\d{4})\] (?P<Request>".+")'
m = re.search(r, line)

In [11]: m.group('IP')
Out[11]: '129.0.0.1'

In [12]: m.group('Time')
Out[12]: '13/Nov/2019:14:43:30 -0800'

In [13]: m.group('User')
Out[13]: 'swills'

In [14]: m.group('Request')
Out[14]: '"GET /assets/234 HTTP/1.0"'
```

> Note: Python automatically allocates and frees memory.The Python garbage collector can be controlled using the `gc` package, though this is rarely needed.

For large files. If the files contain data that can be processed one line at a time, the task is easy with Python. You can read one line at a time, process the line, and then move to the next. The lines are removed from memory automatically by Python’s garbage collector, freeing up memory.

```py
In [23]: with open('big-data.txt', 'r') as source_file:
    ...:     with open('big-data-corrected.txt', 'w') as target_file:
    ...:         for line in source_file:
    ...:             target_file.write(line)
```

# Chapter 3 Command Line
Python offers tools for interacting with systems and shells. You should become familiar with the `sys`, `os`, and `subprocess` modules, as are all essential tools.


```py
import sys

## little or big endian
sys.byteorder
## python object size
sys.getsizeof(1)
## platform
sys.platform
## python version
sys.version_info.major
sys.version_info.minor
```

The most common usage of the os module is to get settings from environment variables.
```py
import os

## pwd and cd
os.getcwd()
os.chdir('/tmp')
## get and set env var
os.environ.get('HOME')
os.environ['HOME'] = '/tmp'
## login user
os.getlogin()
```

With `subprocess` you can run your favorite shell command or other command line software and collect its output from within Python. For the majority of use-cases, you should use the `subprocess.run` function to spawn processes
```py
import subprocess
## text = True: convert to string
sub = subprocess.run(['ls', '-ltr'], capture_output=True, universal_newlines=True [,text=True, stdout=<file>])
sub.stdout
sub.stderr
print(sub.stdout.decode())
## exception will be raised when use
sub = subprocess.run(['ls', '/non'], capture_output=True, universal_newlines=True, check=True)
```

## Creating Command Line Tools
Invoke python script usually by:
```
python xx.py
```
or you can eliminate python by adding `#!/usr/bin/env python`(or `python3`) at first line of the script, then `chmod` the script to executable:
```
./xx.py
```

The simplest and most basic way to process arguments from the command line is to use the argv attribute of the `sys module`:
```py
#!/usr/bin/env python
"""
Simple command line tool using sys.argv
"""
import sys

if __name__ == '__main__':
  ## sys.argv is a list
  sys.argv[0]

  if '--help' in sys.argv:
    help_message = f"Usage: {sys.argv[0]} ..."
    print(help_message)
    sys.exit()
## can get index
idx = sys.argv.index('--namespace')
namespace = sys.argv[idx]
```
This is not enough, we need argument parser! Luckily there are modules and packages designed for the creation of command line tools. These packages provide frameworks to design the user interface for your module when running in a shell. Three popular solutions `argparse`, `click`, and `fire`. All three include ways to design required arguments, optional flags, and means to display help documentation. The first, argparse, is part of the Python standard library, and the other two are third-party packages that need to be installed separately (using pip).

### argparse
这个有专门的tutorial，大概看了一下，it does take much work on your part but you get lots of control.

Automatically generates help and usage messages and issues errors when users give the program invalid arguments.
```py
#!/usr/bin/env python
"""
Command line tool using argparse
"""
import argparse

parser = argparse.ArgumentParser(description='Process some integers.')
parser.add_argument('integers', metavar='N', type=int, nargs='+',
                    help='an integer for the accumulator')

## If the name begins with a dash, it is treated as an optional, flag, argument, otherwise as a position-dependent command. 
parser.add_argument('--sum', dest='accumulate', action='store_const',
                    const=sum, default=max,
                    help='sum the integers (default: find the max)')

args = parser.parse_args()
print(args.accumulate(args.integers))
```
You can also define sub-commands, like `git stash ...`.

### click
It uses Python `Function Decorators` to bind the command line interface directly with your functions. 

Python decorators are a special syntax for functions **take other functions as arguments**. Python functions are objects, so any function can take a function as an argument. The decorator syntax provides a clean and easy way to do this.

```py
#!/usr/bin/env python
"""
Simple Click example
"""
import click

@click.command()
@click.option('--greeting', default='Hiya', help='How do you want to greet?')
@click.option('--name', default='Tammy', help='Who do you want to greet?')
def greet(greeting, name):
    print(f"{greeting} {name}")


if __name__ == '__main__':
    greet()
```
Please refer to [click documents](https://click.palletsprojects.com/en/7.x/).

### fire
[fire document](https://github.com/google/python-fire).

for example:
```py
#!/usr/bin/env python
"""
Simple Fire example
"""
import fire

def greet(greeting='Hiya', name='Tammy'):
    print(f"{greeting} {name}")
def goodbye(goodbye='Bye', name='Tammy'):
    print(f"{goodbye} {name}")

if __name__ == '__main__':
    fire.Fire()
```

An exciting feature of fire is the ability to enter an interactive mode easily. By using the `--interactive` flag, fire opens an IPython shell with the object and functions of your script available:
```py
./fire_example.py <command> <args> -- --interactive
```

Overall, We recommend `click` for most use cases. It balances ease and control. In the case of complex interfaces where you want to separate the UI code from business logic, `argparse` is the way to go. Moreover, if you need to access code that does not have a command line interface quickly, `fire` is right for you.

## Implementing plugins
Once you’ve implemented your applications command line user interface you might want to consider a plugin system. Plugins are pieces of code supplied by the user of your program to extend functionality. 

A key part of any plugin system is plugin discover. Your program needs to know what plugins are available to load and run. Create a file named `add_plugins.py`
```py
#!/usr/bin/env python
import fire
import pkgutil
import importlib

def find_and_run_plugins(plugin_prefix):
    plugins = {}

    # Discover and Load Plugins
    print(f"Discovering plugins with prefix: {plugin_prefix}")
    # pkgutil.iter_modules returns all modules available in the current sys.path
    for _, name, _ in  pkgutil.iter_modules():
        # Check if the module uses our plugin prefix
        if name.startswith(plugin_prefix):
            # Use importlib to load the module, saving it in a dict for later use.
            module = importlib.import_module(name)
            plugins[name] = module

    # Run Plugins
    for name, module in plugins.items():
        print(f"Running plugin {name}")
        # Call the run method on the plugin.
        module.run()

if __name__ == '__main__':
    fire.Fire()
```
then you can wirte modules for example: `module1.py` and put it in `sys.path` directory. If you run `./add_plugins.py find_and_run_plugins module`, then it will search, load and run the `module1.py` module.

## Turbocharging Python with Command Line Tools
Here are the raw ingredients that will be used to make several solutions:
- Click Framework
- Python CUDA Framework
- Numba Framework
- Scikit-learn Machine Learning Framework

These are tools to speed up the performance.

# Chapter 4 Useful Linux Utilities
This chapter will go through some common patterns in the shell and will include some useful Python commands that should enhance the ability to interact with a machine.

As a seasoned performance engineer once said, it depends on what is measured and how.

## disk utility
If we had to work in an isolated environment with a server that doesn’t have access to the internet or that we don’t control and therefore can’t install packages, we would have to say that the `dd` tool can provide help.

This will get thoughput of the new device, for example, the throughput is 1.4GB/s
```
dd if=/dev/zero of=<new device> count=10 bs=100M

10+0 records in
10+0 records out
10506731520 bytes (11 GB) copied, 3.12099 s, 1.4 GB/s
```
how to get IOPS, update every 1 second:
```
iostat -d <device> 1
```

Other common test tool is `fio`, you may need to install this package. It can help clarify the performance behavior of a device in a read-heavy or write-heavy environment (and even adjust the percentages of reads vs. writes).


## network utility
- ssh tunneling (ssh port forwarding)

For example, the server hello.com can only access by ssh with port 3345 and not exposed,let's forward hello.com:3345 to a local port in my machine:
https://www.youtube.com/watch?v=AtuAdk4MwWw
```
ssh -f -L 12333:hello.com:3345 root@hello.com -N
```
`-f` means run in bachground
`-L` is forwarding rule
`-N` means don't get into remote shell
`root@hello.com` is the username and address of that server

This tech can also be used to bypass firewall for some ports. Then we can access localhost:12333 to access the server.
疑问：如果port已经被firewall block了，ssh是怎么连接上的呢？





















































]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>book</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Operators</title>
    <url>/2020/06/03/book-k8s-operator/</url>
    <content><![CDATA[
这段时间开始研究Operator了，刚好有这本书，计划快速过一遍，recap quick start. 看完了一遍，最深得感受就是，如果K8s是云的操作系统，那么Operator就是一个云应用程序自己的管理工具，也就是书中说的application SRE。

Book accompanying git repo:
https://github.com/chengdol/chapters/ (this is forked from origin)
还推荐几本书, from O'reilly:
- Programming Kubernetes (dive deeper into API)
- Extending Kubernetes

My K8s operator-sdk demo git repo, step by step guide you setup a go-based operator and deploy in K8s cluster:
https://github.com/chengdol/k8s-operator-sdk-demo

其他一些资料收集在了这篇blog中:
[Kubernetes Operator Learning](https://chengdol.github.io/2020/01/23/k8s-operator/)

# Chapter 1 Introduction
Operators grew out of work at CoreOS during 2015 and 2016. User experience with the Operators built there and continuing at Red Hat.

An Operator continues to monitor its application as it runs, and can back up data, recover from failures, and upgrade the application over time, automatically.

An Operator is a custom Kubernetes controller watching a CR type and taking application-specific actions to make reality match the spec in that resource.

Making an Operator means creating a CRD and providing a program that runs in a loop watching CRs of that kind. 

The Operator pattern arose in response to infrastructure engineers and developers wanting to extend Kubernetes to provide features **specific** to their sites and software.

看到这里产生了一个疑问: Helm and Operator.

- [Kubernetes Operators and Helm — It takes Two to Tango](https://medium.com/@cloudark/kubernetes-operators-and-helm-it-takes-two-to-tango-3ff6dcf65619)
- [Make a Kubernetes Operator in 15 minutes with Helm](https://www.openshift.com/blog/make-a-kubernetes-operator-in-15-minutes-with-helm)


# Chapter 2 Running Operators
这是最基本的operator 演示，一个etcd cluster，有很大的启发价值，注意下面例子中各自的创建顺序:
https://github.com/kubernetes-operators-book/chapters/tree/master/ch03

First need cluster-wise privilege:
```bash
## need cluster wide privilege
kubectl describe clusterrole cluster-admin

## good
Name:         cluster-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  *.*        []                 []              [*]
             [*]                []              [*]
```

Start with `etcd` as 'hello world' example. Deviation:
- Raft protocol: https://raft.github.io/

you’ll deploy the etcd Operator, then have it create an etcd cluster according to your specifications. You will have the Operator recover from failures and perform a version upgrade while the etcd API continues to service read and write requests, showing how an Operator automates the lifecycle of a piece of foundation software.

A `CRD` is akin to a schema for a `CR`, defining the `CR`’s fields and the types of values those fields contain:
```yaml
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: etcdclusters.etcd.database.coreos.com
spec:
  group: etcd.database.coreos.com
  names:
    kind: EtcdCluster
    listKind: EtcdClusterList
    plural: etcdclusters
    shortNames:
    - etcdclus
    - etcd
    singular: etcdcluster
  scope: Namespaced
  version: v1beta2
  versions:
  - name: v1beta2
    served: true
    storage: true
```

The `CR`’s group, version, and kind together form the fully qualified name of a Kubernetes resource type. That canonical name must be unique across a cluster.

Defining an Operator Service Account:
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: etcd-operator-sa
```

Defining role:
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: etcd-operator-role
rules:
- apiGroups:
  - etcd.database.coreos.com
  resources:
  - etcdclusters
  - etcdbackups
  - etcdrestores
  verbs:
  - '*'
- apiGroups:
  - ""
  resources:
  - pods
  - services
  - endpoints
  - persistentvolumeclaims
  - events
  verbs:
  - '*'
- apiGroups:
  - apps
  resources:
  - deployments
  verbs:
  - '*'
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - get
```

Defining rolebinding, assigns the role to the service account for the etcd Operator:
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: etcd-operator-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: etcd-operator-role
subjects:
- kind: ServiceAccount
  name: etcd-operator-sa
  namespace: default
```

The Operator is a custom controller running in a pod, and it watches the EtcdCluster CR you defined earlier.
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: etcd-operator
spec:
  selector:
    matchLabels:
      app: etcd-operator
  replicas: 1
  template:
    metadata:
      labels:
        app: etcd-operator
    spec:
      containers:
      - name: etcd-operator
        image: quay.io/coreos/etcd-operator:v0.9.4
        command:
        - etcd-operator
        - --create-crd=false
        env:
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        imagePullPolicy: IfNotPresent
      serviceAccountName: etcd-operator-sa
```

Declaring an etcd cluster:
```yaml
apiVersion: etcd.database.coreos.com/v1beta2
kind: EtcdCluster
metadata:
  name: example-etcd-cluster
spec:
  size: 3
  version: 3.1.10
```
After create `CR` resource, operator will generate 3 replicas pod (the pod definition is written by operator logic).

This example etcd cluster is a first-class citizen, an `EtcdCluster` in your cluster’s API. Since it’s an API resource, you can get the etcd cluster spec and status directly from Kubernetes.
```bash
## etcdcluster is a resource just like pod/deploy/sts
kubectl describe etcdcluster example-etcd-cluster
```
The etcd Operator creates a Kubernetes service in the etcd cluster’s namespace:
```bash
kubectl get services --selector etcd_cluster=example-etcd-cluster
```

Run the etcd client on the cluster and use it to connect to the client service and interact with the etcd API. 
```bash
kubectl run --rm -i --tty etcdctl --image quay.io/coreos/etcd --restart=Never -- /bin/sh
```
From the etcd container’s shell, create and read a key-value pair in etcd with etcdctl’s put and get verbs:
```bash
export ETCDCTL_API=3
export ETCDCSVC=http://example-etcd-cluster-client:2379
etcdctl --endpoints $ETCDCSVC put foo bar
etcdctl --endpoints $ETCDCSVC get foo

## check etcd cluster general health
etcdctl --endpoints http://example-etcd-cluster-client:2379 cluster-health
```

You can try to delete etcd pod or upgrade the version (edit cr file then apply) and watching the operator recover the health.

kubectl tricks for upgrade:
```bash
kubectl patch etcdcluster example-etcd-cluster --type='json' \
  -p '[{"op": "replace", "path": "/spec/version", "value":3.3.12}]'
```

# Chapter 3 Operators at the Kubernetes Interface
Operators extend two key Kubernetes concepts: `resources` and `controllers`. The Kubernetes API includes a mechanism, the CRD, for defining new resources.

这2段话把一般通用控制器和operator的区别讲清楚了:
> The actions the ReplicaSet controller takes are intentionally general and application agnostic. It does not, should not, and truly cannot know the particulars of startup and shutdown sequences for every application that might run on a Kubernetes cluster.

> An Operator is the application-specific combination of CRs and a custom controller that does know all the details about starting, scaling, recovering, and managing its application.

Every Operator has one or more custom controllers implementing its application-specific management logic.

An Operator, in turn, can be limited to a namespace, or it can maintain its operand across an entire cluster.

For example, cluster-scoped operator:
> Istio operator: https://github.com/istio/operator
> cert-manager: https://github.com/jetstack/cert-manager

A `service account` is a special type of cluster user for authorizing programs instead of people. An Operator is a program that uses the Kubernetes API, and most Operators should derive their access rights from a service account. 

# Chapter 4 The Operator Framework
This chapter introduced the three pillars of the Operator Framework: the Operator SDK for building and developing Operators; Operator Lifecycle Manager for distributing, installing, and upgrading them; and Operator Metering for measuring Operator performance and resource consumption.

The `Red Hat Operator Framework` makes it simpler to create and distribute Operators. It makes building Operators easier with a `software development kit (SDK)` that automates much of the repetitive implementation work. The Framework also provides mechanisms for deploying and managing Operators. `Operator Lifecycle Manager (OLM)` is an Operator that installs, manages, and upgrades other Operators. `Operator Metering` is a metrics system that accounts for Operators’ use of cluster resources.

`Operator SDK`: https://github.com/operator-framework/operator-sdk
The SDK currently includes first-class support for constructing Operators in the `Go` programming language, with support for other languages planned. The SDK also offers what might be described as an adapter architecture for `Helm` charts or `Ansible` playbooks. 

`Operator Lifecycle Manager` takes the Operator pattern one level up the stack: it’s an Operator that acquires, deploys, and manages Operators on a Kubernetes cluster.

`Operator Metering` is a system for analyzing the resource usage of the Operators running on Kubernetes clusters.

**Install operator SDK**: https://sdk.operatorframework.io/docs/install-operator-sdk/
注意k8s version是否与当前operator sdk兼容，比如我实验的时候k8s version 1.13.2，它支持的crd api version is `apiextensions.k8s.io/v1beta1`, 而最近的operator sdk生成的crd api version is `apiextensions.k8s.io/v1`. 书中用的operator sdk version `0.11.0`.


# Chapter 5 Sample Application: Visitors Site
In the chapters that follow, we’ll create Operators to deploy this application using each of the approaches provided by the Operator SDK (**Helm, Ansible, and Go**), and explore the benefits and drawbacks of each.

读到这里，疑惑Helm是如何处理这个问题的，特别是对同一个charts之中的依赖:
When deploying applications through manifests, awareness of these relationships is required to ensure that the values line up.

The manifest-based installation for this demo:
https://github.com/kubernetes-operators-book/chapters/tree/master/ch05
Now deploying it manually with correct order:
```bash
kubectl create -f database.yaml
kubectl create -f backend.yaml    
kubectl create -f frontend.yaml
```
Deletion:
```bash
kubectl delete -f database.yaml
kubectl delete -f backend.yaml    
kubectl delete -f frontend.yaml
```

# Chapter 6 Adapter Operators
You would have to create `CRDs` to specify the interface for end users. 
Kubernetes controllers would not only need to be written with the Operator’s domain-specific logic, but also be correctly hooked into a running cluster to receive the proper notifications. Roles and service accounts would need to be created to permit the Operator to function in the capacity it needs. An Operator is run as a pod inside of a cluster, so an `image` would need to be built, along with its accompanying deployment manifest.

这章节主要是利用已有的Helm or Ansibel去构造Adapter Operator:
The Operator SDK provides a solution to both these problems through its `Adapter Operators`. Through the command-line tool, the SDK generates the code necessary to run technologies such as Helm and Ansible in an Operator.

First understand the role of `CRDs`.
- A CRD is the specification of what constitutes a CR. In particular, the CRD defines the allowed configuration values and the expected output that describes the current state of the resource.
- A CRD is created when a new Operator project is generated by the SDK.
- The SDK prompts the user for two pieces of information about the CRD during project creation: `kind`, `api-version`


Official operator SDK sample:
https://github.com/operator-framework/operator-sdk-samples

## Helm Operator
demo git repo to generate helm operator:
https://github.com/kubernetes-operators-book/chapters/tree/master/ch06/visitors-helm

A Helm Operator can deploy each instance of an application with a different version of `values.yaml`. The Operator SDK generates Kubernetes controller code for a Helm Operator when it is passed the `--type=helm` argument.
As a prerequisite, be sure to install the `Helm` command-line tools on your machine. 

### New Chart
Generate a blank helm chart structure within the operator project code:
```bash
OPERATOR_NAME=visitors-helm-operator
operator-sdk new $OPERATOR_NAME --api-version=example.com/v1 --kind=VisitorsApp --type=helm

```
At this point, everything is in place to begin to implement your chart.

There are several direcotyies created:
- build: it contains Dockerfile for operator image
- deploy: crds definition, role and rolebinding, service account
- helm-charts: helm chart structure for your app
- watches.yaml: maps each CR type to the specific Helm chart that is used to handle it. 

### Existing Chart
Helm install command 其实有很多参数可以customize，比如选择values yaml file, 但这里没有这么灵活，用的是默认的values.yaml.

一定要事先检查template validation，比如对于helm3:
```bash
helm template <chart dir or archive file> [--debug] | less
```
查看每个rendering 是否格式正确，helm template对format issue并不会报错。

Generate Helm operator atop existing helm archive, 对于OpenShift，要先`oc login`，否则operator-sdk 不能获得cluster info:
```bash
OPERATOR_NAME=visitors-helm-operator
## download existing chart archive
wget https://github.com/kubernetes-operators-book/chapters/releases/download/1.0.0/visitors-helm.tgz
## generate helm operator
operator-sdk new $OPERATOR_NAME --api-version=example.com/v1 --kind=VisitorsApp --type=helm --helm-chart=./visitors-helm.tgz
```
- `--helm-chart`: A URL to a chart archive, The repository and name of a remote chart, or The location of a local directory
- `--helm-chart-repo`: Specifies a remote repository URL for the chart 
- `--helm-chart-version`: Tells the SDK to fetch a specific version of the chart. If this is omitted, the latest available version is used.

You will see `deploy/crds/example.com_v1_visitorsapp_cr.yaml` has the fields exactly the same as `values.yaml` in helm chart.

Before running the chart, the Operator will map the values found in the custom resource’s spec field to the `values.yaml` file.

如此生成的CRD 和 role (extremely permissive) 可以直接使用，但可能不满足具体要求，比如constraints 以及权限限制，需要自己调整:
- [CRD structure schema](https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#specifying-a-structural-schema)


## Ansible Operator
More or less the same as Helm operator generation. 
Generate blank Ansible operator project:
```bash
OPERATOR_NAME=visitors-ansible-operator
operator-sdk new $OPERATOR_NAME --api-version=example.com/v1 --kind=VisitorsApp --type=ansible
```

## Test Operator
An Operator is delivered as a normal container image. However, during the development and testing cycle, it is often easier to skip the image creation process and simply run the Operator **outside of the cluster**.
这个用在开发测试的时候，它不会部署一个真正的operator deployment，只是一个process，但实验效果和真实的一样。这里只是针对helm and ansible的类型.

```bash
## go to root path of operator project
## set full path in `chart` field to chart
cp watches.yaml local-watches.yaml
kubectl apply -f deploy/crds/*_crd.yaml
## start opeerator process
operator-sdk up local --watches-file ./local-watches.yaml
```
The process is up and running, next is to apply your `cr` yaml:
```bash
kubectl apply -f deploy/crds/*_cr.yaml
kubectl delete -f deploy/crds/*_cr.yaml
```
你会看到log的变化，以及application在k8s cluster中的更新。
Once the test is complete, end the running process by pressing `Ctrl-C`.

During development, repeat this process to test changes. On each iteration, be sure to restart the Operator process to pick up any changes to the Helm or Ansible files

## Deploy Operator
Running an Operator outside of the cluster, is convenient for testing and debugging purposes, but production Operators run as Kubernetes deployments.

1. Build the operator image. The Operator SDK’s `build` command chains to the underlying Docker daemon to build the Operator image, and takes the full image name and version when run:
  ```bash
  operator-sdk build jdob/visitors-operator:0.1
  ```
  You can check the Dockerfile, no additional changes are needed. the `${HOME}` is consistent with the path in `watches.yaml`.
  
  Once built, push the image to an externally accessible repository

2. Configure the deployment. Update the `deploy/operator.yaml` file that the SDK generates with the name of the image. 

3. Deploy CRD

4. Deploy Service account and Role

5. Deploy Operator deployment

# Chapter 7 Operators in Go with the Operator SDK
这一节的code参考，没有把所有logic都写到一个文件下，而是针对不同的resource分开写的，还将公用的部分单独分出来了，很有参考价值, 我总结了一下实现，看这里:
https://github.com/chengdol/k8s-operator-sdk-demo

The Operator SDK provides that flexibility by making it easy for developers to use the `Go` programming language, including its ecosystem of external libraries, in their Operators. Write acutall business logic of operator.

While you can write all these pieces manually, the Operator SDK provides commands that will automate the creation of much of the supporting code, allowing you to focus on implementing the actual `business logic` of the Operator.

We will explore the files that need to be edited with custom application logic and discuss some common practices for Operator development.

## Create Go Based Operator
关于如何command line创建go-based operator书中的表述不清楚，我参考了Red Hat的文档:
- [Building a Go-based Memcached Operator using the Operator SDK](https://docs.openshift.com/container-platform/4.3/operators/operator_sdk/osdk-getting-started.html#building-memcached-operator-using-osdk_osdk-getting-started)

这描述太含糊了:
In particular, the Operator code **must** be located in your `$GOPATH`, 关键是怎么设置`$GOPATH`:
- [Understand the GOPATH](https://www.digitalocean.com/community/tutorials/understanding-the-gopath)
- [Setting up local Go environment](https://www.digitalocean.com/community/tutorials/how-to-install-go-and-set-up-a-local-programming-environment-on-ubuntu-18-04)

如果用`go env | grep GOPATH`，发现已经有默认值了`$HOME/go`，但还需要在bash env中export:
```bash
export GOPATH=$HOME/go
export PATH=$PATH:$GOPATH/bin
#export GO111MODULE=on

OPERATOR_NAME=visitors-operator
## 这个路径和后面的controller import中的路径要一致！
OPERATOR_PATH=$GOPATH/src/github.com/jdob
mkdir -p $OPERATOR_PATH
cd $OPERATOR_PATH
## no --type specified, default is go
operator-sdk new $OPERATOR_NAME
```
针对operator-sdk new出现的错误信息，我export了`GO111MODULE=on`，但后来重做一遍后这个错误又消失了:
- [Why is GO111MODULE everywhere, and everything about Go Modules](https://dev.to/maelvls/why-is-go111module-everywhere-and-everything-about-go-modules-24k)

The generation can take a few minutes as all of the Go dependencies are downloaded. 

## Add CRDs
You can add new CRDs to an Operator using the SDK’s add api command. Run from the Operator project `root` directory to generate CRD: 这应该说明一个Operator可以有多个CRDs.
```bash
cd $OPERATOR_PATH/$OPERATOR_NAME
operator-sdk add api --api-version=example.com/v1 --kind=VisitorsApp
## from command outputs, you will see what files are generated
```

3 files are important:
- `deploy/crds/*cr.yaml`
- `deploy/crds/*crd.yaml`
- `pkg/apis/example/v1/visitorsapp_types.go`: contains a number of struct objects that the Operator codebase leverages

For example, in `pkg/apis/example/v1/visitorsapp_types.go` edit the Spec and Status struct:
```go
// VisitorsAppSpec defines the desired state of VisitorsApp
// +k8s:openapi-gen=true
type VisitorsAppSpec struct {
	// INSERT ADDITIONAL SPEC FIELDS - desired state of cluster
	// Important: Run "operator-sdk generate k8s" to regenerate code after modifying this file
	// Add custom validation using kubebuilder tags: https://book.kubebuilder.io/beyond_basics/generating_crd.html

	Size       int32  `json:"size"`
	Title      string `json:"title"`
}

// VisitorsAppStatus defines the observed state of VisitorsApp
// +k8s:openapi-gen=true
type VisitorsAppStatus struct {
	// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
	// Important: Run "operator-sdk generate k8s" to regenerate code after modifying this file
	// Add custom validation using kubebuilder tags: https://book.kubebuilder.io/beyond_basics/generating_crd.html

	BackendImage  string `json:"backendImage"`
	FrontendImage string `json:"frontendImage"`
}
```

After editing, run
```bash
## After any change to a *_types.go file, you need to update any generated code
operator-sdk generate k8s
```
Then customize `deploy/crds/example_v1_visitorsapp_crd.yaml` file to reflect the struct content, for example:
https://github.com/chengdol/chapters/tree/master/ch07/visitors-operator/deploy/crds

这里并没有特意修改RBAC，用的默认Operator permission:
https://github.com/chengdol/chapters/tree/master/ch07/visitors-operator/deploy

## Write Control Logic
Inside of the Operator pod itself, you need a controller to watch for changes to CRs and react accordingly. Similar to adding a CRD, you use the SDK to generate the controller’s skeleton code.

```bash
## generate controller code skeleton
operator-sdk add controller --api-version=example.com/v1 --kind=VisitorsApp
```
The file `pkg/controller/visitorsapp/visitorsapp_controller.go` will be created, the is the controller file that implements the Operator’s custom logic.

More information on K8s controller:
https://kubernetes.io/docs/concepts/architecture/controller/

主要有2个func需要customize: `add` and `Reconcile`，一个是watch也就是告诉K8s哪些resource需要监控，一个是控制逻辑.
While the bulk of the Operator logic resides in the controller’s `Reconcile` function, the `add` function establishes the watches that will trigger reconcile events:
https://github.com/chengdol/chapters/tree/master/ch07/visitors-operator/pkg/controller/visitorsapp

The first watch listens for changes to the `primary resource` that the controller monitors. 也就是自定义的kind类型。
The second watch, or more accurately, series of watches, listens for changes to any `child resources` the Operator created to support the primary resource. 也就是自定义kind类型中间接的其他resources，比如deployment, sts, service等

**Reconcile function**
The Reconcile function, also known as the `reconcile loop`, is where the Operator’s logic resides:
https://github.com/chengdol/chapters/blob/master/ch07/visitors-operator/pkg/controller/visitorsapp/visitorsapp_controller.go

The Reconcile function returns `two` objects: a ReconcileResult instance and an error. 
有几种可能:
```go
return reconcile.Result{}, nil
return reconcile.Result{}, err
return reconcile.Result{Requeue: true}, nil
return reconcile.Result{RequeueAfter: time.Second*5}, nil
```

Since Go-based Operators make heavy use of the Go Kubernetes libraries, it may be useful to review:
https://pkg.go.dev/k8s.io/api
the `core/v1` and `apps/v1` modules are frequently used to interact with the common Kubernetes resources.

这里提到了update status value，应该对应的是resource yaml中底部的status 信息:
```go
instance.Status.BackendImage = "example"
err := r.client.Status().Update(context.TODO(), instance)
```
如同我在这章开头提到的，作者将不同resource的逻辑分开到不同的go file了，可以仔细观察怎么写的.

**关于Child resource deletion:**
If the child resource’s owner type is correctly set to the primary resource, when the parent is deleted, Kubernetes garbage collection will automatically clean up all of its child resources

It is important to understand that when Kubernetes deletes a resource, it still calls the Reconcile function. 

There are times, however, where specific cleanup logic is required. The approach in such instances is to block the deletion of the primary resource through the use of a `finalizer`. A finalizer is simply a series of strings on a resource, 感觉就是一个mark.

```go
finalizer := "visitors.example.com"

beingDeleted := instance.GetDeletionTimestamp() != nil
if beingDeleted {
    if contains(instance.GetFinalizers(), finalizer) {

        // Perform finalization logic. If this fails, leave the finalizer
        // intact and requeue the reconcile request to attempt the clean
        // up again without allowing Kubernetes to actually delete
        // the resource.

        instance.SetFinalizers(remove(instance.GetFinalizers(), finalizer))
        err := r.client.Update(context.TODO(), instance)
        if err != nil {
            return reconcile.Result{}, err
        }
    }
    return reconcile.Result{}, nil
}
```

## Idempotency
It is critical that Operators are idempotent. Multiple calls to reconcile an unchanged resource must produce the same effect each time.

1. Before creating child resources, check to see if they already exist. Remember, Kubernetes may call the reconcile loop for a variety of reasons beyond when a user first creates a CR. Your controller should not duplicate the CR’s children on each iteration through the loop.

2. Changes to a resource’s spec (in other words, its configuration values) trigger the reconcile loop. Therefore, it is often not enough to simply check for the existence of expected child resources. The Operator also needs to verify that the child resource configuration matches what is defined in the parent resource at the time of reconciliation.

3. Reconciliation is not necessarily called for each change to the resource. It is possible that a single reconciliation may contain multiple changes. The Operator must be careful to ensure the entire state of the CR is represented by all of its child resources.

4. Just because an Operator does not need to make changes during a reconciliation request doesn’t mean it doesn’t need to update the CR’s Status field. Depending on what values are captured in the CR’s status, it may make sense to update these even if the Operator determines it doesn’t need to make any changes to the existing resources.

## Operator Impact
If the Operator incorrectly handles operations, they can negatively affect the performance of the entire cluster.

## Test Operator
> 如果operator test有错误，则image build之后运行也会出现同样的错误！

The process running the Operator may be outside of the cluster, but Kubernetes will treat it as it does any other controller.

Go to the root project directory:
```bash
## deploy CRD
kubectl apply -f deploy/crds/*_crd.yaml
## start operator in local mode
operator-sdk up local --namespace default
## deploy CR
kubectl apply -f deploy/crds/*_cr.yaml
```
The Operator SDK uses credentials from the kubectl configuration file to connect to the cluster and attach the Operator. The running process acts as if it were an Operator pod running inside of the cluster and writes logging information to standard output.

# Chapter 8 Operator Lifecycle Manager
这章节概念性的东西较多，建议多读几遍。
`OLM` git repo:
https://github.com/operator-framework/operator-lifecycle-manager

Once you have written an Operator, it’s time to turn your attention to its installation and management. As there are multiple steps involved in deploying an Operator, a management layer becomes necessary to facilitate the process. 就是管理Operator的东西.

`OLM`’s benefits extend beyond installation into Day 2 operations, including managing upgrades to existing Operators, providing a means to convey Operator stability through version channels, and the ability to aggregate multiple Operator hosting sources into a single interface. OLM在Openshift 上是自带的，K8s上没有。OLM也是通过CRD实现的，在Openshift 中run `oc get crd` 就可以看到相关CRDs.

1. ClusterServiceVersion
You can think of a CSV as analogous to a Linux package, such as a Red Hat Package Manager (RPM) file.

Much like how a deployment describes the “pod template” for the pods it creates, a CSV contains a “deployment template” for the deployment of the Operator pod. 

2. CatalogSource
A CatalogSource contains information for accessing a repository of Operators. OLM provides a utility API named packagemanifests for querying catalog sources, which provides a list of Operators and the catalogs in which they are found. 
```bash
kubectl -n olm get packagemanifests
```

3. Subscription
End users create a subscription to install, and subsequently update, the Operators that OLM provides. A subscription is made to a `channel`, which is a stream of Operator versions, such as "stable" or "nightly."

To continue with the earlier analogy to Linux packages, a subscription is equivalent to a command that installs a package, such as yum install.

4. InstallPlan
A subscription creates an InstallPlan, which describes the full list of resources that OLM will create to satisfy the CSV’s resource requirements.

5. OperatorGroup
An Operator belonging to an OperatorGroup will not react to custom resource changes in a namespace not indicated by the group.

## Installing OLM
version `v0.11.0`, 我用的k8s `v1.13.2`，最近的版本不兼容了
https://github.com/operator-framework/operator-lifecycle-manager/releases
```bash
kubectl apply -f https://github.com/operator-framework/operator-lifecycle-manager/releases/download/0.11.0/crds.yaml
kubectl apply -f https://github.com/operator-framework/operator-lifecycle-manager/releases/download/0.11.0/olm.yaml
```
After applying, The CRDs for OLM are created, the olm pods are up and running in `olm` namespace. OLM可以用于和`OperatorHub.io` 进行交互，如同Helm 和HelmHub, Docker 和DockerHub. 书中用了个例子说明如何部署etcd operator from operatorHub.

后面主要是讲如何publish自己的operator了, 目前用不到。

# Chapter 9 Operator Philosophy
Let’s try to connect those tactics to the strategic ideas that underpin them to understand an existential question: what are Operators for?

An Operator reduces human intervention bugs by automating the regular chores that keep its application running. `Operators: Kubernetes Application Reliability Engineering`

有些启发价值:
You can build Operators that not only run and upgrade an application, but respond to errors or slowing performance.

Control loops in Kubernetes watch resources and react when they don’t match some desired state. Operators let you customize a control loop for resources that represent your application. The first Operator concerns are usually automatic deployment and self-service provisioning of the operand. Beyond that first level of the maturity model, an Operator should know its application’s critical state and how to repair it. The Operator can then be extended to observe key application metrics and act to tune, repair, or report on them.

Site Reliability Engineering lists the `four golden signals` as `latency`, `traffic`, `errors`, and `saturation`.

Highly Successful Operators:
1. An Operator should run as a single Kubernetes deployment.
2. Operators should define new custom resource types on the cluster.
3. Operators should use appropriate Kubernetes abstractions whenever possible.
4. Operator termination should not affect the operand.
5. Operator termination should not affect the operand.
6. Operator termination should not affect the operand.
7. Operators should be thoroughly tested, including chaos testing.

# Appendix 
## Running an Operator as a Deployment Inside a Cluster
Please see my git repo for more details.
```bash
## build operator image
## go to project root directory
operator-sdk build image:tag
```
Then docker push image to docker registry, replace the image placeholder in `operator.yaml` file. Then apply the CR yaml.

书中另外2个appendix 是关于CRD validation and RBAC control的设置。

















]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>book</tag>
        <tag>kubernetes</tag>
        <tag>operator</tag>
      </tags>
  </entry>
  <entry>
    <title>Practical Vim, 2nd Edition</title>
    <url>/2019/08/06/book-vim/</url>
    <content><![CDATA[
看这种书挺枯燥，还是learn by doing 比较好，使用一段时间就很明确自己需要什么功能了。

> By default `oc/kubectl edit` will use `vi` in Linux if environment variable `OC _EDITOR` is empty, if you have `.vimrc` that is not compatible with `vi`, will get error message, to use `vim` set `export OC_EDITOR=vim` or `export KUBE_EDITOR=vim`.

# Other resources
[How I booted my Vim](https://nvie.com/posts/how-i-boosted-my-vim/)
Informative!! I grab lots of configurations from it, go and poke around the author's vimrc file.

[Vim Tutorial Videos](http://derekwyatt.org/vim/tutorials/index.html)
There are a lot, see O`Reilly.

Actually you can learn `Vim` by running command `vimtutor` in your terminal, it is a simple and quick tutorial. For a quick review, just see summary after each lesson.

# Daily Foundation
掌握好这些操作基本上就满足日常需求了。

## Configuration
`.vimrc` is just like `.bashrc`. you create a `.vimrc` (even it's empty) to tell vim use vim not vi-compatiable mode!

关于vimrc的配置，参考my github repo:
<https://github.com/chengdol/vim-configuration>

1. Basic configuration
2. Custom key mapping
3. Vimscript plugin

About mapping:

- nmap: normal mode
- vmap: visual mode
- imap: insert mode
- map: normal, visual and operating-pending mode
- map!: command and insert mode

To list mapping: `:map` or more specific `:vmap`

Which mode is currently use should be clear:  `Normal` mode, `Insert` mode and `Visual` mode, `Command-Line` mode, `Replace` mode, etc. `Visual` mode lets us select text in the `buffer` and then operate on the selection.

If you suspect that your customizations are causing interference, here’s a quick test. Try quitting Vim and then launching it with these options:
```
vim -u NONE -N
```
The `-u NONE` flag tells Vim not to source your `vimrc` on startup. That way, your customizations won’t be applied and plugins will be disabled. In older versions of Vim, not loading a `vimrc` file activates `vi` compatible mode, which causes many useful features to be disabled. The `-N` flag prevents this by setting the `nocompatible` option. Since version 8.0 of Vim, the `nocompatible` option is set by default, making the `-N` flag unnecessary.

`vim -N file`: `-N` not using vi compatible mode, use newer vim mode. or can configuring in `.vimrc` file.

`:set list`, `:set nolist`, `:set list?`, `:set list&`: set, unset, check value, set as default. 以此类推.
```
set autochdir "auto set current dir as working dir
set wildmode=list:longest "activate TAB auto-complete file path
```

**Reload .vimrc**
Reload .vimrc without restarting vim: `:source ~/.vimrc`. 已经加入vimrc了。

## Plugin
Vim 8.0 has its own built-in package manager. For Vim version less than 8.0, I use `vim-plug` as the plugin manager:
https://github.com/junegunn/vim-plug
You can even specify git address of the plugin, for example:
```
Plug 'https://github.com/tfnico/vim-gradle.git'
```

If the plugin does not get auto installed, run `:PlugInstall`, check the plug status by `:PlugStatus`. 这些在`vim-plug` README中都有说明。

> Please see my [vim-configuration git repo](https://github.com/chengdol/vim-configuration), download the vimrc file.

- nerdtree: file system explorer for vim editor
- fuzzyfinder: fzf
- buffer explorer: 比默认的buffer要好看
- taglist: source code browser

NERDTree，在目录界面中通过`m`启动常规文件/文件夹操作。`C`回车 进入子文件夹。

## Display Management
Vim中切换编辑多个文件：
`:cd /tmp` change vim working directory
`:pwd` show current working directory
`:set hidden` (put it in `.vimrc`): https://medium.com/usevim/vim-101-set-hidden-f78800142855
`:e .` browse current directory, select file to edit

`:ls` list buffer (open files)
- %: buffer in current window
- +: unsave changes
- =: read-only

`:bn` go to next buffer
`:bp` go to previous buffer
`:bd` delete buffer, such as `:bd5`
`:bf` go to first buffer
`:bl` go to last buffer
`:b3` go to 3rd buffer
`gf` jump to the file under the cursor, use `ctrl+o` jump back

可以在vim `:e <path>` 中直接创建，删除文件或者文件夹，显示long format, sort等

> 在使用`vim-plug`加载插件后，这部分功能失效了, 不过可以使用command mode去做查看。
- i: long, thin, tree format
- s: sort by name, time, file size
- r: reverse sort order
- gh: hide or unhide dotfiles
- d: make a directory
- D: delete file or directory
- R: rename file or dir
- -: go up a directory

### windows & splits
All start with `ctrl+w`:
- s: split horizontally 
- h: move focus left
- l: move focus right

- v: split vertically
- j: move focus down
- k: move focus up

- w: cycle focus
- p: focus previous win

- c: close win current focus
- o: close all win except current focus

`:h window-resize`, check for window resize.
- +: enlarge windows, `5+`
- -: reduce windows, `3-`

### Tab
各个文件单独的tab，不用划分window了, 这个部分是放在vimrc中的:
```
" Tab mappings
" The default leader key is \
let mapleader=","                        " remap leader key to ,
" invoke for example: ,tt
map <leader>tt :tabnew<cr>
map <leader>te :tabedit
map <leader>tc :tabclose<cr>
map <leader>to :tabonly<cr>
map <leader>tn :tabnext<cr>
map <leader>tp :tabprevious<cr>
map <leader>tf :tabfirst<cr>
map <leader>tl :tablast<cr>
map <leader>tm :tabmove
```

What is `Leader key` in Vim?
https://stackoverflow.com/questions/1764263/what-is-the-leader-in-a-vimrc-file
The `Leader key` is a way of extending the power of VIM's shortcuts by using sequences of keys to perform a command.

## Operation
Using `hjkl` (在后续许多命令中都有涉及) or `arrow` keys to move around, can have number ahead to indicate how many line to move.

`:q!` quit without saving.
`:wq` quit with saving, always retouch the timestamp.
sometimes using `:q!` and `:wq!`: https://unix.stackexchange.com/questions/88247/use-of-in-vim

`:x` the same as `:wq`, but write only when changes have been made.
`:w filename` used to save file to `filename`.
`:w !sudo tee %` write to sudo with current file name, `%` represent current file name. (用在保存更改read-only文件的内容)

[Quick Movement](https://medium.com/usevim/vim-101-quick-movement-c12889e759e0)
`A` append at end of the line, `a` appends after cursor.
`I` insert ata head of the line, `i` insert at cursor.
`W` jump contiguous words, `w` jump one word.
`o` will open a line below cursor, `O` open above.
`3w` move 3 words forward, `0` move to the start of the line.
`^` go to first non-empty char in line
`3fn` find 3rd `n` char at this line. repeat by `;`

### Screen line movement
screen line指被terminal因宽度限制产生的行，并不是指原始的很长的那一行。
`g0`, `gm`, `g$`: start, middle, end of a line.
`gk`, `gj`: move up/down in screan line (can use arrow instead k/j ).

### Scrolling
Press `G` to move you to the bottom of the file.
Type `gg` to move you to the start of the file.
`line number + G` or `line number + gg` will go to the line specified.
To go back to where you came from press `Ctrl-o` (Keep `Ctrl` down while pressing the letter `o`). To go forward `Ctrl-i`
`H`, `M` and `L` move the cursor to top, medium and bottom of current page
`zt`, `zz`, `zb`: move cursor line to top, middle, bottom of screen

### Make Mark
For example, jump back and forth spots:
https://www.linux.com/news/vim-tips-moving-around-using-marks-and-jumps/
`:marks`

### Shift
Shift configuration is in `.vimrc`:
```
" shift with tab
set tabstop=2                    " Global tab width.
set shiftwidth=2                 " And again, related.
set expandtab                    " Use spaces instead of tabs
" extremely useful to edit yaml file
set autoindent                   " always set autoindenting on
set copyindent                   " copy the previous indentation on autoindentin
```
`:retab`: replace all tabs with spaces in current buffer.

`>>`, `<<` shift current line, `4>>`, `4<<` block shift with 4 lines together.
In insert mode, can use tab itself to shift (now override by auto-complete in my vimrc file), or using `ctrl+t`, `ctrl+d`: shift right and left.

对于code block indention，我在.vimrc 中设置了vmap, 用visual mode选中后shift就很方便了。

### Search and Replace
`/` search forward, `?` search backword. when hit the search, `n` go forward, `N` go backword. If the cursor is at search word, use `*` to search forward, `#` search back, use `g*`, `g#` do partial match search.

`/case\c` search case-insensitive, `/CaSe\C` search case-sensitive. `\c` and `\C` can be anywhere in pattern.

`:s/old/new/g` to substitute `old` by `new` in a line for all occrurences. you must place the cursor in that line.
To change every occurrence of a character string between two lines,
`:#,#s/old/new/g` where `#,#` are the line numbers of the range of lines where the substitution is to be done.
`:%s/old/new/g`  to change every occurrence in the whole file.
`:%s/old/new/gc` to find every occurrence in the whole file, with a prompt whether to substitute or not, `c` is confirmation.

For case-sensitive: `:%s/old/new/Igc`, but actually can be `:%s/old\C/new/gc`.

### Copy and Paste
这里和vim 的 register有关. Default is unnamed register, delete, change, substitute, search and yank all use registers.

> Pasting text into a terminal running Vim with automatic indentation enabled can destroy the indentation of the pasted text: https://vim.fandom.com/wiki/Toggle_auto-indenting_for_code_paste, use `vim-bracketed-paste` can fix this.

Show register contents: `:registers`

`y` is copy operator, for example, `yw` copy one word, `y$` copy to end of the line and `yy` used to copy a line, then you can paste through `p`. you can use `v` to select and copy. By default it uses unnamed register. Register `0` always stores the yank content, so you can use `"0p` to paste.

`"ayy`, `"ap` use register `a` to yank and paste. Usually we use `a~z` and `A~Z` register, the uppercase register can append text to current content.

使用上不同文件之间copy/paste没啥问题，但如果需要copy到system clipboard，需要设置:
https://stackoverflow.com/questions/3961859/how-to-copy-to-clipboard-in-vim
vim has to be compiled with clipboard support for any of the suggestions mentioned here to work. Mine wasn't configured that way on Mac OS X by default and I had to rebuild vim. Use this the command to find out whether you have it or not:
```
vim --version | grep 'clipboard'
```
`+clipboard` means you're good and the suggestions here will work for you, while `-clipboard` means you have to recompile and rebuild vim.

`d` is the **delete operator**, use **motion** to specify the action, for example `dw`, `de`, `d$`. Without `d`, vim just move the cursor: `w`, `e`, `$`
`2dw` delete 2 words ahead
`4dd` delete 4 lines in a row
Type `p` to put previously `deleted` text after the cursor. for example, you delete a line and replace it in another place

`v` and then you move the cursor to select the text you want, if you want to delete them next, type `d`, if you want to save the selected text to another file, press `:`, then type `w filename`. `V` is moved linewise.

### Folding
这个非常有用，比如编辑yaml 文件。
Fold by syntax or indent (yaml). `:sed foldmethod=indent/syntax`
`zM`, `zR`: fold and unfold all
`zi`: toggle fold all
`zc`, `zo`: close, open fold block
`za`: toggle fold block
`zk`, `zj`: move fold focus up and down

还可以根据file type去设定, for example:
```
" Automatic fold settings for specific files. Uncomment to use.
" autocmd FileType ruby setlocal foldmethod=syntax
" autocmd FileType css  setlocal foldmethod=indent shiftwidth=2 tabstop=2
```
### Recording
暂时没用到，对重复的复杂操作有用。

### Block comment and uncomment
[quick way to comment and uncomment block of codes](https://stackoverflow.com/questions/1676632/whats-a-quick-way-to-comment-uncomment-lines-in-vim)
comment:
1. press `Esc`
2. hit `ctrl+v`, enter visual block mode
3. use arrow keys select the lines, it won't highlight everthing, it's ok
4. `shift+i`
5. insert the text you want, e.g: #, //, %
6. press `Esc` twice

uncomment:
1. press `Esc`
2. hit `ctrl+v`, enter visual block mode
3. use arrow keys select text you want to delete
4. press `x` to delete the text

### Others
Press `u` to undo the last commands, `U` to undo all the changes in a line.
`crtl+r` to undo undo.

Type `rx` to replace the one character at the cursor with  `x`. `R` is used to replace more then one chars, type `R` then get into insert mode.

`ce` deletes the word and places you in Insert mode, so you can type new word, it will replace old ones. `c` is **change operator**, just like `d`.
比如要更改3个连续的words 在某行中: `c3w`, then type.

`Ctrl+g` will show cursor location in the file and the file status, also the file name.

`%` is used tp match another part of `(, [ or {`. This is very useful in debugging a program with unmatched parentheses. 也可以用来match其他block语句，比如if-end。

`:!<commands>` is used to execute external commands, for example `:!ls -ltr /tmp`. (the same as IPython)
 
`:r filename` will retrive the file placed under the cursor. you can also read the output of an external command, `:r !ls -ltr`.




]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>book</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title>Systems Performance, Second Edition</title>
    <url>/2021/04/13/book-systems-performance/</url>
    <content><![CDATA[
对于理解，排查，解决系统性能问题有非常大的助益, 这本书简直就是 oncall saver, 我对这方面非常感兴趣，很幸运目前能遇到它而不是在几年之后。

此外我还有一篇博客总结了一下系统性能调优: `<<Linux Performance Tuning>>`.

[x] 希望在6个月内把整本书通读一遍，结合实验掌握书中的要点。
[x] 09/18/2021 revisit







]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>book</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>The Grand Chessboard 大棋局</title>
    <url>/2021/05/14/book-%E5%A4%A7%E6%A3%8B%E5%B1%80/</url>
    <content><![CDATA[
犹豫了一段时间，最后还是决定把non-technical的书也放上来记录一下。

大概是在2006年的时候，那一年歼十装备部队并全面形成战斗力的消息正式通过央视曝光。第一次见到这种摆脱苏式造型的主力空优战斗机，很是好奇和惊喜，特地去书店买了一本介绍歼十的画册细细品读，也就是从那一年，我开始关注空军的发展，它的过去和未来，后来逐渐扩展到其他军种，总之，算是一个不是那么专业的军迷了吧。大家也都知道，军事手段是政治的延伸，政治这里我主要是指国际关系有关的部分，毕竟过去的十几年中国周边也不太平，我个人也跟着网友们叽叽歪歪，时而群情激愤，时而摇头叹气，不亦说乎。

我有时也在想，该从哪一点入手去理解这个变幻莫测的国际形式呢？除了黑天鹅事件，到底很多事情，为什么会重复发生，它们看上去注定就会发生。有的国家就是没有国运，战争，地区冲突，种族主义，外部势力干涉，这几十年来不消停，而有的国家，却可以涅班重生，亦或是虽然没什么存在感，偏安一隅，但就是可以安安稳稳地发展，很少有动乱。当然，有不同的角度可以在一定程度上解释，比如经济，货币，对外政策，能源争夺，全球化等等，但总觉得不是很直观，看了后也没有给我留下什么深刻的印象。

直到我阅读了<<大棋局>> 这本书，发现地缘政治这一角度还挺不错也容易理解(主要是冷战后)，布热津斯基的著作虽然过去快30年了，里面有的观点不敢苟同，有的与现在的实际情况很有出入，但可能是地缘单位时间尺度很大的原因，它仍然帮助我理解了近几年以及目前周边正在发生的很多事情，特别是在中亚和东欧地区（写这篇记录时，距离中印，阿塞拜疆和亚美尼亚，哈萨克斯坦和吉尔吉斯斯坦边境冲突结束不久，战争的乌云正笼罩在东乌克兰地区，台海以及南中国海也是明的暗地斗争不断）。

此外，张文木的`<<新时代：国家战略能力与地缘博弈>>` 也是很有深度的文章，值得一看，之后我还会认真阅读他的其他著作，比如`<<全球视野中的中国国家安全战略>>`。


除了中文序和卷首语，大概总结一下:

欧亚大陆五个地缘战略棋手：德国，法国，俄罗斯，中国，印度 （是的，英国，日本还有印尼都不够格）
欧亚大陆五个地缘政治支轴国家：乌克兰，阿塞拜疆，韩国，土耳其，伊朗 （回过头来看，这几个国家或周边十几年就没太平过，支轴名副其实）

当然这是30年前的设定了，目前来看，中国无疑是欧亚大陆最重量级的棋手。刚好最近宁南山发表了一篇文章`<<谁掩护了中国的崛起>>`，可以看看哈。








]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>The Go Programming Language</title>
    <url>/2020/06/09/book-the-go-programming-lang/</url>
    <content><![CDATA[
Recently I am working on Kubernetes Operator, using golang to implement the logic for operators, after having a brief understanding of Go value and Philosophy, and basic syntax, structure, this book is my next step.

There are additional, comprehensive resources on Go web site:
https://go.dev/

Other 2 Chinese Go programming books, looks good:
- [Go语法树入门](https://github.com/chai2010/go-ast-book)
- [Go语言高级编程](https://github.com/chai2010/advanced-go-programming-book)





]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>book</tag>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis</title>
    <url>/2023/01/01/cache-redis/</url>
    <content><![CDATA[
# Redis Introduction
[Redis](https://redis.io/docs/about/) is an open source (BSD licensed), 
in-memory data structure store used as a database, cache, message broker, and
streaming engine. Redis provides data structures such as strings, hashes, lists,
sets, sorted sets with range queries, bitmaps, hyperloglogs, geospatial indexes,
and streams. 

Redis has built-in replication, Lua scripting, LRU eviction, transactions, and
different levels of on-disk persistence, and provides high availability via
Redis Sentinel and automatic partitioning with Redis Cluster.

You can run atomic operations on these types, like appending to a string;
incrementing the value in a hash; pushing an element to a list; computing set
intersection, union and difference; or getting the member with highest ranking
in a sorted set.

You can learn from [Redis university](https://university.redis.com/).

# Codelab
Dockerhub Redis: https://hub.docker.com/_/redis.

To try Redis in a quick way, spinning up a server and client instance
separately in the same docker network(You can also use docker compose to 
assemble them):
```bash
# Create default bridge network for Redis instances.
docker network create redis-network

# Create Redis server using default config with port 6379.
docker run -d --network redis-network \
--name redis-server \
redis:latest

# Create Redis client and connect to redis-server with port 6379 by default.
docker run -it --network redis-network \
--name redis-client \
redis:latest redis-cli -h redis-server
```
From the client terminal, try example
[commands](https://redis.io/docs/getting-started/#exploring-redis-with-the-cli).

The usage of other commands please start with `Data Types`(below section) and
explore comprehensive commands [here](https://redis.io/commands/).

# Client Libraries
Client [libraries](https://redis.io/resources/clients/) for various languages.

# Data Types
The supported [data types](https://redis.io/docs/data-types/)

# Usage Cases

## Cache
Redis can perform different roles based on user demands, the caching is one key
role from them, please see 
[client side caching](https://redis.io/docs/manual/client-side-caching/) and
[eviction](https://redis.io/docs/reference/eviction/) strategies for reference.

## In-Memory DB with Persistence
Redis can [persist](https://redis.io/docs/management/persistence/) the data with
different mechanism.

## Message Broker
Redis also provides [pub/sub](https://redis.io/docs/manual/pubsub/)
functionality, 

[ ] vs [Stream](https://redis.io/docs/data-types/streams/) data type?

## Other Extensions
Redis can be extended to fit more roles, for example, storing and query JSON
objects like Mongodb, full-text search as Elasticsearch , graph DB like Neo4j,
please see [here](https://redis.io/docs/stack/) for details.

]]></content>
      <categories>
        <category>Cache</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Career Growth</title>
    <url>/2019/04/30/career-growth/</url>
    <content><![CDATA[
Experience and good learnings from past.

1. Be familiar with the product, ask for quota/env to use it like the customer,
learn the customers experience.

2. Be more proactive/leadership, such as meeting driving to make progress,
action items figure out.

3. Always note down meeting summary and share with stakeholders, link to
calendar tab.

4. Know what is going on around you, set bi-weekly sync up with colleagues to
understand what are they working on, this could inspire you to get solution on
your projects.

5. Domain knowledge sharing, contribute to team tech talks.

6. External dependencies are always risks.

7. Different levels of test are necessary: unit test, functional test,
integration test, e2e test and UAT, CUJ test(if it is customer facing product).

8. Document your project progress, as detailed as possible: time, context,
screenshot, link, what have done so far, TODOs.

9. Automate repeated work: utilities scripts, bash alias, command notes.

10. Always consider buffers on project timeline.

11. Back up important work frequently.

12. Use engineer survival guide suggestions.]]></content>
      <categories>
        <category>Miscellanea</category>
      </categories>
  </entry>
  <entry>
    <title>Renew Driver License</title>
    <url>/2019/09/19/car-driver-license/</url>
    <content><![CDATA[
我的临时驾照过期了快一周了，忘记去renew，新的驾照也没寄过来（也不知道是不是给我办的real ID？,
准备明天去DMV。仔细观察了一下，当时办事人员在我的临时驾照上写了一个Legal Presence号码（916-657-7790），应该是快过期的时候打过去查询进度和催促用的，但是我忘了。。。我印象里以为10月
份才过期😌。

所以说，明天去了之后有几件事需要确认：
1. 继续renew
2. 查询驾照审批进度
3. 再次确认地址

保险起见还是带上[必要的材料]:
- 护照
- I-94
- Old DL
- SSN Card
- EAD
- H1B

后续：
上次去renew的时候用的是OPT学生身份，但在审批过程中我的身份变成了H1B，不一致导致了pending，这
次去更新了一下，所以一定要用最新的身份去办理。

09/30/2019, 今天收到了新的driver license，居然不是Real ID, 之后还得再去更换，否则2020以后
乘机还得带护照了。决定2020之后再去办理了，因为会搬家。记得要2份不同的住址证明，否则不给办.

10/18/2019, 过了一个月sticker还是没有收到，去DMV一问才知道地址居然还是以前的，原来car registration的地址和Drive license的地址都要修改。花了22刀现场办了一个，算是了结一件事.

## 2022-12 DL Renewal
I have received the notice from DMV to renew my DL(expiration date 2023/01).
Book a appointment if in person renew is needed.

I have also completed the H1B extension application, the file has been received.

I did not see any documents that are required for renewal, in case, just bring
them together:
- Renewal Notice from DMV
- Application fee
- Passport
- H1B Extenstion Notice (I-94 included)
- Old DL
- SSN Card
- New Address Memo

I was not required to redo the test, just needs to finish the application,
inputed your personal attributes such has eye color, hight, weight, etc (from
the old DL).

For my case, only passport, H1B(I-94) were collected, whole process took about 1
hour, pretty efficient.
]]></content>
      <categories>
        <category>Car</category>
      </categories>
      <tags>
        <tag>car</tag>
      </tags>
  </entry>
  <entry>
    <title>Car Insurance</title>
    <url>/2020/06/04/car-insurance/</url>
    <content><![CDATA[
最近疫情比较严重，在家办公了，大家出行减少，于是上个月保险公司给我退了$11的汽车保费。今年我感觉疫情很难结束，并且wfh将会持续很长一段时间，平时出门也就买个菜，于是想把保险换成更便宜一些的。

我目前的投保的公司是Progressive，之前是Farmers。但我开车一向比较注意，所以从来没出过事故。

这里有篇文章介绍了一下美国汽车保险的情况:
https://www.bangli.us/post/3842
https://www.guruin.com/guides/car-insurance
https://www.dealmoon.com/guide/773024

后来换成了GEICO，价格便宜了近一半，直接在GEICO app 上申请的，续费和查看也很方便。

]]></content>
      <categories>
        <category>Car</category>
      </categories>
      <tags>
        <tag>car</tag>
      </tags>
  </entry>
  <entry>
    <title>Car Registration Renewal</title>
    <url>/2020/09/13/car-registration-renewal/</url>
    <content><![CDATA[
前几天收到了vehicle registration renewal的通知，这次需要做smog check (对于比较老的非电动
车辆，一般2年一次), 在google map上选择了一家评分较好又比较近的smog check station, 要知道
美国这边smog check都是私人承包的，可以大概在评论中看一下价格如何，此外记得携带DMV的通知单去现
场。

一共花了50刀，smog check做完之后你不需要做其他任何事情，只需要到网上缴付Renewal fee即可，但
不要做完立即支付，因为DMV那里还没有收到你的smog check记录 (如果你去支付的页面，会发现有一个警
告如此)。一般等待一天左右警告会消失，说明DMV已经收到记录，这时就可以进行支付了。

> Aside: 上次遇到过sticker被寄到了之前的地址的情况，所以在搬家renew驾照的时候一定要记得提醒
更改renewal的地址。

> 此外，每年renew car sticker也不要忘了，否则超时会被罚款。去DMV可以网上交钱renew然后过一两
周就会给你寄过来了。]]></content>
      <categories>
        <category>Car</category>
      </categories>
      <tags>
        <tag>car</tag>
      </tags>
  </entry>
  <entry>
    <title>汽车的日常</title>
    <url>/2019/10/18/car-repair/</url>
    <content><![CDATA[
这篇blog主要记录一下汽车维修和保养方面的总结吧。

来美国的第一辆车是2009 Camry，主要是为了方便平时买菜，图个省心买了辆二手北美神车，到这篇blog创
建的时候已经开了快3年，只能说名副其实，已经10年的车了，大毛病一个都没有。目前为止的车况我只能说
非常棒，10年的车，接近16万公里，只有一些小部件损耗的更换，丰田，了不起。
![](https://drive.google.com/uc?id=1ff9jmfxgTJhXn5TO8Y1kLf5jl-TjC5RW)

说实话，10年了，外形真没有过时😃，经济适用，打算还继续使用一段时间🌹。

02/04/2020 100k miles! 纪念一下
![](https://drive.google.com/uc?id=1j9zDS1ZE-dnzIdN8IPTwXIIBmf9uN65c)


## 目前遇到的问题：

1. 刹车尾灯
![](https://drive.google.com/uc?id=1tfLBOmP_42bb07in1trmjCXbNxmbDU11)
某次出游发现右边的刹车尾灯不亮了，很好办，网上买一对和车型兼容的尾灯就行了，自己安装非常容易。花
费$4.99.

2. 遮阳挡板
![](https://drive.google.com/uc?id=1a8Ocq57jXSBVqjD3h7RYE48J0zi-YRkC)
这个买来的时候就是破损的，我一直没管它，但有时阳光太强这个挡板活动有点问题导致体验不是很好，于是
就在网上买了一个自己装上，花费$29.98.

3. 胎压传感器
这种传感器的电池寿命一般在5~10年之间，看来这车之前都没换过，不巧被我遇到了。我在亚马逊上一次性买
了4个兼容的传感器，准备把四个轮胎上的都换掉。
![](https://drive.google.com/uc?id=1LONYlXCAsOcAX82hj2JQmMqUIKG3LCMw)

需要注意的是，TPMS胎压传感器需要专业人士和工具更换，要先确认工具能正确的识别它，然后再安装。安
装步骤一般是先将轮胎放气，卸载，然后更换。特别要注意传感器需兼容车载电脑。我买的是
pre-programmed的产品，315MHz + 433MHz兼容，不过安装后仍需要relearn车载控制器。一定要仔细
阅读说明书哦。

我找了Costco Tire Center帮我更换，很不幸，他们的工具无法识别我买的sensor (I doubt)...于是
就只能使用他们的sensor了，价格$44.99/个。最后加上labor fee总共花了$252.61。之前我去咨询了
其他的auto repair，有的4个要charge $500，呵呵😑。

要注意的是他们会询问车的年份和型号，以及发动机类型（几缸）。然后交钱，给你一个磁性号码牌，放在车
顶上，然后你就可以到处闲逛，比如去Costco看看烤鸡，完事后会打电话叫你去取车。

还需要注意的是，随着气温骤降，TPMS warning light may go on，这是因为气温降低，轮胎里的空气
收缩导致胎压下降。可以参考这篇文章: https://www.lesschwab.com/article/tpms-light-coming-on-in-cold-weather-heres-why.html

可以去gas station去自己打气，最好自己买个tire gauge，Amazon上很多选择，感觉这很必要。
https://www.dummies.com/home-garden/car-repair/wheels-tires/how-to-add-air-to-your-tires/
我仔细研究了一下Amazon上售卖的tire gauge with inflation and deflation，感觉一般般呀，特
别是便携12DV车充的，看差评可能会烧保险丝。。。 最后就没买😂，不过可以考虑入手一个机械式测压的。

我直接去了costco，在 https://www.costcotireappointments.com 上进行预约即可。或者早上早
点去直接听到garage门口让工作人员帮忙补补气即可，打气后一般过一会就正常了。


## 车辆正常保养维护
又到了该保养的时候了，maintenance light blinks everyday! 为了做到心里有数，pre-research
is a must! 最该看的，其实就是car manual了，每个车都会有的，里面会告诉你一些基本的使用和保养
常识，当然了，很多人也不关心这个，反正交给4S店或者其他auto repair去做了。

就拿我的车来说吧，5k miles左右会做一个保养，我一般会做的项目包括:
1. change engine oil (must)
2. change engine oil filter (must)
3. tire check, brake pads check
4. battery get tested
5. change engine air filter (depends, but should)
6. change cabin air filter (recommand)
7. washer fluid

机油不说了，保养主要就是换机油，有的auto repair如果你不说，他不会给你换机油过滤器的。。。但最
好换了。轮胎检查一下，特别是spare，没气了打气，否则爆胎了你拿啥顶上？刹车片看看需不需要更换。

电池看看是否正常，电压测测，现代车辆都是车载电脑操控，需要一个稳定输出的电池。

引擎滤网需要更换，我这几次观察了一下，如果汽车的使用环境比较好，污染物少，用了10k miles的滤网
还算干净，但还是换了，很简单的操作. 车厢空气滤网，这个很容易脏，建议更换。更换engine air
filter非常简单，工具只需要Ratchet Socket Wrench and Sockets, 注意socket的直径匹配就行:
![](https://drive.google.com/uc?id=1T15bCJjRchNlHtuKXpvJGueovD54g0wl)
其他相关工具的如下:
![](https://drive.google.com/uc?id=1fooPl36_amOsChHWWDYKEPydzg37EMjL)


雨刮水不够了，自己加满，但最好不要用tap water，网上有很多去污剂可以考虑混合一下，如果在零下的
环境中使用，还需要加防冻剂。我买的是QWIX windshield washer fluid, 1/4 oz makes one
gallon windshield washer fluid.

100k miles保养，还需要考虑:
1. coolant
2. power steering fluid
3. transmission fluid
4. brake fluid
5. change spark plugin
6. tire rotate

这些项目都有自己的更换周期，特别是那几个fluid。取决于你车的具体情况。
所有这些保养，经过研究，都可以自己完成😃，就是要自己买工具。这个以后准备妥当了再更新一下。

这次保养，我除了change engine oil (filter), spark-plugin, 检查rear brake pads磨损殆尽
也和brake fluid一起更新了，花费$420 (＃￣～￣＃)~ Coolant 和 power steering fluid 没有
更换，说没什么必要，人工费也挺贵的。其实我觉得brake change去costco或许会便宜很多，但当时嫌麻
烦就没去问，下次就注意了。

## Battery replacement
今天早上发现发动不了车了！原因是电池电量不足，毕竟已经快6年没换电池了!

首先，如何紧急发动，用jumper starter, 这个东西最好备一个在车后备箱里，买最简易的那种需要另一
个车供电的, 在网上买一个road应急包里很多工具!

使用jumper starter 的步骤参考car manual，非常的详细，注意的点：
1. connect positive first: discharged car then assistant car
2. connect negative then: assistant car then ground on discharged car
3. boot assistant car for 5 mins and slightly push gas pedal
4. boot discharged car
5. remove in reverse order: negative on ground then assistant car
6. remove positive on assistant then discharged car

电池的更换, 对于我这款老车，非常简单，去costco 买一个同款电池自己安装(当时买成 $138 total),
废旧电池可以去costco 回收 $15. 工具: socket wrench set and screwdriver set.

电池的接线：
1. uninstall: remove negative first
2. install: put positive first

安装完成后，按照car manual 的指示，re-initialize tire pressure value. 其他车可能在安装
完成后需要重新初始化车在电脑等等.
]]></content>
      <categories>
        <category>Car</category>
      </categories>
      <tags>
        <tag>car</tag>
      </tags>
  </entry>
  <entry>
    <title>Azure Cloud</title>
    <url>/2020/07/17/cloud-azure/</url>
    <content><![CDATA[
//TODO
[ ] custom azure container: vim, zsh or startship, auto completion
[ ] Azure subscription and tenant IDs? In ~/.azure/credentials

# Setup Azure CLI
Using docker azure CLI container:
https://docs.microsoft.com/en-us/cli/azure/run-azure-cli-docker
```bash
docker pull mcr.microsoft.com/azure-cli

docker run -d \
--name azure \
--entrypoint=/bin/bash \
mcr.microsoft.com/azure-cli:latest \
-c "tail -f /dev/null"
```

## Commands
To access a aks cluster:
```bash
# similar to company in gcp
# list subscriptions
az account list --output table

# set subscription
# you can see subscription from akz record
az account set -s "SUBSCRIPTION NAME"


# group, similar to project in gcp
# you can see group from akz record
# config default group
az configure --defaults group=<resource group name>


## or direct get zks credential by
az aks get-credentials -n <cluster name> -g <group name> --subscription
```

The command reference links:
- [subscription](https://docs.microsoft.com/en-us/cli/azure/account?view=azure-cli-latest)
- [resource group](https://docs.microsoft.com/en-us/cli/azure/group?view=azure-cli-latest)
- [aks](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest)]]></content>
      <categories>
        <category>Cloud</category>
      </categories>
      <tags>
        <tag>azure</tag>
        <tag>cloud</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Cloud</title>
    <url>/2020/07/17/cloud-aws/</url>
    <content><![CDATA[
//TODO:
- https://app.pluralsight.com/course-player?clipId=8ada28fb-a440-477a-af89-84b1eceb5440

From Pluralsight: `AWS Certified Developer – Associate (DVA-C01)`

别人提到的:
1. 工作中经常用到， 需要花时间学习理解的: vpc, subset, security group, eni, route53, acm, IAM
2. 其次: load balancer, auto scaling, ddb throttle, api gateway。
3. 其他: lambda, step function, cloudformation, s3, ec2, sqs, sns, cloudwatch, ecs, ecr, code deploy 等等

# Core Services
`EC2`: Elastic Cloud Compute
`AMI`: Amazon Machine Image
`EBS`: Elastic Block Storage, used for EC2 files systems
`Security Group`: set of firewall rules that control the traffic for your **single** instance, for example, control who can ssh to EC2 instance, `VPC` is for **groups** of instance

`S3`: Simple Storage Service, maxmium file size is 5T, bucket is accessed via URL, the same as gcloud storage. Can be used for hosting static web site.

`RDS`: Relation Database Service
`Route53`: Domain Name System (DNS) servics, you can register your domain name!

## EC2



# Enhancing Services
`EB`: Elastic Beanstalk, application service running on EC2

`Lambda`: Serverless option for executing code, function as a service, only pay when the code is running, significant cost savings if you have infrequent activity. Great for small, irregular tasks, for example, nightly ETL kickoffs, notification type functions

`DynamoDB`: a managed NoSQL database, supports both key-values and document models

`VPC`: for securing your services, components in the VPC can connect each through private IP. Multiple `subnets` can be in VPC, for example, you can configure public subnet and private subnet.

How does `VPC` work? 
- route table: control what goes where
- network ACL(access control list): act as subnet-level firewalls, control who can come and go 

`CloudWatch`: monitoring resources and acting on alerts, for example, CPU usage on EC2 instances, DynamoDB read/write throughput, estimated billing charges

`CloudFront`: super fast CDN, works seamlessly with S3, EC2, load balancing and route53

## CloudWatch
For example, Increasing network traffic -> EC2 -> alarm CloudWatch -> action -> Auto Scaling Group -> EC2. `SNS` can also be integrated to CloudWatch.

`SNS`: simple notification service, Pub/sub messaging for microservices and serverless applications. First create `topic`, then subscribe this with from email or SMS, etc

## IAM
`MFA`, multi-factor authentication, reuqire more than one factor to authenticate.
MFA process: password + device code (app generated code refresh every 60 seconds) 类似将军令, 要先在手机上下载一个MFA app.

After loging aws console, click the account user name -> My security credentials -> MFA

IAM **policy** make it easy to assign permissions to users or groups in an administrative way. Users have no permission by default. Policy properties:
- Effect: allow, deny
- Action: operations user can perform
- Resources: user performs on

Root account permission is dangerious, follows amazon suggested best practices to have more securities. For example, create a admin grouo, attch policy to it, then add user to this group, use this user to login.

# Access AWS 
- Web console
- SDK, programming application uses: https://github.com/aws
- Command line, great for shell scripting: https://github.com/aws/aws-cli

To generate the access key for SDK and cli, after loging aws console, click the account user name -> My security credentials -> Access Keys.

Create `~/.aws/credentials` file with content from your access key:
```ini
[default]
aws_access_key_id=AKIAIVHU6XLsd3J7IAKA
aws_secret_access_key=Vemdu3nD65uY1cWC0fznCEfUhvsUT9NIjMT790zqK
region=us-west-2
output=json
```

I use aws cli docker to run the commands, the docker container is ephemeral for each command (for convenience, set alias for docker run command), you need to mount the `~/.aws` to container:
```bash
docker run --rm -ti -v ~/.aws:/root/.aws amazon/aws-cli s3 ls
```

For other methods installing
- https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html

To aviod installing dependencies, you can use virtual machine to setup environment.


# Demo Components
A pizza web site:
- EC2, host web application
- DynamoDB, store users & toppings
- RDS, store pizza
- S3, store images & assets
- ElastiCache, store sessions

]]></content>
      <categories>
        <category>Cloud</category>
      </categories>
      <tags>
        <tag>cloud</tag>
        <tag>aws</tag>
      </tags>
  </entry>
  <entry>
    <title>Google Cloud CLI</title>
    <url>/2022/03/31/cloud-gcp-cli/</url>
    <content><![CDATA[
When working with gcloud CLI, sometimes there are strong needs to filter and
format the output for subsequent processing.

A use case for example [here](https://stackoverflow.com/questions/53497318/gce-how-to-find-all-disks-attached-to-an-instance):

```bash
gcloud compute instances \
list \
--project example \
--filter='name~test*' \
--format="value(name,zone,disks[].deviceName)"
```

They are all from
[gcloud topic group](https://cloud.google.com/sdk/gcloud/reference/topic), from
the leftside panel there are other gcloud command reference.

## Resource Available Key
To find the [keys](https://cloud.google.com/sdk/gcloud/reference/topic/resource-keys)
for filter and format, check `--format flattened` option, it will list all keys
for the specified resource.

## Filter Syntax
https://cloud.google.com/sdk/gcloud/reference/topic/filters

## Format Syntax
https://cloud.google.com/sdk/gcloud/reference/topic/formats

## Projection Syntax:
https://cloud.google.com/sdk/gcloud/reference/topic/projections


]]></content>
      <categories>
        <category>Cloud</category>
      </categories>
      <tags>
        <tag>gcloud</tag>
      </tags>
  </entry>
  <entry>
    <title>GCP Log Explorer</title>
    <url>/2022/07/04/cloud-gcp-log-explorer/</url>
    <content><![CDATA[
The quick revisit is by query examples.

## Query Example:
To run a query you can go through this typical order:
```sql
-- find resurce type
resource.type="k8s_container"
-- any label to narrow down the scope
resource.labels.cluster_name="us-east4"
resource.labels.namespace_name="default"
resource.labels.container_name="foo"
-- strings in json or text payload
textPayload:"there is a high bar"

-- apply logic operator or regexp in searching
```

Other query examples please see
[here](https://cloud.google.com/logging/docs/view/query-library).

## Query Language
It is recommended to read through the detailed syntax
[here](https://cloud.google.com/logging/docs/view/logging-query-language#syntax_summary).

## Key Takeaways
1. Comment line starts with `--` in query expression.

2. The boolean operator precedence order: `NOT`, `OR` and `AND` and they must
use upper case in query.

3. The query expression leftside [field](https://cloud.google.com/logging/docs/view/logging-query-language#field_path_identifiers) is a path from LogEntry group,
you can also explore them in the unfolded query result, for example, usually we
want to check if `jsonPayload` or `textPayload` contains desired substrings.

4. Regular expression on text query [examples](https://cloud.google.com/logging/docs/view/logging-query-language#example-regular-expressions).

5. The [timestamp](https://cloud.google.com/logging/docs/view/logging-query-language#right-time-period)
in query is on UTC, you can get it by date command:
```bash
# 2023-07-04T18:50:36+00:00
date --rfc-3339=s --date="5 hours ago" | sed -e 's/ /T/g'
# Then use it in query, for example:
timestamp >= "2023-07-04T18:50:36+00:00"
```

]]></content>
      <categories>
        <category>Cloud</category>
      </categories>
      <tags>
        <tag>gcloud</tag>
      </tags>
  </entry>
  <entry>
    <title>Google Cloud Subnet Expansion</title>
    <url>/2022/05/09/cloud-gcp-subnet-expand/</url>
    <content><![CDATA[
If the VPC subnet address space in a region ran out, the simplest way is to create a new subnet with the same mask in the same region(depends on your needs). Or you can expand the original subnet IP range.


# VPC Overview Highlight
https://cloud.google.com/vpc/docs/vpc
- VPC is global resource
- resource within a VPC(regardless subnet) can communicate to each other, subject to firewall rules
- shared VPC, keep a VPC in a common host project


# Subnet Overview Highlight
https://cloud.google.com/vpc/docs/subnets
- subnet is regional resource
- subnet [creation mode](https://cloud.google.com/vpc/docs/vpc#subnet-ranges): auto and custom
- you can create more than one subnet per region(for example to extend subnet capacity).
- subnet [IPV4 valid range](https://cloud.google.com/vpc/docs/subnets#ipv4-ranges): primary and secondary

Note, there is no need to create secondary subnet IP range for [`Alias IP`](https://cloud.google.com/vpc/docs/alias-ip). From observation, the subnet will have seconard IP ranges auto created if GKE is used in that network: it will create `pods` and `services` secondary IP ranges.

For example:
```bash
# check which node has pods secondary IP range:
gcloud compute instances list \
# the attribute path can be found through --format flattened
--filter="networkInterfaces[0].aliasIpRanges[0].subnetworkRangeName~'pods'" \
--project <project name>
```

Found VM attached with specified subnet:
```bash
gcloud compute instances list \
--filter="networkInterfaces[].subnetwork~'regions/us-east4/subnetworks/us-east4'" \
--project <project name>
```


## Create and Modify Network
- [VPC MTU setting and impact](https://cloud.google.com/vpc/docs/vpc#mtu)
- [subnet creation and editing](https://cloud.google.com/vpc/docs/create-modify-vpc-networks#subnet-rules)

The primary IPv4 range for the subnet can be [`expanded`](https://cloud.google.com/vpc/docs/create-modify-vpc-networks#expand-subnet), but not replaced or shrunk, after the subnet has been created. For example, the original primary IP range is `192.168.2.0/24`(in private address space defined in gcloud), now set the prefix length to `21`:
```bash
gcloud compute networks subnets expand-ip-range <subnet name> \
--region us-east4 \
--prefix-length=21 \
--project <project name>
```
Then the new IP range will be `192.168.0.0/21`(bit set exceeds mask length is removed as it does not make sense), the expansion will fail if the new IP range conflicts with others.
]]></content>
      <categories>
        <category>Cloud</category>
      </categories>
      <tags>
        <tag>cloud</tag>
        <tag>gcp</tag>
      </tags>
  </entry>
  <entry>
    <title>Intro Containerd</title>
    <url>/2021/05/10/containerd-intro/</url>
    <content><![CDATA[
今天第一次在生产环境中看到了 k8s 跳过 docker 直接使用containerd runtime, 这里大概记录一下.

什么是 k8s 跳过 docker 使用 containerd run time? 需要清楚它们的关系, please read throughly:
- [The differences between Docker, containerd, CRI-O and runc](https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/#dockershim-docker-in-kubernetes)
- [Dockershim removal is coming. Are you ready?](https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/)

这次事件的开始是一个在 GKE node 上的incident: High CPU Usage Critical, 去查看了一下，不仅仅是CPU usage高，load average也是超出相当多了，4个CPU核心的平均负载竟达到了20+(3个参数都是)。当然，对比其他几个正常的monitoring节点以及相关分析，最后的分析结果是uneven loads，一个解决办法是在资源定义中对亲和性做一些安排，把负载分散到其他合格的节点上。

关于如何查看节点上的负载情况，可以用以下的命令:
```bash
# 观察running/runnable的队列长度
# -t: append timestamp to each line output
vmstat -t 1

# -b: batch mode
# -n: page number
# -i: idle process hide
# -c: show command
# -H: thread
# -w: width output
top -b -n 1 -i -H -c -w | grep -E '^[0-9]+' | awk '{ if ($8 == "R" || $8 == "D") print $0 }' 
```
`grep` 部分是为了过滤一些 pid 非常大，不需要关心的线程。注意我们只查看state为`D` 和 `R`的线程，因为平均负载和这两个有关。

此外，我发现`docker images/ps -a`输出的是空，开始以为是权限的问题，但并不是，后来查看docker daemon的状态，并没有什么问题，除了Memory的用量非常少，这让我意识到这个节点可能使用的是其他的container runtime。

于是从`top`中摘取了一个应该是容器相关的进程，查看其父进程PPID，比如:
```bash
# BSD
ps axo pid,ppid,comm | grep <pid>
# standard
ps -eo pid,ppid,comm | grep <pid>
```
最后展开查看，得到了如下的结果，就很明显了:
```bash
root     3515874  0.0  0.0 111720  3712 ?        Sl   Apr03  28:56 /usr/bin/containerd-shim-runc-v1 
-namespace k8s.io -id 93a341648e8833e0212a257f1fd6aa2cded3f825f2475c16c15dc576b8c949a2 -address /run/
containerd/containerd.sock
```
> 注意，以上描述其实不准确，因为docker uses containerd as well, 只是歪打正着了，要查看k8s 用的什么container runtime:
```
kubectl get nodes -o wide
```

Containerd可以以一个守护进程方式存在，可以用systemctl查看状态和配置。
关于Containerd 的参考资料:
[Getting started with containerd](https://containerd.io/docs/getting-started/)
- If you are a developer working on containerd you can use the `ctr` tool to quickly test features and functionality without writing extra code
- If you want to integrate containerd into your project, you can use a simple client package. In this guide, we will pull and run a Redis server with containerd using that client package.

You can use `crictl` command to explore in `containerd` runtime k8s node:
https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/

## Readings
The team has done some work to switch container runtime in k8s:
[How to switch container runtime in a Kubernetes cluster](https://dev.to/stack-labs/how-to-switch-container-runtime-in-a-kubernetes-cluster-1628)

此外，GKE文档中也有关于Containerd使用的说明[Containerd images](https://cloud.google.com/kubernetes-engine/docs/concepts/using-containerd).]]></content>
      <categories>
        <category>Containerd</category>
      </categories>
      <tags>
        <tag>containerd</tag>
      </tags>
  </entry>
  <entry>
    <title>Metadata Server</title>
    <url>/2021/12/28/cloud-gcp-metadata/</url>
    <content><![CDATA[
The thing is I ran docker container in the VM instance from gcloud, the docker container has gcloud SDK installed beforehand, without mounting user `~/.config` folder, I found that the container SDK has already been set with the service account from host, for example:
```bash
# gcloud_test is image built with gcloud SDK
docker run -it --rm --entrypoint=/bin/bash gcloud_test:latest
```

Inside container, executing:
```bash
gcloud auth list
```
Instead of asking you login, the host associated service account was displayed.

It turns out that it is related to [Metadata Server](https://cloud.google.com/compute/docs/metadata/overview) provided by Google Cloud: Your VM automatically has access to the metadata server API without any additional authorization. You can only query the metadata server programmatically from within a VM.

For example, to get the service account of the VM:
```
curl -H "Metadata-Flavor:Google" http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/
```
More metadata items see [here](https://cloud.google.com/compute/docs/metadata/default-metadata-values).

The gcloud SDK inside of container will do something like this to automatically fetch host's service account and use it, if I disable the container networking during creation, this mechanism will not work anymore:
```bash
# disable network
docker run -it --rm --network none --entrypoint=/bin/bash gcloud_test:latest
```

Also notes that this is a common concept for most of the cloud providers, not something unique to Google.]]></content>
      <categories>
        <category>Cloud</category>
      </categories>
      <tags>
        <tag>cloud</tag>
        <tag>gcp</tag>
      </tags>
  </entry>
  <entry>
    <title>Google Cloud</title>
    <url>/2020/07/10/cloud-gcp/</url>
    <content><![CDATA[
From PluralSight Google Cloud path. 
下载的配套slides 讲得很详细，可以参考，特别是讲解了如何选择资源组合, 每个课时中有Quick Labs.

The best way to learn is to read official document along with operating on glcoud console.

Some useful GCP projects:
- [Networking 101 GCP Sheet](https://github.com/jesuispy/networking-101-gcp-sheet)
- [Google Cloud 4 Words ](https://github.com/priyankavergadia/google-cloud-4-words#the-google-cloud-developers-cheat-sheet)


# Commands
```bash
## after install gcloud SDK
## init
gcloud init --console-only

## can login multiple user accounts
gcloud auth login
## it is ADC(application default credential)
## for code to interact with GCP, such as terraform CLI
gcloud auth application-default login [--project]

## same as gcloud auth login but using SA credential
## and roles
gcloud auth activate-service-account [--key-file]

## list auth accounts or service account
gcloud auth list
## switch account
gcloud config set account <account name>
## revoke account
gcloud auth revoke <account name>


## show and install components, i.e alpha, beta, kubectl...
gcloud components list
gcloud components install [beta]


## all projects under my account, not the used one
gcloud projects list 


## set which project to use
gcloud config set project <project name>
## current project in use
gcloud config list project
## get project ID
gcloud config get-value project

## list service account in project
gcloud iam service-accounts list [--project <project ID>]
## create service-account after auth login and set project
gcloud iam service-accounts create <SA name>  [--display-name=<"description">] [--project <project id>]
## can update display name and description
gcloud iam service-accounts update ...
## disable service account
gcloud iam service-accounts enable/disable ... 
## delete: When a service account is deleted, its role bindings 
## are not immediately removed; they are automatically purged from 
## the system after a maximum of 60 days.
gcloud iam service-accounts delete ... 
## generate credentials json file for terrform
## can also delete it
gcloud iam service-accounts keys create ~/key.json \
  --iam-account <SA name>@<project ID>.iam.gserviceaccount.com
## see the roles bind to service account
gcloud iam service-accounts get-iam-policy <SA>



## see available context
## -o name: show context name
kubectl config get-contexts [-o name]
## switch context
kubectl config use-context <context name>
## rename context to human readable
kubectl config rename-context <old> <new>

## export current configuration to yaml file
kubectl config view --minify --flatten > cluster.yaml
## the same as this gcloud command
## KUBECONFIG=clusters.yaml: specify cluster.yaml to store the credentials
KUBECONFIG=clusters.yaml gcloud container clusters \
get-credentials <cluster name> --zone=<cluster zone>


## current enabled API list
gcloud services list [--porject <project ID>]
gcloud services enable <API>
gcloud services disable <API>


## create default VPC network
gcloud compute networks create default


## create K8s cluster in default network
gcloud container clusters create gke-eu --zone europe-west1-c \
  --release-channel stable --enable-ip-alias
## list cluster
gcloud container clusters list
gcloud container clusters list \
  --project <project name> \
  --filter "name:cluster-name" \
  --format "get(location)"
## describe
gcloud container clusters describe <cluster name> --region <region/zone>
## delete cluster
## -q: quiet
gcloud container clusters delete gke-eu --zone=europe-west1-cd [-q]


## grant IAM roles to end user in project
## member can be serviceAccount:email
gcloud projects add-iam-policy-binding <project ID> \
 --member user:<member> \
 --role=roles/gkehub.admin \
 --role=roles/resourcemanager.projectIamAdmin
```


# Terms
Cloud SDK commands:
- gcloud
- kubectl
- gsutil (google storage)
- bq (big query)

Cloud shell is acutally running on a ephemeral compute engine instance. 其实command line 操作创建各种资源 比UI 更方便 (这也是Terraform的基础)

`Zone` is under `Region`, you can think of a zone as data center in a region.

`Anthos` is google's morden solution for hybird and multi-cloud systems and services management. (下面一章会专门总结一下)

`GCP cloud functions`: serverless execution environment for building and connecting cloud services. With Cloud Functions you write simple, single-purpose functions that are attached to events emitted from your cloud infrastructure and services. Your Cloud Function is triggered when an event being watched is fired. Your code executes in a fully managed environment. There is no need to provision any infrastructure or worry about managing any servers.

`GCP deployment manager`: like `Terraform`, infrastructure as code.

`GCP Dataproc` for running Apache Spark and Apache Hadoop clusters.
`GCP Dataflows` offers managed data pipelines, serverless fully managed data processing.
`GCP Dataprep` visually explore, clean and prepare data for analysis and machine learning.

`BigQuery` is fully managed data warehouse.
`Pub/Sub` (publisher/subscriber) is scalable, reliable messaging.
`DataLab` offers interactive data exploration. Build on Jupyter.


# Kubernetes Architecting
Build on top of compute engine.
Container is isolated in user space to running application code, lightweight, represent as a process:
- process
- linux namespace
- cgroups
- nuion file systems

GKE abstracts away the master, only show the worker nodes on dashboard.
Use `Node Pool` to manage different kinds of nodes.
Google maintains a container registry: `gcr.io`
`Cloud Run`: build on `Knative`, for serverless workloads.

`Cloud Build`: Build, test, and deploy on serverless CI/CD platform.

`Private Cluster`, google products and authorized networks can access.

# Fundations
`Compute Engine` let you run virtual machine. In GCP, K8s nodes are actually virtual machine running in Compute Engine, just like IBM Fyre, you can see them in Compute Engine dashboard.
- Fully customized virtual machines
- Persistent disk/SSD or optional local SSDs
- Global load balancing and autoscaling
- Per-second billing

VM has built-in SDK commands.
A `vCPU` is equal to 1 hardware hyper-thread.

`Preemptible VM`: can be terminated by GCP if the resources is needed in other places.
Cloud storage is binary large-object storage. 不同的storage针对不同的对象.

`VPC`: virtual private cloud, VPC is global scope, subnet is regional, can have different zone on the same subnet. Each VPC network is contained in a GCP project. VPC make componets connect to each other or isolated from each other.

You control the VPC network, use its `route table` to forward traffic within network, even across subnets.

`VPC`: 3 types:
- default mode 
- auto mode
- custom mode (for production)

`VPN` can connect the on-premises network to GCP network.

VMs can be on the same subnet but different zones. Every subnet has four reserved IP addresses in its primary IP range: `.0` for subnet network itself, `.1` for subnet gateway, second-to-last address in the range and the last address.

The external IP is transparent to VM, managed by VPC. You will not see it by `ip a s` command. 
In `/etc/hosts`:
```bash
10.128.0.2 instance-1.us-central1-a.c.terraform-k8s-282804.internal instance-1  # Added by Google
## internal DNS reslover
169.254.169.254 metadata.google.internal  # Added by Google
```
For example:
```bash
nslookup instance-1

Server:         169.254.169.254
Address:        169.254.169.254#53

Non-authoritative answer:
Name:   instance-1.us-central1-a.c.terraform-k8s-282804.internal
Address: 10.128.0.2
```

Setup VPC peering or VPN to allow internal network connection between VPCs.

You can delete the whole default network setting, and create your own, for example, auto or custom mode network.

`Private google access` (for example to access cloud storage) and `Cloud NAT` (only outbound is allowed) help VM without external IP to access internet.

`RAM Disk`: tmpfs, fast scratch disk or cache, faster then disk but slower then memory.

VM comes with a single root persistent disk, can attach additional disk to VM, it is network storage! The extended disk needs to be formated and mounted by yourself, for example:
```bash
sudo mkfs.ext4 -F -E lazy_itable_init=0 \
                     lazy_journal_init=0,discard \
                     /dev/disk/by_id/<disk name>

sudo mount -o discard,defaults /dev/disk/by_id/<disk name> /home/<target directory>
```

`App engine` is not like Compute engine, it does not comprise of virtual machines, instead get access a family of services that application needs. Container(K8s, hybird) is in the middle of Compute engine (IssA) and App engine (PaaS). You don't want to focus on the infrastructure at all, just want to focus on your application code. Especially suited for for building scalable web application/web site and mobile backends, RESTful API.

App engine flexible environment is rely on container running in virtual machine in compute engine.


# Core Services
## IAM
除了GCP, 其他public cloud也采取同样的RBAC策略。

忘了就多看几遍: [Regulating Resource Usage Using Google Cloud IAM](https://app.pluralsight.com/library/courses/google-cloud-platform-iam-regulating-resource/table-of-contents)

首先理解`RBAC`，在很多场合都有应用: 分为3个部分: identity, roles and resources. Identity 可以是google account, google group and service account(not human). Role 有几种分类，比如primitive role, predefined role, custom role.

`IAM`: identity and access management, who can do what on which resources. user of IAM can be person, group and application. Always select the least privilege to reduce the exposure to risk.

IAM add new member中 GCP 和 G suite  是共享用户(human)信息的。

Identities:
- google accounts
- service accounts, belongs to your applications
- google groups (collection of google accounts and service accounts)
- G suite domains
- Cloud identity domains

`Service Account`: used by application or virtual machine running code on your behalf, can have IAM policies attach to it:
- user-managed SA: for example `service-account-name@project-id.iam.gserviceaccount.com`, you choose the service account name.
- default SA: 常见的比如使用App engine, compute engine时自动创建的service account. 
- google-managed SA: GCP 内部使用，不用管。

IAM roles:
- primitive role: Owner, Editor, Viewer.
- predefined role: 针对不同资源的roles，比如compite, gke, network等等.
- custom role: 自定义的, user maintain, for more granular access.

Bindings 就是把Identity 和 roles结合起来，形成一个`policy`. IAM把policy 赋予不同的对象, 比如: IAM hierarchy: Organization -> folder -> project -> resource.

Project level policy operations (or organiation level)，意思是在project level上，这些member可以做规定的事情。
```bash
## add and revoke
## member can be user:xx or serviceAccount:xx
gcloud projects add-iam-policy-binding <project ID> \
  --member=member \
  --role=<role ID>
gcloud projects remove-iam-policy-binding <project ID> \
  --member=member \
  --role=<role ID>

## batch operation, role bindings 都在yaml file中
gcloud projects set-iam-policy <project id> <file path>
gcloud projects get-iam-policy <project id> [--format=json/yaml] > [file path]
```

注意这2个命令，这里service account被当做了resource而不是identity, 所以这里设置了其他identity去操作这个service account:
```bash
gcloud iam service-accounts set/get-iam-policy <service account>
```


## Storage and Database
Storage access control has many options, IAM is one of them and usually is enough. others like ACLs, signed URL and Signed policy document.

`Cloud Storage`: fully managed object store. In the demo, `gsutil` command can do versioning, acl, set restrictions, etc.
```bash
# if want to skip heep_proxy setting
gs='env -u http_proxy gsutil'
```

The slide has info about how to choose which service: SQL, NoSQL ...?

`Cloud SQL`: a fully managed database service (MySQL or PostgreSQL), If the Cloud SQL located in the same VPC and the same region, connect it with private IP, otherwise using cloud SQL proxy connection (setup via a script).

`Cloud Spanner`: Cloud Spanner combines the benefits of relational database structure with non-relational horizontal scale. Used for financial and inventory applications.

`Cloud Firestore`: the next generation of Cloud Datastore. Cloud Firestore is a NoSQL document database

`Cloud Bigtable`: a fully managed, wide-column NoSQL database that offers low latency and replication for high availability.

`Cloud Memorystore`: creates and manages Redis instances on the Google Cloud Platform.

## Resource Management
Resource manager, quotas, labels and billing.

## Resource Monitor
From stackdriver collection.


# Scaling and Automation
## Interconnecting Networks
In the demo, Two VMs in differen region and subnet, setup the `VPN tunnel` they can ping each other via private IP.

理解了这部分，可以自己搭建VPN翻墙了.
`Cloud VPN`: securely connect your infrastructure to GCP VPC network, useful for low-volume data connections.

Options: IPsec VPN tunnel, dedicated interconnect (for large traffic) and partner interconnect (via other service provider network)

Configure cloud VPN gateway and on-premises VPN gateway, setup VPN tunnel (encrypted traffic), must be paired.

## Load Balancing and Auto Scaling
Managed instance groups, typically used with autoscaler.

HTTP(s) load balancing: level 7 application layer load balancer.

In the demo, create VM with detached disk, install apach2 then keep the disk to create custom image, use this image to create instance template then creating instance groups.

## Infrastructure Automation
Deployment manager and Terraform, can also use Ansible, Chef, Puppet..

Terraform is integrated in Cloud Shell.

GCP marketplace, production-ready solutions.


# External HTTP(S) Load Balancing
https://cloud.google.com/load-balancing/docs/https



# Anthos
建议把这个系列的slides下载复习。
Qucik Labs and slides are from [PluralSight Anthos special](https://app.pluralsight.com/paths/skills/architecting-hybrid-cloud-infrastructure-with-anthos)


Built on open source technologies pioneered by Google—including Kubernetes, Istio, and Knative—Anthos enables consistency between on-premises and cloud environments.
```
      On-premises                           Public Cloud
|----------------------|               |-----------------------|
|                       Config Management                      | Anthos Configuration Management
|      <==============================================>        |
|                       Service Mesh                           | Istio, communications &
|      <==============================================>        |        observability
|-----------|           |              |                       |
| Enterprise| |---------|              |-----------|           |
| workload  | |Containers              |Containers |           | Kubernetes, deployment &
|           | |---------|              |-----------|           |             run-time platform
|           |   K8s     |              |   GKE                 |
|           | on-premise|              |                       |
|-----------|-----------|              |-----------------------|

```
这个系列先讲了Anthos是什么，组成结构，然后讲了service mesh, 最后讲了anthos config management (ACM).

几个要点:
1. on-premises cluster中安装运行有一个agent pod, 用来主动注册该cluster到anthos control plane.
2. 所有注册过的cluster是统一管理和可视的，在同一个control plane，cluster中的资源也可见.
3. Anthos中很重要的部分就是service mesh, 使用的是Istio，所以要理解这部分。见我的关于Istio的博客。
4. config management is the single source of truth, 可以把所有的policies都放在一个git repo中，是为desired state, 使用时会传播到所有被managed的objects中，是否被managed 在object manifest中有annotation标记.
5. multiple control planes DNS using Istio CoreDNS, not kube-dns (for local).

## Ingress of Anthos
https://cloud.google.com/kubernetes-engine/docs/concepts/ingress-for-anthos
这里将ingress of anthos的概念，组成以及图示都列出来了，很清晰。
Ingress for Anthos is designed to meet the load balancing needs of multi-cluster, multi-regional environments. It's a controller for the external HTTP(S) load balancer to provide ingress for traffic coming from the internet across one or more clusters.

Ingress for Anthos updates the load balancer, keeping it consistent with the environment and desired state of Kubernetes resources.

Ingress for Anthos uses a centralized Kubernetes API server to deploy Ingress across multiple clusters. This centralized API server is called the `config cluster`. Any GKE cluster can act as the config cluster. The config cluster uses two custom resource types: `MultiClusterIngress` and `MultiClusterService`. By deploying these resources on the config cluster, the Anthos Ingress Controller deploys load balancers across multiple clusters.

There can have multiple `mcs` and only one `mci`. `mcs` can select specific clusters with `clusters` field. `mci` can specify default backend and other backends with rules.

Clusters that you register to an `environ`(An environ is a domain that groups clusters and infrastructure, manages resources, and keeps a consistent policy across them) become visible to Ingress, so they can be used as backends for Ingress.

Environs possess a characteristic known as `namespace sameness` which assumes that resources with the identical names and same namespace across clusters are considered to be instances of the same resource. ]]></content>
      <categories>
        <category>Cloud</category>
      </categories>
      <tags>
        <tag>cloud</tag>
        <tag>gcp</tag>
      </tags>
  </entry>
  <entry>
    <title>Cloud Init Quick Start</title>
    <url>/2020/12/21/cloud-init/</url>
    <content><![CDATA[
当时的项目用到了cloud-init 进行本机系统启动后的配置，替代之前Ansible的配置操作(也可以做ansible之前的一些更为基础的配置，比如设置network, SSH等)，使其在boot后到达可用状态。其实和Ansible 一样都是configuration management tool, Ansible is push-based, cloud-init is pull-based.

LXD/LXC container can be used with cloud-init.

# Cloud-init
[cloud-init](https://cloudinit.readthedocs.io/en/latest/) official document, [User data](https://cloudinit.readthedocs.io/en/latest/topics/examples.html) config example.

这[段话](https://cloud-init.io/)解释得很清楚了:
Cloud images are operating system templates and every instance starts out as an identical clone of every other instance. It is the `user data` that gives every cloud instance its personality and cloud-init is the tool that applies user data to your instances automatically.

To use cloud-init, need to install packages, for example in CentOS:
```bash
yum install -y cloud-init
# you can see these services
systemctl cat cloud-init-local
systemctl cat cloud-init
systemctl cat cloud-config.service
systemctl cat cloud-final.service
```
See this IBM [post](https://www.ibm.com/support/knowledgecenter/SSB27U_6.4.0/com.ibm.zvm.v640.hcpo5/instslesmore.htm) for how to install cloud-init on Centos


目前各大云厂商都支持cloud-init, 在infra as code中，cloud-init可以通过传递一个`cloud-init.tpl` metadata file 到 Terraform instance resource `metadate`的 `user-data` 中进行设置. 这样在instance 启动时，相应的就会自动配置了。
```ini
data "template_file" "setup" {
  # template file
  template = file("${path.module}/cloud-init.tpl")
  # pass var for rendering
  vars = {
    foo              = "/dev/sdb"
    foo_config    = base64encode(data.template_file.foo_config.rendered)
  }
}

# instance definition
resource "google_compute_instance" "backup" {
  # pass to it
  metadata = {
    user-data  = data.template_file.setup.rendered
  }
}
```
If you are working on gcloud, go to instance detail page, check `Custom metadata` -> `user data` will display the rendered script.

要点是如何写这个`cloud-init.tpl` metadata file, notice that must include this line at very beginning and no space after `#`:
```bash
#cloud-config
```

# Debug Cloud-init
[Troubleshooting VM provisioning with cloud-init](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/cloud-init-troubleshooting)

The log of cloud-init is in `/var/log/cloud-init.log`. It will show you errors if something failed.

上次还遇到一个问题，就是当时`#cloud-config`格式没对，导致cloud-init 无法解析这个文件，所以user metadata没有得到执行，这时如果看log file 不是很明显，需要查看`/var/log/boot.log`文件，通过对比发现这个错误:
```bash
Unhandled non-multipart (text/x-not-multipart) userdata ...
```
这说明格式错了，当时这个问题卡了几个小时，一直没注意到这个地方。


# Others
在构造user的password的时候，需要一个hash的数值:
[openssl passwd](https://ma.ttias.be/how-to-generate-a-passwd-password-hash-via-the-command-line-on-linux/)
[Why is the output of “openssl passwd” different each time?](https://unix.stackexchange.com/questions/510990/why-is-the-output-of-openssl-passwd-different-each-time)
```bash
# -1: MD5
openssl passwd -1
# -salt: add salt
openssl passwd -1 -salt yoursalt
# from stdin
echo 'admin' | openssl passwd -1 -stdin -salt yoursalt
```













]]></content>
      <categories>
        <category>Cloud</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>cloud</tag>
        <tag>cloud-init</tag>
      </tags>
  </entry>
  <entry>
    <title>Spanner Google SQL</title>
    <url>/2024/01/15/database-spanner-googlesql/</url>
    <content><![CDATA[
The [reference](https://cloud.google.com/spanner/docs/reference/standard-sql/overview)
of Google SQL language.

Here are some sample Spanner SQL for quick revisit.

# Sample Interactive SQLs
```sql
-- Launch spanner interactive terminal with parameters
span sql --max_value_lines=300

-- help
help

-- Select DB in target scope
use /span/xxxx/cloud-example-domain:test;

-- For write and update operation
set dml_concurrency interactive_transactional;


-- Partial string match with like operator
select Device
from Devices
where Device.name like '%foo-%';

-- Use ltrim and cast
select device.region,device.area,device.name
from Devices
where device.id in ('bar','foo')
-- "apple23" will be trimmed to "23" and cast to number 23
order by region,area,cast(ltrim(d.name, 'apple') as int32)
desc
limit 3;

-- Use UNEST, in and limit
select distinct Device.reference_id
from Devices
where "example_id" in UNNEST(Device.ids)
limit 1;

-- To check timestamp change between a time period
-- it is UTC timestamp: current - 9 hour <= target time period < current - 8 hour
select Device.id,timestamp_seconds(Metadata.last_updated_timestamp.seconds) 
from Devices 
where Metadata.last_updated_timestamp.seconds 
          >= UNIX_SECONDS(TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 9 HOUR))
      and Metadata.last_updated_timestamp.seconds 
          < UNIX_SECONDS(TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 8 HOUR))

-- Use array_length for repeated proto field
select Device.id
from Devices
where array_length(Device.ids) > 1;

-- Use group by and count(*)
select Device.service_level,count(*)
from Devices
where Device.type='SWITCH'
group by Device.service_level;

-- Insert with proto
insert into ProjectMappings(ProjectMapping, Metadata)
values(
  {project_number: 1357913579,
  billing_account: "test-billing-account",
  project_state: "PROJECT_STATE_ACTIVE",
  alias: "test-alias"
  },
  {
  last_updated_timestamp: {seconds: 1689901201},
  version_id: 1555444333222111000,
  state: "LATEST",
  created_by: "example@gmail.com"
  }
);

-- Compare timestamp seconds
SELECT *
from PrivateCloudMetadata
WHERE PrivateCloud.region in ('us-east4', 'us-central1')
      AND Metadata.last_updated_timestamp.seconds 
          > UNIX_SECONDS(TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 5 DAY));
```

# Redirect
```sql
-- Run sql from input.txt, absolute path
Source /home/user/chengdol/input.txt;

-- Capture into output.txt, absolute path
TEE /home/user/chengdol/output.txt;

-- cancel output redirect
NOTEE
```
]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>spanner</tag>
      </tags>
  </entry>
  <entry>
    <title>SQL Quick Revisit</title>
    <url>/2023/07/01/database-sql-quick-revisit/</url>
    <content><![CDATA[
Common Relational DB SQL concepts for quick revisit, recapped mainly from 
[w3school](https://www.w3schools.com/sql/) and go/Bard, not every DB supports
the listed syntax, so please use them accordingly.

## SQL Clauses Layout Order
```sql
SELECT column_name(s)
FROM table1
JOIN table2 ON table1.column = table2.column
WHERE condition
GROUP BY column_name(s)
HAVING condition
ORDER BY column_name(s);

-- you can use multiple tables, it is the same as inner join
SELECT column_name(s)
FROM table1, table2
WHERE condition
GROUP BY column_name(s)
HAVING condition
ORDER BY column_name(s);
```

## Distinct
```sql
SELECT COUNT(DISTINCT Name) FROM Customers;

-- Distinct applies on the combination in SELECT, not just the first column
SELECT DISTINCT collectors.first_name, collectors.last_name
FROM collectors
JOIN sales
  ON collectors.id = sales.collector_id;
```

## Operators in Where
`WHERE` [operators](https://www.w3schools.com/sql/sql_where.asp):
* -, >, <, >=, <=, !=(<>)
* BETWEEN
* LIKE
* IN

## Quotes for Text
```sql
-- Single quote for text value
SELECT * FROM Customers
WHERE Country='Mexico';

-- No quote around number value
SELECT * FROM Customers
WHERE CustomerID=1;
```

## Using Parenthesis
```sql
SELECT * FROM Customers
WHERE Country='Germany' AND (City='Berlin' OR City='München');
```

## Min and Max
```sql
SELECT MAX(Price) AS LargestPrice
FROM Products;
```

## Count, Avg and Sum
```sql
SELECT COUNT(ProductID)
FROM Products;
```

## Order By
```sql
SELECT * FROM Customers
ORDER BY Country ASC, CustomerName DESC;

-- order by field can be irrelevant from select
SELECT Address
FROM Customers
ORDER BY CustomerName ASC
```

## NULL
Not every DB supports NULL for empty values, be aware.
```sql
SELECT CustomerName, ContactName, Address
FROM Customers
WHERE Address IS NULL;

SELECT CustomerName, ContactName, Address
FROM Customers
WHERE Address IS NOT NULL;
```

## Limit
```sql
SELECT * FROM Customers
LIMIT 3;
```

## In
The `IN` operator is a shorthand for multiple OR conditions.
```sql
SELECT * FROM Customers
WHERE Country NOT IN ('Germany', 'France', 'UK');

-- subquery
SELECT * FROM Customers
WHERE Country IN (SELECT Country FROM Suppliers);
```

## Alias
```sql
-- New column with a Literal value
SELECT 'Customer' AS Cus, ContactName, City, Country
FROM Customers

-- CONCAT is a SQL function
SELECT CustomerName,
CONCAT(Address,', ',PostalCode,', ',City,', ',Country) AS Address
FROM Customers;

-- you can omit AS for table alias
SELECT o.OrderID, o.OrderDate, c.CustomerName
FROM Customers AS c, Orders AS o
WHERE c.CustomerName='Around the Horn' AND c.CustomerID=o.CustomerID;
-- the same as
SELECT o.OrderID, o.OrderDate, c.CustomerName
FROM Customers c, Orders o
WHERE c.CustomerName='Around the Horn' AND c.CustomerID=o.CustomerID;
```

## Joins
* (INNER) JOIN: Returns records that have matching values in **all** tables.
* LEFT (OUTER) JOIN: Returns all records from the left table, and the matched
records from the right table.
* RIGHT (OUTER) JOIN: Returns all records from the right table, and the matched
records from the left table.
* FULL (OUTER) JOIN: Returns all records when there is a match in either left or
right table.

See explanation diagrams [here](https://www.w3schools.com/sql/sql_join.asp).

### Inner Join
```sql
SELECT column_name(s)
FROM table1
INNER JOIN table2
ON table1.column_name = table2.column_name;
-- the same as JOIN
SELECT column_name(s)
FROM table1
JOIN table2
ON table1.column_name = table2.column_name;

-- Inner join 3 tables
SELECT Orders.OrderID, Customers.CustomerName, Shippers.ShipperName
FROM 
(
  (
    Orders
    INNER JOIN Customers
    ON Orders.CustomerID = Customers.CustomerID
  )
  INNER JOIN Shippers
  ON Orders.ShipperID = Shippers.ShipperID
);
```

### Left Join
```sql
-- Show both CustomerID can better explain the left join
SELECT 
  Customers.CustomerName, Customers.CustomerID as id1,
  Orders.CustomerID as id2, Orders.OrderID
FROM 
  Customers
LEFT JOIN 
  Orders 
ON 
  Customers.CustomerID = Orders.CustomerID
ORDER BY 
  Customers.CustomerName;
```

The LEFT JOIN keyword returns all records from the left table (Customers), even
if there are no matches in the right table (Orders) and if there is no that
CustomerID in Orders table, you get `null` for the Orders selected fields.

### Right Join
```sql
SELECT
  Orders.OrderID, Orders.EmployeeID as id1,
  Employees.EmployeeID as id2,
  Employees.LastName,
  Employees.FirstName
FROM Orders
RIGHT JOIN Employees
ON Orders.EmployeeID = Employees.EmployeeID
ORDER BY Orders.OrderID;
```

The RIGHT JOIN keyword returns all records from the right table (Employees),
even if there are no matches in the left table (Orders).

### Full Join
```sql
SELECT Customers.CustomerName, Orders.OrderID
FROM Customers
FULL OUTER JOIN Orders ON Customers.CustomerID=Orders.CustomerID
ORDER BY Customers.CustomerName;
```

The FULL OUTER JOIN keyword returns all matching records from both tables
whether the other table matches or not. So, if there are rows in "Customers"
that do not have matches in "Orders", or if there are rows in "Orders" that do
not have matches in "Customers", those rows will be listed as well.

### Self Join
```sql
-- there will be duplicates introduced by switch CustomerName in A and B tables
SELECT 
  A.CustomerName AS CustomerName1,
  B.CustomerName AS CustomerName2,
  A.City
FROM
  Customers A, Customers B -- can omit AS in table alias
WHERE
  A.CustomerID <> B.CustomerID
AND
  A.City = B.City 
ORDER BY
  A.City;
```

## Union
The UNION operator is used to combine the result-set of two or more SELECT
statements.

* Every SELECT statement within UNION must have the same number of columns.
* The columns must also have similar data types.
* The columns in every SELECT statement must also be in the same order.

```sql
-- 'Customer' and 'Supplier' are literal value under the 'Type' column
-- here is used to distinguish the rows are from which select
SELECT 'Customer' AS Type, ContactName, City, Country
FROM Customers
-- UNION ALL -- can have duplicates
UNION -- union distinct row only
SELECT 'Supplier', ContactName, City, Country
FROM Suppliers;
```

## Group By
The GROUP BY statement is often used with aggregate functions (COUNT(), MAX(),
MIN(), SUM(), AVG()) to group the result-set by one or more columns.

```sql
SELECT column_name(s)
FROM table_name
WHERE condition
GROUP BY column_name(s)
ORDER BY column_name(s);

-- Example
SELECT Shippers.ShipperName, COUNT(Orders.OrderID) AS NumberOfOrders
FROM Orders
LEFT JOIN Shippers 
ON Orders.ShipperID = Shippers.ShipperID
GROUP BY ShipperName;
```

## Having
The HAVING clause was added to SQL because the WHERE keyword cannot be used with
aggregate functions.

```sql
SELECT column_name(s)
FROM table_name
WHERE condition
GROUP BY column_name(s)
HAVING condition
ORDER BY column_name(s);

-- Example
SELECT COUNT(CustomerID) as CustomerNumber, Country
FROM Customers
GROUP BY Country
HAVING CustomerNumber > 5
ORDER BY CustomerNumber;
```

## Exist
The EXISTS operator is used to test for the existence of any record in a
subquery.

```sql
SELECT column_name(s)
FROM table_name
WHERE EXISTS
  (
    SELECT column_name
    FROM table_name
    WHERE condition
  );

-- Example
SELECT SupplierName
FROM Suppliers
WHERE EXISTS
(
  SELECT ProductName
  FROM Products
  -- Suppliers is from outer layer
  WHERE Products.SupplierID = Suppliers.supplierID
  AND Price < 20
);
```

## Any and ALL
The ANY and ALL operators allow you to perform a comparison between a single
column value and a range of other values.

```sql
SELECT column_name(s)
FROM table_name
WHERE column_name operator ANY
  (SELECT column_name
  FROM table_name
  WHERE condition);

-- Example
SELECT ProductName
FROM Products
WHERE ProductID = ANY -- can be rewritten as IN operator
  (SELECT ProductID
  FROM OrderDetails
  WHERE Quantity = 10);
```

## Subquery
This is how you can boost your query, basically you can use subquery in below
clauses:
* SELECT
* FROM
* WHERE
* HAVING
* JOIN

Examples:
```sql
-- SELECT clause
SELECT
  first_name,
  last_name,
  ( -- inner query runs for every row of collectors table
    SELECT count(*) AS paintings
    FROM sales
    WHERE collectors.id = sales.collector_id -- correlated with outer query
  )
FROM collectors;

-- WHERE clause
SELECT customer_name
FROM customers
WHERE customer_id IN (
    SELECT customer_id
    FROM orders
    WHERE order_date > '2023-03-08'
);

-- HAVING clause
SELECT product_category, average_price
FROM products
GROUP BY product_category
-- no where as where cannot use aggragate functions
HAVING average_price > (
    SELECT AVG(price)
    FROM products
    WHERE product_category = 'Electronics'
);

-- FROM clause
SELECT product_name, quantity
FROM (
    /* Generate a new table from subquery */
    SELECT product_name, SUM(quantity) AS quantity
    FROM order_items
    GROUP BY product_name
);

-- with JOIN, the example no special meaning.
SELECT
  res.region,res.availability_zone,
  res.machine_type,
  res.billing_account_id,
  res.reservation_code
FROM 
(
  (
    SELECT DISTINCT r.earmark.region,
    r.earmark.availability_zone,
    r.earmark.machine_type,
    r.earmark.billing_account_id,
    r.earmark.reservation_code
  FROM 
    Reservations AS r
  WHERE
    r.ProvenanceMetadata.version_state = 'LATEST' AND
    r.earmark.reservation_code!=""
  ) AS res
  JOIN
  (
    SELECT distinct p.PlacementGroup.region,
    p.PlacementGroup.availability_zone,
    p.PlacementGroup.machine_type
    FROM
      PlacementGroupCapacity AS p
    WHERE
      p.ProvenanceMetadata.version_state = 'LATEST' AND
      p.PlacementGroup.pg_pool=""
  ) AS pgc
  ON 
    res.region=pgc.region AND 
    res.availability_zone=pgc.availability_zone AND
    res.machine_type=pgc.machine_type
);
```
]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title>Apache Cassandra</title>
    <url>/2020/06/28/database-cassandra/</url>
    <content><![CDATA[
>  Revisit on 2023 March because of SPLA.

# Intro
Cassandra [quick started](https://cassandra.apache.org/_/quickstart.html) with
single docker node. You can extend it to test any Cassandra client, for example
`gocql`, build the app with golang image and bring it up in the same docker
network.

Cassandra [basics](https://cassandra.apache.org/_/cassandra-basics.html) to
understand consepts such as ring(cassandra cluster, masterless), horizontal
scaling(aka scale-out), partitioning, RF(replication factor), CL(consistency
level), quorum(RF/2+1), CAP(cassandar by default AP).

How the quorum is [calculated](https://docs.datastax.com/en/cassandra-oss/3.x/cassandra/dml/dmlConfigConsistency.html#HowQUORUMiscalculated).

## Replication Strategy
`Keyspace`: a namespace that defines data replication on nodes, one keyspace may
have mutliple related tables, the `Replicatin Strategy` is keyspace-wide:

- simply strategy for single data center
- network topology strategy for multiple data centers

Data center can have multiple racks.

For example, one data center has 2 racks, rack1 has 2 nodes, rack2 has one node,
if the simply strategy is 1, then rack1 owns 50% data, each node in rack1 owns
25%, rack2 owns 50%, since rack2 only contains 1 node, so that node owns 50%.


## Tunable Consistency
`Coordinator Node`: client connect to perform actions. Each connection to
Cassandra may have a different coordinator node, any node can be the coordinator.

You can configure consistency on a cluster, datacenter, or per individual read
or write operation. see this doc for
[details](https://docs.datastax.com/en/cassandra-oss/3.x/cassandra/dml/dmlConfigConsistency.html).

Consistency level for **write**: 
- `ONE`, `TWO`, `THREE` 
- `QUORUM`(majority of nodes succeeds)
- `ALL`(must all good)
- `ANY`(include coordinator itself).

`Hinted Handoff`: when one write node is unavaiable, the data is written to
coordinator node, the coordinator node will try repeatedly write to the
unavailable node until succeeded.

Consistency level for **read**: how many nodes to consult to return the most
current data to caller.
- `SERIAL`: see this [doc](https://docs.datastax.com/en/cassandra-oss/3.x/cassandra/dml/dmlConfigConsistency.html)
- `ONE`, `TWO`, `THREE`
- `QUORUM`(majority of nodes succeeds)
- `ALL` (must all good)

`Read Repair`: 当对一个node写失败了but back online later，在read时如果有多个replicas
的数据可以参考，则对那个node可重新写入上次wirte失败的数据. Run `nodetool repair`
periodically will resolve the inconsistencies in cluster.

Achieving strong consistency:
- write consistency + read consistency > replication factor

Multiple data center consistency level:
- `EACH_QUORUM`
- `LOCAL_QUORUM`: local means current coordinator node data center
- `LOCAL_ONE`: the same as `ONE`

# Partition, Composite and Clustering Key

To correctly run cql, especially the `order by`, you need to understand how to
define primary key and use partition, composite and clustering
keys efficiently:

https://www.baeldung.com/cassandra-keys


# CQL
A single Cassandra docker node is enough for CQL.

Cassandra Query Language:
- https://cassandra.apache.org/doc/latest/cql/index.html
- Keyspace, table and basic data type
- CRUD operations
- Counters
- Aggregate functions

With `cqlsh` script, you can specify remote Cassandra node with port to connect,
by default it will connect to localhost 9042 port.

Keyspace -> Tables -> partitions -> row.

In brief, each table requires a unique `primary key`. The first field listed is
the `partition key`, since its hashed value is used to determine the node to
store the data. If those fields are wrapped in parentheses then the partition
key is composite. Otherwise the first field is the partition key. Any fields
listed after the primary key are called `clustering columns`. These store data
in ascending or descending order within the partition for the fast retrieval of
similar values. All the fields together are the primary key.

```bash
## help
cqlsh> help

Documented shell commands:
===========================
CAPTURE  CLS          COPY  DESCRIBE  EXPAND  LOGIN   SERIAL  SOURCE   UNICODE
CLEAR    CONSISTENCY  DESC  EXIT      HELP    PAGING  SHOW    TRACING

## specified
cqlsh> help consistency;

CQL help topics:
================
AGGREGATES               CREATE_KEYSPACE           DROP_TRIGGER      TEXT     
ALTER_KEYSPACE           CREATE_MATERIALIZED_VIEW  DROP_TYPE         TIME     
ALTER_MATERIALIZED_VIEW  CREATE_ROLE               DROP_USER         TIMESTAMP
ALTER_TABLE              CREATE_TABLE              FUNCTIONS         TRUNCATE 
ALTER_TYPE               CREATE_TRIGGER            GRANT             TYPES    
ALTER_USER               CREATE_TYPE               INSERT            UPDATE   
APPLY                    CREATE_USER               INSERT_JSON       USE      
ASCII                    DATE                      INT               UUID     
BATCH                    DELETE                    JSON            
BEGIN                    DROP_AGGREGATE            KEYWORDS        
BLOB                     DROP_COLUMNFAMILY         LIST_PERMISSIONS
BOOLEAN                  DROP_FUNCTION             LIST_ROLES      
COUNTER                  DROP_INDEX                LIST_USERS      
CREATE_AGGREGATE         DROP_KEYSPACE             PERMISSIONS     
CREATE_COLUMNFAMILY      DROP_MATERIALIZED_VIEW    REVOKE          
CREATE_FUNCTION          DROP_ROLE                 SELECT          
DROP_TABLE               SELECT_JSON 
```

Create a keyspace with:
```sql
create keyspace pluralsight with replication = {'class':'SimpleStrategy', 'replication_factor':1};
```
Create a table in this keyspace with:
```sql
use pluralsight;
create table courses (id varchar primary key);
```
Optionally attempt to create the table again with:
```sql
create table if not exists courses (id varchar primary key);
```
(and note that you will not get an error as long as the 'if not exists' is present)

Add a few columns to the courses table with:
```sql
alter table courses add duration int;
alter table courses add released timestamp;
alter table courses add author varchar;
```
Add a comment to the table with:
```sql
alter table courses with comment = 'A table of courses';
```
View the complete table and all its default properties with:
```sql
-- describe
desc table courses;
```
Drop and recreate a more complete courses table with:
```sql
drop table courses;

create table courses (
    id varchar primary key,
    name varchar,
    author varchar,
    audience int,
    duration int,
    cc boolean,
    released timestamp
    ) with comment = 'A table of courses';
```
(Note that when entering the lines as above cqlsh will automatically detect a
multi-line CQL statement)

Exit cqlsh:
```sql
exit
```

Load course data by running a series of CQL commands from an external file
```bash
cat courses.cql | cqlsh
```
Verify that the CQL commands in the file were indeed executed:
```sql
use pluralsight;
desc tables;
select * from courses;
```
(The 'desc tables' should show a single 'courses' table, and the 'select'
statement should show 5 rows of sample data.)

The 'expand' cqlsh command will display the query results in a 'one column per
line' format:
```sql
-- pretty format
expand on;
select * from courses;
expand off;
```


You can display the time a piece of data was written with the 'writetime'
function:
```sql
select id, cc, writetime(cc) from courses where id = 'advanced-javascript';
```
We can update this cc column with an 'update' statement:
```sql
update courses set cc = true where id = 'advanced-javascript';
```
Now re-run the select statement containing the 'writetime' function and notice
that the time has changed. You can prove to yourself that this write time is
stored on a per column basis by selecting this for a different column:
```sql
select id, name, writetime(name) from courses where id = 'advanced-javascript';
```
Note that this writetime value is the same as the one returned by our first 'cc' query.


Cassandra also provides a function for returning the token associated with a
partition key:
```sql
select id, token(id) from courses;
```

If you try to select from a column other than the primary key, you'll get an
error:
```sql
select * from courses where author = 'Cory House';
```
(We'll show how to do this in a later module.)

Let's create a users table:
```sql
create table users (
    id varchar primary key,
    first_name varchar,
    last_name varchar,
    email varchar,
    password varchar
    ) with comment = 'A table of users';
```
Then we'll insert and "upsert" two rows of data:
```sql
insert into users (id, first_name, last_name) values ('john-doe', 'John', 'Doe');
update users set first_name = 'Jane', last_name = 'Doe' where id = 'jane-doe';
select * from users;
```
(Note that the net effect of the insert and update are the same.)

Now we'll add a new 'reset_token' column to this table, and add a value to this
column with a TTL:
```sql
alter table users add reset_token varchar;
update users using ttl 120 set reset_token = 'abc123' where id = 'john-doe';
```

We can retrieve the time remaining for a ttl with the 'ttl' query function:
```sql
select ttl(reset_token) from users where id = 'john-doe';
```

We can turn on tracing and do a select to see that there are currently no
tombstones:
```sql
tracing on;
select * from users where id = 'john-doe';
```
(Re-run this several times until the 2 minutes have elasped and the token_value
will be gone, and tracing will show a tombstone.)

Turn off tracing:
```sql
tracing off;
```


Create a ratings table with two counter columns:
```sql
create table ratings (
    course_id varchar primary key,
    ratings_count counter,
    ratings_total counter
    ) with comment = 'A table of course ratings';
```
Now let's increment both counter columns to represent receiving a new course
rating of 4:
```sql
update ratings set ratings_count = ratings_count + 1, ratings_total = ratings_total + 4 where course_id = 'nodejs-big-picture';
select * from ratings;
```
(The select should show the data we just upserted.)

Now let's add a second course rating of 3:
```sql
update ratings set ratings_count = ratings_count + 1, ratings_total = ratings_total + 3 where course_id = 'nodejs-big-picture';
select * from ratings;
exit
```
This should show the new values of "2" and "7" for ratings_count and ratings_total respectively.

Drop and re-create "ratings" to use with the "avg" aggregate function
```sql
drop table ratings;

create table ratings (
    course_id varchar,
    user_id varchar,
    rating int,
    primary key (course_id, user_id)
);
```
Insert a few sample ratings
```sql
insert into ratings (course_id, user_id, rating) values ('cassandra-developers', 'user1', 4);
insert into ratings (course_id, user_id, rating) values ('cassandra-developers', 'user2', 5);
insert into ratings (course_id, user_id, rating) values ('cassandra-developers', 'user3', 4);
insert into ratings (course_id, user_Id, rating) values ('advanced-python', 'user1', 5);
```
You can select the average for a single course (across users):
```sql
select course_id, avg(rating) from ratings where course_id = 'cassandra-developers';
select course_id, avg(rating) from ratings where course_id = 'advanced-python';
```
However, you can't apply aggregate functions across partition keys:
```sql
select course_id, avg(rating) from ratings;  -- incorrect results
```

# Multi-Row Partition

## Composite Key
Previously we only have one primary key in table, that primary is the partition
key. But it could be:
```sql
-- composite key
PRIMARY KEY (partition_key, clustering_key, ...)
```
`partition_key` can also be composite.

There is no `join` operation in Cassandra.

Drop this table and create a new one to hold both course and module data
```sql
drop table courses;
create table courses (
    id varchar,
    name varchar,
    author varchar,
    audience int,
    duration int,
    cc boolean,
    released timestamp,
    module_id int,
    module_name varchar,
    module_duration int,
    primary key (id, module_id)
) with comment = 'A table of courses and modules';
```
Insert data for the course, plus the first two modules
```sql
insert into courses (id, name, author, audience, duration, cc, released, module_id, module_name, module_duration)
values ('nodejs-big-picture','Node.js: The Big Picture','Paul O''Fallon', 1, 3240, true, '2019-06-03',1,'Course Overview',70);

insert into courses (id, name, author, audience, duration, cc, released, module_id, module_name, module_duration)
values ('nodejs-big-picture','Node.js: The Big Picture','Paul O''Fallon', 1, 3240, true, '2019-06-03',2,'Considering Node.js',900);
```
Select the data we just inserted
```sql
-- get same result
select * from courses;
select * from courses where id = 'nodejs-big-picture';
```
Now we can include both id and module_id in our where clause
```sql
select * from courses where id = 'nodejs-big-picture' and module_id = 2;
```
We can't select by just module, unless we enable 'ALLOW FILTERING'
```sql
-- if no partition_key, performance downgrade
select * from courses where module_id = 2;                  // fails
select * from courses where module_id = 2 allow filtering;   // succeeds
```
Now insert the remaining modules for the course
```sql
insert into courses (id, name, author, audience, duration, cc, released, module_id, module_name, module_duration)
values ('nodejs-big-picture','Node.js: The Big Picture','Paul O''Fallon', 1, 3240, true, '2019-06-03', 3, 'Thinking Asynchronously', 1304);

insert into courses (id, name, author, audience, duration, cc, released, module_id, module_name, module_duration)
values ('nodejs-big-picture','Node.js: The Big Picture','Paul O''Fallon', 1, 3240, true, '2019-06-03', 4, 'Defining an Application and Managing Dependencies', 525);

insert into courses (id, name, author, audience, duration, cc, released, module_id, module_name, module_duration)
values ('nodejs-big-picture','Node.js: The Big Picture','Paul O''Fallon', 1, 3240, true, '2019-06-03', 5, 'Assembling a Development Toolset', 489);
```
We can also use module_id as part of an "in" clause
```sql
select * from courses where id = 'nodejs-big-picture' and module_id in (2,3,4);
```
And we can order by module_id
```sql
select * from courses where id = 'nodejs-big-picture' order by module_id desc;
```
We can "select distinct" just the id, but not the id and course name:
```sql
select distinct id from courses;         // succeeds
select distinct id, name from courses;   // fails
```

## Static Columns
Static Columns are static within the partition.
Its the common data in a partition.

From cqlsh, drop and recreate the courses table, using static columns
```sql
use pluralsight;
drop table courses;
create table courses (
    id varchar,
    name varchar static,
    author varchar static,
    audience int static,
    duration int static,
    cc boolean static,
    released timestamp static,
    module_id int,
    module_name varchar,
    module_duration int,
    primary key (id, module_id)
) with comment = 'A table of courses and modules';
```
Insert just the course data, and select it back
```sql
insert into courses (id, name, author, audience, duration, cc, released)
values ('nodejs-big-picture','Node.js: The Big Picture','Paul O''Fallon', 1, 3240, true, '2019-06-03');

select * from courses where id = 'nodejs-big-picture';
```
Now insert the module data for the first two modules
```sql
insert into courses (id, module_id, module_name, module_duration)
values ('nodejs-big-picture',1,'Course Overview',70);

insert into courses (id, module_id, module_name, module_duration)
values ('nodejs-big-picture',2,'Considering Node.js',900);
```
Selecting from courses now returns both course and module data in each row
```sql
select * from courses where id = 'nodejs-big-picture';
select * from courses where id = 'nodejs-big-picture' and module_id = 2;
```
Insert the third module, but also change the name of the course.  Select all rows to show the course title changed everywhere.
```sql
insert into courses (id, name, module_id, module_name, module_duration)
values ('nodejs-big-picture', 'The Big Node.js Picture', 3, 'Thinking Asynchronously', 1304);

select * from courses where id = 'nodejs-big-picture';
```
Insert the fourth module, and fix the course name
```sql
insert into courses (id, name, module_id, module_name, module_duration)
values ('nodejs-big-picture', 'Node.js: The Big Picture', 4, 'Defining an Application and Managing Dependencies', 525);
```
Insert the remaining course module
```sql
insert into courses (id, module_id, module_name, module_duration)
values ('nodejs-big-picture', 5, 'Assembling a Development Toolset', 489);
```
The 'in' and 'order by' clauses work the same as before
```sql
select * from courses where id = 'nodejs-big-picture' and module_id in (2,3,4);

select * from courses where id = 'nodejs-big-picture' order by module_id desc;
```
Select course info, repeated based on the number of modules in the course
```sql
select id, name, author, audience, duration, cc, released from courses;
```
Now "select distinct" course info and only get one row back
```sql
select distinct id, name, author, audience, duration, cc, released from courses;
```
Select just the module information for the course
```sql
select module_id, module_name, module_duration from courses where id = 'nodejs-big-picture';
```


Load module-level course data by running a series of CQL commands from an
external file
```bash
cat data/courses2.cql | cqlsh
```

Select module information for the 'advanced-javascript' course
```sql
use pluralsight;
select module_id, module_name, module_duration from courses where id = 'advanced-javascript';
```
Select module information for the 'docker-fundamentals' course
```sql
select module_id, module_name, module_duration from courses where id = 'advanced-python';
```
Select just the course-level information for all 5 courses
```sql
select distinct id, name, author from courses;
```

## Time Series Data

Launch our one Cassandra node and (when it's ready) load our sample course data
```bash
    cat data/courses2.cql | cqlsh
```
From cqlsh, create a new table to hold course page views
```sql
use pluralsight;
create table course_page_views (
    course_id varchar,
    view_id timeuuid,
    primary key (course_id, view_id)
) with clustering order by (view_id desc);
```
Insert a row into this table, using "now()" to create a timeuuid with the current date/time.  Include a one year TTL.
```sql
insert into course_page_views (course_id, view_id)
values ('nodejs-big-picture', now()) using TTL 31536000;
```
Insert another row into the table with a manually generated v1 UUID (also with a TTL)
```sql
insert into course_page_views (course_id, view_id)
values ('nodejs-big-picture', bb9807aa-fb68-11e9-8f0b-362b9e155667) using TTL 31536000;
```
Insert two more rows using "now()"
```sql
insert into course_page_views (course_id, view_id)
values ('nodejs-big-picture', now()) using TTL 31536000;

insert into course_page_views (course_id, view_id)
values ('nodejs-big-picture', now()) using TTL 31536000;
```
Select the rows, and then use dateOf() to extract the date/time portion of the view_id
```sql
select * from course_page_views;
select dateOf(view_id) from course_page_views where course_id = 'nodejs-big-picture';
```
Reverse the date order of the results
```sql
select dateOf(view_id) from course_page_views where course_id = 'nodejs-big-picture' order by view_id asc;
```
Select only those dates based on Timeuuids that span a 2 day range
```sql
select dateOf(view_id) from course_page_views where course_id = 'nodejs-big-picture'
and view_id >= maxTimeuuid('2019-10-30 00:00+0000')
and view_id < minTimeuuid('2019-11-02 00:00+0000');

-- adjust these dates as necessary to match a more current date range
```
Truncate the table, and add a static column
```sql
truncate course_page_views;
alter table course_page_views add last_view_id timeuuid static;
```
Now insert three rows, using "now()" for both Timeuuids (with TTLs)
```sql
insert into course_page_views (course_id, last_view_id, view_id)
values ('nodejs-big-picture', now(), now()) using TTL 31536000;

insert into course_page_views (course_id, last_view_id, view_id)
values ('nodejs-big-picture', now(), now()) using TTL 31536000;

insert into course_page_views (course_id, last_view_id, view_id)
values ('nodejs-big-picture', now(), now()) using TTL 31536000;
```
Selecting all rows shows different view_ids but the same last_view_id for all rows
```sql
select * from course_page_views;
```
Use 'select distinct' to get just the latest page view for this course
```sql
select distinct course_id, last_view_id from course_page_views;
```
For just one course, this can also be accomplished with the view_id and a LIMIT clause
```sql
select course_id, view_id from course_page_views where course_id = 'nodejs-big-picture' limit 1;
```
However, a 'limit' won't work across multiple courses.  Insert multiple views for another course.
```sql
insert into course_page_views (course_id, last_view_id, view_id)
values ('advanced-javascript', now(), now()) using TTL 31536000;

insert into course_page_views (course_id, last_view_id, view_id)
values ('advanced-javascript', now(), now()) using TTL 31536000;

insert into course_page_views (course_id, last_view_id, view_id)
values ('advanced-javascript', now(), now()) using TTL 31536000;
```
Select latest view_id from each course, using the limit clause
```sql
select course_id, view_id from course_page_views where course_id = 'nodejs-big-picture' limit 1;
select course_id, view_id from course_page_views where course_id = 'advanced-javascript' limit 1;
```
Retrieve the latest course page view for all courses with 'select distinct' and the static column
```sql
select distinct course_id, last_view_id from course_page_views;
```
Select all the individual views for each course, one at a time
```sql
select course_id, view_id from course_page_views where course_id = 'nodejs-big-picture';
select course_id, view_id from course_page_views where course_id = 'advanced-javascript';
```

课程后面的东西目前用不到，到时候再接着看。
]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>cassandra</tag>
      </tags>
  </entry>
  <entry>
    <title>Lookup Bounded Memory Support</title>
    <url>/2018/09/18/design-pxengine-lookup/</url>
    <content><![CDATA[
# How Does lookup operator work now
1. Process every record that would compose to be a lookup table,  read one record at a time, and add the record to the memory (`may overflow here`). Until all the records are processed, then save the lookup table in the memory into disk. 

2. Reads the lookup table file back to memory (`may overflow here`) to setup the hash bucket for each record (set next record offset) , then write to disk again.

3. Load the entire lookup table file from the disk to memory to do lookup(`may overflow here`).

As we can see, there are 3 parts that will cause memory overflow, the main reason is that we load the **whole** lookup table into memory. 

To solve this problem, the easy and straightforward way to do is to divide the lookup table into several parts (we call this as `section`), then process these sections accordingly.

# New Design
Let’s see the diagram to illustrate the workflow:
![](https://drive.google.com/uc?id=1Dzv6L3U7TA_aqFxboe6h6mZaoH6N9UAB)

**Header**: meta data about lookup table: address, offset…
**Bucket vector**: speed up lookup by chaining the records to different bucket using hash. 

![](https://drive.google.com/uc?id=10fgUeAvzYKlp8YKlj3vjgPRypo7KUiQh)
Here assume we have 5 records and 2 buckets, so when we lookup record 4, after hashing, we search from bucket2, so we will only walk through record 2, skip record 1,3 and 5. Of course, the pointer is stored in each record, actually it’s an offset value.

**section file container**: we fix the size of section file container in memory (can be configured by client or set by some logic according to the size of memory in client machine)

## Current Design to minimize the memory usage
1.	Set a max memory cap (section file container size).

2.	Process each record as it got read in, add the size of the record, when the size will exceed or be the same as max memory cap, save the records that collected so far into a file (called section file, size of section file `<=` max memory cap size).   

3.	Repeat the step above until all records have been saved into section file. Therefore, one big lookup table file now has been divided into several chunks of section files.

4.	Process the input record and use its key to get the hash bucket number, chain the records(namely add offect in placeholder left, then we need to wirte new content back to disk, need to swap section files with writing and reading operations).

5.	when do lookup operation, hash the key and get the entry to lookup it, walk through and compare each record until find it or not, may need to read several section file from disk (now write operation).

If the size of input data `<=` 2 * available memory size (ignore other small overhead), we will only have 2 section files, the performance will be only corroded slightl(assume the read/write file operation is good).







]]></content>
      <categories>
        <category>PXEngine</category>
      </categories>
      <tags>
        <tag>pxengine</tag>
      </tags>
  </entry>
  <entry>
    <title>System Design</title>
    <url>/2019/11/29/design-system/</url>
    <content><![CDATA[
This blog is for system design, please revisit frequently to refresh. The notes are mainly from `https://www.educative.io/` and Youtube channel.

有的系统设计主要是各功能部件合理组合:
1. Design Instagarm
2. Design Dropbox
3. Design Twitter
post tweets(photos, videos), follow others, favorite tweets
generate timeline of top tweets
low latancy
highly available
consistency can take a hit

storage: text + photo + video
ingress (write): new generated storage / sec
egress (read): read volume / sec

read heavy system
data sharding: user id -> tweet id -> (creation time + tweet id, sort by time)
query all servers and aggregate

cache for hot users and tweets

4. Designing Twitter Search
5. Designing a Web Crawler (BFS, modular, url frontier, DNS, fetcher, DIS, content filter, extractor, url filter)

6. Designing Facebook Messenger
each chat server serves a bunch of users, LB maps user to it's chat server, chat server commuicate with each other to send/receive message
message handling: long polling to receive message
hashtable keep track of online user, if offline, notify delivery failure to sender
handle message order: 单独靠timestamp不行，use sequence number with every message for each user

database: support high frequence write/read row, quick small updates, range based search: HBase, column-oriented key-value NoSQL database
partition by UserID, low latency

有的主要涉及到了数据结构和算法:
1. Typeahead suggestion (trie, reference)
2. API rate limiter (dynamic sliding window)
3. Designing Facebook’s Newsfeed (offline feed generation)
contain updates, posts, video, photos from all people user follows
user average has 200 followers, 300M DAU, fetch 5 times a day, 1KB each post, so can get traffic.
cache each users' news feed in mem for quick fetch.
feed generation: 
retrieve, rank, store
offline generate by dedicated servers, `Map<UserID, LikedHashMap/TreeMap<PostID, PostItem>> + LastGenerateTime` in memory, LRU cache for user or find user's activity pattern to help generate newsfeed
feed publishing:
push to notify, pull for serving

4. Designing Yelp (querying, objects don't change often, QuadTree)
解释一下我的理解，这里partition讲的是partition quadtree.
从DB中读location id, 通过hashing map to different quadtree server (这个mapping其实就是quadtree index，可以在quadtree server fail后用来重新构造它的数据)，然后各自构造自己的quadtree.这些quadtree servers有一个aggregator server（它有自己的copies）。于是每次request要去所有quadtree server查询，然后聚合返回的数据。对于每个quadtree server，它所包含的location id也有一个本地的mapping, to know which DB servers contains this locatio id info. 这个mapping也使用的hashing实现。

5. Designing Uber backend (requirements, objects do change often, QuadTree)
6. Design Ticketmaster (first come first serve, highly concurrent, financial transactions ACID)

# CAP Theorem
CAP theorem states that it is impossible for a distributed software system to simultaneously provide more than two out of three of the following guarantees (CAP): `Consistency`, `Availability`, and `Partition tolerance`.

When we design a distributed system, trading off among `CAP` is almost the first thing we want to consider.

# Thinking process
1. requirements clarification
2. back of the envelope estimation: scale, storate, bandwidth.
3. system interface definition
4. defining data model
5. high level design
6. detailed design
7. identifying and resolving bottlenecks

# Crucial Components
这里的笔记主要根据以下几点展开:
1. Database (book: 7 weeks 7 databases)
2. Cache system (redis, memcache)
3. Message queue (kafka && zookeeper or others)
4. Load balancer (nginx, Round Robin approach)
5. Log systems
6. monitor system
6. My domain of knowledge k8s, docker, micro-services

# Key Characteristics of Distributed Systems
Scalability: scaling without performance loss (but actually will).
Reliability: keep delivering services when some components fail.
Availability: reliable means available, but not vice versa
Efficiency: latency and (throughput)bandwidth.
Manageability: ease of diagnosing and understanding problems when they occur.


# 常用技术知识
## 备份的说法:
Standby replicas
Failover to other healthy copies
Duplicates
Backup (spare)
Redundancy (redundant secondary copy)

## NoSQL Database:
[An Introduction To NoSQL Databases](https://www.youtube.com/watch?v=uD3p_rZPBUQ)
**Big Data**: social network, search engine, traditional methods of processing and storage are inadequate.

1. Key-value stores: Redis, Dynamo (redis can also be cache)
2. Document database: MongoDB, Couchbase
3. [Wide-column database](https://www.youtube.com/watch?v=8KGVFB3kVHQ): Cassandra, HBase
4. Graph database: Neo4J

Advantage of NOSQL database：
no data models(no pre-defind schema), unstructed , easy to scale up and down (horizontal data sharding), high performance with big data.

Advantage of SQL database:
relational data, normalization (eliminate redundancy), SQL, data integrity, ACID compliance.

## Consistent Hashing (with virtual replicas)
https://www.youtube.com/watch?v=ffE1mQWxyKM
Using hash `mod` strategy is not efficient, think about that add a new server, then original 20 % 3 = 2 now is 20 % 4 = 0. We have to **re-organize** all the existing mappings.

https://www.youtube.com/watch?v=zaRkONvyGr8
Consistent hashing can be used in many situations, like distributed cache, load balancing, database, etc.

For example, we have `n` servers.
Hash the request and get the location of it in the `ring`, find the server with hash value equal or larger than it and send this request to that server (clockwise move). But server may not distributed in ring evenly or the requests is not uniformly (thus server load factor is not `1/n`), so we can use **virtual replicas**, this can implement by other hash function.

With contsistent hashing, add or remove servers will not cause much overhead. The new added server will grab objects from its near servers and removed server, all original objects will move to next server after the removed one.

## Long Polling (轮询)
https://www.jianshu.com/p/d3f66b1eb748?from=timeline&isappinstalled=0
和一般的polling都属于pull(拉模式)。

> 题外话: push模式其实也是建立了一个持久的connection，但server一旦有新的信息就会push给client，而不会去在乎client的处理能力，这是一个缺点, long polling对于client要更灵活一些（因为client会request first）。

This is a variation of the traditional polling technique that allows the server to push information to a client whenever the data is available. With Long-Polling, the client requests information from the server exactly as in normal polling, but with the expectation that the server may not respond immediately (keep the connection connected). That’s why this technique is sometimes referred to as a `Hanging GET`.

Each Long-Poll request has a `timeout`. The client has to reconnect periodically after the connection is closed due to timeouts or receive the disconnect from server.

如果client突然unavailable了，如何检测呢？这个connection是如何保持的？我猜想的是connection保持期间，并不需要额外的sync查看server client是否健在(我记得TCP有一个机制会检测这个connection是否健康？)。如果server 发送了message未收到acknowledge则说明client不在了，则connection中断。

## Data Sharding
https://medium.com/@jeeyoungk/how-sharding-works-b4dec46b3f6
`Horizontal partitioning` is also called as Data Sharding

## Web Server vs Application Server
https://stackoverflow.com/questions/936197/what-is-the-difference-between-application-server-and-web-server

## proxy server
https://www.educative.io/courses/grokking-the-system-design-interview/N8G9MvM4OR2
A proxy server is an intermediate server between the client and the back-end server.

Typically, proxies are used to filter requests, log requests, or sometimes transform requests (by adding/removing headers, encrypting/decrypting, or compressing a resource). Another advantage of a proxy server is that its cache can serve a lot of requests.
1. open (forwarding) proxy: hide clients
2. reverse proxy: hide servers

## Map Reduce
We can have a Map-Reduce (MR) set-up These MR jobs will calculate frequencies of all searched terms in the past hour.

## Exponential Moving Average (EMA)
In EMA, we give more weight to the latest data. It’s also known as the exponentially weighted moving average.

# Some Design Bottlenecks
1. data compression 需要吗, 如何选择？

2. capacity estimation: metadata + content 两方面都要考虑，high level estimations 主要包括: storage for each day, storage for years, incoming bandwidth, outgoing bandwidth. 这些主要来自于: Total user, Daily active user (DAU), size of each request, how many entries each user produce, data growth, 有时对某个量单独估计比较好。

3. read heavy or wirte heavy? bandwidth, ingress: 每日新增数据总量/秒; egress: 用户浏览或下载总量/秒.
4. database需要有哪些符合场景的特点? 比如quick small updates, ACID, range based search, etc.
5. how about consider the peak time read and wirte throughput.
6. hot user in database handle, 怎么设计database去减轻这个问题.

7. we may need aggregator server for fetching and process data from different DB or caches.
8. monitoring system, collect metrics: daily peak, latency.  we will realize if we need more replication, load balancing, or caching.

9. load balancer can sit: between client and web server, web server and application server (or cahce), application server and database. load balancer can be single point of failure, need redundancy to take over when main is down.
10. load balancer: Round Robin approach, or more intelligent.

11. cache policy, LRU, 80-20 rule.


# Other System Design Videos:
## Introduce to System Design
[Introduce to System Design](https://www.youtube.com/watch?v=UzLMhqg3_Wc&list=PLrmLmBdmIlps7GJJWW9I7N0P0rB0C3eY2)
同样推荐了这本书`<<Designing Data Intensive Applications>>`, 会对这些topics有更深入的讲解。
1. ask good question:
   which features care about, which not?
   how much to scale (data, request, latency)
2. don't use buzzword (be clear about the tech you use)
3. clear and organized thinking
4. drive discussion (80% I talk)

**Things to consider**:
1. Features
2. API
3. Availability
4. Latency
5. Scalability
6. Durability
7. Class Diagram
8. Security and Privacy
9. Cost-effective

**Concepts to know**:
1. Vertical vs horizontal scaling
2. CAP theorem
3. ACID vs BASE
4. Partitioning/Sharding 
5. Consistent Hashing
6. Optimistic vs pessimistic locking
7. Strong vs eventual consistency
8. RelationalDB vs NoSQL
9. Types of NoSQL
     Key value
     Wide column
     Document-based
     Graph-based
10. Caching
11. Data center/racks/hosts
12. CPU/memory/Hard drives/Network bandwidth
13. Random vs sequential read/writes to disk
14. HTTP vs http2 vs WebSocket
15. TCP/IP model
16. ipv4 vs ipv6
17. TCP vs UDP
18. DNS lookup
19. Http & TLS
20. Public key infrastructure and certificate authority(CA)
21. Symmetric vs asymmetric encryption
22. Load Balancer
23. CDNs & Edges
24. Bloom filters and Count-Min sketch
25. Paxos 
26. Leader election
27. Design patterns and Object-oriented design
28. Virtual machines and containers
29. Pub-sub architecture 
30. MapReduce
31. Multithreading, locks, synchronization, CAS(compare and set)

**Tools**:
1. Cassandra
2. MongoDB/Couchbase
3. Mysql
4. Memcached
5. Redis
6. Zookeeper
7. Kafka
8. NGINX
9. HAProxy
10. Solr, Elastic search
11. Amazon S3
12. Docker, Kubernetes, Mesos
13. Hadoop/Spark and HDFS


## Design Spotify| Apple Muisc | Youtube Music
[Design Spotify| Apple Muisc | Youtube Music](https://www.youtube.com/watch?v=ks-CS41AiQs)
1. scope: cover and what else you are not going to cover
2. key components (具体分析了一下spotify工作的过程，比如存储，传输protocol转换，low latency, CDN)
3. data model
4. scaling

if data size is high, consider compress audio data
quality, user use different device and network condition
distribution CDN

scaling group 比如一些stateless servers可以使用k8s, containers去管理。
]]></content>
      <categories>
        <category>System Design</category>
      </categories>
      <tags>
        <tag>system design</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker USER Directive</title>
    <url>/2019/07/23/docker-USER/</url>
    <content><![CDATA[
> 09/29/2021 `docker commit` is not a good way to create docker image as it makes image size bigger, using multi-stage build instead.

From major security hardening items: `Try to explicitly set a USER/uid – to avoid even accidental startup as root`. This is a good point I agree. But in our case it's impossible to use `USER` directive with non-root user in dockerfile at beginning, as we need root user to install and configure services in containers, then alter the settings that cater to non-root user.

The solution is in the last `docker commit`, use `--change 'USER 1000'` to set default user as non-root. For example:
```bash
docker commit --change 'USER 1000' \
--change 'ENTRYPOINT ["/opt/xx/initScripts/startcontainer.sh"]' \
-c 'ENV SETUPINPROGRESS ""' \
${SERVICES_HOST} ${DOCKER_TEMPIMAGE_TAG_SERVICES}:3
```

> Note that the non-root user must **exist** in image, if it belongs to multiple groups (one primary and several supplementaries), only specify `id` is enough.

If later we need to run as root, just specify `runAsUser: 0` in K8s yaml or `--user 0` in `docker run` command, it will **overwrite** the default setting.

You can use `docker inspect` to check default `USER`:
```
docker inspect <image>:<tag> | grep -i user
```




]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Database Two-Column Table Design</title>
    <url>/2023/10/15/database-2-column-table-design/</url>
    <content><![CDATA[
This is a design and good practice about how to manage data in table for
Spanner, and you can extend it to other types of DB accordingly.

## Table Schema
Basically, the table will have 2 columns:

* `Object`: proto or encoded JSON, can be easily backward compatible in case of
adding new fields or deprecating old fields.
* `Metadata`: proto or encoded JSON, stores the info associated with the paired
Object.

For Metadata, it consists of:
* `version_id`: uint64, such as 100000000023
* `version_state`: enum
  * UNDEFINED: should never be used
  * LATEST: current object
  * ARCHIVED: stale object (must be followed by either LATEST or DELETED)
  * DELETED: deleted object
* `created_by`: string, such as bob@corp.com
* `last_update_time`: UTC, timestamp or uint64 for seconds

The primary key of the table is the combination of Metadata.verion_id and
certain fields from Object.

## How it works
The primary idea is that we never physically delete the record from the table
within the data retention period.

1. A new object foo is created by user alice@corp.com, the metadata#1 is
```bash
version_id: 100000000001
version_state: LATEST
created_by: alice@corp.com
```

2. Some fields of the object foo's are updated by leo@corp.com, the original
foo record metadata#1 will be changed to
```bash
version_id: 100000000001
version_state: ARCHIVED
created_by: alice@corp.com
```

At meanwhile, a new foo object is written into the table with the metadata#2
below:
```bash
version_id: 100000000002
version_state: LATEST
created_by: leo@corp.com
```

3. Later, user john@corp.com want to delete the foo object, the metadata#2 will
be changed to:
```bash
version_id: 100000000002
version_state: ARCHIVED
created_by: leo@corp.com
```

At meanwhile, the foo object with metadata#3 is created:
```bash
version_id: 100000000003
version_state: DELETED
created_by: john@corp.com
```

In this foo's lifecycle, we totally have 3 records and we know who did what and
when.

Please note that each operation(create, update, delete) should be in a 
`transaction` as old one(if exists) needs to be marked as ARCHIVE and new one
will be created (LATEST or DELETED), otherwise there will be duplicate records
from Object perspective although the Metadata.version_id is different.

> For DB which has changelogs for history, auditing, debugging, you may not need
this design as it compliates the client side implementation, but they may be not
that straightforward in real case for quickly checking.

## Data Access Layer
You need to create data access layer for common operations like above, the app
or service should not access the DB directly via SQL query but using the API
provided by data access layer, for example:

* list object
* upsert object
  * archive object
* delete object
  * archive object
* query object by certain fields

> For human, it is still available to run CRUD on DB directly, but should be 
cautious and with AOD(access on demand) for production.

## Base Management Class
A base management class can be created to handle common operations and specific
Object class inherits from it, for example:

## Data Retention
If physical deletion is required in terms of data retention policy, you can
have scheduled job to scan the table in off-peak time in 2 steps:

1. loop through all LATEST objects, if invalid, logically delete it in a
transaction.

2. loop through all DELETED objects, if invalid, physically delete it and all
ARCHIVED ones, no transaction is needed.
]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title>Cassandra CQLSH and CQL</title>
    <url>/2023/06/03/database-cassandra-cql/</url>
    <content><![CDATA[
Here I write down some very common CQL/CQLSH commands for quick revisit.

The statements are case-insensitive, for cql shell commands, no comma at end,
for cql, there is.

* [cqlsh](https://cassandra.apache.org/doc/latest/cassandra/tools/cqlsh.html) guide.
* [cql](https://cassandra.apache.org/doc/latest/cassandra/cql/index.html) guide.

```bash
# launch CQL shell to enter interactive mode
cqlsh <host IP>

# better format for multiple columns query
expand on

# list keyspace names
describe keyspaces
# describe shows the all table schemas in that keyspace
describe keyspace <keyspace name>

# list table names
describe tables
# describe shows the table schema, with partition key and clustering key
describe table <table name>


# entry keyspace
use <keyspace name>;

# empty table data
truncate <table name>;

# capture query result into a file
# check current capture location
capture
# off the paging in query result for capture
paging off
# set capture location
capture '~/20230603_xxx'
# turn off capture
capture off

# number of row
select count(*) from <table>;

# Allow filtering clause
# Please use single quote for text value
# The column1 is not the table partition key(because of using Allow filtering),
# then this query will do a full cluster scan, not efficient
select * from <table> where column1='column1 value' allow filtering;

# Limit output
select * from <table> limit 3;

# Order by clause, to use it, the partition key must be defined in the WHERE
# clause and the ORDER BY clause uses the clustering key for ordering.
select pkey, rkey FROM <table> WHERE pkey='xxx' order by rkey desc limit 1;

# decode blob(binary large object) from "value" column
select blobastext(value) from <table> WHERE rkey='xxx' allow filtering;

# there is also textasblob() for insert
insert into <table> (id, data) values (1,textasblob('This is a string'));

# update general fields, you need to specify partition and clustering keys to
# identify the row, the "in" operator is helpful for multiple deletion
update <table> set field1='value1',field2='value2' where <partitionKey>='xxx' and cluseringKey in (1111,2222,3333);

# Deletion only needs partition key
# If the partition key is timestamp type, it will show as
# "2023-08-21 18:28:31.489000+0000" in the table, in deletion, please remomve
# the 000 right before the +0000, otherwise it won't work:
delete from <table> where updated_at='2023-08-21 18:28:31.489+0000';
```

You can also run commands without the interactive cql shell:

```bash
cqlsh <host IP> -e "describe tables"
```
]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>cassandra</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker Base Image Choice</title>
    <url>/2021/09/24/docker-base-image/</url>
    <content><![CDATA[
最近针对目前的pipeline 有2个优化:
1. 把deployer base image 单独分出来，在这一阶段安装一些general common packages.
2. 把base image flavor 从 Alpine 换到 Debian.

对于1， 很容易理解，因为每次CI/CD pipeline 都会rebuild deployer image，大量的packages 安装工作比较耗时，如果大部分工作都在 base image中安装好了，剩余的工作量就少很多。

对于2，以前我没有想到这一点，怎么选择合适的base image 以及为什么需要切换base image呢？首先从Python image的各种类型开始了解(不同的Application 情况不同，这里只讨论Python).

[Python Image Variants](https://hub.docker.com/_/python)
There are 4 types:
- python:[version]
- python:[version]-slim
- python:[version]-alpine
- python:[version]-windowsservercore

前2种都是Debian-based, 只是内部安装的packages 数量不同，slim 只安装了运行Python所需的最小packages数量. 关于Debain release code name, such as buster, stretch. See this [reference](https://wiki.debian.org/DebianReleases).

可以查看使用的Python image Dockerfile 了解哪些packages 已经安装，避免重复，比如slim [Dockerfile](https://github.com/docker-library/python/blob/master/3.9/buster/slim/Dockerfile).

至于为什么对于Python image需要从Alpine 切换到 Debian呢？这篇文章总结到了: [Using Alpine can make Python Docker builds 50× slower](https://pythonspeed.com/articles/alpine-docker-python/). 针对我们使用的Python image, 我也做了比较，确实debian based image对于PIP requirements.txt 安装要更快，但随着Alpine base的更新以后情况可能会改变。

这篇文章所在网站的其他内容也很有参考价值:
- [Best Docker Base Image for Python Applciation 2021](https://pythonspeed.com/articles/base-image-python-docker-images/)
- [Overall articles list for Python production-ready practices](https://pythonspeed.com/docker/)

Other good articles:
- [A Comparison Of Linux Container Images](http://crunchtools.com/comparison-linux-container-images/),这篇article的table很不错。


# Dockerfile Best Practice
[Dockerfile Best Practices](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/).

```bash
# --no-cache: do not rely on build cache
# -f: specify dockerfile
# context: build context location, usually .(current dir)
docker build --no-cache -t helloapp:v2 -f dockerfiles/Dockerfile <context path>
```
Make sure do not include unnecessary files in your build context, that will result in larger image size. Or using `.dockeringore` to exclude files from build context.

Pipe in dockerfile, no files will be sent to build context:
```bash
# cannot use COPY in this way
# -: read Dockerfilr from stdin
echo -e 'FROM busybox\nRUN echo "hello world"' | docker build -
# here document
docker build -<<EOF
FROM busybox
RUN echo "hello world"
EOF
```
Omitting the build context can be useful in situations where your Dockerfile does **not** require files to be copied into the image, and improves the build-speed, as no files are sent to the daemon. 

[Multi-Stage builds](https://docs.docker.com/develop/develop-images/multistage-build/) allow you to drastically reduce the size of your final image, without struggling to reduce the number of intermediate layers and files. For [example](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#use-multi-stage-builds), the Elasticserach curator [Dockerfile](https://github.com/elastic/curator/blob/master/Dockerfile) also adopt this workflow:

- Install tools you need to build your application
- Install or update library dependencies
- Generate your application

```dockerfile
# the simplest base image
FROM scratch
```

To reduce complexity, dependencies, file sizes, and build times, avoid installing extra or unnecessary packages just because they might be “nice to have.” For example, you don’t need to include a text editor in a database image.

**Only** the instructions `RUN`, `COPY`, `ADD` create layers. Other instructions create temporary intermediate images, and do not increase the size of the build.

Sort multi-line arguments, for example debian:
```dockerfile
# Always combine RUN apt-get update with apt-get install in the same RUN
# otherwise apt-get update clause will be skipped in rebuild if no --no-cache
RUN apt-get update && apt-get install -y \
  bzr \
  cvs \
  git \
  mercurial \
  subversion \
  && rm -rf /var/lib/apt/lists/*
# clean up the apt cache by removing /var/lib/apt/lists it reduces the image size
```

## Dockerfile Clause
`LABEL` can be used to filter image with with `-f` option in `docker images` command.

Using pipe:
```bash
# Docker executes these commands using the /bin/sh -c interpreter
RUN set -o pipefail && wget -O - https://some.site | wc -l > /number

# or explicitly specify shell to support -o pipefail
RUN ["/bin/bash", "-c", "set -o pipefail && wget -O - https://some.site | wc -l > /number"]
```

`CMD` should rarely be used in the manner of CMD `["param", "param"]` in conjunction with `ENTRYPOINT`, unless you and your expected users are already quite familiar with how ENTRYPOINT works. `CMD` should almost always be used in the form of CMD `["executable", "param1", "param2"…]`.

Use `ENTRYPOINY` with `docker-entrypoint.sh` helper script is also common:
```dockerfile
COPY ./docker-entrypoint.sh /
ENTRYPOINT ['/docker-entrypoint.sh']
# will be substituted with command in docker run end
# docker run --it --rm image_name:tag <param1> <param2> ...
CMD ["--help"]
```

Each `ENV` line creates a **new** intermediate layer, just like `RUN` commands. This means that even if you unset the environment variable in a future layer, it still persists in this layer and its value can be dumped. To prevent this, and really unset the environment variable, use a `RUN` command with shell commands, to set, use, and unset the variable all in a single layer. You can separate your commands with ; or &&:
```dockerfile
# syntax=docker/dockerfile:1
FROM alpine
RUN export ADMIN_USER="mark" \
    && echo $ADMIN_USER > ./mark \
    && unset ADMIN_USER
CMD sh
```

Although `ADD` and `COPY` are functionally similar, generally speaking, `COPY` is preferred. If multiple files need to be `COPY`, copy them separately in use rather than all in one go, this can help to invalidate the cache.

Because image size matters, using `ADD` to fetch packages from remote URLs is strongly **discouraged**; you should use `curl` or `wget` instead. That way you can delete the files you no longer need after they’ve been extracted and you don’t have to add another layer in your image.

You are strongly encouraged to use `VOLUME` for any mutable and/or user-serviceable parts of your image. (I rarely use)

Avoid installing or using sudo as it has unpredictable TTY and signal-forwarding behavior that can cause problems. If you absolutely need functionality similar to sudo, such as initializing the daemon as root but running it as non-root, consider using `gosu`.

Lastly, to reduce layers and complexity, avoid switching `USER` back and forth frequently.

For clarity and reliability, you should always use absolute paths for your `WORKDIR`. 

Think of the `ONBUILD` command as an instruction the parent Dockerfile gives to the child Dockerfile.









]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker Commit</title>
    <url>/2019/10/11/docker-commit/</url>
    <content><![CDATA[
When build docker images, sometimes we need to use some files to install some packages inside container, for example when build redhat docker image: `redhat.repo`, `entitlement/` and `rpm-gpg/` are needed for package installation.

But we don't want to use `COPY` command in dockerfile to copy them into image, that will add layers to store them when run `docker build`, not safe. The solution is mount these files in `docker run`, after install then commit, `docker commit` **will not** include any data in volumes mounted inside the container.

For example:
```bash
## mount redhat repo and keys, install packages
docker run --detach \
  --name=serviceosbase \
  --user 0 \
  -v /etc/yum.repos.d/redhat.repo:/etc/yum.repos.d/redhat.repo \
  -v /etc/pki/rpm-gpg:/etc/pki/rpm-gpg \
  -v /etc/pki/entitlement:/etc/pki/entitlement \
  --entrypoint=/bin/sh \
  ${DOCKER_IMAGE_TAG}:1 \
  -c 'tail -f /dev/null'

docker exec serviceosbase /bin/sh -c "yum install -y glibc glibc-common systemd
    systemd-libs openssl-libs && yum update -y && rm -rf /var/tmp/yum-* && yum
    makecache fast"

docker commit serviceosbase ${DOCKER_IMAGE_TAG}:1
```

You can check the layers with `docker history <image>` command:
```bash
IMAGE               CREATED              CREATED BY                                      SIZE                COMMENT
1f6e112efb83        About a minute ago   /bin/sh -c #(nop)  ENV LANG=en_US.UTF-8 LANGU   0 B
6060bfb14056        About a minute ago   /bin/sh -c rm /etc/yum.repos.d/ubi.repo &&      10.83 MB
543fa76542de        2 minutes ago        /bin/sh -c #(nop)  MAINTAINER XXX               0 B
6558c4297a5d        2 minutes ago        /bin/sh -c #(nop)  LABEL name=IIS Services ve   0 B
6fecccc91c83        5 weeks ago                                                          7.06 kB
<missing>           5 weeks ago                                                          204.8 MB            Imported from -
```
Compare with dockerfile, no layer is for mount data after commit.]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker Runtime Capabilities</title>
    <url>/2019/05/15/docker-capability/</url>
    <content><![CDATA[
In my blog [`<<Linux Capability>>`](https://chengdol.github.io/2019/05/13/linux-capability/). I talk the basic and general knowlwdge about `Capability`. This blog will focus on Capability in Docker container.

In `docker run` command, there are some flags about runtime privilege and capabilities:
```
--cap-add: Add Linux capabilities
--cap-drop: Drop Linux capabilities
--privileged=false: Give extended privileges to this container
--device=[]: Allows you to run devices inside the container without the --privileged flag.
```

By default, Docker containers are **unprivileged** and cannot, for example, run a Docker daemon inside a Docker container. This is because by default a container is not allowed to access any devices (`/dev`) on host, but a “privileged” container is given access to all devices on host.

The `--privileged` flag gives **all** capabilities to the container, and it also lifts all the limitations enforced by the device cgroup controller. In other words, the container can then do almost everything that the host can do. This flag exists to allow special use-cases, like running Docker within Docker.

How to verify? you can run a busybox with `--privileged` enabled or not, first try enable it:
```
docker run --rm -it --privileged busybox sh
```
then let's check init process capabilities (busybox doesn't have `getpcaps`):
```
# cat /proc/1/status | grep -i cap

CapInh: 0000001fffffffff
CapPrm: 0000001fffffffff
CapEff: 0000001fffffffff
CapBnd: 0000001fffffffff
CapAmb: 0000000000000000
```
then decode in another machine, we can see full capabilities here:
```
# capsh --decode=0000001fffffffff

0x0000001fffffffff=cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,
cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,
cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,
cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,
cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,
cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,
35,36
```
if not enabled, only see default ones:
```
# capsh --decode=00000000a80425fb

0x00000000a80425fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,
cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_sys_chroot,cap_mknod,
cap_audit_write,cap_setfcap
```

By default, Docker has a default list of capabilities that are kept. The following table lists the Linux capability options which are allowed by default and can be dropped.

1. SETPCAP: Modify process capabilities.
2. MKNOD: Create special files using mknod(2).
3. AUDIT_WRITE: Write records to kernel auditing log.
4. CHOWN: Make arbitrary changes to file UIDs and GIDs (see chown(2)).
5. NET_RAW: Use RAW and PACKET sockets.
6. DAC_OVERRIDE: Bypass file read, write, and execute permission checks.
7. FOWNER	Bypass: permission checks on operations that normally require the file system UID of the process to match the UID of the file.
8. FSETID: Don’t clear set-user-ID and set-group-ID permission bits when a file is modified.
9. KILL: Bypass permission checks for sending signals.
10. SETGID: Make arbitrary manipulations of process GIDs and supplementary GID list.
11. SETUID: Make arbitrary manipulations of process UIDs.
12. NET_BIND_SERVICE: Bind a socket to internet domain privileged ports (port numbers less than 1024).
13. SYS_CHROOT: Use chroot(2), change root directory.
14. SETFCAP: Set file capabilities.

Further reference information is available on the [capabilities(7) - Linux man page](http://man7.org/linux/man-pages/man7/capabilities.7.html)


## Resource
[Docker run reference](https://docs.docker.com/engine/reference/run/)
[Docker security](https://docs.docker.com/engine/security/security/)]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>capability</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker Daemon Log</title>
    <url>/2019/12/02/docker-daemon-log/</url>
    <content><![CDATA[
When I was working on securing docker registry, I followed the instructions but when run `docker push` I always get `x509: certificate signed by unknown authority` error, this means the self-signed certificate is not identified by docker daemon.

This time to get more detail information, need to check the docker daemon log.

# How to Enable Debug Mode?
By default, the debug mode is off, check here to enable debugging section:
https://docs.docker.com/config/daemon/

Edit the `daemon.json` file, which is usually located in `/etc/docker/`. You may need to create this file if it is not there.
```json
{
  "debug": true
}
```
Then send a `HUP` signal to the daemon to cause it to reload its configuration. On Linux hosts, use the following command:
```
sudo kill -SIGHUP $(pidof dockerd)
```

# Where is The Log?
https://stackoverflow.com/questions/30969435/where-is-the-docker-daemon-log
```bash
Ubuntu (old using upstart ) - /var/log/upstart/docker.log
Ubuntu (new using systemd ) - sudo journalctl -fu docker.service
Amazon Linux AMI - /var/log/docker
Boot2Docker - /var/log/docker.log
Debian GNU/Linux - /var/log/daemon.log
CentOS - /var/log/daemon.log | grep docker
CoreOS - journalctl -u docker.service
Fedora - journalctl -u docker.service
Red Hat Enterprise Linux Server - /var/log/messages | grep docker
```

In Red Hat, from `/var/log/messages` file I clearly see that the docker daemon pick certificate under `/etc/docker/certs.d/<domain, no port number!>` folder.

If your OS is using `systemd`, the `journalctl` command can help, but the output from container is also dumping here, see this issue: https://github.com/moby/moby/issues/23339.

You can filter it by (works fine in Red Hat):
```bash
journalctl -fu docker _TRANSPORT=stdout + OBJECT_EXE=docker
```



]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Configuring Container DNS</title>
    <url>/2019/07/30/docker-dnsdomainname/</url>
    <content><![CDATA[
I have some doubts about `dnsdomainname` command in docker container when I developed non-root DataStage, in `Engine Conductor` tier.

References to [Configure container DNS](https://docs.docker.com/v17.09/engine/userguide/networking/default_network/configure-dns/)

How can Docker supply each container a `hostname` and `DNS configuration` without having to build a custom image with the hostname written inside? The trick is to **overlay** three crucial `/etc` files inside the container with virtual files where it can write fresh information.

In container, run:
```
# mount | grep etc

/dev/mapper/rhel-root on /etc/resolv.conf type xfs (rw,relatime,attr2,inode64,noquota)
/dev/mapper/rhel-root on /etc/hostname type xfs (rw,relatime,attr2,inode64,noquota)
/dev/mapper/rhel-root on /etc/hosts type xfs (rw,relatime,attr2,inode64,noquota)
```

Four different options affect container domain name services, please see official documentation for explanation:
* `-h HOSTNAME` or `--hostname=HOSTNAME`
* `--link=CONTAINER_NAME` or `ID:ALIAS`
* `--dns=IP_ADDRESS`
* `--dns-search=DOMAIN`
* `--dns-opt=OPTION`

So what should `dnsdomainname` return exactly? for non-root engine docker container, the hostname is `is-en-conductor-0.en-cond`, but why sometimes `dnsdomainname` return empty but sometimes return `en.cond`.

I find the return depends on `/etc/hosts` file first non-self-loop IP hostname pair, for example:
```
9.30.223.186    is-en-conductor-0.en-cond is-servicesdocker is-xmetadocker
```
the `dnsdomainname` will return `en-cond`, but if change to 
```
9.30.223.186    is-xmetadocker is-en-conductor-0.en-cond is-servicesdocker
```
the command returns empty, and if you run `hostname -f` (long host name), it returns `is-xmetadocker`.

]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker Daemon Start Failure</title>
    <url>/2022/02/27/docker-daemon-fail/</url>
    <content><![CDATA[
A strange issue happened without a clear cause and by the time I didn't fully understand why/how the solution worked, documented here for revisit if necessary.

> Note: Run docker command with sudo is not encouraged, add user to docker group is preferred.

**Issue:**
Docker daemon startup fails contineously, checking deamon status and journal:
```java
× docker.service - Docker Application Container Engine
     Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)
     Active: failed (Result: exit-code) since Mon 2022-02-28 04:42:32 UTC; 94ms ago
TriggeredBy: × docker.socket
       Docs: https://docs.docker.com
    Process: 6747 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock (code=exited, status=1/FAILURE)
   Main PID: 6747 (code=exited, status=1/FAILURE)
        CPU: 266ms

Feb 28 04:42:32 chengdolgob.c.googlers.com systemd[1]: docker.service: Scheduled restart job, restart counter is at 2.
Feb 28 04:42:32 chengdolgob.c.googlers.com systemd[1]: Stopped Docker Application Container Engine.
Feb 28 04:42:32 chengdolgob.c.googlers.com systemd[1]: docker.service: Start request repeated too quickly.
Feb 28 04:42:32 chengdolgob.c.googlers.com systemd[1]: docker.service: Failed with result 'exit-code'.
Feb 28 04:42:32 chengdolgob.c.googlers.com systemd[1]: Failed to start Docker Application Container Engine
```

First to adjust the docker daemon unit file to disabke restart by commenting out:
```ini
#TimeoutSec=0
#RestartSec=10
#Restart=always
```
Then run `systemctl daemon-reload` and restart docker daemon to see detailed error message:
```yaml
...
Feb 28 04:43:46 chengdolgob.c.googlers.com dockerd[7890]: failed to start daemon: Error initializing network controller: list bridge addresses failed: PredefinedLocalScopeDefaultNetworks List: [192.168.11.0/24]: no availabl
...
```
Check the network interface `ip a s`, there is no docker0 bridge.

**Solution:**
To fix it, create new docker network bridge, [reference ticket](https://github.com/docker/for-linux/issues/123):
```bash
# delete docker0 bridge
sudo ip link delete docker0

# 192.168.9.1/24 is from docker daemon.json bip field
sudo ip link add name docker0 type bridge
sudo ip addr add dev docker0 192.168.9.1/24
# or default
sudo ip addr add dev docker0 172.17.0.1/16
```
The inet IP is from `/etc/docker/daemon.json` bip field, for example:
```json
{
  "data-root": "/usr/local/google/docker",
  "bip": "192.168.9.1/24",
  "default-address-pools": [
    {
      "base": "192.168.11.0/24",
      "size": 24
    }
  ],
  "storage-driver": "overlay2",
  "debug": true,
  "registry-mirrors": ["https://mirror.gcr.io"]
}
```
See daemon.json attribute [description](https://docs.docker.com/engine/reference/commandline/dockerd/#daemon).

Then after bridge is created, restart docker daemon:
```bash
sudo systemctl restart docker
```]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker Compose Quick Start</title>
    <url>/2020/09/01/docker-compose-learn/</url>
    <content><![CDATA[
**Docker Compose vs Docker Swarm**
The difference lies mostly in the backend, where docker-compose deploys container on a **single** Docker host, Docker Swarm deploys it across **multiple nodes**.

现在想想，当时组员在本地搭建DataStage也应该用docker compose, it is especially good for web development:
- accelerate onboarding
- eliminate app conflicts
- environment consistency
- ship software faster


# Install
https://docs.docker.com/compose/install/
For Mac docker-compose is self-contained with Docker desktop:
```bash
## check location
which docker-compose
## show version
docker-compose version
```

For Linux, first install `Docker Engine`, then download docker-compose to executable path:
```bash
## 1.26.2 is current stable version
sudo curl -L "https://github.com/docker/compose/releases/download/1.26.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
```
or download binary from github release directly:
https://github.com/docker/compose/releases


Uninstall docker-compose, remove binary:
```
sudo rm -f /usr/local/bin/docker-compose
```

# Getting Started
https://docs.docker.com/compose/gettingstarted/
If you know how to write yaml file for Kubernetes, then quick easy to understand the `docker-compose.yml`.

Some basic commands:
```bash
## build or rebuild service images
## if not specify service name, will build all images
docker-compose build [--no-cache] [service name]

## run services
## will build images if not yet
## -d: detch
## --no-deps: just bring up specified service
docker-compose up [-d | --no-deps] [service name]

## check logs
docker-compose logs [-f] <service name>

## see what is running
docker-compose ps

## one-off command in container
docker-compose run <service name> <commands>

## start/stop service, will not remove container
docker-compose start
docker-compose stop

## remove stopped containers
docker-compose rm [service name]

## shutdown, similar to docker rm -f ...
## -v,--volumes: remove the mounted data volume
## --rmi all: remove all images
docker-compose down [-v] [--rmi all]
```

## Command Completion
https://docs.docker.com/compose/completion/#install-command-completion
for oh-my-zsh, add `docker` and `docker-compose` to plugins list in `~/.zshrc`:
```
plugins=(... docker docker-compose)
```

## Config File
遇到没见过的指令，[查阅这里](https://docs.docker.com/compose/compose-file/), see left sidebar `version 3`.

Define services relationship in `docker-compose.yml` file.
```yaml
## docker-compose.yml can parse env variables in current running environment
## 通过这些环境变量控制读取的文件类型,比如development, staging, production
## export APP_ENV=development

## 这里假设下面的文件夹和文件都存在，比如dockerfile 等等

## docker-compuse version
version: "3.7"
services:
  web:
    container_name: web
    build:
      ## relative path is base on docker-compose.yml directory
      ## can be url
      context: .
      ## args used in dockerfile, must declaure using ARG in dockerfile
      ## only seen in build process!
      args:
      - buildno=1
      ## relative path to context
      dockerfile: ./web/web.dockerfile
    ## specify built image and tag
    ## if no image here, docker-compose will use it's name convension
    image: webapp:v1
    
    ## dependencies
    ## docker-compose will start mongo and redis first
    depends_on:
    - mongo
    - redis

    ## host: container
    ## 外界访问采用host port
    ## container 之间互相访问port 不用在这里expose
    ports:
    - "80:80"
    - "443:443"
    - "9090-9100:8080-8100"
    
    ## add env vars from a file
    env_file:
    - web.env/app.${APP_ENV}.env
    - /opt/runtime_opts.env
    
    ## other form of env vars
    environment:
      RACK_ENV: development
      ## boolean muct be quoted
      SHOW: 'true'
      ## value is from current running environment
      SESSION_SECRET:

    ## short syntax
    ## https://docs.docker.com/compose/compose-file/#short-syntax-3
    volumes:
    ## host path, relative to docker-compose.yml
    - "./web:/opt/web"
    ## named volume
    - "mydata:/opt/data"

    ## overwrite WORKDIR in Dockerfile
    working_dir: /opt/web

    ## containers in the same network are reachable by others
    networks:
    - nodeapp-network
  
  mongo:
    container_name: mongo
    build:
      context: .
      dockerfile: mongo/mongo-dockerfile
    ports:
    - "27017"
    env_file:
    - mongo.env/mongo.${APP_ENV}.env
    networks:
     nodeapp-network:
       ## can be accessed by `mongo` or `db` in nodeapp-network
       aliases:
         - db

  redis:
    container_name: redis
    ## pull image from other places
    image: redis:latest

    ## long syntax
    ## https://docs.docker.com/compose/compose-file/#long-syntax-3
    volumes:
    - type: volume
      source: dbdata
      target: /data
      volume:
        nocopy: true

    ports:
    - "6379"
    env_file:
    - redis.env/redis.${APP_ENV}.env
    networks:
    - nodeapp-network

networks:
 nodeapp-network:
   ## docker defaults to use bridge driver in single host
   driver: bridge

## named volumes
volumes:
  ## default use 'local' driver
  mydata:
  dbdata:
```

More about environement variables:
By default, the docker-compose command will look for a file named `.env` in the directory you run the command. By passing the file as an argument, you can store it anywhere and name it appropriately, for example, `.env.ci`, `.env.dev`, `.env.prod`. Passing the file path is done using the `--env-file` option:
```bash
docker-compose --env-file ./config/.env.dev up 
```
`.env` contains `key=value` format equaltions.


## Storage
Image is set of read-only layers (shared), whereas container has its unique thin read write layer but it is ephemeral.

关于storage的讲解:
https://docs.docker.com/storage/
这里主要弄清楚volumes, bind mounts and tmpfs的区别和使用:

如果docker host的文件系统和docker container使用的不一样，bind mounts如何处理呢?
并且内外user, group都不一样，如果在docker container中新建一个文件，bind mounts 在host中如何映射呢?

**Docker Compose Volume:**
https://docs.docker.com/compose/compose-file/#volumes
这里讲了如何mount host path or named volumes.

### Volumes
https://docs.docker.com/storage/volumes/
类似于K8s的volume claim, general preferred.

Stored in a part of the host filesystem which is managed by Docker (`/var/lib/docker/volumes/` on Linux). Non-Docker processes should not modify this part of the filesystem. Volumes are the best way to persist data in Docker.

A given volume can be mounted into multiple containers simultaneously. When no running container is using a volume, the volume is still available to Docker and is not removed automatically. 

```bash
docker volume create <volume name>
docker volume ls
docker volume rm <volume name>
docker volume inspect
## remove unused volumes
docker volume prune
```

When you mount a volume, it may be named or anonymous.

Volumes also support the use of volume drivers, which allow you to store your data on remote hosts or cloud providers, among other possibilities.

If you need to specify volume driver options, you must use `--mount`, `-v` 的表示比较局限, 这里只是一个简单的例子, 实际上用`--mount`的配置选项很多:
```bash
## docker will create volume myvol2 automatically if it does exist
docker run -d \
  --name devtest \
  -v myvol2:/app[:ro] \
  nginx:latest

## or
docker run -d \
  --name devtest \
  --mount source=myvol2,target=/app[,readonly] \
  nginx:latest
```
If the container has files or directories in the directory to be mounted (such as `/app/` above), the directory’s contents are copied into the volume, other containers which use the volume also have access to the pre-populated content.


### Bind Mounts
https://docs.docker.com/storage/bind-mounts/
类似于K8s的hostpath, use case for example, mount source code for development.

May be stored anywhere on the host system. They may even be important system files or directories. Non-Docker processes on the Docker host or a Docker container can modify them at any time.

The file or directory does not need to exist on the Docker host already. It is created on demand if it does not yet exist.

Bind mounts are very performant, but they rely on the host machine’s filesystem having a specific directory structure available.

```bash
docker run -d \
  -it \
  --name devtest \
  -v "$(pwd)"/target:/app[:ro] \
  nginx:latest
## or
docker run -d \
  -it \
  --name devtest \
  --mount type=bind,source="$(pwd)"/target,target=/app[,readonly] \
  nginx:latest

## check Mounts section
docker inspect devtest
```
If you bind-mount into a non-empty directory on the container, the directory’s existing contents are obscured by the bind mount.


### tmpfs
https://docs.docker.com/storage/tmpfs/
Only Linux has this option, useful to temporarily store sensitive files.
Stored in the host system’s memory only, and are never written to the host system’s filesystem.

```bash
docker run -d \
  -it \
  --name tmptest \
  --tmpfs /app \
  nginx:latest
## or
docker run -d \
  -it \
  --name tmptest \
  --mount type=tmpfs,destination=/app,tmpfs-mode=1770,tmpfs-size=1024 \
  nginx:latest
```


## Network
这个章节讲到了所有docker network的类型，作用，区别:
https://docs.docker.com/network/

Docker network labs:
https://github.com/docker/labs/tree/master/networking

From docker's perspective, Steps to create a container network:
1. create a custom bridge network
```bash
## create a bridge network isolated_network
docker network create --driver bridge isolated_network

## see created network, the driver is bridge
## you can see host and none are there
docker network ls

## inspect 
docker network inspect isolated_network
```

2. run containers in the network and ping each other by container name
```bash
## create 2 busybox in the same network
docker run -d --name=test1  --net=isolated_network --entrypoint=/bin/sh busybox -c "tail -f /dev/null"
docker run -d --name=test2  --net=isolated_network --entrypoint=/bin/sh busybox -c "tail -f /dev/null"

## you can see containers in this network
docker network inspect isolated_network

## ping each other
docker exec test1 ping -c 5 test2
docker exec test2 ping -c 5 test1
```

3. remove network created
```bash
docker network rm isolated_network
docker network ls
```

同样的思路，可以用docker command查看docker-compose中建立的network的信息。

列出了docker compose中top-level networks 创建时的options, after creating top-level networks, they can be referenced by service-level to use:
https://docs.docker.com/compose/compose-file/#network-configuration-reference

这篇文章说得很明白, docker compose中network是如何作为的:
https://docs.docker.com/compose/networking/

这个例子很有意思，把frontend, backend的网络分开了, only app can reach both networks:
https://docs.docker.com/compose/networking/#specify-custom-networks
```yaml
version: "3"
services:

  proxy:
    build: ./proxy
    networks:
      - frontend
  app:
    build: ./app
    networks:
      - frontend
      - backend
  db:
    image: postgres
    networks:
      - backend

networks:
  frontend:
    # Use a custom driver
    driver: custom-driver-1
  backend:
    # Use a custom driver which takes special options
    driver: custom-driver-2
    driver_opts:
      foo: "1"
      bar: "2"
```

## Resource
类似于K8s, 也有quota的配置在deploy key下面，但是docker compose file v3 并不支持: `ignored by docker-compose up and docker-compose run`，虽然可以转换成v2，比如:
```bash
docker-compose --compatibility up
```
但是是best effort , 见这里讨论: [How to specify Memory & CPU limit in docker compose version 3](https://stackoverflow.com/questions/42345235/how-to-specify-memory-cpu-limit-in-docker-compose-version-3)


Docker Compose file version 2是支持quote设置的:
https://docs.docker.com/compose/compose-file/compose-file-v2/#cpu-and-other-resources
]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker Image Default Parameters</title>
    <url>/2019/05/15/docker-image-default/</url>
    <content><![CDATA[
This blog is a follow-up of
[`<<Docker Run Reference>>`](https://chengdol.github.io/2019/05/15/docker-run/).

When builds an image from a Dockerfile or by committing from a running
container, we can set startup parameters for the new image.

Four of the Dockerfile commands cannot be overridden at runtime: `FROM`,
`MAINTAINER`, `RUN`, and `ADD`. Everything else has a corresponding override in
`docker run` command.

# CMD
The `CMD` can be the default startup command for a container or the arguments
for entrypoint.

```bash
docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]
```
If the image has an `ENTRYPOINT` specified then the `CMD` or `COMMAND` is
appended as arguments to the `ENTRYPOINT`(see next section).

For example, overrides the `CMD` in busybox by `/bin/sh -c ls -ltr`:
```bash
docker run -it busybox /bin/sh -c ls -ltr
```

You can use inspect to check the default `CMD` in image, it shows the default
`CMD` for busybox is `[sh]`. If you override it by `/bin/sh -c ls -ltr` like
above example, then you run 
```bash
# There is also a "ContainerConfig" section, but it is not related to CMD.
docker inspect -f "{{.Config.Cmd}}" busybox
```

You can see under the `COMMAND` column, it changes to `/bin/sh -c ls -ltr`, easy
to verify.
```bash
# --no-trunc: no truncate output
docker ps -a --no-trunc
```

# ENTRYPOINT 
The `ENTRYPOINT` is the default start point of the running container.

```bash
# Overwrite the default entrypoint set by the image
--entrypoint="":
```
The `ENTRYPOINT` of an image is similar to a `COMMAND` because it specifies what
executable to run when the container starts, but it is (purposely) more
difficult to override. The `ENTRYPOINT` gives a container its default nature or
behavior, so that when you set an `ENTRYPOINT` you can run the container as if
it was that binary, complete with default options, and you can pass in more
options via the `COMMAND`.

Check the default entrypoint of a image by:
```bash
docker inspect -f "{{.Config.Entrypoint}}" <image or container>
```

To override the entrypoint as `/bin/sh` and pass parameters `tail -f /dev/null`
to it:
```bash
docker run -d \
--entrypoint=/bin/sh \
<image>:<tag> \
-c "tail -f /dev/null"
```

> NOTE: `--entrypoint` will clear out any default command in image.

# EXPOSE
The `EXPOSE` is used for incoming traffic when published.

```
--expose=[]: Expose a port or a range of ports inside the container.
             These are additional to those exposed by the `EXPOSE` instruction
-P         : Publish all exposed ports to the host interfaces
-p=[]      : Publish a container's port or a range of ports to the host
             format: ip:hostPort:containerPort | ip::containerPort | hostPort:containerPort | containerPort
               Both hostPort and containerPort can be specified as a
               range of ports. When specifying ranges for both, the
               number of container ports in the range must match the
               number of host ports in the range, for example:
                   -p 1234-1236:1234-1236/tcp

               When specifying a range for hostPort only, the
               containerPort must not be a range.  In this case the
               container port is published somewhere within the
               specified hostPort range. (e.g., `-p 1234-1236:1234/tcp`)

               (use 'docker port' to see the actual mapping)

--link=""  : Add link to another container (<name or id>:alias or <name or id>)
```

With the exception of the `EXPOSE` directive, an image developer hasn’t got much control over networking. The `EXPOSE` instruction defines the initial **incoming** ports (listens on specific network ports) that provide services. These ports are available to processes inside the container. An operator can use the --expose option to add to the exposed ports.

> NOTE: `EXPOSE` will not allow communication between container and host or
other containers from different network. To allow this you need to publish the
ports.

> NOTE: using `-P` or `-p` rather than `--net=host` for incoming traffic.

To expose a container’s internal port, using the `-P` or `-p` flag. The exposed
port is accessible by any client that can access the host.

> NOTE: in K8s, if the pods are in the same namespace, the pods can communicate
with each other, no additional config is needed except you want to access the
pods from outside of the cluster.

# USER
```
-u="", --user="": Sets the username or UID used and optionally the groupname o
                  GID for the specified command.

The followings examples are all valid:
--user=[ user | user:group | uid | uid:gid | user:gid | uid:group ]
```
root (id = 0) is the **default** user in a container. The developer can create
additional users.

# ENV
Docker automatically sets some environment variables when creating a Linux
container.

The following environment variables are set for Linux containers:
* HOME: Set based on the value of USER
* HOSTNAME: The hostname associated with the container
* PATH: Includes popular directories, for example:
  `/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin`
* TERM: xterm if the container is allocated a pseudo-TTY

Additionally, the operator can set any environment variable in the container by
using one or more `-e` flags. If the operator names an environment variable
without specifying a value, then the current value of the named variable is
populated into the container’s environment.

# VOLUME
```
-v, --volume=[host-src:]container-dest[:<options>]: Bind mount a volume.
The comma-delimited `options` are [rw|ro], [z|Z],
[[r]shared|[r]slave|[r]private], and [nocopy].
The 'host-src' is an absolute path or a name value.

If neither 'rw' or 'ro' is specified then the volume is mounted in
read-write mode.

The `nocopy` mode is used to disable automatically copying the requested volume
path in the container to the volume storage location.
For named volumes, `copy` is the default mode. Copy modes are not supported
for bind-mounted volumes.

--volumes-from="": Mount all volumes from the given container(s)
```
The volumes commands are complex enough to have their own
[documentation](https://docs.docker.com/storage/volumes/). 

The `container-dest` must always be an absolute path such as `/src/docs`. The
`host-src` can either be an absolute path or a name value. If you supply an
absolute path for the `host-src`, Docker bind-mounts to the path you specify. If
you supply a name, Docker creates a named volume by that name.

For example, you can specify either `/foo` or `foo` for a `host-src` value. If
you supply the `/foo` value, Docker creates a bind mount. If you supply the
`foo` specification, Docker creates a named volume.

# Other Resources
[Docker run reference](https://docs.docker.com/engine/reference/run/)
[Dockerfile reference](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact)
[Expose vs publish: Docker port commands explained simply](https://medium.freecodecamp.org/expose-vs-publish-docker-port-commands-explained-simply-434593dbc9a3)]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker init Process Signal Caught</title>
    <url>/2021/12/29/docker-init-process/</url>
    <content><![CDATA[
//TODO
关于init process signal handling is clear, how about init process forward signal?

[ ] ancestor ns send sigterm to container init, will it receive it or not.

首先需要理解docker container -> process in different pid namespace

需要理解几个点ancestor namespace kill child init process:
https://man7.org/linux/man-pages/man7/pid_namespaces.7.html
docker kill, docker stop, docker rm difference
https://unix.stackexchange.com/questions/509660/do-docker-container-rm-and-docker-container-kill-effectively-achieve-the-sam


from inside of container kill init process, foreground:
https://devops.stackexchange.com/questions/5613/how-to-explicitly-kill-the-process-with-pid-1-from-inside-a-container
https://docs.docker.com/engine/reference/run/#foreground
we need handler for init process
https://medium.com/@gchudnov/trapping-signals-in-docker-containers-7a57fdda7d86


how to know living process has signal handler:
https://stackoverflow.com/questions/5975315/linux-how-to-see-if-a-living-process-has-signal-handlers-set/8810790


there is a demo to write signal proxy by your own:
https://medium.com/hackernoon/my-process-became-pid-1-and-now-signals-behave-strangely-b05c52cc551c

Best practices for propagating signals on Docker
https://www.kaggle.com/residentmario/best-practices-for-propagating-signals-on-docker

## Kill init Process in Container
Inside container, PID 1 will never be killed by `kill -9 1`, but if PID 1 has registered other signal handlers then it can respond accordingly, need to check the signal bit:
```bash
# sh as PID 1
docker run -it --entrypoint=/bin/sh busybox 

# check signal bitmap PID 1
cat /proc/1/status | grep -i sigcgt
```

The output is:
```
SigCgt:	0000000000010002
```
So `sh` has 2 handlers in bit 2(SIGINT) and bit 17(SIGHLD), so for this container, it will never react to `kill 1` or `kill -9 1` as no handler registered for them.

If the init PID you use has bitmap set for SIGTERM with exit(0), then you can terminate it by `kill 1`.]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker Multi-process</title>
    <url>/2021/10/10/docker-multi-process/</url>
    <content><![CDATA[
应用容器，一般来说只有一个main serivce process(it can spawn child processes). 需要一个 init process(PID 1) 去管理 children reaping, handle signals, 也就是说, 如果你的 service process 有 fork 但是 no reaping，那么你就需要一个 init process 了，否则会造成 zombie process.

特别是在容器中使用第三方 app 的时候，不清楚对方是否会产生 child processes 或者 reaping, 所以最好使用 init process, see this [articale](https://medium.com/@BeNitinAgarwal/an-init-system-inside-the-docker-container-3821ee233f4b) and [what is the advantage of tini](https://github.com/krallin/tini/issues/8).

## Exploration
这里一篇文章关于 [run multiple services in docker](https://docs.docker.com/config/containers/multi-service_container/), the options could be:

- using [`--init`](https://docs.docker.com/engine/reference/run/#specify-an-init-process), it is `docker-init` process backed by `tini`.
```bash
# ps aux can see docker-init
docker run --init -itd nginx
```
- using wrapper script, for example, entrypoint script.
- main process along with temporary processes, set job control in wrapper script.
- install dedicated init process and config them, for example, supervisord, tini, dumb-init, etc.

For catching signals and child process reaping, if not using `tini` or other dedicated init process, you need to write code by yourself. 

## init Process
可以看看container commonly used init processes:
- [tini](https://github.com/krallin/tini)
- [dumb-init](https://github.com/Yelp/dumb-init)
- [supervisord](https://github.com/ochinchina/supervisord)

For `tini`, [using steps](https://github.com/krallin/tini#using-tini):
```dockerfile
# in alpine docker
RUN apk add --no-cache tini
# tini is now available at /sbin/tini
ENTRYPOINT ["/sbin/tini", "--"]
# or
ENTRYPOINT ["/sbin/tini", "--", "/docker-entrypoint.sh"]
```

## How tini Proxies Signal
之前看了一篇关于 Linux delivery signal 之于 container init process  的文章，提到了在 container 中 `kill 1` 的操作为什么有时会失败，然后讲了什么时候 kernel 会把信号推送到 init process，以及什么时候不会。这篇文章只提到了源码的一部分，也就是 init process(SIGNAL_UNKILLABLE) + non-default signal handler + current namespace, see the second if condition: 
https://github.com/torvalds/linux/blob/a76c3d035872bf390d2fd92d8e5badc5ee28b17d/kernel/signal.c#L79-L99
```cpp
static bool sig_task_ignored(struct task_struct *t, int sig, bool force)
{
	void __user *handler;

	handler = sig_handler(t, sig);

	/* SIGKILL and SIGSTOP may not be sent to the global init */
	if (unlikely(is_global_init(t) && sig_kernel_only(sig)))
		return true;

  /***** see this condition *****/
	if (unlikely(t->signal->flags & SIGNAL_UNKILLABLE) &&
	    handler == SIG_DFL && !(force && sig_kernel_only(sig)))
		return true;
  /*******/

	/* Only allow kernel generated signals to this kthread */
	if (unlikely((t->flags & PF_KTHREAD) &&
		     (handler == SIG_KTHREAD_KERNEL) && !force))
		return true;

	return sig_handler_ignored(handler, sig);
}
```
The emphasis is on `sigcgt` bitmask, this is correct as docker has documented here:
https://docs.docker.com/engine/reference/run/#foreground
```
A process running as PID 1 inside a container is treated specially by Linux: it ignores any signal with the default action. As a result, the process will not terminate on SIGINT or SIGTERM unless it is coded to do so.
```
也就是说，用户如果在 init process 注册了 SIGTERM handler(`sigcgt` bit set to 1) 那么 `handler == SIG_DFL` is false，所以 init process 就可以收到了.  

但问题是我查看 tini init process signal bitmask `sigcgt` is 0 for all fields, 所以 kernel 甚至都不会把信号传递过去, so how come the tini forwards signal if no signal would be delivered at all? I have opened a [question](https://github.com/krallin/tini/issues/192) regarding this.

From the author's comment, I know `The way Tini catches signals is by blocking all signals that should be forwarded to the child, and then waiting for them via sigtimedwait`. If takes a closer look at the caller of `sig_task_ignored`:
https://github.com/torvalds/linux/blob/a76c3d035872bf390d2fd92d8e5badc5ee28b17d/kernel/signal.c#L101-L120
```cpp
static bool sig_ignored(struct task_struct *t, int sig, bool force)
{
	/*
	 * Blocked signals are never ignored, since the
	 * signal handler may change by the time it is
	 * unblocked.
	 */
	if (sigismember(&t->blocked, sig) || sigismember(&t->real_blocked, sig))
		return false;

	/*
	 * Tracers may want to know about even ignored signal unless it
	 * is SIGKILL which can't be reported anyway but can be ignored
	 * by SIGNAL_UNKILLABLE task.
	 */
	if (t->ptrace && sig != SIGKILL)
		return false;

	return sig_task_ignored(t, sig, force);
}
```
You will see `Blocked signals are never ignored`! So tini will always receive the signals from kernel. 

关于 tini main loop 中的 `sigtimedwait` 其实就是block execution 等待信号的到来
https://github.com/krallin/tini/blob/378bbbc8909a960e89de220b1a4e50781233a740/src/tini.c#L501-L514
```cpp
int wait_and_forward_signal(sigset_t const* const parent_sigset_ptr, pid_t const child_pid) {
	siginfo_t sig;

	if (sigtimedwait(parent_sigset_ptr, &sig, &ts) == -1) {
		switch (errno) {
			case EAGAIN:
				break;
			case EINTR:
				break;
			default:
				PRINT_FATAL("Unexpected error in sigtimedwait: '%s'", strerror(errno));
				return 1;
		}
	} else {
```
可以参考 [signal(7)](https://man7.org/linux/man-pages/man7/signal.7.html) section `Synchronously accepting a signal` 关于 `sigtimedwait` 的讲解.

还可以通过 `sudo strace -p <pid>` 去观察 tini 是如何 forward signal 的:
```bash
# tini is init process
# bash is child
docker run --name test -itd tini_image:latest bash

# find out pid in host namespace
ps -ef | grep bash

sudo strace -p <tini pid>
sudo strace -p <child bash pid>

docker stop test
```
]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Non-Root Docker Update Summary</title>
    <url>/2019/05/06/docker-nonroot-update-summary/</url>
    <content><![CDATA[
I haven't got time to learn Docker systematically so far, but I still gain a lot from daily tasks. This post is a brief summary for what I have done to upgrade the container from owned by root to noon-root for security reason required by our customers.

I will talk the development workflow instead of the detail about how to set and modify the configurations inside the container:

I have a base image at beginning, let's say `root_is_engine.tar.gz`, load it:
```
docker load -i root_is_engine.tar.gz
```
you can check the loaded image by running:
```
docker images
```

Now I am going to create the container from this image, but wait, I don't want the container to run any script or program when it starts, what I need is it just hangs without doing anything.

That means I need to override the default `entrypoint` (entrypoint sets the command and parameters that will be executed first when spin up a container) in docker run command:
```
docker run --detach \
         --cap-add=SYS_ADMIN \
         --privileged=false --name=${ENGINE_HOST} --hostname=${ENGINE_HOST} \
         --restart=always --add-host="${SERVICES_HOST} ${DB2_XMETA_HOST} ${ENGINE_HOST}":${ENGINE_HOST_IP} \
         -p 8449:8449 \
         -v ${DEDICATED_ENGINE_VOLPATH}/${ENGINE_HOST}/EngineClients/db2_client/dsadm:/home/dsadm \
         --entrypoint=/bin/sh \
         ${DOCKER_IMAGE_TAG_ENGINE}:${DOCKER_IMAGE_VERSION} \
         -c 'tail -f /dev/null'
```

> Note that place the arguments to your entrypoint at the **end** of your docker command `-c 'tail -f /dev/null'`

Run `docker ps` command, you can see under `COMMAND` column the entrypoint is what I specified:
```
CONTAINER ID        IMAGE                   COMMAND                  CREATED             STATUS              PORTS                     NAMES
b462f6123684        is-engine-image:1       "/bin/sh -c 'tail -f..."   2 days ago          Up 2 days           0.0.0.0:8449->8449/tcp   is-en-conductor-0.en-cond

```

Then get into the container by running:
```
docker exec -it <container id or container name> [bash|sh]
```
Then if you check the process status, you can see the init process with PID #1 is running `tail` command to track `/dev/null` device file:
```
ps aux

USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.0   5968   616 ?        Ss   16:47   0:00 tail -f /dev/null
root        27  1.5  0.0  13420  1992 pts/0    Ss   16:50   0:00 bash
root        45  0.0  0.0  53340  1864 pts/0    R+   16:50   0:00 ps aux
```

OK, now I can make changes, for example, create and switch to ordinary user with specified user id and group id, grant them privileges, modify the owner and permission of some files, run and update startup script line by line to see if the applications setup correctly with non-root.

> Note if you have mount path in host machine, you may need to `chown` correct uid and gid, otherwise ordinary user in container may get permission denied issue.

After running some tests and they succeed, I need to **commit** the changes into a new image. 

* First understand how does `docker commit` work? what will be committed into new image?

  The container has a writable layer that stacks on top of the image layers. This writable layer allows you to *make changes* to the container since the lower layers in the image are read-only.

  From the docker documentation, it said: It can be useful to commit a container’s **file** changes or **settings** into a new image.

  The commit operation will **not** include any data contained in volumes mounted inside the container.

  By default, the container being committed and its processes will be paused while the image is committed. This reduces the likelihood of encountering data corruption during the process of creating the commit. If this behavior is undesired, set the `--pause` option to false.

  The `--change` option will apply Dockerfile instructions to the image that is created. Supported Dockerfile instructions: `CMD|ENTRYPOINT|ENV|EXPOSE|LABEL|ONBUILD|USER|VOLUME|WORKDIR`

* How about processes in the running container?

  When you start container from image then process will start here - processes exists **only** in executing container, when container stops there are no processes anymore - only files from container's filesystem.

> Note that before committing, you need to quiesce the services and remove the mount path content to unlink all broken symbolic links.

Also remember to put the old entrypoint back:
```
docker commit \
       --change 'ENTRYPOINT ["/bin/bash", "-c", "/opt/xx/initScripts/startcontainer.sh"]' \
       <container ID> is-engine-image:1
```

> Note that `podman` commit format is different.

> Note that you may use `bin/sh` instead of `/bin/bash`.

OK, now start to run the new image with non-root user:
```
docker run --detach \
         --user 1000 \
         --cap-add=SYS_ADMIN \
         --privileged=false --name=${ENGINE_HOST} --hostname=${ENGINE_HOST} \
         --restart=always --add-host="${SERVICES_HOST} ${DB2_XMETA_HOST} ${ENGINE_HOST}":${ENGINE_HOST_IP} \
         -p 8449:8449 \
         -v ${DEDICATED_ENGINE_VOLPATH}/${ENGINE_HOST}/EngineClients/db2_client/dsadm:/home/dsadm \
         ${DOCKER_IMAGE_TAG_ENGINE}:${DOCKER_IMAGE_VERSION}

```

Let's see the processes in the non-root container:
```
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
dsadm        1  0.0  0.0  13288  1604 ?        Ss   18:29   0:00 /bin/bash /opt/xx/initScripts/startcontainer.sh
dsadm      540  0.0  0.0   5968   620 ?        S    18:29   0:00 tail -f /dev/null
dsadm      568  0.1  0.3 2309632 24792 ?       Sl   18:29   0:00 /opt/xx/../../jdk
dsadm      589  2.5  0.0  13420  2012 pts/0    Ss   18:36   0:00 bash
dsadm      610  0.0  0.0  53340  1868 pts/0    R+   18:36   0:00 ps aux
```

If all things are in good shape, save the image into tar.gz format, of course you can use a new tag before saving:
```
docker save is-engine-image:1 | gzip > ~/nonroot_is_engine.tar.gz
```

> Note that there is a `gzip` to compress the image.
]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>entry point</tag>
        <tag>docker commit</tag>
        <tag>docker save</tag>
      </tags>
  </entry>
  <entry>
    <title>Container Namespace and Cgroup</title>
    <url>/2022/01/03/docker-ns-cgroup/</url>
    <content><![CDATA[
> Different OS the namespace/cgroup path is slightly different.

## Ubuntu
As for Ubuntu, CPU cgroup for example is from:
```bash
# if you run docker container
cd /sys/fs/cgroup/cpu/docker
```
then you will see folder named as docker container ID, for example:
```bash
cd 9a89252ea39e15c5f90cc7b1a606bc64d4acb3a50c112ab53f3e751d06ba85db

# limit: cfs_quota_us/cfs_period_us
cpu.cfs_period_us
cpu.cfs_quota_us
# request: proportional to other container CPU shares
cpu.shares
```
For other cgroups, following the similiar path pattern, for example, the memory path is `/sys/fs/cgroup/memory/docker`.

## GKE
In Google GKE node, the CPU cgroup path is like:
```bash
# burstable is a type of QoS (quality of service)
# you can see QoS in pod description status section
cd /sys/fs/cgroup/cpu/kubepods/burstable/podf13115bf-9e5b-4871-b855-d24430b31aa6/84b07d37a9f07c57fe9e642f2cd951f7821b5d2333c92eae945b99f5dd996491
```

The `84b07d37a9f07c57fe9e642f2cd951f7821b5d2333c92eae945b99f5dd996491` is container ID inside of the pod. Using `kubectl describe` can see it, then you can see the cpu limit/request source of truth.]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker Container IPC Share</title>
    <url>/2019/07/31/docker-ipc/</url>
    <content><![CDATA[
I have a post talks about IPC [`<<Linux IPC>>`](https://chengdol.github.io/2019/05/01/linux-ipc/), it's about IPC resource requirements for DB2 and how to adjust the ipc parameters permanently or temporarily. This time we face different ipc issue, interesting.

## Description
We bring up DataStage container in particular order to make them function correctly, but when Engine is running, Xmeta DB2 went down silently whihout any warning or error messages. unquiesce will make DB2 up again, but it's not stable.

This did not happen in K8s cluster, things get weired.

## Reason
We update docker run command for Xmeta and Engine with `--ipc=host`, so they will share the ipc resouces with host machine (They reside on the same machine) and each other. 

The trick is there are some `ipcrm -a` in the Engine start and quiesce scripts. So when Engine is up, it cleans host ipc resource and DB2 goes down.

we remove all `ipcrm -a` commands inside container and things get work.

Another thing is before start up the containers, **make sure the ipc is clean**, we encountered the problem that the old non-used ipc overstep in new containers, so run `ipcrm -a` in host machine to clean them.

**Why K8s does not have this problem?** Because we schedule Xmeta and Engine in different nodes, the `ipcrm -a` will not affect each other, but docker we hold all containers in one node.

Some usefully commands
```bash
ipcs -l      ## list ipc configuration

ipcrm -a     ## remove all ipc resources
ipcs -a      ## list ipc in use
```

]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>linux</tag>
        <tag>ipc</tag>
      </tags>
  </entry>
  <entry>
    <title>OOP Design</title>
    <url>/2020/03/29/design-OOD/</url>
    <content><![CDATA[
OOD 可以看成是 High Level System Design 后的具体实现。

This blog is for `Object Oriented Design`, please revisit frequently to refresh. The notes are mainly from `https://www.educative.io/`.

[OOP Design video](https://www.youtube.com/watch?v=fJW65Wo7IHI&list=PLGLfVvz_LVvS5P7khyR4xDp7T9lCk9PgE)
`OOP`: `Object-oriented programming`
`Design pattern`: singleton, factory, etc
`Concurrency`

The four principles of object-oriented programming are encapsulation, abstraction, inheritance, and polymorphism.

The process of OO analysis and design can be described as:
1. Identifying the objects in a system;
2. Defining relationships between objects;
3. Establishing the interface of each object;
4. Making a design, which can be converted to executables using OO languages.

要清楚如何定义enum, constants abstract class, interface. 如何选择他们.

`UML` stands for `Unified Modeling Language` and is used to model the Object-Oriented Analysis of a software system. UML is a way of visualizing and documenting a software system by using a collection of diagrams.

Be familiar with:
1. use case diagram (actors， 不是指所有的object: admin, customer, system...)
2. class diagram (relationships)
3. sequence diagram (emphasize on interaction between objects, time series)
4. activity diagram (emphasize on control of flow)

**Summary**:
总的来看，通过class diagram，设计好基本的abstract class, interface, base class，然后再延伸，实例化。实例化的class中设计好attributes, method的实现。

* 定义好enum, constants.

* 把不同的系统分开设计，比如notifiation, payment...

* 对于actor people，有一个account class, 被包含在person abstract class中，然后这个person被实例化为其他诸如guest, admin, member等class. (有类似关系的以此类推)

* 对于有查询需求的任务, search interface中声明search方法，然后被search的对象类implements, 在这个对象内部实现search的各种method, 通常需要与database连接。

* 可能会用到设计模式


### Design a Library Management System
Library management systems help libraries keep track of the books and their checkouts, as well as members’ subscriptions and profiles.

1. get clarity of the requirements, Be sure to ask questions to find the exact scope of the system that the interviewer has in mind

找到actors之后，围绕actors就可以提出具体要求，可以做什么事情:
member can search book by title, name, date, category, author.
book has unique id, rack number, etc.
member can checkout, reserve book copies.
days a member can keep the book.
numbers a member can checkout.
collect fine if after due date.
system can send notification to user.
...

最后确认需具体要实现什么的功能？

2. use case diagram, define top use cases
从object角度, 看看各自可以有什么操作, 对具体的实现功能，画出具体的use case diagram。

3. class diagram
有几个问题:
要从high level考虑有哪些abstract class, enum, constant, interface等。
然后各自的依赖关系画一下。

interface vs inheritace，什么时候谁合适?

在实现代码的时候，class diagram可以作为指导原则

4. activity diagram
画出具体实现**某一**功能的状态图

4. code
Since you are not required to write a fully executable code in an interview, you can assume parts of the code to interact with the database, payment system, etc..

主要写出基本的部分:
Enums and Constants:
```java
// enums
public enum BookStatus
{
  AVAILABLE,
  RESERVED,
  LOANED,
  LOST
}

public enum ReservationStatus
{
  WAITING,
  PENDING,
  CANCELED,
  NONE
}
// constants
public class Constants {
  public static final int MAX_BOOKS_ISSUED_TO_A_USER = 5;
  public static final int MAX_LENDING_DAYS = 10;
}
```

然后是abstrace class, interface 和其他继承，实现的类，根据class diagram去实现，举几个例子:
```java
// For simplicity, we are not defining getter and setter functions. The reader can
// assume that all class attributes are private and accessed through their respective
// public getter methods and modified only through their public methods function.
public abstract class Book {
  private String ISBN;
  private String title;
  private String subject;
  private String publisher;
  private String language;
  private int numberOfPages;
  private List<Author> authors;
}

public class BookItem extends Book {
  private String barcode;
  private boolean isReferenceOnly;
  private Date borrowed;
  private Date dueDate;
  private double price;
  private BookFormat format;
  private BookStatus status;
  private Date dateOfPurchase;
  private Date publicationDate;
  private Rack placedAt;

  public boolean checkout(String memberId) {
    if(bookItem.getIsReferenceOnly()) {
      ShowError("This book is Reference only and can't be issued");
      return false;
    }
    if(!BookLending.lendBook(this.getBarCode(), memberId)){
      return false;
    }
    this.updateBookItemStatus(BookStatus.LOANED);
    return true;
  }
}

public class Rack {
  private int number;
  private String locationIdentifier;
}

// interface
public interface Search 
{
  public List<Book> searchByTitle(String title);
  public List<Book> searchByAuthor(String author);
  public List<Book> searchBySubject(String subject);
  public List<Book> searchByPubDate(Date publishDate);
}

public class Catalog implements Search 
{
  private HashMap<String, List<Book>> bookTitles;
  private HashMap<String, List<Book>> bookAuthors;
  private HashMap<String, List<Book>> bookSubjects;
  private HashMap<String, List<Book>> bookPublicationDates;

  public List<Book> searchByTitle(String query) {
    // return all books containing the string query in their title.
    return bookTitles.get(query);
  }

  public List<Book> searchByAuthor(String query) {
    // return all books containing the string query in their author's name.
    return bookAuthors.get(query);
  }
}
```

### Design parking lot
https://www.youtube.com/watch?v=DSGsa0pu8-k&t=814s
评论里有一些批评意见，可以参考。

* Identify the problem scope:
do you want me to come up a system design, or class hierarchy?
or should we get into certain specific question and wirte some methods?

* How you approach the problem:
show a clear and systematic approach how you tackle this problem
how is the parking lot designed? building? open space? free or pay?
how many spots are we talking about? floors?
are there mutliple entrances or exits?
should we first fill out floor from top?
prices strategy? premium, general customer?

进口的识别系统会识别plate number + vehicle type，然后再ticket上标明应该停放到哪个building，哪一层的哪个位置。(如果有多个entrance, 则有concurrency问题需要解决)

code some part of the design? which part do you want me to implement?
We have database backend but here for Simplicity can we assume we store the data in memory?

算法去实现spot -> vehicle的分配。


1. system requirements
multi-floors
multi-entries or exits
parking ticket
pay cash or credit card
pay at info panel or at exit
display message when full
different parking spots for different type car
electric car spots, charge station support
step price hourly

2. use case diagram
get clarity what functions we need to implement.

3. class diagram
ParkingLot
ParkingFloor
ParkingSpot
Account: admin and parking attendant
ParkingTicket
Vehicle: many types
Payment: credit card or cash
ParkingRate
ParkingDisplayBoard
ParkingAttendantPortal
CustomerInfoPortal
ElectricPanel

4. code
see: https://www.educative.io/courses/grokking-the-object-oriented-design-interview/gxM3gRxmr8Z


### Design Amazon online shopping system
1. get clarity
* what scope do you want me to focus on?
* high level design or dive into a specific function component, wirte class and method.

`Objects`: guest, member, admin and system
分别讨论一下各自可以做什么操作。

几个主要的system:
register, search, review, order, shipment, payment, notification.

2. use case diagram
3. activity diagram
4. sequence diagram

#### Design shopify
[shopify eCommerce platform](https://www.youtube.com/watch?v=lEL4F_0J3l8)
类似于淘宝, 有自己的网店portal.

### Design stack overflow
Stack Overflow is one of the largest online communities for developers to learn and share their knowledge.

Users of Stack Overflow can earn reputation points and badges. For example, a person is awarded ten reputation points for receiving an “up” vote on an answer and five points for the “up” vote of a question. The can also receive badges for their valued contributions. A higher reputation lets users unlock new privileges like the ability to vote, comment on, and even edit other people’s posts.

`Actors`: admin, guest, member, system, moderator(close,delete,undelete question)

### Design a Movie Ticket Booking System

1. high level overview
movie theater -> halls -> moive -> shows
search movie
select show and boot ticket
select seat
notification
payment
**concurrency** issue when booking:
  在数据库层面实现的，没有使用Java的concurrency package
  We can use transactions in SQL databases to avoid any clashes.
  lock the rows before we update them
discount/coupon apply

2. use case diagram
3. class diagram
4. activity diagram

### Design ATM
Automated teller machine (ATM)
withdraw and deposit money

components of ATM:
card reader
kaypad
screen
Cash dispenser
Deposit slot
printer
network

checking and saving account of user
都放在**transaction中，保证原子性**:
Balance inquiry
Deposit cash
Deposit check
Withdraw cash
Transfer funds

The ATM will maintain an internal log of transactions that contains information about hardware failures; this log will be used by the ATM operator to resolve any issues.

`actors:`
operator
customer
bank manager
system

### Design an Airline Management System
This system involves the scheduling of flights, air ticket reservations, flight cancellations, customer support, and staff management. Daily flights updates can also be retrieved by using the system.

roundtrip
one way
mutli-city

回忆一下预定的界面
departing
returning

### Design a Hotel Management System
和ticket booking类似
a online portal, keep track of available rooms, book rooms and generate bill.
booking of different room types like standard, deluxe, family suite, etc
housekeeping log to keep track of all housekeeping tasks

1. use case diagram
guest
manager
system
housekeeper
Receptionist

2. class diagram
3. activity diagram

### Restaurant Management system
同上，补充几个特点:
The system allows the manager to keep track of available tables in the system as well as the reservation of tables and bill generation.
这里是西餐的形式，没人一个座位，单独点餐。

menu -> menu sections -> items
rooms -> tables (reservation or walk-in)

### Design Facebook
Facebook is an online social networking service where users can connect with other users to post and read messages. Users access Facebook through their website interface or mobile apps.


### Design LinkedIn
Similar to Facebook design but carter to professionals. 各种功能几乎就一样。

A LinkedIn member’s profile page, which emphasizes their skills, employment history, and education, has professional network news feeds with customizable modules.

1. use case diagram
member
admin
system

2. class diagram
3. activity diagram














]]></content>
      <categories>
        <category>OOP Design</category>
      </categories>
      <tags>
        <tag>OOP design</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker Entrypoint Script</title>
    <url>/2020/09/23/docker-entrypoint/</url>
    <content><![CDATA[
I have a post [`Shell Arguments Format`](https://chengdol.github.io/2019/09/17/shell-argument-format/) talked about `exec "$@"` (当时并没有在意为什么在docker中这么使用), the script `docker-entrypoint.sh` is a very common and flexible way to accept different parameters when start docker container and run application as `PID 1`, this allows the application to receive any Unix signals sent to the container (之前遇到过这个问题, 非PID 1的进程在对container 的终止signal 没有反应).

See this docker best practice about entrypoint:
https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#entrypoint

The same pattern in envoy image, see this [`docker-entrypoint.sh` link](https://github.com/envoyproxy/envoy/blob/master/ci/docker-entrypoint.sh):
```bash
#!/usr/bin/env sh
set -e

# shell parameter expansion
# if LOGLEVEL is null or unset, then assign null to it
# env variable can set from `-e` with docker run
loglevel="${LOGLEVEL:-}"

# if the first argument looks like a parameter (i.e. start with '-'), run Envoy
# ${1#-} 的意思是对于第一个positional parameter, 是不是以-开头
# ${1#-} expand 的结果是去掉最短的match部分
if [ "${1#-}" != "$1" ]; then
  # re-set $@ value
  # see below explanation
	set -- envoy "$@"
fi

# have CMD ['envoy'] in dockerfile
# this is the default parameter to entrypoint
if [ "$1" = 'envoy' ]; then
	# set the log level if the $loglevel variable is set
	if [ -n "$loglevel" ]; then
    # 更新位置参数
		set -- "$@" --log-level "$loglevel"
	fi
fi

# ENVOY_UID is the environment variables you specified
# to set envoy userid/group
if [ "$ENVOY_UID" != "0" ]; then
    if [ -n "$ENVOY_UID" ]; then
	    usermod -u "$ENVOY_UID" envoy
    fi
    if [ -n "$ENVOY_GID" ]; then
	    groupmod -g "$ENVOY_GID" envoy
    fi
    # Ensure the envoy user is able to write to container logs
    chown envoy:envoy /dev/stdout /dev/stderr

    # su-exec switch user exec
    # https://github.com/ncopa/su-exec
    su-exec envoy "${@}"
else
    # becomes PID 1
    # 注意有double quote
    exec "${@}"
fi
```

For example:
```bash
# start as bash
docker run --rm -it envoy_image_name:tag bash

# pass parameters `--help` and run
docker run --rm -it envoy_image_name:tag --help

# run as default CMD `envoy` with ENTRYPOINT
docker run --rm -it -e ENVOY_UID=1001 -e LOGLEVEL="info" envoy_image_name:tag
```

The new thing to me is `set --`, see this question:
[What does “set --” do in this Dockerfile entrypoint?](https://unix.stackexchange.com/questions/308260/what-does-set-do-in-this-dockerfile-entrypoint/308263)

The `set --` command sets the positional parameters and link new tokens with existing position parameters, The `--` is the standard "don't treat anything following this as an option"，也就是说这是要排列位置参数了，而不是重置位置参数:
```bash
set a b c
# output "a b c"
echo $1 $2 $3

# 相当于$@ = newToken "$@"
set -- newToken "$@"
# it actually exec "newToken a b c"
exec "${@}"
```

## Nginx Entrypoint
Let's see one more about [nginx docker entrypoint](https://github.com/nginxinc/docker-nginx/blob/master/entrypoint/docker-entrypoint.sh) and explain:
```bash
#!/bin/sh
# vim:sw=4:ts=4:et

set -e

# if quiet log, then redirect to /dev/null
# otherwise redirect to 1 (stdout)
if [ -z "${NGINX_ENTRYPOINT_QUIET_LOGS:-}" ]; then
    # link file descriptor 3(additional) to 1 (stdout)
    # 3 is file descriptor see below explanation

    # exec can preserve the setting
    exec 3>&1
else
    exec 3>/dev/null
fi
# why use new fd 3: 3 is used by echo later, easy to manage redirect to where

# -o: or
if [ "$1" = "nginx" -o "$1" = "nginx-debug" ]; then
    if /usr/bin/find "/docker-entrypoint.d/" -mindepth 1 -maxdepth 1 -type f -print -quit 2>/dev/null | read v; then
        # redirect message from 1 (implicitly) to 3 (stdout in essence)
        # echo >&3 facilitate reading, see below explanation
        echo >&3 "$0: /docker-entrypoint.d/ is not empty, will attempt to perform configuration"

        echo >&3 "$0: Looking for shell scripts in /docker-entrypoint.d/"
        find "/docker-entrypoint.d/" -follow -type f -print | sort -V | while read -r f; do
            case "$f" in
                *.sh)
                    # -x: file exists and executable
                    if [ -x "$f" ]; then
                        echo >&3 "$0: Launching $f";
                        "$f"
                    else
                        echo >&3 "$0: Ignoring $f, not executable";
                    fi
                    ;;
                *) echo >&3 "$0: Ignoring $f";;
            esac
        done

        echo >&3 "$0: Configuration complete; ready for start up"
    else
        echo >&3 "$0: No files found in /docker-entrypoint.d/, skipping configuration"
    fi
fi

# process replacement
exec "$@"
```

Explanation:
- [exec: process replacement and redirect fd](https://www.putorius.net/exec-command.html)
- [File descriptor 3](https://stackoverflow.com/questions/12194616/what-does-this-shell-command-mean-exec-31-logger-t-okok)
- [echo and >& the order does not matter](https://stackoverflow.com/questions/23489934/echo-2-some-text-what-does-it-mean-in-shell-scripting)]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Podman</title>
    <url>/2020/02/26/docker-podman/</url>
    <content><![CDATA[
[`Podman`](https://podman.io/) is a pod manager tool, a daemonless container engine for developing, managing, and running OCI Containers on your Linux System. Containers can either be run as **root** or in **rootless** mode. Simply put: `alias docker=podman`.

> 实际使用中发现podman commit命令和docker格式有不同，且commit后的image使用上有不正常的地方，比如HOSTNAME不见了。

[`Container tool guide`](https://github.com/containers/buildah/tree/master/docs/containertools), it shows the difference between `buildha` and `podman`:

Both Buildah and Podman are command line tools that work on OCI images and containers. The two projects differentiate in their specialization.

Buildah specializes in building OCI images. Buildah's commands replicate all of the commands that are found in a Dockerfile. Buildah’s goal is also to provide a lower level coreutils interface to build images, allowing people to build containers without requiring a Dockerfile. The intent with Buildah is to allow other scripting languages to build container images, without requiring a daemon.

Podman specializes in all of the commands and functions that help you to maintain and modify OCI images, such as pulling and tagging. It also allows you to create, run, and maintain containers created from those images.

A major difference between Podman and Buildah is their concept of a container. Podman allows users to create "traditional containers" where the intent of these containers is to be long lived. While Buildah containers are really just created to allow content to be added back to the container image. An easy way to think of it is the buildah run command emulates the RUN command in a Dockerfile while the podman run command emulates the docker run command in functionality. Because of this you cannot see Podman containers from within Buildah or vice versa.

> so buildah mainly is used to **build** images (to build images you need to run containers before commit updates), podman is used to **run** container in production environment.

In short Buildah is an efficient way to create OCI images while Podman allows you to manage and maintain those images and containers in a production environment using familiar container cli commands.

> Some commands overlaps between the projecs

Let's see some example when I was working on deploy ds assembly on portworx:
```bash
## install podman in redhat/centos
yum install podman -y

## the same as docker login
podman login -u <user name> -p <password> docker.io

## the same as docker pull/tag/push
podman pull k8s.gcr.io/pause:3.1
podman tag k8s.gcr.io/pause:3.1 <regisrty path>/pause:3.1

## --tls-verify=false 
## disable HTTPS and verify certificates when contacting registry
## you may also need is when login
podman push <registry path>/pause:3.1 --tls-verify=false
```







]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>podman</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker Registry Configure</title>
    <url>/2019/06/16/docker-registry-config/</url>
    <content><![CDATA[
From JFrog:
- A Docker `repository` is a hosted collection of tagged images that, together, create the file system for a container
- A Docker `registry` is a host that stores Docker repositories
- An `Artifactory repository` is a hosted collection of Docker repositories, effectively, a Docker registry in every way, and one that you can access transparently with the Docker client.

https://docs.docker.com/registry/configuration/

If you change the setting in docker registry running container, try
```bash
docker restart <container id>
```

]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>registry</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker Mount Access Permission Deny</title>
    <url>/2021/12/25/docker-selinux/</url>
    <content><![CDATA[
When performed the ES upgrade from a Linux jumpbox docker container, interestingly on one of the regions' jumpbox I cannot read the mounted folder and got permission denied error. This is related to Selinux setting on docker daemon.

For example, on that jumpbox:
```bash
# test is a folder in host user home I want to mount
sudo docker run \
--rm \
-v ~/test:/test \
busybox sh \
-c "ls /test"

# got access denied
ls: can't open '/test': Permission denied
```

First, verify the Selinux mode is `enforcing`, you can check by
```bash
getenforce
# enforcing
```

Then I see the docker daemon Selinux is `enabled`, this is why I get permission denied:
```bash
sudo docker info | grep Security -A5

Security Options:
 seccomp
  Profile: /etc/docker/seccomp.json
 # below keyword means Selinux is enabled
 selinux
Kernel Version: 3.10.0-1062.12.1.el7.x86_64
```

On other regions' jumpbox, although the Selinux is `enforcing` mode but the docker daemon does not enable it specifically, so I can still read/write mounted foler.

Solutions:
1. set Selinux to `permissive` mode and mount as usual
```bash
sudo setenforce 0
```

2. mount with label `Z`, see this [question](https://stackoverflow.com/questions/24288616/permission-denied-on-accessing-host-directory-in-docker)

From docker official, [Configure the selinux label](https://docs.docker.com/storage/bind-mounts/#configure-the-selinux-label)
```bash
# test is a folder in host user home I want to mount
# append Z and ro(read-only) labels
# Z: the mount is private and unshared
sudo docker run \
--rm \
-v ~/test:/test:Z,ro \
busybox sh \
-c "ls /test"
```

# Reference
[Secure your containers with SELinux](https://opensource.com/article/20/11/selinux-containers)]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker Run Reference</title>
    <url>/2019/05/15/docker-run/</url>
    <content><![CDATA[
This is a summary from [Docker run reference](https://docs.docker.com/engine/reference/run/#entrypoint-default-command-to-execute-at-runtime)

Docker runs processes in isolated containers. **A container is a process** which runs on a host. The host may be local or remote. When an operator executes `docker run`, the container process that runs is isolated in that it has its own file system, its own networking, and its own isolated process tree separate from the host.

```bash
docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]
```

The `docker run` command can override nearly all the defaults set from docker image.

Let's first see the `docker run` command I encountered:
```bash
docker run --detach \
--name=${DB2_XMETA_HOST} \
--restart=always \
--privileged=false \
--cap-add=SYS_NICE \
--cap-add=IPC_OWNER \
--cap-add=SETFCAP  \
--user 1000 \
-e MY_POD_NAMESPACE=${MY_POD_NAMESPACE} \
-e SHARED_VOL=${SHARED_REPOS_VOLPATH} \
--hostname=${DB2_XMETA_HOST} \
-p ${DB2_XMETA_PORT}:${DB2_XMETA_PORT} \
-v ${SHARED_VOL}:${SHARED_VOL} \
is-xmetadocker:11.7.1 \
```

## Detched [-d]
To start a container in detached mode, you use `-d=true` or just `-d` option. By design, containers started in detached mode exit when the root process used to run the container exits, unless you also specify the `--rm` option. If you use `-d` with `--rm`, the container is removed when it exits or when the daemon exits, whichever happens first.

> This is why we specify `tail -f /dev/null` at end of start script in container.

## Foreground
In foreground mode (the default when `-d` is not specified), docker run can start the process in the container and attach the console to the process’s standard input, output, and standard error. It can even pretend to be a TTY (this is what most command line executables expect) and pass along signals.

For interactive processes (like a shell), you must use `-it` together in order to allocate a tty for the container process. 

For example:
```
docker run -it --rm busybox /bin/sh
```
This will directly open a shell to operate on container, once exit, container will be removed.

## Name [`--name`]
Specify container name. If you do not assign a container name with the `--name` option, then the daemon generates a random string name for you. Defining a name can be a handy way to add meaning to a container.

### IPC settings [`--ipc`]
```
--ipc="MODE"  : Set the IPC mode for the container
```
IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores and message queues.

Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or through the network stack. 

```
--ipc=<Value>
```
Value	Description
* "":	Use daemon’s default.
* "none":	Own private IPC namespace, with /dev/shm not mounted.
* "private":	Own private IPC namespace.
* "shareable":Own private IPC namespace, with a possibility to share it with other containers.
* "container: <_name-or-ID_>": Join another (“shareable”) container’s IPC namespace.
* "*host": Use the host system’s IPC namespace.

If not specified, daemon default is used, which can either be `private` or `shareable`, depending on the daemon version and configuration.

If these types of applications are broken into multiple containers, you might need to share the IPC mechanisms of the containers, using "shareable" mode for the main container, and `container:<donor-name-or-ID>` for other containers.

## Network settings
```bash
--dns=[]           : Set custom dns servers for the container
--network="bridge" : Connect a container to a network
                      'bridge': create a network stack on the default Docker bridge
                      'none': no networking
                      # set this to join other's network
                      'container:<name|id>': reuse another container's network stack
                      'host': use the Docker host network stack
                      '<network-name>|<network-id>': connect to a user-defined network
--network-alias=[] : Add network-scoped alias for the container
--add-host=""      : Add a line to /etc/hosts (host:IP)
--mac-address=""   : Sets the container's Ethernet device's MAC address
--ip=""            : Sets the container's Ethernet device's IPv4 address
--ip6=""           : Sets the container's Ethernet device's IPv6 address
--link-local-ip=[] : Sets one or more container's Ethernet device's link local IPv4/IPv6 addresses
```

I meet `--add-host` flag in service docker:
```
--add-host="${SERVICES_HOST} ${DB2_XMETA_HOST} ${ENGINE_HOST}":${SERVICES_HOST_IP} \
```

## Restart policies (`--restart`)
Using the `--restart` flag on Docker run you can specify a restart policy for how a container should or should not be restarted on exit.

When a restart policy is active on a container, it will be shown as either `Up` or `Restarting` in `docker ps`.

## Exit Status
The exit code from `docker run` gives information about why the container failed to run or why it exited. When `docker run` exits with a non-zero code, the exit codes follow the chroot standard.

## Clean up [`--rm`]
By default a container’s file system persists even after the container exits. This makes debugging a lot easier (since you can inspect the final state) and you retain all your data by default. But if you are running **short-term** foreground processes, these container file systems can really pile up. If instead you’d like Docker to automatically clean up the container and remove the file system when the container exits, you can add the `--rm` flag.

## HOSTNAME [`--hostname`]
```
--hostname="xxx"		Container host name
```
Set the hostname of the container.

## Runtime privilege and Linux capabilities
I separate this section to the blog [`<<Docker Capability>>`](https://chengdol.github.io/2019/05/15/docker-capability/) since it's important to me.

## Logging drivers [`--log-driver`]
The container can have a different logging driver than the Docker daemon. Use the `--log-driver=VALUE` with the `docker run` command to configure the container’s logging driver.

Default logging driver is json format. The `docker logs` command is available only for the `json-file` and `journald` logging drivers.

## Overriding Dockerfile image defaults
I separate this section to the blog [`<<Docker Image Defaults>>`](https://chengdol.github.io/2019/05/15/docker-image-default/) since it's important to me.

## -p
Remember, the first part of the -p value is the host port and the second part is the port within the container

## VOLUME (shared filesystems)
When use `-v` option binds mount a volume from host machine to inside container, if the container originally has contents inside the mount target folder, they will all be removed when mount and replaced by contents from source host machine folder.

> Note that `docker commit` will not include any data contained in volumes mounted inside the container.

]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker Registry API</title>
    <url>/2019/06/10/docker-registry-api/</url>
    <content><![CDATA[
How to list images and tags in the docker registory? How to delete image(layers) in docker registory? These are general demands in my daily work, let's figure them out.

A brife digression: The `OpenShift` platform has web UI to deal with images in integrated docker registry (it is called `imagestream` in OpenShift), usually after you login to terminal, run `oc version` will show you the web address. You can list and delete imagestream there.

For example, I use `OpenShift` integrated docker registry and push my docker images to a project called `datastage` (I configuring the setting so other project can pull images from this project):
![](https://drive.google.com/uc?id=1T55Gro5-_WZDTkP3lNuIrDUk6C7f8qWl)

## Resurces
[Docker Registry HTTP API V2](https://docs.docker.com/registry/spec/api/)
[Registry 清理镜像](https://blog.csdn.net/qq_35904833/article/details/80807592)
[v2 Docker registry authentication](https://github.com/docker/distribution/blob/master/docs/spec/auth/token.md)
[Registry tool Git project](https://github.com/byrnedo/docker-reg-tool)
[Cleanup Your Docker Registry](https://medium.com/@mcvidanagama/cleanup-your-docker-registry-ef0527673e3a)

## Quick Set up
After installing docker, get and run docker registry from [Docker Offical Images - registry](https://hub.docker.com/_/registry).

```
docker pull registry
```
you will get:
```
docker images

REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
registry            latest              f32a97de94e1        3 months ago        25.8MB
```
then run it locally with image deletion enabled:
```
docker run -d -p 5000:5000 -e REGISTRY_STORAGE_DELETE_ENABLED=true --restart always --name registry registry
```
> To remove images, you need to setup docker registry with **delete enabled**(by default it's off), see my blog [Docker Registry Configure](https://chengdol.github.io/2019/06/16/docker-registry-config/)

```
docker ps

CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES
3021266dca1f        registry            "/entrypoint.sh /etc..."   2 seconds ago       Up 1 second         0.0.0.0:5000->5000/tcp   registry
```

Next, let's use busybox to illustrate:
```
docker pull busybox
docker tag busybox localhost:5000/busybox:v1
docker push localhost:5000/busybox:v1
```

## Insecure Docker Registry
Quick set up will give you a insecure private docker registry (means no `docker login` and use `http` to access API).

> Note that you can use `-v` option in `curl` command to get verbose message such as HEADER information.

### Check Availability
```
curl -k --head -X GET http://localhost:5000/v2/

HTTP/1.1 200 OK
Content-Length: 2
Content-Type: application/json; charset=utf-8
Docker-Distribution-Api-Version: registry/2.0
X-Content-Type-Options: nosniff
Date: Sun, 23 Jun 2019 05:20:01 GMT
```
This means registry is accessable and user has permission.

### List Images
```
curl -k -X GET http://localhost:5000/v2/_catalog

{"repositories":["busybox"]}
```
### List Image Tags
```
curl -k -X GET http://localhost:5000/v2/busybox/tags/list

{"name":"busybox","tags":["v1"]}
```

### Delete Images
Deletion of unused digests of docker images to avoid unnecessary space growth in a private docker registry

Deletion is more complicated than list, from [Deleting an Image API](https://docs.docker.com/registry/spec/api/#deleting-an-image), there are 2 main steps:

#### Delete through API
1. Get the `digest` of the image with tag `v1`
```
curl -k --head -H "Accept: application/vnd.docker.distribution.manifest.v2+json" -X GET http://localhost:5000/v2/busybox/manifests/v1

HTTP/1.1 200 OK
Content-Length: 527
Content-Type: application/vnd.docker.distribution.manifest.v2+json
Docker-Content-Digest: sha256:bf510723d2cd2d4e3f5ce7e93bf1e52c8fd76831995ac3bd3f90ecc866643aff
Docker-Distribution-Api-Version: registry/2.0
Etag: "sha256:bf510723d2cd2d4e3f5ce7e93bf1e52c8fd76831995ac3bd3f90ecc866643aff"
X-Content-Type-Options: nosniff
Date: Sun, 16 Jun 2019 20:12:07 GMT
```
> Note when deleting a manifest from a registry version 2.3 or later, the following header must be used when HEAD or GET-ing the manifest to obtain the correct digest to delete: `Accept: application/vnd.docker.distribution.manifest.v2+json`.
>
> You can refer this [Image Manifest V 2, Schema 2](https://docs.docker.com/registry/spec/manifest-v2-2/) to get more header details.

Here, we use the digest from `Docker-Content-Digest` field in the header, the vaule is `sha256:bf510723d2cd2d4e3f5ce7e93bf1e52c8fd76831995ac3bd3f90ecc866643aff`.

Actually, if the docker image is loaded, you can inspect it by:
```
docker inspect localhost:5000/busybox:v1 | less
```
There is a `RepoDigests` field that also contains the same digest:
```
...
        "RepoDigests": [
            "busybox@sha256:7a4d4ed96e15d6a3fe8bfedb88e95b153b93e230a96906910d57fc4a13210160",
            "localhost:5000/busybox@sha256:bf510723d2cd2d4e3f5ce7e93bf1e52c8fd76831995ac3bd3f90ecc866643aff"
        ],
...
```

2. Issue delete command
```
curl -k -v -X DELETE http://localhost:5000/v2/busybox/manifests/sha256:bf510723d2cd2d4e3f5ce7e93bf1e52c8fd76831995ac3bd3f90ecc866643aff

* About to connect() to localhost port 5000 (#0)
*   Trying 127.0.0.1...
* Connected to localhost (127.0.0.1) port 5000 (#0)
> DELETE /v2/busybox/manifests/sha256:bf510723d2cd2d4e3f5ce7e93bf1e52c8fd76831995ac3bd3f90ecc866643aff HTTP/1.1
> User-Agent: curl/7.29.0
> Host: localhost:5000
> Accept: */*
>
< HTTP/1.1 202 Accepted
< Docker-Distribution-Api-Version: registry/2.0
< X-Content-Type-Options: nosniff
< Date: Sun, 16 Jun 2019 20:27:05 GMT
< Content-Length: 0
```
The response `HTTP/1.1 202 Accepted` means the deletion succeeds, let's check the tag again:
```
curl -k -X GET http://localhost:5000/v2/busybox/tags/list

{"name":"busybox","tags":null}
```

> Note that if the docker registry deletion is not enabled, you will get response
`{"errors":[{"code":"UNSUPPORTED","message":"The operation is unsupported."}]}`.

#### Delete in File System
> Note this way doesn't required docker registry is deletion enabled!

Actually, docker registry stores image in `/var/lib/registry/docker/registry/v2/`, there are `blobs` and `repositories` directories. `blobs` directory is where images reside and `repositories` is where metadata and reference locate.

You need to delete two dirs if you mount docker registry storage in host:
```
rm -rf <mount path>/registry/v2/repositories/busybox/_manifests/tags/v1/index/sha256/<hash dir>

rm -rf <mount path>/registry/v2/repositories/busybox/_manifests/revisions/sha256/<hash dir>
```

At the time of deleting those dirs; the docker registry should be in read only mode. Nobody should push to registry.

#### Garbage Collection
However, the API and file system deletions above only remove the **metadata** or dereference the connection between manifest with layers data in disk, we need to run **garbage collection** in docker registry to remove layers:
```
docker exec -it registry sh
```
check space used before clean:
```
du -sch /var/lib/registry/docker/

764.0K  /var/lib/registry/docker/
764.0K  total
```
then run garbage collection:
```
bin/registry garbage-collect /etc/docker/registry/config.yml
```
> Note that `/etc/docker/registry/config.yml` is the configuration file for docker registry.

then if you check space used again
```
du -sch /var/lib/registry/docker/

8.0K    /var/lib/registry/docker/
8.0K    total
```

### Other Notice
If you have one image with multiple tags and the digests are the same, delete one of them will remove them all.

If you have one image with multiple tags and the digests are different for each tag, deletion is tag-separate.

## Secure Docker Registry
In ICP4D cluster, we use secure docker registry with `https` and login credentials. But first let's understand how to set up secure docker registry, see my blog [`<<Secure Docker Registry>>`](https://chengdol.github.io/2019/06/16/docker-secure-registry/).

login, see .docker/config
curl works?

### Check Availability
If you don't have authentication, you will get `401` Unauthorized status, for example, here `https://mycluster.icp:8500` is the private secure docker registry location: 
```
curl --head -k -X GET https://mycluster.icp:8500/v2/

HTTP/1.1 401 Unauthorized
Content-Type: application/json; charset=utf-8
Docker-Distribution-Api-Version: registry/2.0
Www-Authenticate: Bearer realm="https://mycluster.icp:8600/image-manager/api/v1/auth/token",service="token-service"
Date: Mon, 24 Jun 2019 16:14:10 GMT
Content-Length: 87
```
Here `Www-Authenticate` tells you Auth Server address.

In my `OpenShift` cluster:
```
curl -k --head -X GET https://172.30.159.11:5000/v2/

HTTP/1.1 401 Unauthorized
Content-Type: application/json; charset=utf-8
Docker-Distribution-Api-Version: registry/2.0
Www-Authenticate: Bearer realm="https://172.30.159.11:5000/openshift/token"
X-Registry-Supports-Signatures: 1
Date: Mon, 24 Jun 2019 16:37:40 GMT
Content-Length: 87

{"errors":[{"code":"UNAUTHORIZED","message":"authentication required","detail":null}]}
```
Need to apply token from Auth Server.


```
======================================================================================
I think I get stuck here...
the situation is:
1. I have docker login ability
2. where can I get the token to do API access? auth server, where, how?
3. the platform use what to secure docker registry?

icp4d cluster is more transparent the Openshfit
======================================================================================
```

```
curl -u openshift:NOyEoOrA0FDm2IgYqlHCDkDepQ7I0vw-7Sx8RzPUmzw -X GET "https://172.30.159.11:5000/openshift/token?service=172.30.159.11:5000&scope=repository:demo1-ds/busybox:pull,push"
```
https://docs.docker.com/registry/spec/auth/token/

### List Images
```

```
### List Tags
```

```

### Remove Images
```

```



]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>registry</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker Smart Use</title>
    <url>/2020/09/15/docker-smart-use/</url>
    <content><![CDATA[
这里主要记录一下docker container 一些有意思的用法。

## Version Release
可以将源码封装到 docker(tag) 或者 helm chart 中，通过check-in pre/post-CI/CD pipeline 发布，然后在 K8s or Jenkins pipeline 中使用，这样整个流程就非常规范，易于管理。


## Run in Background
之前一直通过更改`--entrypoint` 为`/bin/sh` 加上 `-c "tail -f /dev/null"`的方式，使容器在后台running，其实可以默认使用 `-d`(detach) 的方式，但必须了解entrypoint/cmd的设置，比如:
```bash
# the busybox build with CMD ["sh"], here it run as default sh
# and with -it, this container hang in background
docker run -itd busybox

# if image has entrypoint different than sh or bash, to make it idle
# we have to reset the entrypoint
docker run -itd --entrypoint=/bin/sh <image>:<tag>
```


## Wrap Dependencies
把一些需要依赖的tool build到docker image中，然后运行docker container去工作，把需要处理的资源mount到container中:
比如 helm linter cv:
```bash
docker run --rm -it \
-v pathto/chart:/chart \
cv:latest cv lint helm /chart
```
Logs may also be captured by passing an additional volume: `-v ~/path/to/logs:/tmp/cv`

再比如jsonnet，搭建本地运行环境比较困难:
```bash
# check jsonnet help
docker run --rm -it bitnami/jsonnet:latest --help

# convert Chart.yaml.jsonnet to Chart.yaml
# -V: pass parameters to chart.yaml.jsonnet
# -S: plain text display
# sed -e 's/"//g': remove double quotes, but need to quote the version number

# the entrypoint is jsonnet cli
docker run \
--rm \
--user root \
-v pathto/Chart.yaml.jsonnet:/Chart.yaml.jsonnet \
bitnami/jsonnet:latest /Chart.yaml.jsonnet -V version="1.0.0" -S | sed -e 's/"//g'
```

]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Set up Secure Docker Registry Container</title>
    <url>/2019/06/16/docker-secure-registry/</url>
    <content><![CDATA[
> 现在想想，可以使用cert-manager 以及 Let' encrypt去做，这样就不用再去配置cert trust in OS以及可以自动更新certificate~ 唉，当时完全不知道!

This post is about configuring your own secure docker registry in the form of **docker container**, check [this](https://chengdol.github.io/2020/01/06/k8s-secure-registry/) to set up a secured docker registry in K8s. 

> More about SSL please check my blog [`SSL Demystify`](https://chengdol.github.io/2019/11/29/ssl-demystify/). It contains the theory, workflow and practice.

Securing access to your docker images is paramount, the docker registry natively supports `TLS` and `basic authentication`, let's do it.

## Generate Self-signed Certificate
See [document](https://docs.docker.com/registry/deploying/#run-an-externally-accessible-registry) from docker.
```bash
mkdir -p /root/certs

## generate domain.key and self-signed domain.crt
## I use -days 3650
openssl req \
      -newkey rsa:4096 -nodes -x509 -sha256\
      -keyout certs/domain.key -out certs/domain.crt -days 3650 \
      -subj "/C=US/ST=CA/L=San Jose/O=<Company Name>/OU=Org/CN=chengdol.registry.com"
```
> Notice that the `CN=chengdol.registry.com` must be the registry access url, no port number suffix needed.

Parameters explanation from [here](https://linux.die.net/man/1/req):
```yaml
openssl req: 
  The req command primarily creates and processes certificate requests in PKCS#10 format. It can additionally create self signed certificates for use as root CAs for example.
-newkey: 
   this option creates a new certificate request and a new private key. 
  rsa:nbits: 
    where nbits is the number of bits, generates an RSA key nbits in size.
-nodes:
  if this option is specified then if a private key is created it will not be encrypted.
-x509: 
  this option outputs a self signed certificate instead of a certificate request. This is typically used to generate a test certificate or a self signed root CA .
-[digest]:
  this specifies the message digest to sign the request with (such as -md5, -sha1, -sha256)
-keyout:
  this gives the filename to write the newly created private key to.
-out:
  this specifies the output filename to write to or standard output by default.
-days:
  when the -x509 option is being used this specifies the number of days to certify the certificate for. The default is 30 days.
-subj:
  replaces subject field of input request with specified data and outputs modified request. The arg must be formatted as /type0=value0/type1=value1/type2=..., characters may be escaped by \ (backslash), no spaces are skipped.
```

There are mutli-way to do the same thing，一步一步的构造self-signed certificate:
[OpenSSL Essentials: Working with SSL Certificates, Private Keys and CSRs](https://www.digitalocean.com/community/tutorials/openssl-essentials-working-with-ssl-certificates-private-keys-and-csrs)



## Setup Secure Docker Registry
see [document](https://docs.docker.com/registry/deploying/#run-the-registry-as-a-service) from docker.

We have the `certs` folder with crt and key created by openssl.
Start the docker registry container, using TLS certificate:
```bash
docker run -d \
--restart=always \
--name registry \
-v /root/certs:/certs \
-e REGISTRY_HTTP_ADDR=0.0.0.0:443 \
-e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \
-e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \
-p 443:443 \
registry:2
```
Here we overwrite some env variables to change the default configuration.

Also follow the instruction in docker web, instruct `every` docker daemon to trust that certificate. The way to do this depends on your OS, for Linux:
```bash
mkdir -p /etc/docker/certs.d/<docker registry domain>/
## copy domain.crt (generate by openssl) to this folder on every Docker host
cp /root/certs/domain.crt /etc/docker/certs.d/<docker registry domain>/
```

> 注意，Docker官方文档中用的是`<docker registry domain>:5000/`文件夹名，但如果你配置的是443端口,则会出错，通过Docker daemon中的log，发现对于443端口，这里不需要`:5000`. 但如果设置了basic authentication且用的是5000端口，则需要了。

> 当时还发生了奇怪的事情，我发现不需要这个trust操作居然也能进行push，后来才发现原来是旧配置在docker daemon json 文件中设置了insecure registry，这样一来根本就不会检查证书了。

If you don't do this, when run docker push you will get this error:
```bash
Error response from daemon: Get https://chengdol.registry.com/v2/: x509: certificate signed by unknown authority
```

If docker still complains about the certificate when using authentication?
When using authentication, some versions of Docker also require you to trust the certificate at the OS level.

For RedHat, do:
```bash
cp certs/domain.crt /etc/pki/ca-trust/source/anchors/myregistrydomain.com.crt
update-ca-trust
```

Now you can push and pull like below, no need to specify port number, it will use 443 port:
```
docker pull ubuntu
docker tag ubuntu chengdol.registry.com/ubuntu:v1
docker push chengdol.registry.com/ubuntu:v1
docker pull chengdol.registry.com/ubuntu:v1
```

So far, secure configuration is done, now the docker registry will use `HTTPS` in `443` port to communciate with docker client. If you want to setup basic authentication, see below:

## Setup Basic Authrntication
> Warning: You cannot enable authentication that send credentials as clear text. You **must** configure `TLS` first for authentication to work.

Use `htpasswd` to create the user info:
```
mkdir -p /root/auth
htpasswd -Bbn demo demo > /root/auth/htpasswd
```

Then, we switch back to `5000` port: (注意这里没用443端口)
```
docker run -d \
-p 5000:5000 \
--restart=always \
--name registry \
-v /root/auth:/auth \
-e "REGISTRY_AUTH=htpasswd" \
-e "REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm" \
-e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \
-v /root/certs:/certs \
-e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \
-e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \
registry:2
```

Do the same trust thing in `every` docker host, under `/etc/docker/certs.d/` directory, we create a folder `<docker registry domain>:5000` and put domain.crt in it:
```bash
mkdir -p /etc/docker/certs.d/<docker registry domain>:5000/
## copy domain.crt (generate by openssl) to this folder on every Docker host
cp domain.crt /etc/docker/certs.d/<docker registry domain>:5000/
```

Then, you need to first login to push or pull:
```
docker login <docker registry domain>:5000 -u demo -p demo
```

## Conclusion
OK, now a secure docker registry container with basic authentication is up and running. You can push and pull after docker login.
]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>registry</tag>
      </tags>
  </entry>
  <entry>
    <title>Skopeo</title>
    <url>/2019/09/26/docker-skopeo/</url>
    <content><![CDATA[
`skopeo` is a command line utility that performs various operations on container images and image repositories. see it's [git repos](https://github.com/containers/skopeo). This is really a fantastic tool! Other two complementaries are `buildah` and `podman`.

Command usage see [here](https://github.com/containers/skopeo/tree/master/docs).

In Red Hat/Centos, you can use `yum` to install skopeo.

> Note the docker registry should be secured by SSL/TLS, basic authentication can also apply.

> Skopeo can do paralelly copy, you can run several skopeo processes in background then wait.

## Docker Installed
In docker environmet, skopeo will check `$HOME/.docker/config.json` file for authentication (created by docker login command). If the auth file is there, you are good and you can directly copy image tar (or gzip tar) to docker registry and copy image directly from docker registry to local docker daemon for example (can use docker hub to do test):
```
skopeo copy \
       docker-archive:<absolute or relative path>/<tar name>.tar.gz \
       docker://<registry>[:5000]/<image name>:tag

skopeo copy \
       docker://<registry>[:5000]/<image name>:tag \
       docker-daemon:<image name>:tag
```
This is extremely efficient compare to old load/tag/push method. Many other benefits like no docker daemon needed, rootless please see doc.

Inspect docker image in registry:
```
skopeo inspect \
       docker://<registry>[:5000]/<image name>:tag
```
Delete image in registry
```
skopeo delete \
       docker://<registry>[:5000]/<image name>:tag
```
> You must enable deletion in docker registry [configuration](https://docs.docker.com/registry/configuration/#override-specific-configuration-options): 

## No Docker Installed
If there is no docker daemon, skopeo will still work, but you need to explicitly give the auth creds and ssl/tls certificates path of the target registry, for example, the destination registry login user name and password are both `demo`, and the certificates path for ssl/tls is `/root/certs` (**must** include \*.key, \*.crt and \*.cert, the \*.crt and \*.cert could be the same content if it's self-signed).
```
skopeo copy \
       --dest-creds demo:demo \
       --dest-cert-dir /root/certs \
       docker-archive:<absolute or relative path>/<tar name>.tar.gz \
       docker://<registry>[:5000]/<image name>:tag

skopeo copy \
       --src-creds demo:demo \
       --src-cert-dir /root/certs \
       docker://<registry>[:5000]/<image name>:tag \
       docker-daemon:<image name>:tag
```

Inspect docker image in registry:
```
skopeo inspect \
       --creds demo:demo \
       --cert-dir /root/certs \
       docker://<registry>[:5000]/<image name>:tag
```
Delete image in registry
```
skopeo delete \
       --creds demo:demo \
       --cert-dir /root/certs \
       docker://<registry>[:5000]/<image name>:tag
```
> You must enable deletion in docker registry [configuration](https://docs.docker.com/registry/configuration/#override-specific-configuration-options): 

## Other Use Case 
One important use case is we pre-load image to target host machine because we pre-assign some application runs on some dedicated nodes. Pre-load will save much time from pull if the image is big, the application pod will up and run instantly.

So, how to copy image tarball from local to remote machine docker daemon? Yes there are `--src-daemon-host` and `--dest-daemon-host` options, but how?

Refer [document](https://docs.docker.com/install/linux/linux-postinstall/#configure-where-the-docker-daemon-listens-for-connections) from docker:
By default, the Docker daemon listens for connections on a `UNIX socket` to accept requests from local clients. It is possible to allow Docker to accept requests from remote hosts by configuring it to `listen` on an IP address and port as well as the UNIX socket.

So, let first open the port in target host:
It is conventional to use port `2375` for un-encrypted, and port `2376` for encrypted communication with the daemon.

> 注意，这里我只考虑了un-encrypted，因为设置encrypted connection可能会影响kubelet对docker的操控，我在使用完后关闭了这一端口，并且如果集群有单独的网关且访问worker nodes的IP是内部的，影响也不大。

Here we can configure remote access with **systemd unit file** or with **daemon.json**, I prefer the second one, because after systemd unit file update, I need to reload then restart docker:
```
systemctl daemon-reload
systemctl restart docker
```
Using daemon json file only need to restart, just add this line to it:
```json
{
  "hosts": ["unix:///var/run/docker.sock", "tcp://0.0.0.0:2375"]
}
```
You can listen on port `2375` on all network interfaces(eg: `0.0.0.0`), or on a particular network interface using its IP address(eg: `172.192.10.1`). 

After restart docker service, check the port status:
```
netstat -lntp | grep 2375
```

Now we can execute copy:
```
skopeo copy \
       --dest-daemon-host http://<target machine hostname or ip>:2375 \
       docker-archive:<path>/<image tarball name>.tar.gz \
       docker-daemon:<image name>:<tag>
```

Check these articles:
[Skopeo Copy to the Rescue](https://www.redhat.com/en/blog/skopeo-copy-rescue)
]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>skopeo</tag>
      </tags>
  </entry>
  <entry>
    <title>Index Lifecycle Management</title>
    <url>/2022/08/08/elastic-ILM/</url>
    <content><![CDATA[
Apart from working with data stream and rollover alias, ILM can also apply on individual indices with rollover action disabled.

# Individual Index
For [example](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/set-up-lifecycle-policy.html#apply-policy-manually), create a ILM policy that has hot(no rollover) and cold tiers and deletion phase, using this ILM policy with index template or updating on existing indices. 

Although no rollover, it is still helpful to manage the retention and data tiers of individual index.

# Switch Policy
To switch ILM policy for data stream or alias or individual, [reference](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/set-up-lifecycle-policy.html#switch-lifecycle-policies).

# Debug ILM Error
There is an example steps about debugging the ILM error, [reference](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/index-lifecycle-error-handling.html#index-lifecycle-error-handling).]]></content>
      <categories>
        <category>Elastic</category>
      </categories>
      <tags>
        <tag>elastic</tag>
        <tag>ILM</tag>
      </tags>
  </entry>
  <entry>
    <title>Bird View</title>
    <url>/2019/12/04/drone-view/</url>
    <content><![CDATA[
2019年12月4日，我拿到了DJI Mavic 2 Pro，其他配件正在陆续抵达中。很高兴，要开始一个体验生活的新分支了，并且是从一个前所未有的角度看世界。我准备用几年时间打造一个自己的channel，分享鸟瞰美景，记录足迹。很多东西要学，除了维护保养，操控，还有摄影，剪辑等等目前我也是几乎没有经验，Let's go!

在这里我会记录一些关于无人机拍摄的技巧，以及其他相关的方面，当然，还有我的作品。
我想提醒的是无人机是一件复杂设备，前期学习很重要，否则会对产品，人身安全或其他财产造成伤害。

> 02/01/2020 还未首飞😑，还在忙其他非常重要的事情！春天已经来了.
> 04/09/2020 还未首飞😌，新冠肺炎疫情爆发了，重要的事情还没忙完！唉。。。
> 05/16/2021 终于首飞了😒

# 电池指南
一定要仔细阅读大疆的电池手册，挺多注意事项的，总结一下:

## 使用注意
1. 禁止接触任何液体
2. 不要使用非大疆官方电池
3. 不要给abnormal电池充电
4. 不要在电池turn on时从无人机上安装或拆卸
5. 使用温度范围-10 ~ 40摄氏度，温度过高请自然冷却后使用
6. 远离强电磁环境，可能损害电池控制板
7. 不要重压
8. 不要exhaust电池
9. 注意电池的隔离，防止接口短路
10. 飞行前确保电池充满

## 充电注意
1. 使用大疆认证的充电器
2. 充电前请turn off电池
3. 不要在充电时离开
4. 不要飞行后立马充电，防止温度过高，理想充电温度22 ~ 28摄氏度
5. 大疆充电器会在充满后自己切断，但最好人为及时断开

## 存放注意
1. 长期存贮确保充电至40 ~ 60%
2. 不要在天热时将电池放在车内
3. 电池在10天未使用时会自动发热放电至60%
4. 每3个月至少完整充放电一次
5. 电池存储温度-10 ~ 45摄氏度
6. 长期不使用电池会休眠，充电唤醒
7. 将电池从无人机上取下单独长期存放

## 旅行注意
1. 登机前，将电池释放至30%以下，可以通过飞行消耗实现

# 安全须知

## 飞行环境
1. 飞行远离复杂电磁环境
2. 飞行高度在6000米以上可能影响性能
3. 飞行环境温度:-10 ~ 40摄氏度
4. 环境风速小于10m/s
5. 注意所在地是否允许无人机飞行

## 飞前检查
1. 检查传感器无遮挡
2. 遥控器以及无人机电池充满状态
3. 电池安装正确牢固
4. 螺旋桨臂正确展开
5. 螺旋桨无破损，安装牢固
6. 摄像头清洁无污染，伸缩旋转无阻碍
7. 仅在设备要求时校准罗盘
8. 熟悉选择的飞行模式，理解各项功能以及航行警报

## 飞行操作
1. 在智能飞行模式时，不要靠近强反射面，比如水面或雪地，传感器或受影响
2. 降落后，首先关闭引擎，然后依次关闭电池，遥控器

# Log
05/16/2021
今天跟着这个教程走了一遍: [DJI mavic 2 pro beginner guide](https://www.youtube.com/watch?v=tYrnNYgbglY).

设置好了账号个一些基本配置，然后在院子里进行了首飞。简单的take off/landing. 声音挺大的，下次去公园试试，目前降落没有掌握太好，用的是自主模式，然后就是摄像/照相还没尝试。

## 05/17/2021
charger hub will not charge batteries parallelly, one by one instead.
注意天线侧面面向无人机最好，也就是天线和display其实是有角度的。

[x] Multiple flight modes: turn on
[x] Return to home(the take off point) altitude: 30 ~ 60m 自主返回高度设定
[x] Turn off beginner mode

[x] Caliberate IMU, fold the drone (once of out the box suggested)，校准会在不同姿态下进行
[x] Enable visual obstacle avoidance, and others in advanced settings
[x] Aircraft battery settings: 15% threshold, RTH
[x] Gimbal settings: Gimbal auto caliberation
[x] file index mode: continuous 
[x] set center point: cross

## 05/18/2021
propeller 螺旋桨
直接死按下降stick 就是landing
yaw/turn left/right

左上角的gear 用来操作gimbal的上下视角, 也可以长按屏幕来控制gimbal的上下左右移动
右上角gear 用来调整shutter speed 快门速度

## 05/19/2021
Revisit what I have learned and hands-on

## 05/22/2021
Flying at park, video records, try tacker mode
[x] register the drone: FAADroneZone.FAA.gov, see [here](https://youtu.be/ToRAN1-vDTM?t=1079

## 05/27/2021
尝试了降落在手中，这样回收比较方便，特别是在地形不好的情况下。手中起飞道理相同。

]]></content>
      <categories>
        <category>Drone View</category>
      </categories>
      <tags>
        <tag>drone</tag>
      </tags>
  </entry>
  <entry>
    <title>Cerebro for ELK</title>
    <url>/2021/08/10/elastic-cerebro/</url>
    <content><![CDATA[
Cerebro ([Github Repo](https://github.com/lmenezes/cerebro)) is a Web UI to examine ELK cluster state as well as issuing REST API calls, similiar to Kibana but it is more straightforward through UIs.

Post related:
[Using Cerebro as Web UI to manage an ELK cluster](https://www.redhat.com/sysadmin/cerebro-webui-elk-cluster)

For example, I deploy one cerebro instance on dev cluster and port forward the cerebro service port to my bastion local:
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cerebro-deployment
  labels:
    app: cerebro
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cerebro
  template:
    metadata:
      labels:
        app: cerebro
    spec:
      imagePullSecrets:
      - name: <your image pull secret>
      containers:
      - name: cerebro
        # update to available docker image address and tag
        image: lmenezes/cerebro:0.9.4
        ports:
        - containerPort: 9000
        args:
        # set default elasticsearch cluster endpoint
        - -Dhosts.0.host=http://xxxx:9200
```

Then executing [port forward](https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/):
```bash
# cerebro-deployment-xxxx is the pod name
# 9123:9000: 9123 is local port, 9000 is pod port
kubectl port-forward cerebro-deployment-xxxx 9123:9000
```
Then accessing cerebro web UI from the browser by `localhost:9123`.

There are other forward formats, for example:
```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: cerebro-service
spec:
  selector:
    app: cerebro
  ports:
    - protocol: TCP
      port: 9000
      targetPort: 9000
```
```bash
# 9123: local port
# 9000: svc port
kubectl port-forward service/cerebro-service 9123:9000
```]]></content>
      <categories>
        <category>Elastic</category>
      </categories>
      <tags>
        <tag>elastic</tag>
        <tag>cerebro</tag>
      </tags>
  </entry>
  <entry>
    <title>Python Curator for Elasticsearch</title>
    <url>/2021/09/06/elastic-python/</url>
    <content><![CDATA[
Recently I was working on optimizating deletion of ES stale indices and hot shard rebalancing. 

Every weekend there is plenty of outdated index needs to be deleted, the heavy workload contributes to busy ES cluster along with Kafka topics lag, end up with PD alerts.

Also the lack of intelligence of elasticsearch could occasionally allocate lots of hot/active shards to one data node result in a hotspot with high CPU load average and IOPS.
```bash
# query hot shards distribution on data nodes
curl -s "localhost:9200/_cat/shards/<hot index pattern>?s=node:asc" > shards \
&& cat shards | awk {'print $8'} | uniq -c | sort -rn

# get hot shard number average
cat shards | awk {'print $8'} | uniq -c | sort -rn | \
awk 'BEGIN { sum = 0; count = 0 } { sum += $1; count += 1 } END { print sum / count }'
```

There are 2 basic Python modules can help:
- [elasticsearch](https://pypi.org/project/elasticsearch/), the official Python client for Elasticsearch API.
- [elasticsearch-curator](https://pypi.org/project/elasticsearch-curator/), Elasticsearch curator helps you curate, or manage your indices easily.

> [ILM and Curator](https://www.elastic.co/guide/en/elasticsearch/client/curator/current/ilm-and-curator.html), Curator will not act on any index associated with an ILM policy without setting.

For writing your own specific curator, see this [quick start](https://curator.readthedocs.io/en/latest/index.html), for instance to delete indices:
```py
import elasticsearch
import curator

es_host = "localhost:9200"
client = elasticsearch.Elasticsearch(hosts = es_host)

ilo = curator.IndexList(client)

# filter indices based on conditions
ilo.filter_by_regex(kind='regex',
                    value='^.*info.*_\d{2}\.\d{5}|^.*-\d{2}\.\d{5}')
ilo.filter_by_age(source='creation_date',
                  direction='older',
                  timestring='%Y.%m.%d',
                  unit='days',
                  unit_count=30)

delete_indices = curator.DeleteIndices(ilo)
delete_indices.do_action()
```

BTW, to list indices sorted by creation date on Kibana:
```bash
# view url parameters
GET _cat/indices?help

# specify columns and sort for all indices
GET _cat/indices?h=h,s,i,id,p,r,dc,dd,ss,creation.date.string&s=creation.date

# sort only specified indices
# the index list cannot be too long (<= 4096 bytes)
GET _cat/indices/[indices list]?s=creation.date
```

For using existing curator [docker container](https://hub.docker.com/r/bitnami/elasticsearch-curator/), the `curator` CLI is ready to use with [command syntax](https://github.com/elastic/curator/blob/master/docs/asciidoc/command-line.asciidoc) (the repo is organized bad...):
```bash
curator [--config CONFIG.YML] [--dry-run] ACTION_FILE.YML
```
There are the examples of [config.yml](https://github.com/elastic/curator/blob/master/examples/curator.yml) and [action.yml](https://github.com/elastic/curator/blob/master/examples/actions/delete_indices.yml). I find the well formated and published document is [here](https://www.elastic.co/guide/en/elasticsearch/client/curator/5.8/command-line.html)

K8s Cronjob is easy to have with docker container mentioned above (see [others example](https://medium.com/imaginelearning/a-production-elasticsearch-curator-example-ea08a41131f3)), put config and action yaml file in configMap, BTW, to [manually trigger cronjob](https://www.craftypenguins.net/how-to-trigger-a-kubernetes-cronjob-manually/) for testing purpose:
```bash
kubectl create job --from=cronjob/<cj name> <cj name>-manual-0001
```

Some useful cronJob settings:
```yaml
spec:
  # cron explain: https://crontab.guru/#15_6-23/2_*_*_*
  schedule: "15 3-23/2 * * *"
  successfulJobsHistoryLimit: 5
  failedJobsHistoryLimit: 3
```






]]></content>
      <categories>
        <category>Elastic</category>
      </categories>
      <tags>
        <tag>elastic</tag>
      </tags>
  </entry>
  <entry>
    <title>Data Stream, ILM and Data Tiers</title>
    <url>/2022/08/05/elastic-data-stream/</url>
    <content><![CDATA[
At the time of this writing, we use Elasticsearch version 7.16.2, the reference is based on this version and the content may be subject to change.

# Demo
A quick docker compose [setup](https://github.com/chengdol/InfraTree/tree/master/docker-elasticsearch#hotwarmcold-tiers) to play with data stream and observe how ILM behaves.

A more [detailed way](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/set-up-a-data-stream.html) to set up and use data stream and ILM from official document.

> In reality, consider the tier size in terms of multiple factors: disk space usage, CPU LA, CPU usage, for example, when the disk space usage is low but the CPU LA could beyond the vcpu limits, then you should not shrink the tier aggressively.

# Data Tiers
Details please see [Data Management](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/data-management.html) explanation.

Data tiers is automatically ingetrated with Data Stream, to move cold indices to less performance and cost hardware, as the docker compose setup demo shows.

Things to highlight:
- The content tier is required. System indices and other indices that aren’t part of a data stream are automatically allocated to the content tier.
- The hot tier is required. New indices that are part of a data stream are automatically allocated to the hot tier.

## Decommission Data Nodes
[Cluster-level shard allocation filtering](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/modules-cluster.html#cluster-shard-allocation-filtering).

To decommission data node from tiers, first drain all shards from it:
```bash
# multiple ips separated by comma
PUT _cluster/settings
{
  "transient": {
    "cluster.routing.allocation.exclude._ip" : "<target data node ips>"
  }
}
```
Then check allocation to make sure 0 shard resides:
```bash
GET _cat/allocation?v
```
Then revert the cluster level transient setting:
```bash
PUT _cluster/settings
{
  "transient": {
    "cluster.routing.allocation.exclude._ip" : null
  }
}
```

# Data Stream
[Data Stream](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/data-streams.html) is well-suited for logs, events, metrics, and other continuously generated append-only data.

Data stream consists of **hidden** backing indices, not to be confused with the hidden data stream. To list all hidden or non-hidden data streams:
```bash
GET _data_stream?expand_wildcards=hidden
```
Hidden data streams are usually for facilitating purpose, we don't use it.

The backing index name pattern:
```
.ds-<data-stream>-<yyyy.MM.dd>-<generation>
```

The same index template can be used for **multiple** data streams, the index patterns in settings can use regexp to extend match:
```js
"index_patterns" : ["apple-*"]
```
Then `apple-green` and `apple-yellow` are 2 data streams.

One data stream only has **one** wirte index.

# ILM
[ILM: Manage the index lifecycle](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/index-lifecycle-management.html), ILM is tightly working with Data Steam as the docker compose setup demo shows.

One thing is worth to notice is the age of shard, for instance, the write index at hot tier(always start at hot) is named as `.ds-example-2022.07.26-000002`, at the time of rollover the age of it is **reset** to 0. If from hot to cold tier the min age is 7 days, let's say the rollover is at `2022.08.01`, then the shift to cold tier of this shard will be on `2022.08.08`, instead of `2022.08.02`(2022.07.26 + 7 days).

So, to transition backing index to next tier right after rollover, the window should be set as [0 days](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/ilm-index-lifecycle.html#ilm-phase-transitions).

The age of the shard can be examined by ILM explain [API](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/ilm-explain-lifecycle.html), and list of other important fields:
```bash
GET .ds-example-2022.07.26-000002/_ilm/explain
```

The update on working ILM policy has some [limitations](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/update-lifecycle-policy.html#ilm-apply-changes):
- If changes can be safely applied, ILM updates the cached phase definition. If they cannot, phase execution continues using the previous cached definition.
- Changes to `min_age` are not propagated to the cached definition. Changing a phase’s `min_age` does not affect indices that are currently executing that phase.
- When you apply a different policy to a managed index, the index completes the current phase using the cached definition from the previous policy. The index starts using the new policy when it moves to the next phase.

]]></content>
      <categories>
        <category>Elastic</category>
      </categories>
      <tags>
        <tag>elastic</tag>
        <tag>datastream</tag>
      </tags>
  </entry>
  <entry>
    <title>CNBC News</title>
    <url>/2023/07/23/english-cnbc/</url>
    <content><![CDATA[
I started listening CNBC short news while on shuttle to company at morning. I
think it is good idea to note down new things.

#### [Ether reclaims $1900, and Nasdaq halts crypto custody plans](https://youtu.be/mvKhh7jKqdk)
* reclaim: recover, obtain the return of sth previously lost, given.
* custody/ˈkʌstədi/: protective care
* second straight /stret/ day
* money laundering
* bipartisan: agreement or cooperation of two political parties, bipartisan approval
* senator: a member of senate
* don't hear back **right away**: immediately
* legislation: laws
* coincide/ˌkoʊɪnˈsaɪd/: occur at or during the same time. publication is timed
to coincide with a major exhibition. n: by coincidence.
* maxmium publicity: attention given by the media
* extravagant/ɪkˈstrævəɡənt/: cost too much money
* multi-million: millions
* leaving ... in the dust: outpaces others or leaves them far behind in terms of
progress, quality, or success.
* fraudster: a person who commits fraud, especially in business dealings.
* self-policing: the process of keeping order or maintaining control within a
community without accountability or reference to an external authority 自我监督


#### [How EV range is determined and why the process is flawed](https://youtu.be/LuyHxV1drtY)
* tailpipe
* range anxiety
* critic/ˈkrɪtɪk/: a person who expresses an unfavorable opinion of something.
* steady-state: an unvarying condition in a physical process
* prohibitive: forbidding or restricting something
* EV regenerative braking/breɪk/

#### [How Big Banks Like JPMorgan And Citi Want To Put Wall Street On A Blockchain](https://youtu.be/fppTadeCzMA)
* goldman sachs
* 24x7x365: x does not pronounce
* turbocharge the financial system
* killer app: extremely positive impact and can essentially "kill off" the
competition.
* proponent says it ...: person who advocate/ˈædvəkeɪt/ a proposal
* tightens crackdown on ...: severe measures to restrict

#### [How China Is Threatening U.S. GPS Dominance](https://youtu.be/J064wmGKEZg)
* BeiDou: big dipper
* drone: /droʊn/
* GPS(global positioning system) constellation: /ˌkɑːnstəˈleɪʃn/
* the tech also becomes **indispensable** to lives of civilians /səˈvɪliən/, with
cases going far beyond...
* over the course of the past decade, ...
* this is a rapid advancement: 发展
* China's BeiDou is the newest **entrant** among this system.
* you the user are agnostic /æɡˈnɑːstɪk/ to the GPS selected by the phone.
* Taiwan strait /streɪt/ crisis
* Chinese state media
* real wake up call
* surveillance /sɜːrˈveɪləns/: close observation, especially of a suspected spy
or criminal.
* subsidizing ground infrastructure investment for Beidou adoption in the area
where is not well served by GPS.
* system being used in a much more **pervasive** /pərˈveɪsɪv/ way.
* self-driving or autonomous /ɔːˈtɑːnəməs/ car

#### [How To Fix America's Labor Shortage](https://youtu.be/YVTxute2lhA)
* technician/labor shortage.
* the pandemic **exacerbated** /ɪɡˈzæsərbeɪt/ the problem.
* apprentice  /əˈprentɪs/, **apprenticeship** /əˈprentɪʃɪp/ program: an
arrangement in which someone learns an ari, trade or job under a more experienced person.
* concerted action: jointly arranged, planned, or carried out; coordinated.
* the US economy is quickly approaching a **make or break** moment.

#### [What’s Going On With Reddit?](https://youtu.be/h6JJSkMGdZ0)
* news, memes /mi:m/
* Y combinator
* cracking down hate speech
* purge of offensive content, bans racist communities 
* double down: double a bet

#### [The End Of Free Money At The Federal Reserve](https://youtu.be/kuNCfkDtPNk)
* federal /ˈfedərəl/ reserve.
* deflation: reduction of the general level of prices in an economy.
* long-feared corporate debt **woes**/woʊ/ start to **hit home**击中要害: troubles.
* monetary/ˈmɑːnɪteri/ tightening and U.S. bank fragility in 2023.

#### [Where Thousands Of Tech Workers Went After Mass Layoffs](https://youtu.be/JmD9rohDnjQ)
* Tech **sector**: an area or portion that is distinct from others
* new college **grads**
* known for its boom and bust cycles
* the industry has faced **headwinds** related to crypto crash and the failure
of Silicon Valley Bank.
* a sense of optimism
* a **handful** of coastal /ˈkoʊstl/ areas 
* Google parent Alphabet
* **trim/trimmed** the rosters: another way to express layoff
* downturn: a decline in economic, business, or other activity: a downturn in the
housing market.
* companys **overbuild** during the pandemic
* Holy crap(天哪): an informal and somewhat stronger way of expressing surprise,
astonishment, or disbelief
* I have longevity /lɔːnˈdʒevəti/ in this company.
* the **blow** has been cushioned/ˈkʊʃn/ in many cases by generous
severance/'sɛvərəns/ packages.
* absurd /əbˈsɜːrd/ amounts of money is getting **funneled** into generative AI
startups.

#### [Where Did Americans’ Savings Go?](https://youtu.be/v_qXC-1PH4U)
* overspending
* the top/bottom/lower half ...
* had about 5 days of **liquid savings**: 流动性储蓄 
* be pessimistic /ˌpesɪˈmɪstɪk/ about the current situation, that's **all time high**
* was supposed to: 本应该, the recession that we were supposed to have this year.
* they will stop consuming(消费) and it can be a **self-fulfilling prophecy** /ˈprɑːfəsi/.
* stockpile: a large accumulated stock of goods or materials.
* these **dynamics**(n. 动态: 特定系统或一组元素的行为和相互作用所遵循的模式、力量或过程)
are observable around the world.

#### [Rent vs. Buy: Which Makes More Sense In The Current Real Estate Market?](https://youtu.be/LzK2rviPo4c)
* it was always/others are **out of picture**: Not Included or Considered /
Physically Absent.
* I was living **paycheck to paycheck**: a person's income is just enough to cover
their immediate expenses.
* **ever changing** housing market: constantly and continuously undergoing
alterations.
* extraordinary(very unusual) / ordinary(normal, commonplace)
* it makes more sense to **make this move**.
* a **mortgage** is a type of **loan** that is specifically used to purchase
real estate, while a loan is a more general term referring to borrowed money
that can be used for various purposes. Mortgages are often secured loans with
the property as collateral, and they usually have longer repayment terms than
other types of loans.
* the numbers **fluctuated** /ˈflʌktʃueɪt/(rise and fall irregularly in number
or amount) **throughout** (during) 2022
* more **cost effective** to rent than buy.
* metro: /ˈmetroʊ/: a major city.
* Giving your money to someone else, building equity on their investment
**as opposed to** your own.

#### [How Americans Are Struggling With Car Loans](https://youtu.be/JuR8pQqluWY)
* **auto loan** is at a **record high**.
* outside of work: 工作之余
* the average price is $724 **up/down from** $654 in 2023.
* **up** 2% from the prior /ˈpraɪər/ year.
* signing the **bottom line**: the final total of an account.
**markup** is the additional amount added to the cost price to determine the
selling price, while **profit** is the positive difference between total revenue
and total costs after all expenses have been accounted for.
* shopping around and come back: 再看看
* even if it's been a long, exhausting and **mentally draining** day.

#### [Why Low Economic Growth Is So Dangerous](https://youtu.be/EX4NliS7114)
company's **bottom line**: net income or profit.
improve people's **livelihood**: a means of securing the necessities of life.
the topic of AI is at **fever pitch**: a state of extreme excitement.
derisk(take steps to make less risky) the portfolios(a range of investments).

#### [How America Plans To Get Teens Back To Work](https://youtu.be/bbmHk1p4TRk)
* **reinvigorate** /ˌriɪnˈvɪɡəˌret/ the youth /juːθ/ labor force: give new energy
or strength to.
* In summary, **youth** is a more inclusive term that can cover a wider age range
beyond just the teenage years, while **teen(teenager)** specifically refers to
the age range between 13 and 19.
* minor: a person under the age of full legal responsibility.
* passed laws **rolling back** child laber.
* keeping industries **afloat** /əˈfloʊt/ when there are work shortages: out of
debt or difficulty; not sinking.
* It's extremely rough when people can't recognize they're **being taken advantage of**.
* Companies will do anything **but** pay workers more.

#### [How Amazon Is Making Custom Chips To Catch Up In Generative A.I. Race](https://youtu.be/rViVFrQg4Hk)
* $300M: 亿, $30M: 千万
* that provides acceleration **in terms of** speed, cost efficiency and power
for every general web-based workloads.
* the open source ChatGPT **rival** /ˈraɪvl/.

#### [Why Prosecuting Insider Trading Is So Problematic](https://youtu.be/wA7RrJvBzfY)
* insider trading: the illegal practice of trading on the stock exchange to one's
own advantage through having access to confidential information.
* she was **convicted**(定罪) of the obstruction of justice.
* they were **prosecuted**(起诉) for obstruction of highway.
* market manipulation(操纵), manipluate stock: control or influence (a person or
situation) cleverly, unfairly.
* smoking gun: a piece of evidence or information that is indisputable proof of a
certain fact or wrongdoing.
* 2 types of insider trading: classical and **misappropriation**: dishonestly or
unfairly take.
* spokesperson: 发言人, 代言人
* white collar crime: non-violent, financially motivated criminal activities that
are typically carried out by individuals, professionals, or organizations in
positions of trust, authority, or respectability.
* pump and dump scheme/skiːm/: investment **scam** that involves artificially
inflating the price of a stock or other financial asset through false or
misleading statements. Once the price has been pumped up, the fraudsters then
"dump" their own holdings of the asset at the inflated price, making a profit,
while other investors who were deceived into buying suffer losses.
* meme stock: gain sudden popularity on the internet and lead to sky-high price.

#### [How Temu Makes Money From $10 Smartwatches From China](https://youtu.be/quGoGgbP-aE)
* 100 thousand: 十万
* 100 billion: 千亿
* Temu is addictive /ə'dɪktɪv/
* countdown clock/watch
* team payment/group buy: team up, price down
* they are paying big bounties /'baʊnti/ and referral fees to help bring people
to the site.
* go across the **social sphere**: collective interactions, relationships, and
activities that occur within a specific social context or environment. 社会领域.
* if you are an **influencer** you can earn up to 500$ a month
* Temu is the retailer's major push to overseas market, where its competitiors
jd.com, etc already have **footprint**.
* supply chain advantage 
* not pay for the brand name

#### [How Modelo Dethroned Bud Light In The U.S.](https://youtu.be/wfisB72QPy0)
* **Mexican** beer brand: Modelo(now bestseller), Corona, Bud light.
* Hispanic: people who have a connection to, or an origin from, Spanish-speaking
countries, such as Spain, Mexico, Cuba, etc.
* brewery /'bruəri/

#### [How Did The Sriracha Shortage Happen?](https://youtu.be/EYdU1X2p2ro)
* condiment/'kɑndɪmənt/: used to add flavor to food.
* sriracha **hot sauce**
* chili pepper: 红辣椒
* smooth **paste**: 糊状物, 膏, such as sriracha.
* ranch: a large farm
* 50 truck and trailer loads:
* **renege**/rɪˈneɡ/ on that deal: go back on a promise.

#### [Why Penny Stocks Are So Dangerous](https://youtu.be/-aO--ly05aI)
* trillion: a million million
* OTC(Over-The-Counter): decentralized market where financial instruments
such as stocks, bonds, derivatives, and other securities are traded directly
between buyers and sellers.
* penny stock: often under $1 or $5
* penny stock **schemer**: a person who is involved in making secret or
dishonest plans.
* hgih-stakes gambler.
* Low-priced securities tend to be **volatile**/ˈvɑːlətl/ and trade in low volume.
* at a particular time: 特定时间
* Don't let fear of **missing out** control your investment decisions.
* make the pitch: 推销

#### [How Space Factories Are Becoming A Reality](https://youtu.be/OZJv_YGpjeI?si=grA0lX8u_gyMmk-0) 
* in-space manufacturing sounds like **science fiction**, still a very
**nascent**/ˈneɪsnt/ market: just coming into existence and beginning to display
signs of future potential.
* ISS: International Space Station.
* there is **huge queue and backlog of** people waiting to use space station.
* drug therapy: treatment intended to relieve or heal a disorder
* capsule/vehicle: spacecraft
* **drug delivery** to the patients much easier
* royalty/ˈrɔɪəlti/: a sum of money paid to a patentee for the use of a patent,
not loyatly!

#### [How to Invest in Real Estate Without Buying A Home](https://youtu.be/mW6pb9V0ico?si=inhHbzu6_Xyfk3ZD)
* REITs: real estate investment trusts, such as data center REITs(DLR, EQIX),
residential REITs: INVH(Invitation homes), AMH(american homes 4 rent)
* sarcasm/ˈsɑːrkæzəm/: the use of irony/ˈaɪrəni/ to mock or convey contempt.
* Philadelphia/ˌfilə'delfjə/.


#### [Why Youth Unemployment Is Surging In China](https://youtu.be/iK3VEF2woRY?si=Nn_Roj9HphiKSH5z)
* soaring/surging
* tertiary/ˈtɜːrʃieri/ industry, in China it is service industry, has
**tapered/'tepɚ/ off** in the last 6 months compared to a year ago.
* China's National Bureau/ˈbjʊroʊ/ of Statistics
* Imports and exports to and from the country are down in most sectors.
* limited spending 消费
* They do not wish to **disclose** too much of it, as a result, there are more
**speculation** and more fear in the market.
* Their parents lived through China's big economic boom in the last couple of
decades. 
* The housing market has **skyrocketed** in the last 10/15 years in China.
* First, second tiers city.
* sixfold, tenfold: To express an increase by a multiple of a certain number of
times, you can use the "fold" suffix along with a specific number.
* China's overall urban unemployment rate has remained relatively steady all year,
**hovering** around 5.2%.
* Finanically affected.
* **lie/lying flat/flæt/**: tang ping vs **rat race**: an exhausting, usually competitive
routine.
* Laber intensive manufacturing.

#### [Qualcomm Turns To Auto And AI With Future Apple Business Uncertain](https://youtu.be/2aD36UakFk4?si=mls8572nG-je5hID)
* Qualcomm is a **fabless chip** company: designs microchips but contracts out
their production rather than owning its own factory.
* OEMs, such as Intel, Samsung: original equipment manufacturer, produce components, parts, or products
that are used by other companies.

#### [Why The U.S. Won’t Pay Down Its Debt](https://youtu.be/9yU_bVZqqYk?si=Omccew_HEHjCQNTD)
* the national debt clock: https://www.usdebtclock.org/
* owe/oʊ/ you 25 cents.
* intragovernmental debt holdings: government owes itself that money.
* the debt in some way is a **contingency** plan: a provision for an unforeseen
event or circumstance
* with no long term benefit, you are actually **deteriorating**/dɪˈtɪriəreɪt/
your financial or fiscal future over the long run.
* Debt to GDP ratio
* service the debt: pay back the debt and the interest on the amount to the
lender.
* windfall: a piece of unexpected good fortune
* fiscal **solvency** crisis:ability to pay one's debts.
* Japan has been running **fiscal deficits** basically for the last 3 decades.
* be supposed to: 本应该
* sluggish/ˈslʌɡɪʃ/: slow-moving or inactive

#### [Why You Should Buy Everything With Credit Cards](https://youtu.be/SnBu2_l_sZ0?si=dZcXUNKH1RNNKDnJ)
* that's a really big hole you are contining to dig and it's going to be really
hard to get out of.
* credit card balance: In banking and accounting, the balance is the amount of
money owed/oʊ/ (or due) on an account.
* how do you build your credit history?
* the higher your credit score, the more responsible you are **deemed**.
* it depends on your personal **circumstances**: a fact or condition connected
with or relevant to an event or action.

#### [Why The NYC Subway Is Such A Mess](https://youtu.be/EOWeZ7ZFU0c?si=Ry2QAvAotfftVRHx)
* revenue in 2022 **plummeted**/ˈplʌmɪt/ 37% from its 2019 levels.
* fare/fer/ evader/i'veidə/, fare revenue, fare gate
* public transit/ˈtrænzɪt/: more commonly used in North America, particularly
in the United States and Canada.
* train/treɪn/, bridge, tunnel
* subway bench
* weekend vs weekday
* the system is **decaying**/dɪˈkeɪ/ rapidly, I **think of** it as a heart attack.

#### [President Biden’s Ambitious New Plan To Help Student Debt, Explained](https://youtu.be/qypcBJPyF5c?si=wtzT1aF10iBEOkKq)
* student loan relief
* **one in four** student loan borrowers are in **delinquency**/dɪˈlɪŋkwənsi/
or default: mainly US: a failure to pay an outstanding debt.
* income-driven repayment plan
* **discretionary**/dɪˈskreʃəneri/ plan: 自行决策计划
* discretionary income: income remaining after deduction of taxes, other mandatory
charges, and expenditure on necessary items
* not all of its beneifts will **go into effect** right away.

#### [How Consumers Can Manifest Inflation](https://youtu.be/3kw3Uasvu_4?si=NVF_BQVnrC7qanoB)
* manifest: reveal, indicate; consumers have a role or influence in the
occurrence or increase of inflation through their actions or behaviors. 
* prices have slowly begun to **descend**.
* people really do behave **in accordance with** their expectations and with their
sentiments and attitudes/ˈætɪtuːd/ towards the economy.
* price in: market has already taken into account and adjusted for that particular
information or event, and it is now reflected in the asset's current valuation.
* inflation expectation.
* consumers had some recognition over the last year that there were factors that
were **peculiar**/pɪˈkjuːliər/ to the current situation, even if they might last
for a while, they didn't expect them to last forever.
* inflation is **here to stay**: something has become a permanent or long-lasting part
of a situation or context
* **preliminary**/prɪˈlɪmɪneri/ results from the Sept survey...
* Wage-price **spiral**/ˈspaɪrəl/: rising wages lead to higher prices for goods
and services, which in turn lead to demands for even higher wages.

#### [Could Deep-Sea Mining Fix The Global Minerals Shortage](https://youtu.be/7s_JdZuERbU?si=3kfg7yyJxGAIEamb)
* Minerals/ˈmɪnərəl/ **vital**/ˈvaɪtl/ to electric vehicle batteries and clean
energy transition as a whole are found in abundance/əˈbʌndəns/ ...
* fragile/ˈfrædʒl/
* transfer, transmit(transmission), transit, transport

#### [The Truth Behind 'Unlimited' Vacation Plans](https://youtu.be/_Bh8pdUxfo8?si=97ZfEwluWjeFvReL)
* offer this **perk**/pɜːrk/.
* it is not a formal promise but just a **catchphrase**/'kætʃfreiz/: 醒目的广告用语
* accrued/ə'krud/ benefits: received or accumulated in regular or increasing
amounts over time
* pension/ˈpenʃn/
* liability: costs associated with running a business, such as rent, payroll and
interest payments

#### [Why Tesla May Be The Big Winner Of The UAW Strikes](https://youtu.be/xqJiJmDszQ0?si=yYC1MWGQPQpx5uBC)
* automaker Detroit/di'trɔit/ three(D three): GM, Ford and Stellantis
* worker union
* intensity of the **strike**
* audacious/ɔːˈdeɪʃəs/: showing an impudent lack of respect.
* This time they **caught** the industry **off guard**: unprepared for a surprise
or difficulty
* foreign or homegrown company
* has **slashed** prices considerably throughout 2023
* speak out: express one's feelings or opinions frankly and publicly
* wishful thinking: 如意算盘；痴心妄想
* stockholm/'stɔkhəum/ syndrome/ˈsɪndroʊm/: 斯德哥尔摩综合症(被劫持人质对劫持者产生好感
并同情、宽容他)

#### [Why GlobalFoundries’ Chips Are So Important To The U.S.](https://youtu.be/wdP_goQU0ZU?si=LHWkT4eTGDnbvtVA)
* wafer/'wefɚ/: 晶片
* foundry: a workshop or factory for casting metal
* **Roughly** we **put out**(create or produce) about a million chips a day.
* isn't the microchip company with most **hype/haɪp/** right now: 夸张地宣传.
* making chip at the **bleeding edge**: the very forefront of technological
development
* behind the scenes: out of sight of the public at a theater or organization
* misnomer/ˌmɪsˈnoʊmər/: a wrong or inaccurate name or designation, 用词不当
that's **patently** false: easily recognizable; obvious
* technological **tug of war**/tʌɡ/: 拔河
* older node: 老节点 In computer science and networking, the term "older node"
typically refers to a node or device within a network that has been in operation
for a longer period of time compared to other nodes or devices in the same
network.
* legacy node: 遗留节点 refers to a node or device within a network that uses
older, often outdated, technology, protocols, or hardware. 
* back out: withdraw from a commitment
* reshore: (of a company) transfer (a business operation that was moved overseas)
back to the country from which it was originally relocated, eg: increasing
numbers of US-based manufacturing businesses are planning to **reshore**
production from China back to the US

#### [Why Tornadoes Are More Destructive Than Ever](https://youtu.be/wAmNUMdn1p4?si=56xIsuRujng3q7Xv)
* tornado alley/ˈæli/: a narrow passageway between or behind buildings.
* the cost of disasters each year is just **skyrocketing**: (of a price, rate, or
amount) increase very steeply or rapidly
* We are seeing ....happening more and more regularly 
* building code: a set of regulations and standards that specify the minimum
requirements for the design, construction, and maintenance of buildings and
structures. 
* including but not limited to: xxx
* hailstorm/'helstɔrm/: 雹暴

#### [Can The U.S. Compete With Chinese Drones?](https://youtu.be/wnJzv01jgb8?si=C5BR46LS6mPMR0W7)
* warfare, combat, conflict
* civilian/səˈvɪliən/ drone: non-military purposes, 民用无人机
* trusted allies /ˈælaɪ/
* fears of xx espionage /ˈespiənɑːʒ/: the practice of spying or of using spies
* concerns are overblown: 过分渲染
* the war in Ukraine has **elevated** the need for cheap, reliable drones
* **has** a large **lead** on cost effective and **capable**/ˈkeɪpəbl/ 能干的 drones
* some in the US think this **dynamic**(see again) is not permanent
* they are known for being **well-made** and come with the great support in the
form of DJI **care**.
* ministry of commerce/ˈkɑːmɜːrs/ released **export restrictions** on certain
drones: long flight times or **certain**(specific but not explicitly named or
stated) sensors
* don't have all of the **requisite** supply chain ...
* putting that aside 抛开这个不谈
* overreliance/'ovɚrɪ'faɪn/: excessive dependence on or trust in someone or something

#### [Why The U.S. Is Now Obsessed With Soybeans](https://youtu.be/9kW5vm0yj40?si=AiWHe6u5joZT3EcJ)
* soybean is used as food/fuel/feed crop/krɑːp/
* valuable commodity/kəˈmɑːdəti/: a raw material or primary agricultural product
that can be bought and sold, such as copper or coffee
* soybeans take nitrogen from the air and **replenish**/rɪˈplenɪʃ/ the soil:
restore, fill up again
* soybean crop **yield** 收益；产量
* soybean's primary use is protein for producing **livestock**, such pork,
poultry/ˈpoʊltri/, aquaculture /'ækwəkʌltʃɚ/, even dairy/ˈderi/
* global demand for animal protein is on the rise
* the us-china relationship **soured** as the trade dispute escalated in 2018
* subsidies to **offset** farmer's losses
* renewable diesel/ˈdiːzl/柴油, biofuel, drop-in fuel

#### [Why EVs Are Piling/paɪl/ Up At Dealerships In The U.S.](https://youtu.be/cZlsZwcIgpc?si=HeLCgLSJ_ubkBgpa)
* these efforts have hit **unnerving**/ˌʌn'nɝv/ **speed bump**: make (someone)
lose courage or confidence
* gas/fuel burning vehicle
* ICE(internal combustion engine) car
* the demand of EVs has **leveled off**: begin to fly horizontally after
climbing or diving
* that number **has since fallen** to 67% in 2023.
* the market for EVs is **upside down**. 颠倒，混乱
* sticker price(initial or advertised price of a product or service) or
MSRP(manufacturer's suggested retail price).
* pain point = sore/sɔːr/ spot 痛处；伤心事
* current, past or **prospective**: expected or expecting to be something
particular in the future
* there is a **fair** amount of feedback..: considerable though not outstanding
in size or amount.
* if perhaps we could hit the **rewind button** and do things differently than
we have.

#### [Quiet Cutting: How Power In U.S. Offices May Be Shifting Back To Bosses](https://youtu.be/j_V2Qxpc5jE?si=7oC4mh8lmIrgkCQo)
* quiet cutting: reorg, take or leave it
* quiet quitting: doing the bare minimum of your job and putting in no more time,
effort, or enthusiasm than necessary, employee has **the upper hand**: advantage
* buzzword term: 流行术语
* companies **are fearful** what lies ahead.

#### [Why It's Becoming Harder To Get Into Airport Lounges](https://youtu.be/UHYAvU8tYrg?si=SMj4YfQ6fFbip8mB)
* airport lounge/laʊndʒ/, loyalty or elite/eɪˈliːt/ status.
* but as the popularity increases, **so do the** challenges of operating them,
like overcrowding.
* there is a lot of ways that you can **qualify** to get into them.
* Status at an airline comes with **a host of**(a large number or multitude of
something) benefits like free seat upgrade, free checked bags, premium customer
service, lounge access.
* modern lounge often includes **amenities/ə'mɛnəti/**....
* taproom: a room in which alcoholic drinks, especially beer, are available on tap

#### [How Walmart Is Beating Everyone In Groceries](https://youtu.be/mEgZajtRJmI?si=i5t4RfkOVetY_2cc)
* supermarket checkout counter
* price sensitive customer
* along with its **rivals/ˈraɪvl/**, has also faced criticism that it does not
compete fairly with small independent grocers/ˈɡroʊsər/
* grocery chain
* automation could make some jobs **obsolete**: no longer produced or used; out
of date
* move cutomers to high margin **discretionary**(the freedom to decide what
should be done) spending categories like **apparel/ə'pærəl/**(clothes) and
**home furnishings**
* waterbed effect: used in economics and finance to describe a situation where
a change or intervention in one area has ripple effects or unintended
consequences in other related areas.

#### [Should The U.S. Bury Its Power Lines?](https://youtu.be/52iCmSFW2Wk?si=o7l_-5F04uCo_g0o)
* **bury/ˈberi/** power line underground (vs overhead line):  put or hide under ground
* **utility-caused** fire **devastated** towns **throughout** California: an
organization supplying the community with electricity, gas, water, or
sewerage/'suərɪdʒ/
* the result **blaze** destoryed the town of...: a very large or fiercely
burning fire
* **vegetation** maintenance cost:  plants considered collectively, especially
those found in a particular area or habitat:

#### [Why The U.S. Is Getting Serious About UFOs](https://youtu.be/ptynaaiGVqk?si=uxwwco9-15NSBG-C)
* UFO: unidentified flying objects
* **conspiracy/kənˈspɪrəsi/ theory** around the U.S. **concealing/kənˈsiːl/** alien
life and technology in secret compounds like Area 51 have run rampant/ˈræmpənt/.
* science fiction movies and **novels/ˈnɑːvl/**
* the **poll** found that 68% of **respondents** believe ...
* UAP: unidentified **anomalous/əˈnɑːmələs/** phenomane/fə'nɒmɪnə/
* shift the converstaion about UAPs from **sensationalism/sen'seɪʃ(ə)n(ə)lɪz(ə)m/**
to science. <贬>(指行文或报道)耸人听闻,哗众取宠
* this could **be attributed to** ordinary explanations like drones, balloons,
etc: regard something as being caused by.
* uncharacterized 未定性的
* flying saucer: 飞碟
* capitol/'kæpɪtl/ hill 美国国会
* the possibility cannot be completely **discounted**: 不考虑；不全信

#### [The Recession Has Finally Begun, But Only For America's Rich](https://youtu.be/zG_l_7bCR8g?si=IENYu8N2kSxQz1jQ)
* the recession fears **fade**
* **sticky inflation** and a weakened stock market continue to
**disproportionately** pressure wealthy Americans
* fewer people buy fewer things can lead to a **stagnant/ˈstæɡnənt/** economy
* stock market was down very **sharply**
* market has rebounded in 2023
* the budgets are **stretched**: 预算捉襟见肘
* credit card debt has **swelled** to more than $1 trillion
* buy discretionary items: 购买非必需品
* non-discretionary: essential

#### [Why Americans Fell Out Of Love With Canned Tuna](https://youtu.be/PCivX9A8mOo?si=rSYU3bT7ZFdYNKLF)
* canned/kænd/ tuna, tuna rich zone, tune migration pattern
* learn to be more **adaptable**: able to adjust to new conditions
* a decline mainly triggered by **shifting consumer preference**
* **price fixing** scandal/ˈskændl/
* illegal, unregulated, unreported fishing is a major concern
* third party suppliers
* vessel costs are rising, import tariffs/'tærɪf/ are pricey, oil prices are
constantly fluctuating
* not a high **margin** business with **profit margin** only 3%, tight margin
industry

#### [How Homeowners Associations Took Over American Neighborhoods](https://youtu.be/fnLMeotB0c0?si=B5wttHjRI_BKSNO7)
* HOA: Homeowners associations: organizations that oversee properties in a
community
* cozy/ˈkoʊzi/: giving a feeling of comfort, warmth, and relaxation
* board director
* lien/'liən/: a right to keep possession of property belonging to another
person until a debt owed by that person is discharged
* foreclosing home 查封房屋
* garnishing/ˈɡɑːrnɪʃ/ wage: take (money, especially part of a person's salary)
to settle a debt or claim
* prior notice
* handle landscaping 景观美化
* trash pickup
* milk the community: exploit or defraud (someone), typically by taking regular
small amounts of money over a period of time

#### [How Unprofitable Companies Stay In Business](https://youtu.be/miWLE1yhMG0?si=b7k5E7TR-u1N9RoD)
* taking on debts that they probably cannot **repay**
* zombie firms: unviable/ʌn'vaiəbl/ companies
* the **share of** zombie firmes has been increasing over time, this has
**detrimental/ˌdetrɪˈmentl/** effects to healthy firms that compete in the same
sector
* bailout/'belaʊt/ 紧急援助
* the problems will go away quickyl as **things normalize**

#### [Is The Golden Age Of Remote Work Over?](https://youtu.be/mxpnk6F2EXc?si=nfnRdUgHoRrbu_Rf)
* laptop classes
* the corporate leaders have reversed their approach to remote work.
* bureau /ˈbjʊroʊ/: department
* 5 consecutive quarters of year over year declines in worker productivity.
* employee can perform their **duties**.
* rethink: think again 

#### [Who Makes Money From America’s Firetrucks](https://youtu.be/jXAy34A6gyU?si=w-5IYy6EGRc2lejG)
* fire department, fire station, station garage, zero-emission firetruck
* purposed build vehicle: news van, mail truck, garbage truck, construction lift
* the company is **electrifying**
* articulating boom: snorkel/'snɔrkl/, elevated platform for firefighting.
* make the best use of..
* dual sourcing
* suffered some **setbacks**: a reversal or check in progress

#### [What's Wrong With U.S. Cash](https://youtu.be/isI6O-22cTg?si=LV_2SBuvmFqKS2-q)
* paper notes: physical, paper-based representations of a country's currency,
issued by the central bank or government
* denomination/dɪ,nɑmɪ'neʃən/: the face value of a banknote, coin, or postage
stamp. 面额
* banknote: a piece of paper money 纸币
* bill: a banknote; a piece of paper money: a ten-dollar bill
* Polymer/'pɑlɪmɚ/ banknote: 塑胶钞票
* people are ditching/dɪtʃ/ cash for electronic payments: get rid of or give up
* mint vs bureau of engraving and printing
* move **lower** currency denominations to coins to help with longevity
* counterfeit 仿制品, 伪造物
* not quite there yet: 没准备好
* nailed/neɪl/it: perform (an action or task) perfectly

#### [How The U.S. Lost Thousands Of High-Skilled Workers To Canada](https://youtu.be/C6nOWE8betQ?si=LTmzoSJbB9cyQNpW)
* reform is needed
* Canada has launched the new **initiative** to attact this talent: initiate
things independently.
* nomad/ˈnoʊmæd/: a person who does not stay long in the same place
* bureaucratic/ˌbjʊrə'krætɪk/ visa process pushes workers north: inflexible,
complicated
* it is not straightforward, it is very **convoluted**
* H1B **lottery**: a process or thing whose success or outcome is governed by chance.
* it requires a bachelor's degree or its **equivalent**
* if things don't **work out**, you have to leave the country
* they cannot change jobs or work side **gigs**: a job, especially one that is
temporary or freelance and performed on an informal or on-demand basis
* spouse/spaʊs/
* **highly populated** country: India, China
* the company **underwent** major layoffs in the past year: experience or be
subjected to (something, typically something unpleasant, painful)

#### [Why The Pentagon Is Spending Billions To Bring Laser Weapons To The Battlefield](https://youtu.be/pc_iLCI5RVk?si=_rzw9McIOjKU7uQ_)
* direct energy weapon: laser or microwave
* most lasers have been used for targeting and **illumination(such** as for night
vision).
* curise missile
* The US Army, Navy, Marines(海军陆战队) and Air Force
* Move from testing to operation
* unmanned air vehicle
* interceptor 拦截

#### [Why Oil Giants Shell And BP Are Investing In U.S. Farmland](https://youtu.be/XEXIy4xWgf8?si=s1lQwx483pO7X9Gx)
* area without **extensive** trees and vegetation: covering or affecting a large
area
* shepherding/ˈʃepərd/ sheep
* herbicide/saɪd/, insecticide, pesticide
* sorghum/'sɔrgəm/ 高粱
* ranch: a large farm, especially in the western US and Canada

#### [How Elon Musk’s Starlink Is Bringing In Billions For SpaceX](https://youtu.be/SVgVzEVeP4Q?si=300xxxyD0qvvYQos)
* one after another
* starlink's importance to spaceX as a company is **imperative**/ɪmˈperətɪv/: of
vital importance; crucial:
* achieve breakeven cash flow: 盈亏平衡点
* maritime and aviation industries: 航海的 
* satellite debris/dəˈbriː/: scattered pieces of waste or remains
* **flawed** methodology/ˌmeθəˈdɑːlədʒi/: 有缺陷的
* geopolitics/ˌdʒioˈpɑlɪtɪks/, geopolitical/ˌdʒiopə'litɪkl/
* radio and optical astronomy/əˈstrɑːnəmi/
* condemnation /ˌkɑːndemˈneɪʃn/: the expression of very strong disapproval

#### [Why Divorce Is So Expensive In The U.S.](https://youtu.be/wJVrAkNgKsY?si=3xBoEIQEZH7gRitV)
* cost lots of dough: money [美俚]金钱
* misery/ˈmɪzəri/: a state or feeling of great distress or discomfort of mind or
body
* appraiser/ə'prezɚ/: a person whose job is to assess the monetary value of
something
* contested vs uncontested: in context of divorce, A contested divorce occurs when
the spouses are unable to agree on one or more significant issues related to the
divorce, such as child custody, division of assets and debts
* mediator/ˈmidiˌetɚ/: 调停者；调解人
* aftermath: the **consequences** or aftereffects of a significant unpleasant event
* every fee must be paid **up front**: in advance

#### [What Happened To Wonder Bread?](https://youtu.be/xptUVqwX0h0?si=lG7oyXYIkuEOGlwa)
* loaf/loʊf/(loaves) of bread: a quantity of bread that is shaped and baked in
one piece and usually sliced before being eaten, pre-sliced bread.
* the brand has been around for a long time.
* premiumization: the action or process of attempting to make a brand or product
appeal to consumers by emphasizing its superior quality and exclusivity

#### [Binance’s Changpeng Zhao to step down as part of $4.3 billion DOJ settlement: CNBC Crypto World](https://youtu.be/P-3J1VMfvPo?si=KWK4IWjkfk8W-U1Y)
* step down: withdraw or resign from an important position or office
* He's ready to **tee/ti/ it up** and start the project: getting ready or
preparing for something
* what I mean by that was ...: to emphasize sth you have jus said
* a huge catalyst/'kætəlɪst/ for the industry

#### [How Taxpayers Grow The Private Sector](https://youtu.be/g5-se2mD_PQ?si=D0ZaBGijoGYtTWuZ)
* how companies are **governed**: manage pertains more to handling specific
tasks, resources, or operations, while "govern" involves the broader aspect of
ruling, directing, or controlling based on established rules or principles. 
* privatize the gains, socialize the losses
* baby formula: 婴幼儿配方奶粉
* venture capitalist would tell you that's normal, for every success you need to
bear with multiple failures.
* digital **feudalism/ˈfjuːdəlɪzəm/**: 封建主义

#### [How Nespresso Is Taking On Keurig In The U.S. Coffee Pod Market](https://youtu.be/NYIEBquIhCg?si=1i93ZWSFo4vEOs8g)
* Nestlé’s [nes lei] Nespresso. vs Keurig: coffee pod market players 
* single-serve coffee system: a pod coffee machine
* purchased perpetual/pərˈpetʃuəl/ rights to sell and market Starbucks products
at retail world
* coffee shop, coffee style chart: espresso, macchiato/ˌmækɪ'ɑːtəʊ/, latte,
cappuccino, mocha

#### [How America Racked Up A $1 Trillion Credit Card Bill](https://youtu.be/3R_XbZCJ39Y?si=Syn29xF2PBzHPP2V)
* consumers continuing to be resilient: able to withstand or recover quickly
from difficult conditions
* delinquency: see prior note about its meaning on finance.
* their wages are not increasing at the same pace as the inflation.

#### [How Safe Is Tap Water In The U.S.?](https://youtu.be/7dArIR4OKys?si=qiFPTEeTZ8stQFsB)
* tap water, distilled water
* there are contaminants/kən'tæmɪnənt/ present in just about everyone's water to
some degrees.
* arsenic: 砒霜, lead/led/, PFAS
* water treatment facility: 水处理设施, water is treated: purifying water to make it
safe.
* water main: 总水管
* there are **byproducts** from water treatment process, such as **disinfection**
stage: the process of cleaning something.

#### [How The Escalating U.S.-China Tech War Could Hurt American Companies](https://youtu.be/PVBjjMCWBTs?si=vKD1405u7T6hi7Lr)
* the race for technological supremacy/su'prɛməsi/: the state or condition of
being superior to all others in authority
* high-end chip
* China's response has been sporadic: occurring at irregular intervals
* the US govt was not excited about the previous **status quo**
* biden govt aimed in part at closing that **loophole**: an ambiguity or inadequacy
in the law or a set of rules
* **consequential** breakthrough: important
* we are going to see a **bifurcation**/baɪfɚ'keʃən/ into a chinese led tech and
american lead tech: the division of something.

#### [Why Cars Lose Their Value So Fast](https://youtu.be/Ao7fbajRSKI?si=h7_BfEccadPZioOU)
* drive off the lot, not brand new anymore, start losing the value right away
* keep spotless insdie and out, have dent/dent/, scratch on surface
* that is unlikely to change for a while
* car value depreciation
* trade in car: **wholesale** price + costs + dealer margin (It represents the profit
margin that the dealer earns on the sale of a vehicle.)
* information asymmetries /e'sɪmɪtri/

#### [Why The Airbus A380 Is Making An Unlikely Comeback](https://youtu.be/xbzbXEO10yY?si=zg9DPqeQJw5If4WR)
* Most airports cannot **accommodate** the size of it
* wide-body passager plane

#### [Why The Port Of Baltimore Is Getting A Makeover](https://youtu.be/b6PNc8xzEus?si=_vEmY3A8F27bvo7x)
* The U.S. actually **lags** many ports around the world in terms of efficiency
* ship to shore/land crane/kreɪn/, seaport
* rolling cargo ship 滚装船, container ship

#### [Why CEOs Are Quitting In Record Numbers In 2023](https://youtu.be/UBhR-4oxwVI?si=3G29TQZU2VXJxggQ)
* the huge exodus/ˈeksədəs/ of CEO even from big companies: a mass departure of
people
* none of that is clear at this moment

#### [How LVMH Became A $500 Billion Luxury Powerhouse](https://youtu.be/NBmIFTyiGos?si=H2wDWcFek_AAw3ii)
* powerhouse: a person or thing of great energy, strength, or power
* LV is one of the most storied and **iconic**/aɪ'kɑnɪk/ luxury brands
* Innovating new products that is captivating/ˈkæptɪveɪt/ to customers
* you cannot create heritage/ˈherɪtɪdʒ/ overnight: **denoting** a traditional
brand or product regarded as fine craftsmanship, 传统
* Luxury isn’t about quality it’s about exclusivity/'ɛksklʊ'sɪvəti/. People are
paying more so that they can have a product that less people can afford which
makes them feel special
* It's an interesting business **model**. Selling things for significantly more
than their actual value, just because you've managed to convince the customer
that the social validation of having that logo on your product makes it worth
paying that insane **premium**. It's the perfect business model. Convince these
suckers to pay 10x more than they should and they won't even dislike you for it,
they'll admire you. That's called "Brand" in the world of business. A brand
demands a premium. A Toyota or a Honda will also take you from a point A to a
point B, but people pay premium to buy a Mercedes or a BMW. Not to mention, a 
oyota and a Honda will last longer as well. That's the power of brand building.
It's one thing to build a product and build a company, it's a totally different
thing to build a brand.
* model vs module, see chatcpt

#### [Why hertz's bet on Tesla is not paying off in the U.S.]()
* EV: pricing troubles, skyrocketing repair costs, low resale values
* either kill or pause the EV **initiative**: the ability to assess and initiate
things independently
* suffered from underinvestment
* so to speak: She's the glue that holds the team together, **so to speak**, because
she's excellent at fostering teamwork and cooperation.
* as inflation starts to **subside**/səbˈsaɪd/ some costs are coming down: become
less intense, violent, or severe
* leisure business: economic activities and industries that cater to providing
recreational, entertainment, or relaxation-related products and services for
individuals during their leisure time.

#### [Why more Americans are going child free](https://youtu.be/W5XZ_gJBnns?si=d2Pdttk7PYFn60fo)
* DINKS: dual income, no kids
* DIWKS: dual income, with kids
* fewer expenses leave DINKS with more **disposable income** to play with: 税后
所得
* we have even large **nest egg** that allowed us to buy a more expensive home:
a sum of money saved for the future, 储备金

#### [How Budget Airlines Like Ryanair Make Money](https://youtu.be/Y3wKd24iqHw?si=bXWAu9I2PiewjkQw)
* budget, low-cost, low-budget airlines: U.S. has Spirit, Allegiant, Frontier; in
eur has: easyJet, Ryanair, Wizzair.
* **full-service** airline: typically offers passengers in flight entertainment,
checked baggage, meals, beverages and comforts such as blankets and pillows in
the ticket price.
* paper safty card, seatback entertainment
* they tend to **entice**/ɪnˈtaɪs/ travelers with low base fares and then charge
for **add-ons**(something that has been or can be added to an existing object or
arrangement:) such as seat selection, food and luggage.: attract or tempt by
offering pleasure or advantage
* carry-on bag vs checked luggage
* during onboarding, we hit another **road bump**.
* they might **weigh** it
* they are really efficient at boarding and **deboarding**, we got on/off quickly.
* the biggest **hang-up**(not hang up) was that xxx: an emotional problem or
inhibition. 烦恼,焦虑

#### [How Americans Lose Billions To Fraud](https://youtu.be/bD6UZb3lM1w?si=RV6j9mbsEYG2FZ6s)
* credit card fraud / odometer fraud
* money lost to scam calls / phone scams
* recent surge in used car price that it's becoming more **enticing** for scammers
* anyone can be the victim
* identity theft/θeft/
* it can increase a **prison sentence** by years or eveb decades
* fraudsters are **rolling back** or otherwise **tampering with** odometer and
title info: or make unauthorized alterations.
* I don’t trust most **contractor** or **mechanics**/məˈkænɪk/ here in the us:
a person who repairs and maintains machinery

#### [Credit card fraud](https://youtu.be/bD6UZb3lM1w?si=RV6j9mbsEYG2FZ6s)
* their operations are industralized.
* data breach/briːtʃ/
* chargeback/'tʃɑːdʒbæk/: a demand by a credit-card provider for a retailer to
**make good**(compensate for or rectify a loss or damage that has occurred) the
loss on a fraudulent or disputed transaction.
* screw/skruː/ up: a situation that has been completely mismanaged or mishandled
* sucks: express dissatisfaction, disappointment, or a negative opinion about
something: the weather sucks.

#### [Phone scam victim](https://youtu.be/bD6UZb3lM1w?si=OR_Cd1yEYTwZfdiJ)
* phone/voice/txt scam, usuall they give some **sense of** urgency, ask you
don't **hang up** the phone
* my whole body was **shaking**
* The phone **rang** and I answered it
* the person **hysterically**/hɪsˈt ɛrɪk lɪ/ crying on the phone: deriving from
or affected by uncontrolled extreme emotion
* broken **wrist**/rɪst/, **waist**/weɪst/
* free trial: try out a product or service for a limited period without paying
for it 
* phone scams come in many forms
* you need to **rationalize** your weird behavior: attempt to explain or justify
with logical, plausible reasons, even if these are not true or appropriate
* I feel **sorry** for these people 悲哀

#### [Why Americans Can’t Keep Their Paychecks](https://youtu.be/_QP9HESt3nc?si=gVSAf4YmYuNXmKX1)
* this may be the result of a **sneaky behavioral**(lack of transparency)
phenomenon called **lifestyle creep**/kriːp/: **lifestyle inflation**, refers to
the gradual increase in spending or expenses as an individual's income rises,
it is hard to undo/rollback economically. 由俭入奢易，由奢入俭难, end up with living paycheck to paycheck.
* **eat out** rather than cook at home
* can have serious financial **consequence**: often carries a sense of
importance, significance, or **causality/kɔːˈzæləti/** 因果关系
* the solution to make more money isn't really **realistic**: having or showing
a sensible and practical idea of what can be achieved
* super money saver

#### [Why Pharmaceuticals Are So Complicated In The U.S](https://youtu.be/woZmq6gIZBM?si=3FFGdJRYGP_6tUW2)
* Pharmaceutical/ˌfɑːrməˈsuːtɪkl/: relating to medicinal drugs, or their
preparation, use, or sale
* prescription drug: an instruction written by a **medical practitioner**从业者
that **authorizes** a patient to be provided a **medicine** or **treatment**
* complicated relationship between insurance, pharmacy and coverage
* 1 in 4 Americans have trouble affording their **medication**/ˌmedɪˈkeɪʃn/:
treatment using drugs
* buy a product **in bulk/bʌlk/** in low price and then sell it at a higher price
* COVID-19 global **outbreak**
* I believe that mRNA as the technology will be **transformative**: 有改革能力的
* it has dividend growth for nearly 60 years and has consistently **outperformed**
the S&P 500 over past 25 years
* it is really seen as a **bellwether**/'bɛl,wɛðɚ/ in the space: the leading
sheep of a flock
* focus on **core competency/ˈkɑmpɪtən,see/** or diversity: a defining capability
or advantage that distinguishes a person or enterprise from others
* **baby powder** cancer case
* it's a topic of debate, and it's not clear that over the longer term, if that
might not be sth you could pursue.
* pharma and medical devices still have a lot of **synergies**/'sɪnɚdʒi/: 1+1>2
* consumer business(B2C), the opposite is B2B, Examples of consumer businesses
include supermarkets, clothing stores, online retailers, restaurants,
entertainment providers, etc.
* the **controversy/ˈkɑːntrəvɜːrsi/** behind the **Alzheimers**'s drug
* in addition to that, you have to **take into account the fact** that ...
* covers **the elderly**/ˈeldərli/, a lot of patients will be **influenced** by
the coverage decision, it could be extremely **influential**/ˌɪnfluˈenʃl/: having
great influence on someone or something

#### [How China's BYD Overtook Tesla](https://youtu.be/Ttu55nEtC6o?si=eKcj0vsrT_39tYts)
* BYD has **overtaken** Tesla as the world's largest seller of EV
* what **set BYD apart**: high tech, vertical integration, mobility solution
* the supply chains were just in a state of **chaos** during the pandemic

#### [How China Became KFC’s Most Important Market](https://youtu.be/cTFu2xtqnKQ?si=y6RNqgT6qmHcAN8u)
* KFC: one of world's largest food chains
* in the city of Hangzhou, about **an hour outside of** Shanghai
* rice congee, steamed dumpling, egg tart/tɑːrt/

#### [How Criminals Are Making Millions Counterfeiting Prescription Drugs](https://youtu.be/-VbiHC4gsTE?si=ilIqxStvsBb0gVXw)
* he was the **mastermind** of 230M dollars **drug counterfeiting** operation.
* **drug diversion**: illegal or unauthorized distribution or **trafficking**
(traffic's ing state: deal or trade in something illegal:) of drugs
* **fake drug** in recycled bottle
* who agreed to an interview if we **concealed/kənˈsiːl/** his identity: keep
from sight; hide
* what are the most **lucrative**/ˈluːkrətɪv/ drugs to resell: producing a great
deal of profit
* hidden/'hɪdn/ camera video

#### [Will The U.S. Remain The World's Leading Economy?](https://youtu.be/Zs4Htc8lqM0?si=AOeofnr9sFvakP0H)
* but one bigget factor they have **playing against** them is the various
**demographics** dynamics: aging population, low birth rate, rising dependency
ratio, which means fewer working age people
* we will be **prosperous/ˈprɑːspərəs/** if we are nice to each other
* there is **a lot of synergies** between china and US

#### [How Private Credit Became One of the Hottest Investments on Wall Street](https://youtu.be/nXyot2gnxqg?si=1RAsq2HVlQNrOWD6)
* what private credit is just **in a simplest form**, is xxx
* the best way to explain what xx is is to first start with what it isn't
* why private credit is **booming**?

#### [Are Timeshares Worth It?](https://youtu.be/PkB9UAx2fuI?si=ofpgikp_remaOZUO)
* close the loophole: an ambiguity or **inadequacy** in the law or a set of rules.
* we have come here to **take part in** the major game.
* justice department: 司法部门
* he **allegedly/əˈlɛdʒd/** received more than %30 million: 据称
* it became **common knowledge** that the resale depreciation is almost 100%, the
industry would't collapse, but it would **certainly fundamentally** change
overnight
* he was incapable of leading a bowling team, **let alone** a country: far less
likely, possible
* This is a time when we need to either heavily regulate or **outright** ban
this "timeshare" system: completely, immediately

#### [Why United Airlines Invested $1 Billion In Denver Airport](https://youtu.be/fO-qr4G6E1o?si=Z8aDJeU2_yBXqm_L)
* passangers have returned in a **record breaking number**, much higher than our
**forecast**
* it becomes the U.S. biggest **hub**
* explore how the airport and airline plan to **keep up with** demand: learn
about or be aware of current events or developments
* we are **projecting** a lot of growth: estimate, forecast
* whoever pushed 30 years ago for an airport outside the city with no limits on
size is the real **visionary**: thinking about or planning the future with
imagination or wisdom
* I was just at the Denver Airport with an hour long **layover**. It's a very
nice airport, but needs more **eateries**/'itəri/: a restaurant or other place
where people can be served food
* copy room
* employee **cafeteria**: a restaurant or dining room]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>english</tag>
      </tags>
  </entry>
  <entry>
    <title>Conversation Resume for Small Talk</title>
    <url>/2023/01/01/english-conversation-resume/</url>
    <content><![CDATA[
### Daily life:
* How is your week/day going?
* What are you working on recently?

* What did you do over the weekend?
* Any exciting plans for the weekend/hoilday?
* Any plan on the upcoming weekend?


### Personal
* when did you arrive?
* where are you from?
* How did you get here, flight or driving?
* Where did you go to school, what major you learned?
* What do you do for work?
* Do you have family here?

* How long have you lived at your current location? What do you like about it?
* How long have you worked at your current job?
* Have you ever been to xxx?

* Do you have any pets? What kind?

* Do you have any hobbies you enjoy in your free/leisure time?
* what's your biggest passion outside of work?
* Have you seen any good movies or TV shows recently?
* Have you tried any interesting apps or gadgets/ˈɡædʒɪt/ recently?
* Do you have any favorite artists or types of art?

* Where did you get that xxx ? I really like it!

### Notable
* What are your five most **unique experiences**?
* What are your five most personally significant accomplishments?
* Name the 10 things you cannot live without?
* What is the biggest regret/rɪˈɡret/ in your career?

### Staying current:
* What are the top five current events of the week and month? Learn the basic
and develop an opinion and stance/stæns/ on them.
* What are the four most **funny personal situations** from the past week?
* What are the four interesting things you have heard or read about in past week?
]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>english</tag>
      </tags>
  </entry>
  <entry>
    <title>Rollover Alias, ILM and Data Tiers</title>
    <url>/2022/08/06/elastic-rollover-alias/</url>
    <content><![CDATA[
At the time of this writing, we use Elasticsearch version 7.16.2, the reference is based on this version and the content may be subject to change.

# Demo
Quick [Demo to practice](https://github.com/chengdol/InfraTree/tree/master/docker-elasticsearch#alias-plus-ilm) rollover alias and ILM.

# Why not Data Stream
[Manage time series data without data streams](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/getting-started-index-lifecycle-management.html#manage-time-series-data-without-data-streams).

We recognise there might be use-cases where data needs to be updated or deleted in place and the data streams don’t support delete and update requests directly. In these cases, you can use an index alias to manage indices containing the time series data and periodically roll over to a new index.

# Clarification
Note the difference between the `rollover_alias` field and the `_alias` API, the `rollover_alias` is configed in index template's settings section and paired with ILM to ease rollover. The `_alias` API is more like a group operation to convenient the search, query, etc and you can also set write index for it.

In the Kibana template creation, there is a `Alias` page, it has nothing to do with `rollover_alias`.

After applying the `rollover_alias`, the managed backing index will have alias(you can see from GET or Kibana) and the `_alias` API works on it as well.

Reference for [Alias API](https://www.elastic.co/guide/en/elasticsearch/reference/current/aliases.html).

# Logstash Plugin
Just like the data stream, the rollover_alias has its place in [Logstash Elasticsearch output plugin](https://www.elastic.co/guide/en/logstash/7.16/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-ilm), for example:
```ini
output {
    elasticsearch {
        ilm_rollover_alias => "custom"
        ilm_pattern => "000001"
        ilm_policy => "custom_policy"
    }
}
```
From document, this config will overwrite the index settings and adjust the Logstash template to write the necessary settings for the template to support index lifecycle management, including the index policy and rollover alias to be used, So looks like there is no need to preconfig the template to have rollover_alias and ILM, Logstash will do it.

]]></content>
      <categories>
        <category>Elastic</category>
      </categories>
      <tags>
        <tag>elastic</tag>
        <tag>alias</tag>
      </tags>
  </entry>
  <entry>
    <title>English Vocab Improvements</title>
    <url>/2023/09/24/english-vocab/</url>
    <content><![CDATA[
#### [烹饪方法英文大全](https://youtu.be/VLRkqdnadi0?si=p-8Atdd544sgw0RE)
* specific words to describe cooking techniques
* favorite dish: 一道菜, 2 dishes
* **stir fry**用旺火煸,用旺火炒 some veg/vɛdʒ/ and meat, whatever I have **on hand**
在手边
* pan-fry dumpling /'dʌmplɪŋ/: 煎饺
* brown/braʊn/ all sides of steak: 煎焦
* braise/breɪz/ 红烧, braised pork belly 红烧肉
* stew/stu/ 炖
* gradual simmering/ˈsɪmər/ 小火慢炖
* kettle 水壶
* simmering pot 炖盅
* preserves all nutrients/ˈnuːtriənt/ in your food.
* blanch/blæntʃ/ the vegs before stir fry: 焯水
* marinate/'mærɪnet/ the chicken in the fridge overnight: 腌
* pickled/'pɪkl/ beef: 酱牛肉
* smoked duck: 板鸭，熏鸭
* roasted chicken: 烘烤
* grilled: 明火烤，烧烤
* broil/brɔɪl/: 炙烤
* Oyster sauce 蚝油

#### [How to memorize and activate the words](https://youtu.be/I2nCvxJqSmY?si=SEQ_-n0Wu5-sNGMH)
* rote/roʊt/ memory死记硬背 is usually **counterproductive** 反作用. 
* common sense: 常识
* have a habit to make vocab notes.
* recalibrate/rɪ' kælɪbreɪt/ the notes.

#### [请客吃饭]
* let me pay for the bill
* this one is on me
* let me take care of this one
* we talked about it, it's my treat
* next time you take me out for steak

#### Common Chinese Food
* soya milk 豆浆
* fried dough/doʊ/ stick 油条
* shumai
* rice noodle
* bao
* fried pancake 煎饼
* naan/nɑn/
* steamed bun/bʌn/ 馒头
* soup dumpling: 灌汤包
* egg fried rice/raɪs/: 蛋炒饭
* congee/'kɑndʒi/ 粥
* ramen/ramyun(korean)
* pot sticker 锅贴
* spring roll 春卷
* smoked sausage/ˈsɔːsɪdʒ/ 香肠
* tofu pudding 豆花
* seaweed soup 海带汤
* twice-cooked pork 回锅肉
* kung pao chicken 宫保鸡丁
* Peking/'pi:'kiŋ/ duck 北京烤鸭
* spicy tofu 麻婆豆腐
* sweet and sour pork 糖醋里脊
* stewed chicken with mushroom 小鸡炖蘑菇
* pork belly 五花肉
* kimchi/'kɪmtʃhi/ 朝鲜泡菜
* soft tofu stew 豆腐锅
* beef: primarily the cattle meat
* stew beef brisket 牛腩
* tripe/traɪp/ 金钱肚
* abomasum/ˌæbo'mesəm/ 毛肚
* large intestine/ɪn'tɛstɪn/ 大肠
* seaweed salad 海带丝
* dried seaweed 紫菜
* sour and spicy shredded/ʃred/ potatos 酸辣土豆丝
* lamb/læm/: primary the young sheep
* YuXiang, Fish fragant/ˈfreɪɡrənt//flavored pork slice(sliver)/ˈslɪvər/
* pistachio/pɪ'stæʃɪo/ 开心果
* almond /'ɑmənd/
* cashew/'kæʃʊ/
* pomegranate/'pɑmɪɡrænɪt/
* durian/ˈdʊriən/
* mangosteen/'mæŋgə,stin/
* dragon fruit
* oat/ot/ 燕麦
* pea/piː/ 豌豆
* sunflower seed
* cinnamon/'sɪnəmən/肉桂；肉桂皮
* anise/'ænɪs/ 茴
* star anise 八角，八角茴香
* cumin/'kjuːmɪn/ 孜然
* leechee/'li:tʃi:/
* persimmon/pɚ'sɪmən/ 柿子
* haw/hɔ/ 山楂
* passionfruit 百香果
* fig/fɪɡ/ 无花果
* snow pea 豌豆
* kidney /ˈkɪdni/
* vinegar/ˈvɪnɪɡər/
* abalone/ˌæbə'loni/ 鲍鱼
* roe/roʊ/ 鱼卵,鱼子
* longan/'lɔŋɡən/
* papaya/pə'paɪə/
* sugarcane/ˈʃʊɡɚˌken/
* cucumber/ˈkjuːkʌmbər/

#### Country and Capital near China in south Asia
* Myanmar/'mjænmɑ:/(Burma/ˈbə..ma/): Naypyidaw/nei-pyee-daw/, Yangon(old)
* Laos: Vientiane /vi-yan-chain/
* Thailand: Bangkok/ˈbæŋˌkɑk/
* Vietnam /ˌvjet'næm/: Hanoi/hæ'nɔi/, Ho Chi Minh City
* Cambodia/kæmˈbodiə/: Phnom Penh/pəˈnɔm ˈpɛn/
* Malaysia/mə'leiʒə/: Kuala Lumpur/ˈkwɑlə lʊmˈpʊr/
* Philippines: Manila/mə'nɪlə/, Mindanao/ˌmɪndəˈnɑo/棉兰老岛
* Nepal/ni'pɔ:l/: Kathmandu/ˌkætmænˈdu/
* Bhutan/bu:'tæn/: Thimphu /'θimfu:/


#### Northern America
High tech regions in Canada:
* Toronto/tə'run təu/
* Vancouver/vænˈkuvɚ/ 温哥华
* Ottawa/'ɔtəwə/: capital
* Montreal/ˌmɑntriˈɔl/ 蒙特利尔
* Waterloo Region 滑铁卢

# Southern America
* Argentina/ˌɑ:dʒən'ti:nə/ 阿根廷

#### Europe
* Belarusian/ˌbɛləˈru si en/ 白俄罗斯的，白俄罗斯人的, 白俄罗斯人
* Ukraine/Ukrainian /juˈkreniən/
* Lithuania/Lithuanian /ˌliθju:'einiə/ 立陶宛
* Switzerland

#### Africa
* Algeria /æl'dʒiəriə/ 阿尔及利亚 

#### Middle East
* Israel/'izreiəl/, Israeli/ɪzˈreli/
* Palestine/'pælistain/, Palestinian/ˌpælis'tiniən/
* Arab/'ærəb/, Arabian/əˈrebiən/ 

#### DaiWW
* interfere vs intervene: "intervene" generally suggests getting involved in a
situation to positively influence or assist, while "interfere" implies
involvement that disrupts or negatively impacts the situation. 
  * The teacher intervened to stop the bullying and resolve the conflict between
  the students.
  * I didn't want to interfere with their plans, so I refrained from offering my
  opinion.

* embassy(locates in capital of foreign country) vs consulate(other cities):
Ambassadors lead embassies, while consuls or consuls general lead consulates.

* Import Expo/ˈɛkspo/: a large exhibition
* Foreign Minister, Foreign Ministry spokesperson, ministry of foreign affairs
* **ministry of** economy

* dumb question, cry more pls, spineless govt, money talks
* national sovereignty/ˈsɑːvrənti/, economic sovereignty
* propaganda war
* state department (美国)国务院
* supplement(for adding value to proposal in design) vs complement(互补)
* democratic system, political system, public show

* prisoner/ˈprɪznər/
* I don't oppose/əˈpoʊz/ the ideas such as feminism/'fɛmənɪzəm/ and LGBT
* minority always duped to play US imperialist /ɪm'pɪrɪəlɪst/ game,
legitimate/lɪˈdʒɪtɪmət/ issues and grievance/ˈɡriːvəns/ are then hijacked/ˈhaɪdʒæk/ 
by US agents to play their games.

* electromagnetic catapult/'kætəpʌlt/, stealth/stelθ/ carrier/ˈkæriər/-based jet
* hegemony/hɪ'dʒɛmoni/

* view from a millennium/məˈlɛniəm/ perspective
* they are not in a hurry
* moving forward step by step seems to be the current strategy
* despite the worst year in Chinese real estate history, China's GDP growth can
still maintain a growth rate of 5%. The Chinese economy is clearly transitioning
from real estate to high-end manufacture such as electric vehicles and chips.
* genocide/'dʒɛnəsaɪd/ against XJ muslims /ˈmʌzləm/
* Find a patient who has been severely brainwashed, good luck.
]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>english</tag>
      </tags>
  </entry>
  <entry>
    <title>Elasticsearch Corrupted Shards</title>
    <url>/2021/12/24/elastic-corrupt/</url>
    <content><![CDATA[
We accidently configured all nodes the master role and wrong data path in upgrade and result in all shards unassigned with cluster red status, this led to data loss and corrupted shards.

For example, from the cluster health API the cluster status after upgrade:
```json
{
  "cluster_name": "xxx",
  "status": "red",
  "timed_out": false,
  "number_of_nodes": 5,
  // no data nodes
  "number_of_data_nodes": 0,
  // no primary shards
  "active_primary_shards": 0,
  "active_shards": 0,
  "relocating_shards": 0,
  "initializing_shards": 0,
  // all unassigned
  "unassigned_shards": 33,
  "delayed_unassigned_shards": 0,
  "number_of_pending_tasks": 0,
  "number_of_in_flight_fetch": 0,
  "task_max_waiting_in_queue_millis": 0,
  "active_shards_percent_as_number": 0
}
```

In this case you need to take a glance at node status, it turns out that we have wrong configuration, all node are set to master, data path is wrong too:
```js
curl "localhost:9200/_cat/nodes"

172.16.0.141  5 86 1 0.00 0.10 0.19 im - 172.16.0.141
172.16.0.140 24 86 2 0.02 0.14 0.16 im - 172.16.0.140
172.16.0.138  4 66 0 0.27 0.18 0.24 im - 172.16.0.138
172.16.0.137  4 73 0 0.00 0.07 0.12 im - 172.16.0.137
172.16.0.152  4 86 1 0.00 0.04 0.09 im * 172.16.0.152

// for data path, master and data nodes may different but the same kind 
// should be the same path
```

The solution is to set the right configuration (node role and data path) and restart the whole cluster, Usually, the node rejoin will transform `unassigned` to `assigned/started`, if not, the data may lose and corrupted so still unassigned.

From allocation explain API to get details:
```bash
curl "http://localhost:9200/_cluster/allocation/explain" | jq
```
```json
{
  "note": "No shard was specified in the explain API request, so this response explains a randomly chosen unassigned shard. There may be other unassigned shards in this cluster which cannot be assigned for different reasons. It may not be possible to assign this shard until one of the other shards is assigned correctly. To explain the allocation of other shards (whether assigned or unassigned) you must specify the target shard in the request to this API.",
  "index": "elastalert-status",
  "shard": 0,
  "primary": true,
  "current_state": "unassigned",
  "unassigned_info": {
    "reason": "CLUSTER_RECOVERED",
    "at": "2021-12-20T23:54:16.720Z",
    "last_allocation_status": "no_valid_shard_copy"
  },
  "can_allocate": "no_valid_shard_copy",
  "allocate_explanation": "cannot allocate because a previous copy of the primary shard existed but can no longer be found on the nodes in the cluster"
}
```

How to proceed, in any node:

1. retry reroute
```bash
curl -XPOST "localhost:9200/_cluster/reroute?retry_failed=true"
```

2. force reroute and accept data loss
See [explanation](https://www.elastic.co/guide/en/elasticsearch/reference/6.8/cluster-reroute.html#_forced_allocation_on_unrecoverable_errors) for these 2:
```json
// allocate a primary shard to a node that holds a stale copy
curl -XPOST "localhost:9200/_cluster/reroute" \
-H "Content-Type: application/json" \
-d \
'{
    "commands":
    [
        {
            "allocate_stale_primary":
                {
                    "index" : "elastalert-status",
                    "shard" : 0,
                    "node" : "172.16.0.138",
                    "accept_data_loss" : true
                }
        }
    ]
}'

// this actually deletes target index
curl -XPOST "localhost:9200/_cluster/reroute?pretty" \
-H "Content-Type: application/json" \
-d \
'{
    "commands":
    [
        {
          "allocate_empty_primary" :
          {
                "index" : "elastalert-status",
                "shard" : 0,
                "node" : "172.16.0.138",
                "accept_data_loss" : true
          }
        }
    ]
}'
```

3. reindex from a backup if you have

At that time the #2 solved the issue but we lost the all influenced indicies.

# Reference
[How to resolve unassigned shards in Elasticsearch](https://www.datadoghq.com/blog/elasticsearch-unassigned-shards/), this series is good.]]></content>
      <categories>
        <category>Elastic</category>
      </categories>
      <tags>
        <tag>elastic</tag>
      </tags>
  </entry>
  <entry>
    <title>Adding commits to Others&#39; PR</title>
    <url>/2021/04/16/git-add-commits-other-pr/</url>
    <content><![CDATA[
This kind of operation is useful when the initiator is off-line or the changes are straightforward so no need to wait until initiator gets involved.

For direct PR or MR without fork, the steps are:
1. checkout target branch of the PR
2. perform changes on this branch
3. git add --all
4. git commit --amend
5. git push -f

For fork scenario, see [Adding Commits to Someone Else's Pull Request](https://tighten.co/blog/adding-commits-to-a-pull-request/).









]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Logstash UDP Input Data Lost</title>
    <url>/2022/05/01/elastic-logstash-udp-drop/</url>
    <content><![CDATA[
Regarding Logstash introduction recaps, please have a look at [Elastic Stack Quick Start](https://chengdol.github.io/2020/05/20/elastic-stack/)

Issue description: Over the past few months, we have been seeing some logs that were indeed generated, but they lost in Elasticsearch database, which result in false PD alert as the alert expression relies on these missing logs.

The data path is common and typical: 
```yaml
Data source 
=> Logstash(UDP input + filter + Kafka output)
=> Kafka 
=> Logstash(Kafka input + filter + Elasticsearch output) 
=> Elasticsearch(Kibana)
```
The lost could happen in transit at any point, let's check and narrow the scope down.

At very beginning, I just suspected the data lost on Kafka without any proof and adjusted the `ack` level on Logstash Kafka output plugin, it turns out not the case.

# Does The UDP Packet Reach VM?
As UDP transfer is not reliable, the packets could be lost before they reached Logstash VM, we can have background tcpdump to sniff the UDP packets from target host and port to verify, for example:
```bash
# time limit the process run
# -k/--kill-after: send kill after 5s if TERM signal does not work
# 12h: duration, run tcpdump 12 hours
timeout -k 5 12h \
# -i: interface
# -nn: don't convert protocol and port numbers to name
# -w: write to pcap file
# -U: with -w, write packet to file immediately
# -C: rollover file size limit: 200MB
tcpdump -i eth0 \
# filter capturing with src IP started from 172.16 and port 514 only
-nn src 172.16 and port 514 \
-wU /home/example/logstash-0.pcap \
-C 200 \
&> /dev/null &
```

It is important to limit the pcap file size and roll over new file if necessary, for example, the pcap file generated from above command will be:
```bash
# suffix number starts from 1, auto appended.
/home/example/logstash-0.pcap
/home/example/logstash-0.pcap1
/home/example/logstash-0.pcap2
/home/example/logstash-0.pcap3
```
Each will have file size <= 200MB as `-C` specified.

Then you can use Wireshark or tcpdump itself to read/filter the pcap file:
```bash
# -r: read pcap file
# -v: show verbose packet info
tcpdump -r /home/example/logstash-0.pcap2 \
# filter expression
src 172.16.53.4 and port 514 \
-nn \
-v  \
| grep "target info"
```
Then we know the UDP packets actually reached the VM, move to the next question.

## Other tcpdump Usages
If you want to have file creation in a strict time interval manner:
```bash
# -G: rotated every 1800s duration
# must use strftime filename, otherwise new file will overwrite the previous
tcpdump -i eth0 -G 1800 -w /tmp/trace-%Y-%m-%d-%H:%M:%S.pcap
# -W: limit the file number to 3, and exit 0
# so it will only create 3 files and each 30 mins
tcpdump -i eth0 -W 3 -G 1800 -w /tmp/trace-%Y-%m-%d-%H:%M:%S.pcap
```

We can have `-C`(size) and `-G`(time) together, but need the timestamp file name! For example:
```bash
tcpdump -i eth0 -w /tmp/trace-%Y-%m-%d-%H:%M:%S.pcap -G 3 -C 2
```
The file creation is either every 3s or 2MB, whichever comes first, look the size rollover is within each time interval:
```bash
-rw-r--r--. 1 tcpdump              2.0M May  2 06:30 trace-2022-05-02-06:30:20.pcap
-rw-r--r--. 1 tcpdump              2.0M May  2 06:30 trace-2022-05-02-06:30:20.pcap1
-rw-r--r--. 1 tcpdump             1003K May  2 06:30 trace-2022-05-02-06:30:20.pcap2
-rw-r--r--. 1 tcpdump              2.0M May  2 06:30 trace-2022-05-02-06:30:23.pcap
-rw-r--r--. 1 tcpdump              2.0M May  2 06:30 trace-2022-05-02-06:30:23.pcap1
-rw-r--r--. 1 tcpdump              1.1M May  2 06:30 trace-2022-05-02-06:30:23.pcap2
-rw-r--r--. 1 tcpdump              1.5M May  2 06:30 trace-2022-05-02-06:30:26.pcap
-rw-r--r--. 1 tcpdump              1.7M May  2 06:30 trace-2022-05-02-06:30:29.pcap
-rw-r--r--. 1 tcpdump              301K May  2 06:30 trace-2022-05-02-06:30:32.pcap
```

You can rotate the file by size and limit the number of file created, 
```bash
# -W: file count limit 3
# -C: rollover file size limit: 2MB
tcpdump -i eth0 -w /tmp/trace.pcap -W 3 -C 2
```
The result could be:
```bash
# rotated among these 3 files
-rw-r--r--. 1 tcpdump             tcpdump             2.0M May  2 06:55 trace.pcap1
-rw-r--r--. 1 tcpdump             tcpdump             2.0M May  2 06:55 trace.pcap2
-rw-r--r--. 1 tcpdump             tcpdump             1.5M May  2 06:55 trace.pcap0
```
or by time and size both, note that rotation happens only within the timeslice: every 3s when the size exceeds!
```bash
# must use timestamp file name
tcpdump -i eth0 -w /tmp/trace-%Y-%m-%d-%H:%M:%S.pcap -W 5 -C 2 -G 3
```

# Does Logstash UDP Input Drop Packet?
This can be verified by adding another output plugin `file` to store target info to a local file, for example in logstash config file:
```ini
input {}
filter {}

output {
    kafka {}
    if "target info" in [message] {
      file {
        # 'sourceip' is a field from record
        path => "/var/log/logstash/highlight/%{sourceip}.log"
      }
    }
}
```
By checking the log file, I know the output does not send the message out, plus the filter is good, so it must be `input {}` UDP dropped the data.


# Why Does UDP Drop Packet?
UDP is not a reliable transport. By design, it will drop messages if it does not have space to buffer them. 

There is a [blog](https://linux-tips.com/t/udp-packet-drops-and-packet-receive-error-difference/237/2) talks about the UDP packer error and UDP receive buffer error, see from `netstat -us`, they are the first-hand indicator for packet drop.


# How to Solve It?
Luckly, we can increase the queue size and buffer size, for example:
```ini
input {
  udp {
    port => 514
    type => syslog
    queue_size => 100000 # default is 2000
    receive_buffer_bytes => 16777216 # 16 MB, default uses system sysctl value
    workers => 4 # depends on the vcpu number, default is 2
  }
}
```

The value really depends on your traffc, you can run `sar -n DEV 1` to find a reasonable estimation. Moreover, you need to uplift system socket receive buffer in order to set Logstash `receive_buffer_bytes` correctly if it is larger than system default buffer, for example:
```bash
echo "net.core.rmem_max=16777216" >> /etc/sysctl.conf
sysctl -p
```
Finally, increase the Logstash JVM heap size accordingly, usually half of the RAM size for both `Xms` and `Xmx`.

Then restart and check the status of new config:
```bash
systemctl restart logstash
# verbose to see the config change
systemctl status logstash -l
```


# Horizontal Scaling
If the traffic is heavy, vertical scaling on cpu core or RAM is not enough and the packet drop continues. It turns out the Logtash does not scale will the increasing CPU cores, see this [comment](https://discuss.elastic.co/t/netflow-codec-udp-receive-errors/127111/8).

So in this case you have to do horizontal scaling.


# Monitoring/Alert
It is helpful to have Grafana dashboard to display drop rate as to inbound traffic, for example:
```bash
label_replace(rate(node_netstat_Udp_InErrors{instance=~".*-example-.*"}[5m])
/ on(instance)
rate(node_network_receive_packets_total{device="eth0", instance=~".*-example-.*"}[5m])
, "new_name", "$1", "instance", "prefix-(.+)") 
* 100
```

There is the [list](https://www.robustperception.io/network-interface-metrics-from-the-node-exporter) of metrics exposed by node exporter.


# Postscript
There are other things I practiced:
1. Logstash config and syntax for input, filter, output plugin.
2. Using `nc` as UDP client to test Logstash UDP input and filter.
3. Revisit Kafka config and know possible data loss in Kafka side.
4. Revisit PromQL 


最后把系统优化中的网络部分又看了一遍加深印象:D
]]></content>
      <categories>
        <category>Elastic</category>
      </categories>
      <tags>
        <tag>logstash</tag>
        <tag>tcpdump</tag>
      </tags>
  </entry>
  <entry>
    <title>Ceph</title>
    <url>/2020/01/20/fs-ceph/</url>
    <content><![CDATA[
## ceph github
https://github.com/ceph/ceph

## Ceph for object storage, block storage and network file system
Ceph uniquely delivers object, block, and file storage in one unified system.
differences: 
https://cloudian.com/blog/object-storage-vs-block-storage/

## for cephFS, https://docs.ceph.com/docs/master/
what does NFS/CIFS deployable mean?

## how to begin:
https://docs.ceph.com/docs/master/start/intro/
Whether you want to provide Ceph Object Storage and/or Ceph Block Device services to Cloud Platforms, deploy a Ceph File System or use Ceph for another purpose, all Ceph Storage Cluster deployments begin with setting up each Ceph Node, your network, and the Ceph Storage Cluster. 

> ceph ansible playbook?

## ceph can be installed by cephadm (like k8s by kubeadm)：
https://docs.ceph.com/docs/master/bootstrap/#installation-cephadm
cannot get cephadm from yum install, need to install ceph repos and get rpms

构造好ceph cluster，现在的问题是怎么和k8s联合起来？






]]></content>
      <categories>
        <category>Storage</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ceph</tag>
        <tag>filesystem</tag>
      </tags>
  </entry>
  <entry>
    <title>Public Speaking Anxiety</title>
    <url>/2023/01/01/english-public-speak/</url>
    <content><![CDATA[
Please revisit this
[blog](https://nationalsocialanxietycenter.com/social-anxiety/public-speaking-anxiety)
to help youself understand what caused public speaking anxiety and how to overcome
it.

Brain freeze/ Mind going blank usually come when:
* speak to native speaker.
* speak to more senior people.

The underlying fear is judgment or negative evaluation by others.

1. rehearse/rɪˈhɜːrs/ to increase confidence, don't memorize words but focusing
on message delivery.
2. speak slowly, clearly, take deep breath.
3. practice with written notes, bullet points, keep discussion on track.
4. practice recovering from a brain freeze: purposely stopping the talk and
shifting attention to elsewhere, use notes to bring back.
5. practice for the worest: what to say to the audience if our mind goes blank.

Self-help
[videos](https://nationalsocialanxietycenter.com/social-anxiety-self-help-videos/).]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>english</tag>
      </tags>
  </entry>
  <entry>
    <title>Elastic Stack Quick Start</title>
    <url>/2020/05/20/elastic-stack/</url>
    <content><![CDATA[
//TODO:
[ ] elasticsearch certificate
[ ] es ML node for abnormal detect (这个还有点意思，用来分析数据的)
[ ] logstash input data for testing, [movielens](https://grouplens.org/datasets/movielens/)
[x] [linkedin learning](https://www.linkedin.com/learning/learning-the-elastic-stack-2/start-listening-to-your-infrastructure?u=56685617): 
[x] JKSJ elasticsearch training and [git repo](https://github.com/geektime-geekbang/geektime-ELK/tree/master/part-1)
[x] [cerebro](https://github.com/lmenezes/cerebro), [tutorial](https://www.youtube.com/watch?v=ZjVmg9fftUM)


# ELK Stack
The [Elastic Stack](https://www.elastic.co/elastic-stack) is one of the most
effective ways to leverage open source technology to build a central logging,
monitoring, and alerting system for servers and applications.

* `Elasticsearch`: distributed, fast, highly scalable **document database**.
* `Logstash`: aggregates, filters and supplyments log data, forwards them to
Elasticsearch or others.
* `Kibana`: web-based front-end to visualize and analyze log data.
* `Beats`: lightweight utilities for reading logs from a varity of sources,
sends data to Logstash or other backends.
* `Altering`: send notifications to email, slack, pagerduty so on and so forth.

![](https://drive.google.com/uc?id=1ch29tlydlCpFxh2RARV0EVxH_1AWSzRl)

[ELK Stack vs Prometheus](https://stackoverflow.com/questions/40793901/prometheus-vs-elasticsearch-which-is-better-for-container-and-server-monitoring):
ELK is general-purpose no-sql stack can be used for monitoring, aggregating all
the logging and shipping to elastic search for ease of browsing all the logging
and similar things. Prometheus is dedicated monitoring system, alongside with
service discovery consul and alert-manager.

My [Vagrant elasticsearch cluster setup](https://github.com/chengdol/InfraTree/tree/master/vagrant-elasticsearch).
Java runtime and `/bin/bash` that supports array are required, also note that
elasticserach cannot boot by root user.

Another option is to use docker compose to create testing elasticsearch cluster,
see my repo [here](https://github.com/chengdol/InfraTree/tree/master/docker-elasticsearch).

# Elasticsearch
[Version Rolling Upgrade](https://www.elastic.co/guide/en/elasticsearch/reference/current/rolling-upgrades.html#_upgrading_your_cluster), some highlights:

1. Set `index.unassigned.node_left.delayed_timeout` to hours
2. Starts from data nodes, then master nodes, one by one, ensure config yaml
file is correct for each role
3. Wait for recovery with big retries
4. Revert `index.unassigned.node_left.delayed_timeout`
5. Upgrade kibana version

This [blog series](https://www.elastic.co/blog/just-enough-kafka-for-the-elastic-stack-part1)
talks about `kafka + elastic` architecture.

This [blog](https://mincong.io/en/prevent-data-loss-in-elasticsearch/#message-queue-integration)
shares ways to enable data high reliability as well as extending resources. As
we see the kafka message queue also benefits the data reliability besides
throttling.

There are [several ways](https://www.elastic.co/downloads/elasticsearch) to
install: binary, rpm or on kubernetes. The package is Java self-contained, you
can also specify `ES_JAVA_HOME` to use external Java.

Install using [archive](https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html). 
The Elasticsearch `.tar.gz` package does not include the systemd module (have to
create by yourself). To manage Elasticsearch as a service easily, use the Debian
or RPM package instead.

It is advisable to change the default locations of the config directory, the
data directory, and the logs directory. Usually data directory is mounted on
separate disk.

Before launching, go to edit `$ES_HOME/config/elasticsearch.yml`. The
configuration files should contain settings which are node-specific (such as
node.name, node.role and storage paths), or settings which a node requires in
order to be able to join a cluster, such as cluster.name and network.host.

The config path can be changed by `ES_PATH_CONF` env variable.

- `elasticsearch.yml` for configuring Elasticsearch
- `jvm.options` for configuring Elasticsearch JVM settings, can refer to github
jvm configuration for different release branch: https://github.com/elastic/elasticsearch/blob/master/distribution/src/config/jvm.options
- `log4j2.properties` for configuring Elasticsearch logging, for example the logs
path is `/var/log/elasticsearch`

Important [elasticsearch settings](https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html)

For example:
```yaml
# cluster name
cluster.name: chengdol-es
# node name
node.name: master
# ip to access, the host public IP
# or using interface name such as _eth1_
network.host: 9.30.94.85

# a list of master-eligible nodes in the cluster
# Each address can be either an IP address or a hostname 
# that resolves to one or more IP addresses via DNS.
discovery.seed_hosts:
  - 192.168.1.10:9300
  # port default 9300
  - 192.168.1.11
  - seeds.mydomain.com
  # ipv6
  - [0:0:0:0:0:ffff:c0a8:10c]:9301
```

To form a production cluster, you need to specify, for node roles, see this
[document](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#node-roles)
about how to statically specify master and data nodes.

```ini
cluster.name
network.host
discovery.seed_hosts
cluster.initial_master_nodes
# specify dedicated node role
# lower version have different syntax
node.roles: [ master ]
node.roles: [ data ]
```
The master node is responsible for lightweight cluster-wide actions such as creating or deleting an index, tracking which nodes are part of the cluster, and deciding which shards to allocate to which nodes. 

High availability (HA) clusters require at least three master-eligible nodes, at least two of which are not voting-only nodes. Such a cluster will be able to elect a master node even if one of the nodes fails.

Data nodes hold the **shards** that contain the documents you have indexed. Data nodes handle data related operations like CRUD, search, and aggregations. These operations are I/O-, memory-, and CPU-intensive. It is important to monitor these resources and to add more data nodes if they are overloaded.

Important [system settings](https://www.elastic.co/guide/en/elasticsearch/reference/current/system-config.html):
- disable swapping
- increase file descriptors
- ensure sufficient virtual memory
- JVM DNS cache settings
- temporary directory not mounted with noexec
- TCP retransmission timeout

Regarding JVM settings in production environment, see this [blog](https://www.elastic.co/blog/a-heap-of-trouble):
- set Xmx and Xms the same
- java heap size <= 50% host memory capacity
- heap <= 30GB

Check elasticsearch version
```yaml
# 9200 is the defaul port
# on browser or kibana dev console
curl -XGET "http://<master/data bind IP>:9200"
# response from Elasticsearch server
{
  "name" : "master",
  "cluster_name" : "chengdol-es",
  "cluster_uuid" : "XIRbI3QxRq-ZXNuGDRqDFQ",
  "version" : {
    "number" : "7.11.1",
    "build_flavor" : "default",
    "build_type" : "tar",
    "build_hash" : "ff17057114c2199c9c1bbecc727003a907c0db7a",
    "build_date" : "2021-02-15T13:44:09.394032Z",
    "build_snapshot" : false,
    "lucene_version" : "8.7.0",
    "minimum_wire_compatibility_version" : "6.8.0",
    "minimum_index_compatibility_version" : "6.0.0-beta1"
  },
  "tagline" : "You Know, for Search"
}
```

Check cluster health and number of master and data nodes:
```bash
curl -X GET "http://<master/data bind IP>/_cat/health?v=true&format=json&pretty"
# response example
{
  "cluster_name" : "chengdol-es",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 2,
  "number_of_data_nodes" : 1,
  "active_primary_shards" : 0,
  "active_shards" : 0,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
```
Besides master and data node role, there are ingest node, remote-eligible node, coordinating node(can be dedicated).

## Indiex
[x] how to create and config index
[x] how to check index, status, config
[x] how to create actually document data: check document API
[x] how to reroute shards, through cluster APIs

ES [10 concepts](https://logz.io/blog/10-elasticsearch-concepts/), understand what is [indices, document, fields, mapping, shards](https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#index), primary and replicas shards, document, data node, master node, and so on. 

First, an index is some type of data organization mechanism, allowing the user to partition data a certain way. The second concept relates to replicas and shards, the mechanism Elasticsearch uses to distribute data around the cluster.
```ini
Schema - MySQL => Databases => Tables => Row => Columns (事务性, Join)
Mapping - Elasticsearch => Indices => Types => Document => fields (相关性，高性能全文检索)
```
So just remember, Indices organize data logically, but they also organize data physically through the underlying shards. When you create an index, you can define how many shards you want. Each shard is an independent Lucene index that can be hosted anywhere in your cluster.

`Index module`: the settings for index and control all aspects related to an index, for example:
```yaml
index.number_of_shards: The number of primary shards that an index should have
index.number_of_replicas: The number of replicas each primary shard has. Defaults to 1.
```

`Index template`: tell Elasticsearch how to configure an index when it is created, Elasticsearch applies templates to new indices based on an index pattern that matches the index name. Templates are configured prior to index creation and then when an index is created either manually or through indexing a document, the template settings are used as a basis for creating the index. If a new data stream or index matches more than one index template, the index template with the highest priority is used.

There are two types of templates, `index templates` and `component templates`(注意old version只有index template, see this [legacy index template](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates-v1.html#put-index-template-v1-api-prereqs)), template 其实包含了index module的内容.

[Get index template](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-template-v1.html) details through API.

[Elasticsearch API](https://www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html), for example: cluster status, index, document and shards, and reroute, to examine the node type (master and data nodes), shards distribution and CPU load statistics.

Let's see an example to display doc content:
```bash
# cat indices
curl -X GET "172.20.21.30:9200/_cat/indices?format=json" | jq
# search index
# get list of docs and its ids
curl -X GET "172.20.21.30:9200/<index name>/_search?format=json" | jq
# get doc via its id
curl -X GET "172.20.21.30:9200/<index name>/_doc/<doc id>?format=json" | jq
```

To upload single or bulk document data to elasticsearch, see [document API](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs.html). You can download sample here: [sample data](https://www.elastic.co/guide/en/kibana/6.8/tutorial-load-dataset.html), let's try accounts data:
```bash
# the accounts.json does not have index and type in it, so specify in curl command
# /bank/account is the index and type
# es will create index bank and type account for you automatically
curl -s -H "Content-Type: application/x-ndjson" \
  -XPOST 172.20.21.30:9200/bank/account/_bulk?pretty \
  --data-binary "@accounts.json"; echo

# display indices
curl -XGET 172.20.21.30:9200/_cat/indices
# check doc id 1 of index `bank`
curl -XGET 172.20.21.30:9200/bank/account/1?pretty
```

Query data, run on kibana dev console:
```bash
# query account from CA
curl -XGET 172.20.21.30:9200/bank/account/_search
{
  "query": {
    "match": {
      "state": "CA"
    }
  }
}
```
In the response message, the match has a `_score` field, it tells you how relevant is the match. 


## Plugins
Elasticsearch provides a variety of plugins to extend the system, for example, snapshot plugin, see [here](https://www.elastic.co/guide/en/elasticsearch/plugins/7.15/index.html).
```bash
# list all installed plug-ins
bin/elasticsearch-plugin list
# example
bin/elasticsearch-plugin install analysis-icu
# api
localhost:9200/_cat/plugins
```


# Kibana
Kibana [get startd and download](https://www.elastic.co/guide/en/kibana/current/get-started.html)
written in Node.js, no other dependencies needed.

Do a quick configuration, check my vagrant Kibana provision file and start in background:
```yaml
server.port: 5601
server.host: "172.20.21.30"

server.name: "${KIBANA_SERVER_NAME}"
# 2 es nodes
elasticsearch.hosts: ["http://172.20.21.30:9200", "http://172.20.21.31:9200"]

pid.file: ${KIBANA_HOME}/kibana.pid
kibana.index: ".kibana"
```
Access by `http://172.20.21.30:5601` in firefox browser.

Kibana has built-in [sample data](https://www.elastic.co/guide/en/kibana/current/get-started.html#gs-get-data-into-kibana) that you can play with, Go to add sample data then
move to Analytics -> Discover to query and analyze the data. You need to know
[KQL](https://www.elastic.co/guide/en/kibana/current/kuery-query.html) to query
document, [Dashboard](https://www.elastic.co/guide/en/kibana/current/dashboard.html) is also helpful.

Kibana dev console can issue HTTP request to explore ES APIs, `command + enter`
to run (more shortcut see help menu, helpful!).
```bash
# echo command has a play botton
GET /_cat/indices?v
GET /_cat/nodes?v
```

Or the data can be ingested from Logstash, see below and my Vagrant demo. Need
to create Index Pattern to load data and query.

Also you can install plug-ins for Kibana:
```bash
bin/kibana-plugin install <plugin>
bin/kibana-plugin list
bin/kibana-plugin remove <plugin>
```

## Discover
I usually use `Discover` to filter and check log message, and use `Dashboard` to
make graph, such Area, Bar, etc to extract data pattern.

How to draw graph easily from `Dscovery`:

1. In Discover, query and filter to get the target log records.
2. In the leftside panel, right click one of the selected fields -> Visualize.

I want to highlight that the `Area` graph, it will show you the proportion of
target field value alone with the timeline. For example, in the graph settings,
the horizontal axis is `@timtstamp`, vertical axis uses `count` and break down
by the selected field of the message.

There is a
[`saved object`](https://www.elastic.co/guide/en/kibana/current/managing-saved-objects.html)
management, in `Discover`, `Dashboard` and `Index pattern` section, you can save
items and manage them as well as export/import from other Kibana instance.


# Logstash
Ingest data to elasticsearch or other downstream consumers, [introduction](https://www.elastic.co/guide/en/logstash/current/introduction.html). Usually be paired with Beats.

Logstash offers self-contained architecture-specific downloads that include `AdoptOpenJDK 11,` the latest long term support (LTS) release of JDK. Use the `JAVA_HOME` environment variable if you want to use a JDK other than the version that is bundled.

[Logstash configuration Files](https://www.elastic.co/guide/en/logstash/current/config-setting-files.html)
Two types, pipeline config:
- /etc/logstash/conf.d

and logstash settings:
- startup.options
- logstash.yml
- jvm.options
- log4j2.properties
- pipelines.yml

There is a `pipeline.workers` setting in `logstash.yml` file and also some input plugin such as UDP has its own `workers` setting, what's the difference? Read this post [A History of Logstash Output Workers](https://www.elastic.co/blog/a-history-of-logstash-output-workers). So the input and (filter + output) are separated pools, they have separated worker thread settings, `pipeline.workers` is for (filter + output) part, the default value is equal to number of CPU core.

[Getting started with Logstash](https://www.elastic.co/guide/en/logstash/current/getting-started-with-logstash.html)
```bash
bin/logstash --help

# config file syntax check
# -f: config file
# -t/--config.test_and_exit: 
bin/logstash -f first-pipeline.conf --config.test_and_exit
# start logstash
# -r/--config.reload.automatic: used to avoid restart logstash when change conf file
bin/logstash -f first-pipeline.conf --config.reload.automatic
```

Ad-hoc pipeline config:
```bash
cd /usr/share/logstash
## test running
bin/logstash -e 'input { stdin { } } output { stdout {} }'
bin/logstash -e 'input { stdin { } } output { elasticsearch { hosts => ["<master/data node ip>:9200"] } }'
## then type something to send
```

There are 4 important parts to config processing pipeline:

- [Logstash input plugin](https://www.elastic.co/guide/en/logstash/7.17/input-plugins.html)
- [Logstash filter plugin](https://www.elastic.co/guide/en/logstash/current/filter-plugins.html) 
- [Logstash output plugin](https://www.elastic.co/guide/en/logstash/current/output-plugins.html)
- [Condition and reference in filter and output](https://www.elastic.co/guide/en/logstash/current/event-dependent-configuration.html)

```bash
# this output can be used for testing
output {
    # can have multiple output plugin
    elasticsearch {}
    kafka {}
    if "VERBOSE" in [message] {
      file {}
    }
    else {
      stdout { codec => rubydebug }
    }
}
```
For `stdout` plugin, the output can be examined from `journalctl -ex -u logstash -f`.

## Read Beats Data
To read data from Beats:
1. Input: where is the data from? logs? beats?
2. Filter: how should we parse the data? grok filters, geoip filters, etc.
3. Output: where should we store the logs? backend? Elasticsearch?

Go to `/etc/logstash/conf.d`, create new file for example, `beats.conf`:
```bash
input {
    ## in Beats side, listening on port 5043
    beats {
        port => "5043"
    }
}

filter {
    if [type] == "syslog" {
        ## grok filter
        grok {
            match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
        }
        date {
           match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
        }
    }
}

output {
    elasticsearch {
        ## Elasticsearch address
        hosts => [ "9.30.94.85:9200" ]
        ## write to index
        ## %{[@metadata][beat]}: these are field and sub_field in message
        index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
        document_type => "%{[@metadata][type]}"
    }
}
```
In above output `%{[@metadata][beat]}` is field access, please see [Accessing event data and fields in logstash](https://www.elastic.co/guide/en/logstash/7.15/event-dependent-configuration.html), the logstash [data types](https://www.elastic.co/guide/en/logstash/7.15/configuration-file-structure.html)

Then run command to testing file validity:
```bash
# --config.test_and_exit: parses configuration
# file and reports any errors.
bin/logstash -f beats.conf --config.test_and_exit

# The --config.reload.automatic: enables automatic 
# config reloading so that don’t have to stop and 
# restart Logstash every time modify the configuration file.
bin/logstash -f beats.conf --config.reload.automatic
```


# Beats
https://www.elastic.co/beats/
Beats, written in golang, can output data to Elasticsearch, Logstash and Redis. But usually we send data to Logstash (pre-processing) then forward to Elasticsearch.

Each Beat has configure yaml file with detailed configuration guideline. For example, in the configure yaml file, comment out Elasticsearch output, use Logstash output.

- Filebeat: text log files
- Heartbeat: uptime
- Metricbeat: OS and applications
- Packetbeat: network monitoring
- Winlogbeat: windows event log
- Libbeat: write your own


# X-Pack
X-Pack is an Elastic Stack extension that provides security, alerting, monitoring, reporting, machine learning, and many other capabilities. By default, when you install Elasticsearch, X-Pack is installed, it is open-source now.

Check X-pack, you will see the availability and status of each component:
```bash
curl -XGET http://9.30.94.85:9200/_xpack
```









]]></content>
      <categories>
        <category>Elastic</category>
      </categories>
      <tags>
        <tag>elastic</tag>
      </tags>
  </entry>
  <entry>
    <title>Git Cherry Pick</title>
    <url>/2021/07/24/git-cherry-pick/</url>
    <content><![CDATA[
Only want to bring one specific commit or a range of commits from another branch onto current working branch. Note that `git merge` and `git rebase` will bring all differences.

[Cherry-pick Reference](https://www.atlassian.com/git/tutorials/cherry-pick)
git cherry-pick can be useful for undoing changes. For example, say a commit is accidently made to the wrong branch. You can switch to the correct branch and cherry-pick the commit to where it should belong. Note the `-n` flag means to not create commit but staging the picked content.

> Check if it can be done by UI first.

```bash
# go to from branch to get target commit sha
git log -n 10

git checkout <to branch>
# --no-commit: just pick not creating commit
# -edit: edit commit message before commit
git cherry-pick <commit sha> [--no-commit] [-edit]
```

If there are conflicts after `git cherry-pick`, resolve the conflicts first before next cherry pick operation. As this [comment](https://stackoverflow.com/questions/19830464/git-cherry-pick-and-conflicts) mentioned, you can cherry pick multiple commits all in one go, so you don't have to resolve conflicts for changes that are undone by later commits.]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Create Pull Request</title>
    <url>/2019/12/24/git-create-pull-request/</url>
    <content><![CDATA[
In my blog [Create Working Branch](https://chengdol.github.io/2019/09/12/git-create-working-branch/), when run `git push origin <non-master>`, we actually create a pull request.

More [references](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request), it also talks about creating pull request from fork.

I am sometimes confusing about the term, why it is called `pull request` not `push request` (because I push my code to remote repo)? And I am not alone, a reasonable explanation see [here](https://stackoverflow.com/questions/21657430/why-is-a-git-pull-request-not-called-a-push-request): Because You are asking the target repository grab your changes, stand on their side it is a pulling operation.

If the changes in local branch `develop` are ready, but the remote branch `develop` is out of date, after `git pull origin develop` your local get messy, you can use `git reset --hard` roll back to the last commit you made, next delete remote branch on github GUI, then do the `git push origin develop` to recreate it.






]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Issue Layout</title>
    <url>/2020/02/26/git-issue-layout/</url>
    <content><![CDATA[
What should be the layout for a good bug ticket:

```ini
<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
and verify the issue you are about to submit is not a duplicate.
--->

## Environment information
(cluster info, system info)

## Problem Description
(Summarize the bug encountered concisely)

## Steps to reproduce
(How one can reproduce the issue - this is very important)

## Expected behaviour
(what you should see?)

## Observed behaviour
(what actually happens?)

## Additional info or screenshots
(Paste any relevant logs - please use code blocks (```) to format console output,
logs, and code as it is very hard to read otherwise.)

## Workaround available
(If you can, link to the line of code that might be responsible for the problem)

## Per Meeting Minutes

## TO-DO's
```

More:
1. add right pipelines
2. add right labels
3. @people when reply


]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>NFS Server and Client Setup</title>
    <url>/2019/05/27/fs-nfs/</url>
    <content><![CDATA[
This blog is for Network file system NFS with only `one` server. 但NFS有新版本支持multi-server了。
For distributed file system (multiple servers): openAFS, GlusterFS (native support on Redhat/centOS).
For cluster file systems: GFS2 (linux native support)

So, [what is the difference among network, distributed and cluster file systems?](https://www.quora.com/What-is-the-difference-between-a-distributed-file-system-clustered-file-system-and-a-network-file-system). 补充一点: Distributed or netowrk file system also ensure data availability across multiple server nodes and can usually handle nodes being added and removed more gracefully. Don’t make assumptions about how well this will work; be sure to test not only for performance and latency, but also for the impact of changes to the cluster and for failure scenarios.

NFS是一个概念(protocol)，它不是一个文件系统的类型，它是一种文件系统的共享方式, 这里默认使用本地Linux的文件系统类型了。
NFS allows remote hosts to mount `filesystems`(can be any) over a network and interact with those filesystems as though they are mounted locally. 

NFS lets you leverage storage space in a different location and allows you to write onto the same space from multiple servers or clients in an effortless manner. It, thus, works fairly well for directories that users need to access frequently.

常见用法，比如分享home directories so if switch to different host, just mount them, supports diskless machines, but be careful with UIDs and GIDs, ensure they are the same person on each machine.

# Server Setup
Acutally there are many other package may needed, this [video](https://www.youtube.com/watch?v=MBqZe5d9BNQ) may give you more details.
```bash
# install nfs package
yum -y install nfs-utils
```

Then enable and start nfs server:
```bash
# the rpc by default should be on, but may not
# rpc is called remote producture call
# nfs server needs this to access and operate on others
systemctl enable rpcbind
systemctl start rpcbind

systemctl enable nfs-server.service
systemctl start nfs-server.service
# create server side shared folder
mkdir –p /data
chmod –R 755 /data
```

Edit `/etc/exports` file to expose shared folder
```bash
vi /etc/exports 

# any client can access this folder
/data *(rw,insecure,async,no_root_squash)   
# or specific client can access this folder
/data <client ip>(rw,insecure,async,no_root_squash)
```

Then export the shared folder:
```bash
exportfs -a
systemctl restart nfs-server.service
```
Check shared folder is up `showmount -e <server ip>`:
```bash
showmount -e localhost
```
If you change the content in `/etc/exports`, need to reload:
```bash
exportfs -ra
```
Check mount options:
```bash
exportfs -v
```

# Client Setup
First create or using existing folder as folder to mount, here I use `/mntiis` folder in client machine
```bash
# install nfs package
yum -y install nfs-utils

# create client side shared folder
mkdir -p /mntiis
chmod -R 0755 /mntiis
```

If you use non-persistent mount in command line, this connection will disappear after rebooting:
```bash
mount <server ip>:/data     /mntiis    
```
For persistent mount, go to `/etc/fstab` file, add this line
```bash
<server ip>:/data /mntiis nfs defaults 0 0
# or with other mount options
<server ip>:/data /mntiis nfs defaults,timeo=10,retrans=3,rsize=1048576,wsize=1048576 0 0
```

Enable mount:
```bash
mount /mntiis
# or reload all in fstab file
mount -a
```

Verify all set, you can see the `/mntiis` in the output:
```
df -hk
```

When you remove the entry in `/etc/fstab` filem, also unmount the folder, otherwise you will see `/mntiis` is marked by `?` in filesystem:
```
umount /mntiis
```

# Mount On-demand
You can set NFS auto mount on-demand via `autofs`, this can avoid wasting resources by unmounting mount point when not using and mounting it when access again.
```bash
yum install -y autofs
systemctl start autofs
```




]]></content>
      <categories>
        <category>Storage</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>filesystem</tag>
        <tag>nfs</tag>
      </tags>
  </entry>
  <entry>
    <title>Clean Local Working Branch</title>
    <url>/2019/06/28/git-clean-reset/</url>
    <content><![CDATA[
Sometimes if the local working branch messes up but you want to sync up with your remote branch, for example origin master, first remove all untracked files:
```bash
# -d: remove untracked dirs
# -f: force
git clean -df
# -x: remove ignored files as well
git clean -dfx
```
you can have a dry-run first to see what files will be removed
```bash
# -n: dry run
git clean -dfn
```

Check the commit logs in current branch:
```bash
# verbose
# -n: commit number
git log -n 10

# oneline and color
# color: show color
git log --oneline --color -n 10

# -p: show diff for each commit
git log -p --oneline --color -n 10

# show branch relationship
# --graph: show ascii graph
git log --graph --oneline --color -n 10
```

Then properly select one good commit that also appears in remote master branch, run
```bash
git reset --hard <good commit hash>
```

All the commits later than the good commit hash will be removed, then you can just run the command below to sync up remote master branch.
```bash
git pull origin master
```
> Note that it's safe to do git reset here because only I use this branch and I want to clean

Sometimes I see people use:
```bash
# reset to current commit, discard all uncommitted changes
git reset --hard HEAD
# HEAD~1: move to the commit right before HEAD
git reset --hard HEAD~1
```

`HEAD` points to your current commit, check by:
```bash
# get HEAD short hash
git rev-parse --short HEAD
```
so all that `git reset --hard HEAD` will do is to throw away any uncommitted changes you have and point to your latest commit.

]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Git Repository Browser</title>
    <url>/2019/06/28/git-gitk/</url>
    <content><![CDATA[
> Note in order to run `gitk` from terminal you need to have a Desktop GUI environment (for example VNC).

`gitk` is a graphical history viewer. Think of it like a powerful GUI shell over git log and git grep. This is the tool to use when you’re trying to find something that happened in the past, or visualize your project’s history

## Install gitk
For CentOS and RedHat:
```
yum install -y gitk
```

## Usage
`gitk` is easy to invoke from the command-line. Just `cd` into your target git repository, and type:
```
gitk
```
Then a dedicated GUI will be launched for you:
![](https://drive.google.com/uc?id=19b_uKeMXctg0QPFTqjzQCmL7b4VA3x4R)

You can get a brief usage introduction from `man gitk`, for example, if I want to see the commit history of `hello.sh`
```
gitk <path to file>/hello.sh
```

Show all branches:
```
gitk --all
```

## Resources
[Use gitk to understand git](https://lostechies.com/joshuaflanagan/2010/09/03/use-gitk-to-understand-git/)]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>gitk</tag>
      </tags>
  </entry>
  <entry>
    <title>Git Disable Pull Prompt</title>
    <url>/2019/06/27/git-mute-pull-prompt/</url>
    <content><![CDATA[
When run `git pull origin master` to make your local branch up-to-date, if there are files need to be merged, you will get prompt to confirm the merge and make commit. This is not good for automation, need to mute this prompt.

> Note this method fits this situation, but not sure if this is a general way to go, because `git pull` has different syntax format.

Actually, `git pull` does a `git fetch` followed by `git merge`, so can sepetate it into two steps:
```bash
git pull origin master
```
Changes to
```bash
git fetch origin master
git merge FETCH_HEAD --no-edit
```

> Note that The `--no-edit` option can be used to accept the auto-generated message (this is generally discouraged).
]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Chinese Life in English</title>
    <url>/2023/11/27/english-chinese-life/</url>
    <content><![CDATA[
#### [Unexpected Shenzhen Street Food Bargains](https://youtu.be/J-ElgqgLt_8?si=jYbM73ofGroi1mMI)
* chinese hamburger, chinese pork burger
* claypot beef: thick/θɪk/, meaty soup
* street vender, vibrant/ˈvaɪbrənt/ street/night market
* it is so delicious, so much flavors and all the ingredients are fresh as well
* flaky pastry/ˈpeɪstri/ bread 酥饼, the skin is a little bit dry,
crunchy/'krʌntʃi/ bread, very crunchy texture
* coriander/ˌkɔrɪ'ændɚ/
* meat is tender and juicy 鲜嫩多汁
* roasted potato, outer layer is crispy/'krɪspi/, a bit salty
* sweet potato sheet packed/wrapped with other ingredients, tasty inside
* I wish I have a bigger belly so I can keep on eating

#### [TOP 7 Delicious Chinese Mooncakes You Got to Try](https://youtu.be/qV1uF70VA30?si=0BbGrLmUP5cU575Y)
* mid-autumn/moon festival, the second biggest festival in China
* mooncake, original classic, nutty mooncake 五仁, just sweet enough for me
black sesame/'sɛsəmi/: jet black 乌黑(jet blue 深蓝)
* red bean paste 豆沙, perfect amount of sweetness just chewy/'tʃui/ enough
* jujube/ˈdʒuˌdʒub/, dates paste
* I am not a big fan of durian, very well known for its smell, smelly/'smɛli/
* way better than the previous one, different flavor and texture/ˈtekstʃər/
* the flavor is actually pretty mild, not imagined overwhelmingly strong..

#### [All-You-Can-Eat Buffet in Shenzhen China](https://youtu.be/GqQENs6M4zc?si=YWZN-2bKKt8zyfvC)
* find the cheapest place ever to eat
* meatball: Braised pork ball in brown/braʊn/ sauce 红烧狮子头
* chinese pickle, 泡菜，咸菜，榨菜, taste a bit pickle
* texture is moist/mɔɪst/ not too dry
* it is flavorless/bland so I put sugar on it
* it is still good value but the variety nowhere near as good as here.
* it is very jelly 类似素毛肚的口感
* tray/treɪ/, holder 托盘

#### [Exploring the Vibrant Chinese Street Food Culture in Our Local Neighbourhood](https://youtu.be/dJdKWyz-qvY?si=Hth--cpO_uTdnroP)
* let's check it out
* stool/stuːl/ vs chair, stool typicallylacks a backrest and armrests.
* flat noodle 河粉
* bean sprout
* squid tentacle/'tɛntəkl/

#### [Dragon Boat Festival](https://youtu.be/-9SK5pRiWDI?si=YQ2EIwDreoArQStn)
* let's dig in: 深入研究一下
* yummy/ˈjʌmi/: casual way to express pleasure or satisfaction regarding the taste
of food
* I don't know how to explain its texture
* dragon boat festival
* zongzi: everyone eats them around the dragon boat festival so I have been eating
them as well; weird triangle, sticky, meat insdie and wrapped with leaf; sweet
one and meat one

#### [Shenzhen Top 5 | Why We Love Living in Shenzhen](https://youtu.be/hOY47kB3ots?si=diYsSJuFWz_PX7O0)
* time to do a **top five** of why ...
* ebike culture, moving around on a ebike
* **stick around** to the end of the video to find out sth we don't like
* the **spontaneity/ˌspɑːntəˈneɪəti/** and friendiness of all the people: 自发性
* they don't care about what they look like while having fun, no
**self-conscious/ˈkɑːnʃəs/** in any way, without any fear of being judged or
shamed or mocked: 不自然的,忸怩的,害羞的
* you have **abundance/əˈbʌndəns/** of restaurants, bars, snack bars

#### [Eat Dinner in Supermarket](https://www.bilibili.com/video/BV1MQ4y147AX/)
* spicy crispy: 麻辣脆
* tester/sample: 试吃
* it is nice and steamy, soft, freshly cooked
* giant crab, lobster/'lɑbstɚ/
* look at this feast/fiːst/: 筵席，宴会
* here is your deluxe/dɪˈlʌks/
* beef is very thin cut, very well seasoned
* do you know what reminds me of this? 这个让我想起什么
* dried cranberry/'kræn'bɛri/ 蔓越莓

#### [Veggie Sausage](https://www.bilibili.com/video/BV1Ju4y1w77E)
* what that tastes like?
* made out of/without meat
* flour or starch/stɑrtʃ/ sausage
* I gonna have it **more often** actually

#### [Noodle All You Can Eat](https://www.bilibili.com/video/BV1Ss4y1b7GT)
* You can eat any amount you want
* celery/'sɛləri/ 芹菜
* there is a **misconception** that foreigners don't like spicy food
* reddish pickle 粉红
* square方形, flat/flæt/扁的, round圆的, fat宽的 noodle

#### [Some random eatings](https://www.bilibili.com/video/BV1FP4y1H7T2)
* the texture is **slimy**/ˈslaɪmi//(slimier): a moist, soft, and slippery,
similar to chewy.
* dipping sauce: vinegar, soy sauce, onion, garlic, chilli
* **sizzling hot plate**/'sizliŋ/ with beef and rice: 铁板牛肉
* lamb skewer/'skjʊɚ/ 羊肉串

#### [Mountain campus tour](https://www.bilibili.com/video/BV1494y1N78e)
* tent campus, campus tour
* today is really misty/ˈmɪsti/(foggy) and we are in the clouds
* trampoline/'træmpəlin/: 蹦床
* hermetic sealed meat, you need to **pull it open**
* cannot wait to dine/daɪn/
* diarrhea/ˌdaɪə'riə/ 腹泻, bellyache/'bɛlɪek/ 腹痛
* sticky candy with caramel/'kærəmɛl/ flavor 焦糖

#### [Dongbei Malatang](https://youtu.be/dFZ5Zf4ZFbc?si=9w1Sb4V2ykJGhNv-)
* super good winter food
* that's crazy
* I have never tried it before, today its gonna be the first time, let's go.
* quail egg/kweɪl/, little egg
* peanut sauce 麻酱

#### [Fabric market in GuangZhou](https://www.bilibili.com/video/BV12b4y137Vp)
* We are ***on the hunt** for some new stuff
* let's go and look for that
* This fabric is soft, super cozy for winter
* accessory section, they have buttons, beads/biːd/ 珠子, you name it

#### [Why We LOVE China's CHEAPEST Italian Family Restaurant](https://youtu.be/yI-j4WsUmlo?si=lpXaf9clmTXyIQWb)
* foreign food Aisa style
* Japanse family-style **chain** restaurant
* the flavor has been changed to suit the local customer
* it is really a massive bargain
* we ordered 11 dishes and 4 drinks
* really moist, wet/wɛt/ and juicy and garlicky/'gɑrlɪki/, salty cheese on top
* escargot/ˌɛskɑrˈɡo/: （法）食用蜗牛

#### [Chinese fried rice](https://youtu.be/iqD5lCLyXHI?si=oL3j5T6pwoxdnZtz)
* You speak impeccable/ɪmˈpekəbl/ mandarin: **in accordance with** the highest
standards of propriety; faultless
* how much work is still outstanding(remaining to be paid, done, or dealt with)
* your support has been outstanding(exceptionally good).
* the flavor is mind blowing/'bləuiŋ/: overwhelmingly impressive!
* the flavor is magnificent /mæɡˈnɪfɪsnt/: impressively beautiful or excellent
* pear/per/

#### [Play bowling in China](https://youtu.be/CWQuNGphJT0?si=AgPV87tUtitZwPU_)
* bowling/'bolɪŋ/, bowling alley/ˈæli/
* do you have any **technique**? skill or ability in a particular field
* This is the new technique/skill I learned
* unveil/ˌʌnˈveɪl/ the truth

#### [The CRAZY WORLD of Beef Hotpot in China](https://youtu.be/Y6z6K10ui3M?si=KCTwDzhC8ALmgXHO)
* lamb/beef hotpot
* look, hard work pays off
* Do you have any recommends, what set meal is good for a family 4 people
* the meat is too **fatty**
* it is a little bit lighter than other hotpots
* main dish: 主食
]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>english</tag>
      </tags>
  </entry>
  <entry>
    <title>Create Working Branch</title>
    <url>/2019/09/12/git-create-working-branch/</url>
    <content><![CDATA[Let me record the workflow about how to create your own working branch from master.

I `git clone` the original repo to local, then `git checkout -b` to a `develop` branch on top of `master` branch, for example:
```bash
(cd /GitRepos; git clone git@github.xxx.com:xxx.git)
```

Then go to local git repository
```bash
git checkout -b develop
```

After checkout to new branch, if you do nothing and get files changed, this is sometimes a format issue, go to edit `.gitattributes`, comment out the regex, for example:
```
# * text=auto
```
Save and run `git status`, you will see the change now is on `.gitattributes`, then undo the change and save.

Now you are in `develop` branch, let's check the config information:
```
git config --list
```

Focus on the `user.name` and `user.email`, if they are empty, set them by
```
git config --global user.name xxx
git config --global user.email xxx
git commit --amend --reset-author
```

Then working on `develop` branch, when you run `git push origin develop`, this will create a pull request (you can decide merge to which branch) and a remote `develop` branch, then you can review and merge to `master` branch on git web.

Because there is no git repos fork, so run `git pull origin master` in `develop` branch to get sync up with latest updates from `master` branch.
]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Git Merge and Rebase</title>
    <url>/2019/09/11/git-merge-rebase/</url>
    <content><![CDATA[
之前其实很少用到`git merge` and `git rebase`，一般都是通过Github UI Pull Request merges feature branch's updates into master.

也可在CLI 中merge feature branch into master，但这样master中的git history不是线性的，并且会制造一个merge commit:
```bash
# merge feature to master, first go to master
git checkout master
# squash will summary all commits in feature branch into merge commit
git merge --squash feature
# or 不用fast-forward merge, 用普通的recursive merge
# by default is --ff (try fast-forward first, not create merge commit)
git merge --no-ff feature

# need to commit this merge
git commit -m "feature and master merged"
# then push to master
git push origin master
```

Highlight:
- fast-forward merge: less commits, no merge commit will be generated.
- recusive merge: has merge commit, can do revert, clear what was done on a branch.


关于rebase的使用，可以参考这篇文章，它构造了一个线性的git history，方便以后查看:
https://www.jianshu.com/p/6960811ac89c
注意，这篇文章的例子是将dev branch 本地合并到master 再提交，并不是merge request的方式。

1. 先将本地master 更新, git pull origin master
2. 进入需要rebase的分支, git checkout dev
3. 执行rebase, git rebase master. 这样就把master的commits 线性的合并到dev 分支了
4. rebase可能遇到conflicts, 参考这里去修复[Resolving merge conflicts after a Git rebase](https://docs.github.com/en/github/using-git/resolving-merge-conflicts-after-a-git-rebase)
5. dev 中rebase 完成后，切换到master执行merge, git checkout master, git merge dev (github/gitlab UI就是做了这一步)

这样就把dev合并到master中了，实际上还是先rebase 再 merge.

这个文章讲了更多的rebase 特性:
https://baijiahao.baidu.com/s?id=1633418495146592435
1. git pull --rebase (will not use merge)
2. 修改commit 历史
3. 合并commit
4. 分解commit
5. 重新排序commit

[Git Interactive Rebase, Squash, Amend and Other Ways of Rewriting History](https://thoughtbot.com/blog/git-interactive-rebase-squash-amend-rewriting-history#squash-commits-together)
实际遇到一个要求，存在多次的commits在dev branch中，需要squash multiple commits into one, 解决办法: 
在本地rebase 然后再提交, for example I use `dev` branch:
```bash
# 如果dev branch不干净
# later can run `git stash pop`
git stash

# 可以先在dev中squash一下多个commits
# HEAD~5: rebase HEAD 向下的5个commits
# HEAD 可以用git log看一下位置
git checkout dev
git rebase -i HEAD~5

# pull latest to master branch
# because it may be updated by others
git checkout master
git pull origin master

# current in dev branch, reword commits on top of master history
git checkout dev
# rebase against master，这样之后create pull/merge request就不会产生冲突了
# -i: interactively
git rebase -i master


# may have conflicts, fix conflicts as prompted
git add <fixed files>
git rebase --continue

# abort rebase
git rebase --abort

# after rebase (squash), check commit log changes
git log -n 5

# update merge request on github
# 之前的merge request就会被重写
# -f: force override merge request on remote
git push -f origin dev
```


## Other GitHub tutorials:
GitHub.com Help Documentation:
https://docs.github.com/en/github

]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Versioning Large File in Git</title>
    <url>/2019/02/24/git-lfs/</url>
    <content><![CDATA[
I want to introduce you [Git LFS](https://git-lfs.github.com/), it is a command line extension and [specification](https://github.com/git-lfs/git-lfs/blob/master/docs/spec.md) for managing large files with Git. LFS is great for **large, changing files**, there is basically a text pointer to the large file archived some place else.

Usually we store large file or object in artifactory, for example: `jFrog`, `Nexus`, etc.

### Install Git LFS
> Note: you need to install `Git LFS` if you `git pull` from a remote repository that has it

For example, I am working on a RHEL machine. 
First go to source page, follow the installation guide to install:

![](https://drive.google.com/uc?id=1c2hUDmTQwOnD3GNz8S1QHfkyGwLzO5ur)

![](https://drive.google.com/uc?id=1Zhnjd815gHSBJFySqdAyvfqSDeZtd0oc)

This will create a yum repos for git-lfs:

![](https://drive.google.com/uc?id=1BtoVGRjTz1PHXplO5OuJnhYMy8oJPzKl)

![](https://drive.google.com/uc?id=1F4NO7fUTtq0cP0bfgm8EGNlIrvKAjp1T)

```bash
yum install -y git-lfs
```
you can see git-lfs is installed in your machine:
![](https://drive.google.com/uc?id=12xtxpkkbAzSjSXAecwjqb-dV0SFEt_qz)

Once downloaded and installed, set up Git LFS and its respective hooks by running:
```bash
git lfs install
```
![](https://drive.google.com/uc?id=1gTK8WwQi7i1Vfjq7_ysTU5H9jXN4w_Jo)
> Note: You'll need to run this in your repository directory, once per repository.

### Track Large File
Select the file types you'd like Git LFS to manage (or directly edit your `.gitattributes`). You can configure additional file extensions at anytime.
```bash
git lfs track "*.tar.gz"
```
> Note: run this track command at the top level of your repository, then you need to git add `.gitattributes` file

### Manage Large File
Then, just do normal `git add` and `git commit` to manage your large size file.
```bash
git add *.tar.gz
git commit -m "add tar.gz file"
git push origin <your branch>
```

![](https://drive.google.com/uc?id=1x9TX8NBIEZaaG51_dQxEKljdeL0ziKK8)

Actually, you can check the large files you managed by running:
```bash
git lfs ls-files
```

]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git-lfs</tag>
      </tags>
  </entry>
  <entry>
    <title>Git Partial Merge</title>
    <url>/2019/07/10/git-partial-merge/</url>
    <content><![CDATA[
I was previously working on a separate branch `branch-tmp`, now I want to merge some files in that branch into my main personal branch `chengdol_master`, and finally create a pull request to merge int `master`.

## Resource
[Git tip: How to "merge" specific files from another branch](https://jasonrudolph.com/blog/2009/02/25/git-tip-how-to-merge-specific-files-from-another-branch/)
[Interactive merge](https://stackoverflow.com/questions/18115411/how-to-merge-specific-files-from-git-branches)


## Solution

> Notice that in my case, the target files in `branch-tmp` are completely applicable for `chengdol_master`, so I just want to overwrite corresponding files in `chengdol_master`. If we need to pick some changes and leave others in the file, do an `interactive merge`, run this from `chengdol_master`: 
```bash
git checkout --patch brach-tmp <relative path to target file>
```

First check if you have `origin/branch-tmp` locally
```
git branch -r | grep branch-tmp
```
If not, you need to fetch it alone or fetch all origin branches:
```
git fetch origin branch-tmp
git fetch origin
```
Then go to your target branch `chengdol_master`, use `git checkout` command to do the job:
```bash
git checkout origin/branch-tmp <relative path to target file>
```
then the merged files from `branch-tmp` are in staging phase, you can directly commit or unstage them in the `chengdol_master` branch, then push and handle the pull request.

]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Git Origin Vs Upstream</title>
    <url>/2019/07/09/git-origin-upstream/</url>
    <content><![CDATA[
Using Git everyday and pull push everyday, let's spend time to undestand the concept `origin` and `upstream` fully. 

You can edit the upstream and origin in this file for your local repo only:
```bash
# see local config in your repo
vim .git/config
```

## Resource
[Difference Upstream and Origin](https://stackoverflow.com/questions/9257533/what-is-the-difference-between-origin-and-upstream-on-github)

* `upstream` generally refers to the original repo that you forked from
* `origin` is your fork: your own repo on GitHub, clone of the original repo of GitHub





]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Git Pull Not Clean</title>
    <url>/2019/06/14/git-pull-not-clean/</url>
    <content><![CDATA[
Sometimes after I `git pull` from the master branch, if I run `git status`, there are some files (for me it's mainly `xxx.dsx`) are modified that need to be added and committed, that's strange.

It seems the format issue that can be solved by editting the top level `.gitattributes` in your local repository. Open `.gitattributes`, comment out the formulas, for example:
```
#* text=auto
```

Now if I run `git status` again, the clutters are gone and git outputs only show
```
modified:   .gitattributes
```
Then revert back to origin in `.gitattributes` and run `git status` again, the branch will be clean.

Acutally there are some commands can exterminate editor operation:
```
sed -i -e 's/^\(\*[ ][ ]*text.*\)/#\1/' .gitattributes
git status
sed -i -e 's/^#\(\*[ ][ ]*text.*\)/\1/' .gitattributes
git status
```


]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>gitattributes</tag>
      </tags>
  </entry>
  <entry>
    <title>Git Release Tag and Branch</title>
    <url>/2020/12/09/git-tag-branch/</url>
    <content><![CDATA[
# Release Tag
When to use tag: every time you push to production, tag the release.
Release tag points to a specific commit in history.

```bash
# fetch all remote tags
git fetch --tags
# list all tags
git tag [--list]
# delete tag
git tag -d <tag name>
# Delete a tag from the server with push tags
git push --delete origin <tag name>

# in your current branch tag latest commit
# create annotated tag, add more info
# will open editor
git tag -a 0.1.0

# tag specific commit
git tag -a 0.1.0 <commit hash>

# push tag to remote
git push origin <branch> --tags
```

这个git command很有用
```bash
## can see commit detail, tag annotation, and so on.
git show [tag|or anything]
```

References:
[what is tag](https://stackoverflow.com/questions/35979642/what-is-git-tag-how-to-create-tags-how-to-checkout-git-remote-tags)
[annotated and non-annotated tag](https://stackoverflow.com/questions/11514075/what-is-the-difference-between-an-annotated-and-unannotated-tag)
[move a tag on a branch to different commit](https://stackoverflow.com/questions/8044583/how-can-i-move-a-tag-on-a-git-branch-to-a-different-commit)



# Release Branch
Generally you only need tags for releases.
But when you need to make changes to production release without affecting master, you need release branch. For example, make hotfix. Release branch can be updated with new commits.

Use cases for release branches:
- Manual QA.
- Long running releases.
- On demand hot fixes.

Workflow is like
```bash
# we have some hot fixes on release branch, then create a release tag on it for production
# finally merge into master
master ------ + ----- + -----+ --- + --------------- + ------>
               \                               /
                \       hotfix   hotfix       /
                 \-------- + ----- + --------/
      release branch           (release tag)
```
[x] This is why we have double-commit back to master branch. 我们把release tag放在master branch中了。


You can also create release branch on the fly from release tags, the workflow:
```bash
# on master branch
# checkout to a tag (detched HEAD)
git checkout v1.1
# create release branch on release tag
git checkout -b rb1.1

# or in one command to checkout a tag to a new branch
git checkout tags/v1.1 -b rb1.1

# make fix
<hot fix>

# commit
git commit -m "hotfix"
git tag -a v1.1.1 -m "hotfix"
# merge
git checkout master
# or rebase/squash or fast-forward
git merge rb1.1 -m "merged hotfix"
# delete branch, the tag is still there, because you merged to master
git branch -d rb1.1
```]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Git Rename Limit</title>
    <url>/2019/10/22/git-rename-limit/</url>
    <content><![CDATA[
I encounter a git issue when I run these commands, they are used to sync up with origin/master:
```bash
git_clean() {
  git reset --hard HEAD
  sed -i -e 's/^\(\*[ ][ ]*text.*\)/#\1/' .gitattributes
  git status
  git clean -fdx
  git checkout -- .
  sed -i -e 's/^#\(\*[ ][ ]*text.*\)/\1/' .gitattributes
  git status
}
```
I get errors:
```
warning: inexact rename detection was skipped due to too many files.
warning: you may want to set your merge.renamelimit variable to at least 12454 and retry the command.
Automatic merge failed; fix conflicts and then commit the result.
```

Try to set `rename.limit` to larger value and run commands again but that does not help, https://stackoverflow.com/questions/4722423/how-to-merge-two-branches-with-different-directory-hierarchies-in-git
```
git config merge.renameLimit 999999
git merge --abort
git config --unset merge.renameLimit
```

So far these commands help:
```bash
git reset --hard origin/master
git fetch -p
git pull origin master

## if failed again, run
git merge --abort
git reset --hard origin/master
git pull origin master
```]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang JSON Marshal and Unmarshal</title>
    <url>/2023/04/08/golang-json-unmarshal/</url>
    <content><![CDATA[
Highly recommend to read article [JSON and Go](https://go.dev/blog/json) if you
are new to this area or want to bring the memory back.

In Golang, we usually use struct with struct tag to unmarshl/marshal JSON object
data, for example, if the JSON object is:

```json
{
	"page": 10,
	"user_info": {
		"age": 18,
		"height": 7.2
	}
}
```

The corresponding go struct:
```go
type Response struct {
	Page     int      `json:"page"      xml:"PAGE"`
	UserInfo userInfo `json:"user_info" xml:"USER_INFO"`
}

type userInfo struct {
	Age    uint32  `json:"age"    xml:"AGE"`
	Height float32 `json:"height" xml:"HEIGHT"`
}
```

> What is struct tag in golang please see:
https://stackoverflow.com/questions/10858787/what-are-the-uses-for-struct-tags-in-go


Please note that only `exported field`(upper case) in struct will be
encoded/decoded, and struct tag can help rename, if there is no struct tag, the
exact struct field name will be used as JSON key name in marshalling.

For unmarshal if there is no corresponding JSON field, the struct field will be
assigned the default `zero` value of that type, for example:

```go
// using raw string quote `` to avoid any escape chars
d := []byte(`
		{"page": 1}
	`)

resp := Response{}
err := json.Unmarshal(d, &resp)
if err != nil {
  fmt.Println("%w", err)
}
fmt.Printf("%+v\n", resp)

// output
{Page:1 UserInfo:{Age:0 Height:0}}
```

Because the `user_info` JSON data is missing, so `UserInfo` gets a struct value
with default value on all fields `UserInfo:{Age:0 Height:0}`, but this cannot
tell if the `user_info` is really missing or it has value but all 0.

To solve this, we can use pointer for nested struct field:

```go
type Response struct {
	Page     int       `json:"page"`
	UserInfo *userInfo `json:"user_info"`  // change to pointer
}
```

And run again, the output will be:

```go
{Page:1 UserInfo:<nil>}
```

By checking it is `nil`, we can tell the JSON data is missing.


# For Arbitrary JSON Object Data
If there are unknown fields in JSON data, we can use interface and type
assertion to help decode the JSON data, for example:

From blog [JSON and Go](https://go.dev/blog/json), the `json` module uses
`interface{}` to store arbitrary JSON objects or arrays and access them by
type asseration with underlying `map[string]interface{}`.

I have encountered the case that a struct field the marshal and unmarshal have
to use different type due to some reasons, so I have to use `interface{}` for
that struct field and use assertion to access it.


# Omit The Field If Not Exist
The "omitempty" option specifies that the field should be omitted from the
marshal(encoding) if the field has an empty value, defined as false, 0, a nil
pointer, a nil interface value, and any empty array, slice, map, or string.

```go
type Example struct {
	Count     int      `json:"count,omitempty"` // will not appear in JSON if empty value
	Name      string   `json:"name"`
}
```

# Exclude The Field
This field will be skipped in either Marshal or Unmarshal.
```go
type Example struct {
	Count     int      `json:"-"`
}
```

# Non-Object JSON Data
The JSON is not always in object format, it can be a array, single string or
number, so to unmarshal them we need to use right golang data type:
1. JSON array -> golang slice
2. JSON string -> golang string
3. JSON number -> golang number

For example:
```go
blob := `"hello, json"`
var s string
if err := json.Unmarshal([]byte(blob), &s); err != nil {
	log.Fatal(err)
}
fmt.Printf("\n%+v\n", s) // hello, json
```

# Customizing Marshal/Unmarshal
This `UnmarshalJSON` method can be used to customize the unmarshal process, it
is from [Unmarshaler interface](https://pkg.go.dev/encoding/json#Unmarshaler).

In this example, we want to unmarshal a JSON string to a golang struct, the
JSON string is like:
```json
"{\"name\": \"peter\", \"code\": 20001, \"message\": \"peter is sick off today!\"}"
```

```go
type ExampleError struct {
	Name    string `json:"name"`
	Code    uint   `json:"code"`
	Message string `json:"message"`
}

// Implement this func and it will work when you call json.Unmarshal
func (a *ExampleError) UnmarshalJSON(b []byte) error {
	// b is the raw byte data
	// here we first unarmshal it to string to remove the `\`, the string will be
	// `{"name":"peter","code":20001,"message":"peter is sick off today!"}`
	var s string
	if err := json.Unmarshal(b, &s); err != nil {
		return err
	}
	// then we further unmarshal it to golang struct 
	temp := struct {
		Name    string `json:"name"`
		Code    uint   `json:"code"`
		Message string `json:"message"`
	}{}
	// if the b is not valid, we keep it in Message
	if err := json.Unmarshal([]byte(s), &temp); err != nil {
		a.Message = s
		return nil
	}
	a.Name = temp.Name
	a.Code = temp.Code
	a.Message = temp.Message
	return nil
}

var ee = ExampleError{}
err := json.Unmarshal(<byte data>, &ee)
```

There is another example about customized marshal and unmarshal, redact on top
of golang json [module example](https://pkg.go.dev/encoding/json#example-package-CustomMarshalJSON):
```go
package main

import (
	"encoding/json"
	"fmt"
	"log"
	"strings"
)

type Animal int

const (
	Unknown Animal = iota
	Gopher
	Zebra
)

func (a *Animal) UnmarshalJSON(b []byte) error {
	// b is the element from the JSON array
	var s string
	if err := json.Unmarshal(b, &s); err != nil {
		return err
	}
	switch strings.ToLower(s) {
	default:
		*a = Unknown
	case "gopher":
		*a = Gopher
	case "zebra":
		*a = Zebra
	}
	return nil
}

func (a Animal) MarshalJSON() ([]byte, error) {
	// a is the Animal element from the golang array
	var s string
	switch a {
	default:
		s = "unknown"
	case Gopher:
		s = "gopher"
	case Zebra:
		s = "zebra"
	}
	return json.Marshal(s)
}

func main() {
	animalArray := [3]Animal{Gopher, Zebra, Unknown}
	blob, err := json.Marshal(animalArray)
	if err != nil {
		log.Fatal(err)
	}
	// blob now is:
	// `["gopher","armadillo","zebra","unknown","gopher","bee","gopher","zebra"]`

	var zoo []Animal
	if err := json.Unmarshal([]byte(blob), &zoo); err != nil {
		log.Fatal(err)
	}
	fmt.Printf("\n%+v\n", zoo) // [1 2 0]
}
```

# Unmarshal String to Number
This is tricky and useful in some cases, we want to unmarshal a string encoded
number to golang `uint64`, so in the struct you can use tag `string`, for
example:

```go
type Example struct {
	ProjectNumber uint64 `json:"project_number,string"`
}
```

From [document](https://pkg.go.dev/encoding/json#Marshal):
The `string` option signals that a field is stored as JSON inside a JSON-encoded
string. It applies only to fields of string, floating point, integer, or boolean
types. 

# JSON Encoder/Decoder 
When to use marshal/unmarshal and encoder/decoder, reference [question](https://stackoverflow.com/questions/21197239/decoding-json-using-json-unmarshal-vs-json-newdecoder-decode):

* Use `json.Decoder` if your data is coming from an io.Reader stream, or you need
to decode multiple values from a stream of data, for example the HTTP response,
this can be memory efficiency because it does not load all data into memory.

* Use `json.Unmarshal` if you already have the JSON data in memory.

Also from [JSON and Go](https://go.dev/blog/json):
Due to the ubiquity of Readers and Writers, these Encoder and Decoder types can
be used in a broad range of scenarios, such as reading and writing to HTTP
connections, WebSockets, or files.
]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang OOP</title>
    <url>/2020/11/15/golang-OOP/</url>
    <content><![CDATA[
# Struct
Go 用`struct` 实现OOP, 匿名字段可看作实现了继承关系，子类也可以重写父类的方法。

Note, there is also [exported and unexported](https://golangbyexample.com/exported-unexported-fields-struct-go/) fields in struct, the same pattern like exported and unexported variable, func and method.
```go
// Person 在Student中成为了匿名字段
// 直接被访问，也叫提升字段
type Person struct {
    name string
    age int
}

type Student struct {
    Person // 匿名字段，模拟继承结构 or *Person
    school string
}

type Student struct {
    person Person // 普通嵌套字段，必须逐层访问
    school string
}
// You can declare a method on non-struct types, too.

// method，会自动关联到对应的struct
// non-pointer receiver is not common! please don't use it!
func (p Person) getName() {
    return p.name
}

// 指针类型 is used to modify struct variable!
// Since methods often need to modify their receiver
// pointer receivers are more common than value receivers.
func (p *Person) setAge() {
    p.age = p.age + 10
}

// Student子类重写了 Person 父类的方法
func (s Student) getName() {
    return "studnet " + s.name
}

func main() {
    s1 := Student{Person: Person{name: "xxx", age:12}, school: "yyy"}
    // or omit the field name:
    s1 := Student{Person{"xxx", 12}, school: "yyy"}
    // 匿名字段 和 嵌套字段(逐层)的访问
    // 因为提升字段的原因，可以把Person省掉
    // s1.name or s1.Person.name
    // s1.age  or s1.Person.age
    // s1.school

    // vaule s1 calls a pointer receiver method
    s1.setAge() // call is interpreted as (&s1).setAge()

    s2 := &Person{"people", 23}
    // pointer s2 calls a value receiver method
    s2.getName()  // call is interpreted as (*s2).getName()
}
```

# Interface
在Go中，`interface` 定义方法的声明signature，具体类型实现方法的定义.
```go
// interface define
type Usb interface {
    // normal function definition
    start()
    end()
}

type Mouse struct {
    name string
}

// 实现接口，会自动关联
// 相当于Java 中 class xxx implements xxx
// 虽然这里没有写出来interface的名字
// 不能写成(m *Mouse)
func (m Mouse) start() {
    fmt.Println("Mouse start")
}
func (m Mouse) end() {
    fmt.Println("Mouse end")
}

// 测试使用接口
func testUsb(usb Usb) {
    // usb 实际上就是实现了接口的Mouse
    usb.start()
    usb.end()
}

m := Mouse {name: "xxx"}
testUsb(m)

// 或者声明interface 变量
var u Usb
// u 不能访问 m 的字段
u = m
u.start()
u.end()
```

空接口，没有方法，所以可以认为所有类型都实现了它，可以用作函数的参数去接收任何数据类型。
The main usage of empty interface is func arguments.
```go
type A interface {}
// because all type implicitly implements empty interface
// so we can use interface{} as var type
var a1 A = "hello"
var a2 A = 123
var a3 A = true
var a4 interface{} = "hello"

// map value存储任意类型
var map1 = make(map[string]interface{})
// slice 存储任意类型
var slice1 = make([]interface{})
```

注意`fmt.Println()` 就是这么实现的，用的匿名的空接口。
```go
// 可变参数 + 匿名空接口
func Println(a ...interface{}) (n int, err error) {
	return Fprintln(os.Stdout, a...)
}
```

How to 判断接口对应的具体类型呢，语法:
```go
// instance: 解析后的实际类型
// ok: true or false
// 接口对象: 接口名
// 实际类型: 猜测的实际类型
instance := 接口对象.(实际类型) // panic may occur
instance, ok := 接口对象.(实际类型) // safe way

switch 接口对象.(type) {
    case string:
        ...
    case int:
        ...
    case Person:
        ...
}
```

举例:
```go
type A interface {}

// 实例的指针值也可以传进来
func getType(a A) {
    // 判断a 具体是什么类型
    if ins, ok := a.(int), ok {

    }
    // 结构体类型
    // 如果传进来的是指针，则写成，a.(*Person)
    if ins, ok := a.(Person), ok {

    }

    // 或者用switch
    switch ins := a.(type) {
        case int:
            // pass
        case Person:
            // pass
    }
}
```

接口还可以多继承, 实现`C`的类型必须要实现`C`中自身和继承的所有的方法:
```go
type A interface {
    testA()
}

type B interface {
    testB()
}

type C interface {
    A
    B
    testC()
}
```]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>Gitlab CI</title>
    <url>/2020/12/05/gitlab-ci/</url>
    <content><![CDATA[
目前使用的source code management tool 是gitlab, 除了行使git 的功能外，对每次 merge request 都做了额外的CI/CD操作，这里记录一下相关语法和总结 from course [Continuous Delivery with GitLab](https://www.linkedin.com/learning/continuous-delivery-with-gitlab/use-gitlab-for-code-management?u=56685617)

`CI`: code and feature integraion, combining updates into existing code base, testing with automation. 
`CD`: delivery can mean deployment, the process of building and deploying the app, for example, upload the object to somewhere that customer can download.

Gitlab uses pipelines to do both `CI/CD`, defined in `.gitlab-ci.yml` file at your branch.

# Tips
[x] To navigate the source code in gitlab repo, try launch the `Web IDE`, will show you a structure tree on left side of the files.
[x] Use snippet to share code or file block for issue solving, the same as gPaste.
[x] To-do list is someone mentions you in some events.
[x] Milestone is a goal that needs to track.
[x] Merge request (pull request in github) after merged can auto close the issue, deponds on setting.


# Setup self-managed Gitlab
You can experiment with gitlab community edition locally by bringing up a gitlab server through Vagrant.
For example, Vagrantfile, there are 2 VMs, one VM for configuring docker gitlab runner:
```ruby
# -*- mode: ruby -*-
# vi: set ft=ruby :

# server static ip
GITLAB_IP = "192.168.50.10"
# worker static ip
GITLAB_RUNNER_IP = "192.168.50.11"

Vagrant.configure("2") do |config|
  # gitlab server VM
  config.vm.define "server", primary: true do | server|
    server.vm.hostname = "gitlab"
    server.vm.box = "bento/ubuntu-16.04"
    ## private network
    server.vm.network "private_network", ip: GITLAB_IP

    server.vm.provider "virtualbox" do |v|
      v.memory = 2048
      v.cpus = 2
    end
  end

  # gitlab runner VM with docker installed
  config.vm.define "runner", primary: true do | runner|
    runner.vm.hostname = "runner"
    runner.vm.box = "bento/ubuntu-16.04"
    ## private network
    runner.vm.network "private_network", ip: GITLAB_RUNNER_IP

    runner.vm.provider "virtualbox" do |v|
      v.memory = 1024
      v.cpus = 2
    end
  end

end
```

Vagrant quick commands:
```bash
vagrant up
# ssh to server
vagrant ssh [gitlab]
# ssh to worker
vagrant ssh runner
# destroy uses
vagrant destroy -f
```

Install the packages references from [there](https://about.gitlab.com/install/#ubuntu), but it uses enterprise edition, we use community edition.
```bash
# Update package manager and install prerequisites
sudo apt-get update
sudo apt-get install -y curl openssh-server ca-certificates
# we don't need email in this case, so skip it

# Set up gitlab apt repository
curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.deb.sh | sudo bash

# Install gitlab
# this is the IP address in vagrant file
# gitlab-ce is community edition
sudo EXTERNAL_URL="http://192.168.50.10" apt-get install gitlab-ce
```

After install, go to browser and hit `http://192.168.50.10`, reset root password and login as `root` with the reseted password.

# Experiment
[1] Create a new project `hello world` (You can also create it by setting jenkins pipeline)
Use root user to create a private project, check RAEDME added option.

[2] Create a admin user, so don't need to use `root` user anymore.
Grant the new user as `admin`, edit the password that will be used as temporary password next time you login.
sign out and sign in again with new admin user.

[3] Setup SSH for your user
The same process as setup SSH on github, go to setting -> SSH keys.

[4] Create new project under admin user, set as priviate scope.

[4] Create anthos vagrant VM as gitlab client 
To avoid messing up system git global configuration, then vagrant ssh and git clone the project.

Go to project dashboard, in the left menu:
The `CI/CD` tab is what we will focus on
The `Operations` tab is where gitlab integrate other systems in your stack, for example kubernetes.
The `Settings -> CI/CD` is about configuration.


# CI/CD
[x] SonarQube, code quality testing tool.

`.gitlab-ci.yml` 通过设计stage 搭配完成了both CI/CD 的操作。可以通过不同的条件判断，对特定的branch 进行不同的CI/CD. 每次MR 之前和之后都各有一个 pipeline，针对的是MR前后的branch. 设置了jenkins pipeline double-commit 到master branch, 因为如果需要修改`gitlab-ci.yml` 只会checked in 到 master中， 所以变化要在master中得到体现。

`CI` test levels, each of them is a stage in pipeline, should fail early and fail often.
- syntax and linting
- unit and integration
- acceptance

Gitlab runner is similar to jenkins, support run on VM, bare metal system or docker container or kubernetes.
Here we use docker, so install docker first, can reference [here](https://docs.docker.com/engine/install/ubuntu/)

Here we install docker on gitlab server VM.
[x] You can spin up another VM with 2GB, install docker and run gitlab runner container there. But make sure the VM can ping each other, just like what I did in Vagrantfile.

This docker install is on `Ubuntu`, `Centos` or other linux distro please see different way to install docker:
```bash
sudo apt-get update

sudo apt-get install -y \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg-agent \
    software-properties-common

# add docker's official GPG key
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

# add stable repository
sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"

# install docker
sudo apt-get update
sudo apt-get install -y docker-ce docker-ce-cli containerd.io

# verify install good
sudo docker run hello-world
```

Install docker gitlab runner, reference is [here](https://docs.gitlab.com/runner/install/docker.html)
```bash
# name is gitlab-runner
# -v: will create folder automatically
sudo docker run -d --name gitlab-runner --restart always \
    -v /srv/gitlab-runner/config:/etc/gitlab-runner \
    -v /var/run/docker.sock:/var/run/docker.sock \
    gitlab/gitlab-runner:latest
```

Then register the runner to your gitlab project, go to gitlab project Settings -> CI/CD -> Runners expand to see the registeration token.
```bash
# later gitlab-runner is command
# register is argument
sudo docker exec -it gitlab-runner gitlab-runner register

# command prompt:
Enter the GitLab instance URL (for example, https://gitlab.com/):
# from runner expand
http://192.168.50.10/
Enter the registration token:
# from runner expand
K5G9S5e5wmcdoANUGLF4
Enter a description for the runner:
[5922b65a9261]: docker
Enter tags for the runner (comma-separated):
# gitlab-ci will refer this tag
docker-tag
Registering runner... succeeded                     runner=K5G9S5e5
Enter an executor: docker, docker-ssh, virtualbox, docker+machine, docker-ssh+machine, custom, parallels, shell, ssh, kubernetes:
docker
Enter the default Docker image (for example, ruby:2.6):
# this can be overrided later
alpine:latest
```
Then reload the gitlab runner page, you will see the registered runner is there, click runner name to see specific. This runner is locked to this project, but you can alter it (the edit icon right near runner).


Create `.gitlab-ci.yml` in your repo to specify the pipeline, if you create it on web IDE, you can choose a template for it, for example the bash template, more advanced syntax please see gitlab-ci doc:
```yaml
---
# will override the image alpine:latest above
image: busybox:latest

# global variable, used by ${CHART_NAME}
variables:
  CHART_NAME: xxxx
  VERSION_NUM: xxxx

# specify order or skip some stages
stages:
  - test
  - build
  - deploy

before_script:
  - echo "Before script section"
  - echo "For example you might run an update here or install a build dependency"
  - echo "Or perhaps you might print out some debugging details"

after_script:
  - echo "After script section"
  - echo "For example you might do some cleanup here"

# execute in order if no stages list
build1:
  # tags means run on the docker runner I installed above that taged as `docker-tag`
  tags:
    - docker-tag
  stage: build
  script:
    - echo "Do your build here"

test1:
  tags:
    - docker-tag
  stage: test
  script:
    - echo "Do a test here"
    - echo "For example run a test suite"

test2:
  tags:
    - docker-tag
  stage: test
  script:
    - echo "Do another parallel test here"
    - echo "For example run a lint test"

deploy1:
  tags:
    - docker-tag
  stage: deploy
  script:
    - echo "Do your deploy here"
```
In the Pipeline page, `CI Lint` is the tool can edit and validate the `.gitlab-ci` yaml file syntax.
You can also use Settings -> CI/CD -> Environment variables expand to set the env variables.

[x] where is the run-dev-check.sh script hosted? it is git cloned from another repo.
```yaml
    script:
      - git clone -v $CLOUDSIMPLE_CI_REPO_URL
      - ci-cd/common-jobs/run-dev-check.sh
```]]></content>
      <categories>
        <category>Gitlab</category>
      </categories>
      <tags>
        <tag>gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang K8s Proj1</title>
    <url>/2022/04/18/golang-k8s-proj1/</url>
    <content><![CDATA[
**Porj1 description**: build a small go docker app to fetch and display service A's token from specified GKE cluster through port forwarding. The arguments in token request path are from configMap in the same GKE cluster.

I structure this blog in the order of the problem solving in development process.

# Caveat
The VSC may have [hiccups](https://stackoverflow.com/questions/58518588/vscode-could-not-import-golang-package) on import of go packages(annoying errors at package import statement which prevents code intelligent suggestions from using), troubleshooting could be:
1. check the package is downloaded/installed, run `go get`
2. open VSC editor at Go project root directory rather than others
3. make sure Go tools are up-to-date
4. try move the Go project out of GOPATH, and set go.mod for it
5. restart VSC editor


# How to get GKE cluster kubeconfig file?
To talk to GKE cluster, the first thing to do is running the gcloud command:
```bash
gcloud container clusters get-credentials <cluster> --region <region> --project <project>
```

I don't want to install gcloud CLI, alternatively letting golang do this job. The idea is to figure out how does gcloud command generate the kubeconfig, for example:
```bash
KUBECONFIG=kubeconfig.yml gcloud container clusters get-credentials <cluster> --region <region> --project <project>
```
The result would be a single cluster `kuebconfig.yml` file in the current directory, and export this `KUBECONFIG` will make subsequent `kubectl` commands work on the cluster specified in this yaml file.

To understand in depth I use gcloud option `--log-http` to dump command log:
```
gcloud container clusters get-credentials <cluster> --region <region> --project <project> --log-http
```

Displaying redacted log here:
```yaml
=======================
==== request start ====
uri: https://oauth2.googleapis.com/token
method: POST
== headers start ==
# header info
== headers end ==
== body start ==
Body redacted: Contains oauth token. Set log_http_redact_token property to false to print the body of this request.
== body end ==
==== request end ====
---- response start ----
status: 200
-- headers start --
# header info
-- headers end --
-- body start --
Body redacted: Contains oauth token. Set log_http_redact_token property to false to print the body of this response.
-- body end --
total round trip time (request+response): 0.084 secs
---- response end ----

Fetching cluster endpoint and auth data.
=======================
==== request start ====
uri: https://container.googleapis.com/v1/projects/<cluster>/locations/us-west1/clusters/<cluster>?alt=json
method: GET
== headers start ==
# header info
== headers end ==
== body start ==
 
== body end ==
==== request end ====
---- response start ----
status: 200
-- headers start --
# header info
-- headers end --
-- body start --
# big json file which contains necessary data to generate kubeconfig
-- body end --
total round trip time (request+response): 0.067 secs
---- response end ----
----------------------
kubeconfig entry generated for <cluster>.
```

The first http call is about OAuth2.0, used to authorize caller's request, the gcloud client will do this automatically for you if env variable `GOOGLE_APPLICATION_CREDENTIALS` is set correctly, for example:
```bash
export GOOGLE_APPLICATION_CREDENTIALS="path to adc.json or service account key file"
```
I will mount this credential file into docker when I run container.

Note that I use `GOOGLE_APPLICATION_CREDENTIALS` because the app runs outside of the gcloud environment, if it is inside the attached service account will be used. See demo about [Authenticating as a service account](https://cloud.google.com/docs/authentication/production#automatically).

Next I go to figure out how to run gcloud K8s go client to get the GKE cluster info, from the log dump above the URL is known as:
```yaml
uri: https://container.googleapis.com/v1/projects/<cluster>/locations/us-west1/clusters/<cluster>?alt=json
```
It is gcloud K8s engine REST API v1, the live experiment can play [here](https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1beta1/projects.locations.clusters/get). Once you fill the `name` field and click the EXECUTE button(uncheck the API key), the OAuth2.0 will pop up and let you authorize the request then the call will succeed. 

Next find the corresponding go client call for this REST API:
[Go Cloud Client Libraries ](https://cloud.google.com/go/docs/reference)
=> search in search bar
[Kubernetes Engine API](https://cloud.google.com/go/docs/reference/cloud.google.com/go/container/latest/apiv1)
=> search related func name
[func (*ClusterManagerClient) GetCluster](https://cloud.google.com/go/docs/reference/cloud.google.com/go/container/latest/apiv1#cloud_google_com_go_container_apiv1_ClusterManagerClient_GetCluster)

> Note that gcloud K8s go client is for managing GKE cluster not the K8s resources, it is not the k8s/go-client mentioned later.

From the sample code, the required field structure is the same as the REST API path:
```go
// The name (project, location, cluster) of the cluster to retrieve.
// Specified in the format `projects/*/locations/*/clusters/*`.
Name string `protobuf:"bytes,5,opt,name=name,proto3" json:"name,omitempty"`
```

Now ready, I have `GOOGLE_APPLICATION_CREDENTIALS` exported and gcloud K8s go client library, it is easy to get what `gcloud container clusters get-credentials` does for us and make the kubeconfig yaml file from template.


## What is OAuth2.0
The OAuth(open authorization) 2.0 [google doc and example](https://cloud.google.com/docs/authentication).

Usage: authorization between services, OAuth access token is JWT.
- [Intro OAuth](https://www.youtube.com/watch?v=t4-416mg6iU), access delegation, limited access.
- [OAuth deeper](https://www.youtube.com/watch?v=3pZ3Nh8tgTE): terms: resource, resource owner, resource server, client, authorization server(issue access token)
- [JWT explained](https://www.youtube.com/watch?v=soGRyl9ztjI), vs session token(reference token). The JWT(value token) contains the complete request info, that’s why it uses JSON object. Session token is just a key from a session map on the server side.
- [JWT structure explained](https://www.youtube.com/watch?v=_XbXkVdoG_0), encode and decode JWT object


# How to use K8s go client to access K8s resource?
Note that K8s go client([kubernetes/go-client](https://github.com/kubernetes/client-go)) is a standalone project used to talk to K8s, K8s itself is another go project([kubernetes/kubernetes](https://github.com/kubernetes/kubernetes)).

I have made the kubeconfig yaml file ready and set `KUBECONFIG` env variable, then using the client to do API call, for example [reference code](https://github.com/eddiezane/hello-client-go/blob/main/main.go), can also reference the go client [example](https://github.com/kubernetes/client-go/blob/master/examples/out-of-cluster-client-configuration/main.go) for out-of-k8s cluster.

In my project I need to get date from configMap, use [go client for configmap](https://github.com/kubernetes/client-go/blob/master/kubernetes/typed/core/v1/configmap.go).

## Tutorial
[Youtube: Getting Started with Kubernetes client-go](https://youtu.be/jiKwjnlc7Wk)
[Youtube: client-go K8s native development](https://www.youtube.com/watch?v=vlw1NYySbmQ&list=PLh4KH3LtJvRTb_J-8T--wZeOBV3l3uQhc)


# How to port forward in pure golang?
Next, I need to query a K8s service, to make it easy I need to forward port onto localhost, the kubectl command is:
```bash
# service port forward
kubectl port-forward svc/example 9200:9200 -v=8 &> verbose.txt
```
I add `-v=8` flag to dump the log into verbose.txt file.

Then I see there are consecutive API calls, it first gets service detail(GET), then looking for pod(GET) and pod details(GET) managed by that service and uses that pod to do port forwarding(POST). So it actually does:
```bash
# pod port forward
kubectl port-forward example-0 9200:9200
```

The go client has [port forward package](https://github.com/kubernetes/client-go/blob/28ccde769f/tools/portforward/portforward.go), to use it, import as `k8s.io/client-go/tools/portforward`(just follows the dir path layout).

The usage of port forward package is not obvious, I reference below 2 posts to make it work:
- [Port forward sample](https://github.com/kubernetes/client-go/issues/51#issuecomment-436200428)
- [Programmatically Kubernetes port forward in Go](https://gianarb.it/blog/programmatically-kube-port-forward-in-go)

The go channels(start and stop port forward) and goroutine will be used here, you can check `lsof` or `netstat` to see the target localhost port is listening.

Additionally, You can also see how [kubectl implement port-forward](https://github.com/kubernetes/kubernetes/blob/v1.6.0-alpha.0/pkg/kubectl/cmd/portforward.go#L107)


# How to convert curl to golang?
Next, I need to convert `curl` to golang, it is easy as the underlying is all about http request.

There is a interesting project has exactly what I need:
https://mholt.github.io/curl-to-go/


# How to unmarshal only small set of fields from HTTP response JSON?
I find 2 options:
1. Use struct type which has only the target fields, have to construct multiple struct and embed them to reflect the JSON field nesting.
2. Use [interface + type assertion](https://gobyexample.com/json) to receive and convert the target field, no struct is needed!

For single JSON object, [don’t use json.Decoder](https://ahmet.im/blog/golang-json-decoder-pitfalls/), please use `json.Unmarshal` instead.


# How to reduce go docker image size?
Lastly, I want to build a go docker app to simplify use and has minimum image size.

The docker official [doc](https://docs.docker.com/language/golang/build-images/) for building go docker image is not good, end up with a big size image.

The solution is easy: using multi-stage Dockerfile, build go binary in one go docker image and copy the binary to a new base image of the same Linux distro but has much less image size:
```Dockerfile
# syntax=docker/dockerfile:1

# Need to have google-cloud-sdk-gke-gcloud-auth-plugin
# for go-client to access GKE resource
# https://cloud.google.com/blog/products/containers-kubernetes/kubectl-auth-changes-in-gke
FROM golang:1.18-buster AS auth-plugin
# version > 381.0.0-0 asks to install gcloud cli
# which is not desired and make image size bigger!
ENV AUTH_VERSION="381.0.0-0"
RUN echo "deb https://packages.cloud.google.com/apt cloud-sdk main" \
    | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list \
    && \
    curl https://packages.cloud.google.com/apt/doc/apt-key.gpg \
    | apt-key add -

RUN apt-get update \
    && \
    apt-get install google-cloud-sdk-gke-gcloud-auth-plugin=$AUTH_VERSION

# build binary executable base on alpine image
FROM golang:1.18-alpine AS binary
WORKDIR /deployment
COPY cmd ./cmd
COPY go.mod ./
COPY go.sum ./
RUN go mod download
RUN go build -o ./cmd/app ./cmd

# minimum app image creation
FROM alpine:3.15
WORKDIR /deployment
COPY --from=binary \
    /deployment/cmd/app \
    ./cmd/app
COPY --from=auth-plugin \
    /usr/lib/google-cloud-sdk/bin/gke-gcloud-auth-plugin \
    /usr/local/bin/gke-gcloud-auth-plugin
COPY config ./config
COPY template ./template

WORKDIR /deployment/cmd
ENTRYPOINT [ "./app"]
CMD ["--help"]
```
This approach helps me reduce the docker image size from near 2GB to 60MB.

There is a post has similar idea:
[Build a super minimalistic Docker Image to run your Golang App](https://dev.to/chseki/build-a-super-minimalistic-docker-image-to-run-your-golang-app-33j0).
]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang Packages</title>
    <url>/2022/02/19/golang-packages/</url>
    <content><![CDATA[
Search standard Go packages here:
https://pkg.go.dev/std

What are packages belong to `golang.org/x`:
[These packages]((https://pkg.go.dev/golang.org/x)) are part of the Go Project but outside the main Go tree. They are developed under looser compatibility requirements than the Go core. Install them with "go get".

**List of commonly used packages**
`fmt`:
Common use placeholder: %q, %v, %d, %c(one string), %T(type), %p(address).
fmt.Errorf()
fmt.Sprintf()
`logrus`:
structured logger for Go
`strings`:
strings.LastIndex(s,"/"), strings([]byte{'a','b'})
`strconv`:
strconv.Itoa(55), strconv.Atoi("1"), strconv.ParseInt("23", 10, 64)
`bytes`:
bytes.Buffer, []byte("hello")
`unicode/utf8`:
utf8.RuneCountInString("Hello, 世界")

`kubernetes`:
https://github.com/kubernetes/kubernetes
`kubernetes/client-go`:
https://pkg.go.dev/k8s.io/client-go/kubernetes

`os`:
os.Args, os.Stdin, os.Open, os.Exit(1)
`flag`:
extract options/flags from CLI, like Python Click
`cobra`:
powerful CLI for modern app: k8s, etcd, istio, etc
`viper`:
a complete configuration solution for Go app
`path/filepath`

`httpRouter`
`net/http`:
minimal server, http.Handlefunc(x,handler), http.ListenAndServe(x,x)
func handler(w http.ResponseWriter, r *http.Request) {}
`net/url`
`context`
`encoding/json`
https://pkg.go.dev/encoding/json
json.Marshal(), json.MarshalIndent()
`yaml`:
gopkg.in/yaml.v3

`math/rand`:
rand is a subdirectory in math package.
`bufio`: 
Some help for textual I/O: bufio.NewScanner(os.Stdin), bufio.NewReader(os.Stdin)
`runtime`
runtime.Stack()
`io/ioutil`:
ioutil.ReadAll()
`sync`:
sync.Mutex

`sort`:
sort.Interface
sort.Strings(<[]string>)
sort.Ints(<[]int>)
sort.Reverse(sort.IntSlice(<[]int>))
sort.IsSorted()
`text/template`
`text/tabwriter`
`html/template`

`errors`
`time`:
time.Now(), time.Since(xx).Seconds()

`testify`

`hugo`:
a static site website generator
]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang Import Package</title>
    <url>/2023/02/10/golang-import-package/</url>
    <content><![CDATA[
> CAUTION: When using VSC editor, please open the root directory of that go
module instead of a parent directory that contains many go modules/projects, 
otherwise the editor will not work as expected.

There are several situations of importing package to use in your code:
1. Importing package from the same local module.
2. Importing package from remote module.
3. Importing package from a different local module .

For #1 and #2, the steps are described in
[How to Write Go Code](https://go.dev/doc/code), this article also explains
what are go `package` and `module` and the code organization.

For example, I have a go module with below file structure:
```ini
hello/
|-services/ # services package
  |- service1.go # (define struct A with exported methods)
  |- service2.go # (define other exported methods for struct A)
|-hello.go # main package
|-go.mod
```

The module name is `example.com/hello`, the `services` folder holds a separate
package `serivces`(with 2 go files service1 and service2), `main` package is
defined in `hello.go`.

If run `go list ./...` from hello folder, will get 2 packages:
```bash
# go list ./...
example.com/hello
example.com/hello/services
```

The `hello.go` imports and uses `services` package would be:
```go
import "example.com/hello/services"
```

For package imported from remote module, the downloaded module is in `pkg/mod`
subdirectory of the directory indicated by the `GOPATH` environment variable. To
remove all downloaded modules, run:
```go
go clean -modcache
```

For #3, please see
[Call your code from another module](https://go.dev/doc/tutorial/call-module-code#)

For example, I have 2 go modules with below file structure:
```ini
# Module name: example.com/fruit
|- fruit/
  |- apple.go # package apple with ReturnApple func
  |- peach.go # package peach with ReturnPeach func
  |- go.mod 
# Module name example.com/hello
|- hello/
  |-hello.go # package main
  |-go.mod
```

In the hello.go:
```go
import (
	"fmt"
	"example.com/fruit/apple"
	"example.com/fruit/peach"
)

func main() {
	fmt.Println(apple.ReturnApple())
	fmt.Println(peach.ReturnPeach())
}
```

To make the import for apple and peach work, we need to edit the `go.mod` file in hello folder:
```bash
# In hello folder
go mod edit -replace example.com/fruit=../fruit
go mod tidy
```

The hello `go.mod` file will be something like:
```go
module example.com/hello

go 1.18

require (
	example.com/fruit v0.0.0-00010101000000-000000000000
)
replace example.com/fruit => ../fruit
```
]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>Go Unittest for Http Client/Server</title>
    <url>/2023/06/11/golang-unittest-httptest/</url>
    <content><![CDATA[
The `httptest` package provides utilities for HTTP testing:

* Test server handler logic by mocking the request.
* Test client logic by mocking the server handler.

For server handler testing, you need `ResponseRecorder` object to record the
http.ResponseWriter's mutations, plus `httptest.NewRequest` and pass them into
the server handler for `(w http.ResponseWriter, r *http.Request)`.

For client logic testing, you need to mock specific server handler or
even entire server, using the `httptest.NewServer` Object or other server types.

From the main page, it provides [types](https://pkg.go.dev/net/http/httptest#pkg-examples)
for:

* ResponseRecorder
* NewTLSServer
* Server
* Server(HTTP2)


For client logic testing, if you only set up one handler and call it repeatedly,
it returns the same response. If you do want to have different response when you
call mock server, here is the
[example](https://stackoverflow.com/questions/50081104/how-to-mock-second-try-of-http-call),
I enrich it by adding more in handlers.

```go
package main

import (
	"bytes"
	"encoding/json"
	"fmt"
	"io/ioutil"
	"log"
	"net/http"
	"net/http/httptest"
	"net/url"
	"strings"
)

func main() {
	responseCounter := 0
	responses := []func(w http.ResponseWriter, r *http.Request){
		// Transfer into upper case for word provided
		// Hand GET request
		func(w http.ResponseWriter, r *http.Request) {
			query, err := url.ParseQuery(r.URL.RawQuery)
			if err != nil {
				w.WriteHeader(http.StatusBadRequest)
				fmt.Fprintf(w, "invalid request")
				return
			}
			word := query.Get("word")
			if len(word) == 0 {
				w.WriteHeader(http.StatusBadRequest)
				fmt.Fprintf(w, "missing word")
				return
			}
			w.WriteHeader(http.StatusOK)
			fmt.Fprintf(w, strings.ToUpper(word))
		},
		// Handle POST request to get message
		func(w http.ResponseWriter, r *http.Request) {
			if r.Method == http.MethodPost {
				body, err := ioutil.ReadAll(r.Body)
				if err != nil {
					fmt.Println(err)
					return
				}
				// Parse the request body as JSON.
				var data struct {
					Name  string `json:"name"`
					Email string `json:"email"`
				}
				err = json.Unmarshal(body, &data)
				if err != nil {
					fmt.Println(err)
					return
				}
				// Print the name and email of the user who submitted the form.
				fmt.Println("Name:", data.Name)
				fmt.Println("Email:", data.Email)
				fmt.Println("Header:", r.Header)
			}
		},
	}
	ts := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		responses[responseCounter](w, r)
		responseCounter++
		// Using counter to rotate if you have many more calls
		responseCounter = responseCounter % 2
	}))
	defer ts.Close()

	// call from client with different http method
	clientGetMethod(ts.URL + "?word=school")
	// bad call, will get your error message
	clientGetMethod(ts.URL + "?apple")
	clientPostMethod(ts.URL)
}

func clientGetMethod(url string) {
	res, err := http.Get(url)
	if err != nil {
		log.Fatal(err)
	}
	resBody, err := ioutil.ReadAll(res.Body)
	res.Body.Close()
	if err != nil {
		log.Fatal(err)
	}
	fmt.Printf("%s\n\n", resBody)
}

func clientPostMethod(url string) {
	// Set the request body.
	var jsonStr = []byte(`{
			"name": "John Doe",
			"email": "johndoe@example.com"
		}`)
	req, err := http.NewRequest(http.MethodPost, url, bytes.NewBuffer(jsonStr))
	if err != nil {
		fmt.Println(err)
		return
	}
	// Set the request header.
	req.Header.Set("Content-Type", "application/json")
	resp, err := http.DefaultClient.Do(req)
	if err != nil {
		fmt.Println(err)
		return
	}
	// Close the response body.
	defer resp.Body.Close()
}
```]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title>Golang Quick Start</title>
    <url>/2020/08/22/golang-learn/</url>
    <content><![CDATA[
# Introduction
Go users: https://github.com/golang/go/wiki/GoUsers
web services, devops: docker + k8s

Visual Studio Code go-plugin, vim go-plugin

Install and set up a runnable demo project, a brief guidance can see here:
[Official Setup Go project](https://go.dev/doc/tutorial/getting-started)

If forget, read through the tutorials one by one:
https://go.dev/doc/

Other resources:
- The Go programming Language
- [Effective Go](https://golang.org/doc/effective_go.html) 这个更深入了，特别是用法方面
- [Go Blog](https://blog.golang.org/index)

- [Go by Example](https://gobyexample.com/)
- [Other people web site](https://yourbasic.org/)
- [Go语言中文网](https://studygolang.com/)


What are go package and module is explained here:
https://go.dev/doc/code
A `package` is a collection of source files in the same directory that are
compiled together. Functions, types, variables, and constants defined in one
source file are visible to all other source files within the same package.

A repository contains one or more modules. A `module` is a collection of related
Go packages that are released together. A Go repository typically contains only
one module, located at the root of the repository. A file named go.mod there
declares the module path: the import path prefix for all packages within the
module. The module contains the packages in the directory containing its go.mod
file as well as subdirectories of that directory, up to the next subdirectory
containing another go.mod file (if any).

An `import path` is a string used to import a package. A package's import path
is its module path joined with its subdirectory within the module. For example,
the module github.com/google/go-cmp contains a package in the directory cmp/.
That package's import path is github.com/google/go-cmp/cmp. Packages in the
standard library do not have a module path prefix.

Example:
```go
// A module is a collection of related Go packages that are released together
// in the first line of go.mod, you can see the module name

// main package and func is the entrypoint of app
// the file name can be others not necessary as main.go
package main

import (
  . "fmt" // call fmt funcs directly withou fmt prefix
  err "errors" // alias
  "os"
  "net/http"
  "regexp"
  "builtin" // no need to import
  _ "github.com/ziutek/mysql" // call init in the package
)

// auto append `;`, so let { at the same line
func main() {
  fmt.Println("Hello world")
}
```
`package` name in go file should be the same as folder name. 

Important env variables:
```bash
## https://maelvls.dev/go111module-everywhere/
## In 1.15, it's equivalent to auto. 
## In >= 1.16, it's equivalent to on
GO111MODULE=""

## you can export or use `go env -w`
## GOROOT is set automatically
export GOROOT=/usr/local/go
export GOBIN=$GOROOT/bin

## only need to set GOPATH
## go path is the project workspace
## it has src (user create), pkg and bin (auto create for you when build or install)
export GOPATH=$HOME/go
export PATH=$PATH:$GOBIN

## persistent set and unset
go env -w GOPATH=$HOME/go
go env GOPATH
go env -u GOPATH
```

Understand `run`, `build`, `install`, `get` subcommands. pluralsight has `Go CLI playbook` course.
```bash
go version

## used GO >= 1.16
## has version suffix
## install binary without changing go.mod
go install sigs.k8s.io/kind@v0.9.0
go install sigs.k8s.io/kind@latest

## create a module
## link with your code repo url
## generate go.mod
go mod init example.com/example
## add missing and remove unused modules
## can edit the package version in require block
go mod tidy

# clean mod cache
go clean --modcache

## link dependency to local path
## ../greetings is a local package in relative path
## example.com/greetings is that packages module path
## see this tutorial:
## https://go.dev/doc/tutorial/call-module-code
go mod edit -replace example.com/greetings=../greetings
go mod tidy

# download dependencies for off-line go build
# see this ticket:
# https://stackoverflow.com/questions/68544611/what-is-the-purpose-of-go-mod-vendor-command
go mod vendor

## GO >=1.16, only for editing go.mod
## -x: verbose
## it will modify the go.mod
## or you edit go.mod manually
go get github.com/sirupsen/logrus@v1.8.0

## simliar to python dir()/help()
## case-insensitive
go doc fmt
go doc net/http
go doc time.Since
go doc fmt.Println
## local API server , browser your own package
godoc -http :8080

## linter, but VSC will auto does that when save go files
go fmt <package># it runs gofmt -l -w

## test
## file with _test.go suffix
go test [-v] [-run="Hello|Bye"] [-cover|-coverprofile]

## -n: dry run
go run -n main.go
## -work: print $WORK temp folder
## you will see the intermediary files created
go run -work main.go
## run main package
go run main.go
go run .
go run

## build(compile) not install
go build hello
## it will create executable in bin folder
cd $GOPATH/bin
## executable name is the same as folder name
./hello

## install dir is control by
## env GOPATH and GOBIN
go install hello

# check doc from CLI
go doc rand.Float64
go doc math.Sin
go doc strconv.Atoi
```

## 数据类型
基本数据类型:
int32, float64, bool, string(没有char type)
复合数据类型:
array, slice, map, function, pointer, struct, interface

&(取地址符，没有地址的算数运算), *t

## 变量赋值方式
```go
var xx = 23
var xx int = 23 
var xx float32 = 2.32

// var block
var (
  xx int = 34
  yy = 78.99
  zz = "123"
)
// 简短写法
a := "hello"

// swap
b := "world"
a, b = b, a

// 全局变量不支持简短写法
var GLOBAL int = 100
```

## 常量使用
Use all upper cases

## 字符串类型
```go
str1 := "abc"
str2 := `abc`
```

## 强制类型转换
```go
int()
```

## 算数运算符

## 关系运算符
<, >, <=, >=, ==, !=

## 逻辑运算符
&&, ||, !

## 位运算符
&, | , ^, &^(位清空), <<, >>

## If-else
condition 没有括号, no ternary
```go
// if 还可以有初值，和for loop类似了
// 也可以把初值写到外面
if err := file.Chmod(0664); err != nil {
    log.Print(err)
    return err
}
```

## Switch 
每个case 自带break。
关键字 `fallthrough` 只能放case最后一行, 连接执行2个case且一定执行。
case 后的数据类型必须和switch一致, case 可无序，多个condition可用逗号分开。
switch 省略条件相当于`switch true`(tagless switch), 也可省略后，把condition 写在case 后面，就像if-else if了
也可以有初值`switch t = 100 {..}`, `t` only use in switch block

## For loop
no while loop
语法和C一样: `for init; conditionl; post {}`
condition no bracket
```go
for {}
for ;; {}
for key, value := range map {}
for index, value := range [slice, array, string] {}
```

## Goto
前后跳都可以

## Array
没赋值的默认值和C语言一样:
```go
var a [10]int // element default is 0
var a = [10]int{}
var a = [3]float64 {1, 2, 3}
a := [3]float64 {1, 2, 3}
// 可以设置index对应的值
a := [3]float64 {0:32, 2:99}
a := [...]float64 {0:32, 2:99}
len(a)
cap(a)
// ascending sort 
sort.Ints(a)
```
对于数组 len(a) 长度= cap(a) 容量

```go
// same address usage as C language
var arr = [3]int32{99, 100, 101}

// 数组是值传递, 改变arr2中的值不会改变arr, 函数中注意, 除非传递的是地址
// 可直接复制
arr2 := arr
// true
fmt.Println(arr2 == arr)
// type [3]int
fmt.Printf("%T\n", arr2)
// 注意在go中数组名不是地址了，要专门用取地址符号
// 这2个值一样的
fmt.Printf("%p\n", &arr)
fmt.Printf("%p\n", &arr[0])

// t1 其实是指针类型 *[3]int32
t1 := &arr
fmt.Printf("%d\n", (*t1)[0])
```
二维数组
```go
var arr := [3][3]int{{},{},{}}
len(arr)
len(arr[0])
```

## Slice
动态数组，大小可变，背后有一个底层数组.
```go
// 不写长度, slice = nil, no underlying array
// have to append to use it
var slice1 []int
var slice1 = []float64 {1, 2, 3}
// or
// make(type, len, capacity) len 必须小于或等于 cap
// make function is in builtin package
// used to create object
slice1 := make([]int, 2) // len, cap = 2
slice1 := make([]int, 0, 2) // len = 0, cap = 2
// slice1中存的就是切片的地址，不需要取地址符了
// 而数组的地址则是&arr
fmt.Printf("%p\n", slice1)

slice1 = append(slice1, 1, 2, 3, 4, 5, 6, 7)
slice2 := []int{10, 11}
// 注意这里用...，类似于python的分解操作，如果查看append的定义，其实它的末尾参数是个可变参数...type
// 当append超过容量后，生成新的切片的，地址就变了
// 自动扩容是之前的2倍
slice1 = append(slice1, slice2...)

// map, slice 是地址传递!
// 指向同一个地址
slice3 := slice1

// create slice from array
// slice1和arr1指向同一个地址，共享数据，除非slice1扩容则会新生成一个内存地址
arr1 := [10]int
// [start, end)
slice1 := arr1[:]
slice1 := arr1[0:10]
slice1 := arr1[2:4]
// 这时候slice1地址和arr1不一样了
slice1 = append(slice1, 4, 5, 6)

// deep copy, for []type
copy(dst, src)
copy(slice2[2:], slice[1:])
```

## Map
To understand map, see this question and the comment:
https://stackoverflow.com/questions/40680981/are-maps-passed-by-value-or-by-reference-in-go
this is a go [playground](https://go.dev/play/p/Q6vrAmmJWR6) for map to help understand its behavior:


If a map isn’t a reference variable, [what is it](https://dave.cheney.net/2017/04/30/if-a-map-isnt-a-reference-variable-what-is-it)?


```go
// nil map, 不能直接用
var map1 map[int]float64
fmt.Println(map1 == nil)

map1 := make(map[int]string)
map1 := map[string]int{}
map1 := map[string]int{"hello": 100, "Java": 99}

map1["see"] = -34
val1 := map1["hello"]

// 如果map没有key，取出来的是value类型的"0"值
// 怎么判断呢? ok is bool, if true, then exist
val, ok := map["key"]

delete(map1["see"])
len(map1)
```

## String
```go
// UTF-8 coding
str := "hello"
str := `hello`
// string is acutally byte sequence
slice := []byte{65, 66, 67, 68}
str1 := string(slice1)
slice := []byte(str1)

substr := str[1:3]
// strings package 主要是字符串 操作函数
// strconv package 主要是字符串 和 基本数据类型之间相互转换
// go '+' operands must have same type, so use Sprintf to concat
// different type
"123" + strconv.Itoa(100)
b1, err := strconv.ParseBool("true")
str1 := strconv.FormatBool(b1)
```

## defer
用于延迟函数或方法的执行, 用`defer` 控制的调用会等到它的containing/surrounding function执行完了之后再执行，并且当所有延迟的部分执行结束之后，containing function才执行return 语句。

如果有多个`defer`,则按照后进先出的顺序。
用法：
- 关闭连接，文件，比如defer close() 在函数的开头.
- panic, recover，defer函数执行完毕后，异常才会被抛出到上一层.

注意，延迟函数的arguments的值，在`defer`的时候就固定了，值也可能是地址，则复合对象的内部值可能被改变了。
```go
func deferTest() {
  defer fmt.Println("1")
  defer fmt.Println("2")
  fmt.Println("3")
}
```

# Function
函数也是一种复合数据类型，可以用`%T` 查看。
注意函数的参数类型和参数名是反过来写的，且返回值类型列表也写在最后。
函数名如果大写开头 表示公共函数，可被其他package调用，比如fmt.Println()，否则只能package内部访问。
参数传递没有python这么多形式。

参数传递也有值传递和引用(地址)传递.
值传递: int, float, string, bool, array, struct
引用传递: slice, map, chan
```go
// (int, int) 对应了 return val1, val2的类型
// 注意这里p2是slice, p3是array
func funcName(p1 int, p2 []float, p3 [10]int, p4 map) (int, int) {
  // logic
  return val1, val2
}
// 或者只写return，返回列表里已经有了返回值名字了
func funcName(p1 int, p2 []float) (val1 int, val2 int) {
  val1 = p1 * 3
  val2 = len(p2)
  // return named values
  return
}
// 或者return 单纯用来结束函数
// 注意这里没有定义返回值列表
func funcName() {
  return
}

// 如果只有一个返回值，可以不写括号
// 多个参数类型一致，可以把类型写在后边一起
func funcName(p1, p2 int, p3 string) int {
  return val1
}
```

可变参数`...`，只能放到参数列表的最后，且只能有一个可变参数:
```go
// ...type，0个到多个参数都可以，比如Println()就是
// nums 是个slice类型
func funcName(p1 int, nums ...int) {
  // pass
}
// 调用的时候注意，如果是复合类型数据需要分解
funcName(100, 1, 2, 3)
// split sign `...`
funcName(100, []int {1, 2, 3}...)
```

可以定义函数变量，然后赋值，其实赋值的是函数的地址:
```go
var c func(a ...interface{}) (n int, err error)
c = fmt.Println
```

[Anonymous functions and closures](https://yourbasic.org/golang/anonymous-function-literal-lambda-closure/)
匿名函数，没有函数名，可以赋值给变量 或 直接调用. 所以说Go是支持函数式编程的，匿名函数可以作为其他函数的参数(这个作为参数的函数就叫回调函数)，注意不是函数执行的返回值作为参数，是函数本身! 匿名函数也可以作为返回值(闭包)
```go
// no function name and call it
func () {
  fmt.Println()
}()
// or
fun3 := func () {
  fmt.Println()
}
fun3()
```

回调函数:
```go
// callback function
func add(a, b int) int {
  return a + b
}
// calling function
// here we can define func type to replace 'func(int, int)int' in argument list
func operate(a, b int, fun func(int, int)int) int {
  return fun(a, b)
}
// call
operate(1, 3, add)
```

闭包closure, 这里外部函数返回后，匿名函数把外部函数内部的资源`i`保留了下来。Python中local function一样的道理.
A closure is a function value that references variables from outside its body.
```go
// `func() int` is treated as return type as a whole
// the increment() return a closure
func increment() func() int {
  // stay alive in closure function
  i := 0
  return func () int {
    i++
    return i
  }
}

res := increment()
// 1
fmt.Println(res())
// 2
fmt.Println(res())
// 3
fmt.Println(res())
```

# Pointer
指针类型符号`*`, 和C语言一样.
```go
// nil is pointer's default vaule
var p1 *int
a := 10
p1 = &a
// print pointer value
fmt.Println(p1)
fmt.Printf("%p\n", p1)

// 指针的指针
var pp1 **int
pp1 = &p1
```

数组指针，指向数组的指针
```go
arr := [3]int {1, 2, 3}
var pa *[3]int
pa = &arr
// 这个输出注意，并不是输出的地址数值，而是一种形式
// 如果要输出地址数值，用%p
fmt.Println(pa) // &[1 2 3]

// the address of arr
fmt.Printf("%p\n", pa)
fmt.Printf("%p\n", &arr)

// 相同的表达
(*pa)[0]
pa[0] // (*pa)[0] 的简化写法
arr[0]
```

指针数组，元素是指针的数组
```go
a := 1
b := 2
c := 3
arr := [3]*int {&a, &b, &c}
fmt.Println(arr)
```

For example:
```go
// 指向 指针数组的指针
*[3]*int
// 指向 数组指针的指针
**[5]string
// 指向 指针数组指针的指针
**[3]*string
```

函数指针，指向函数的指针, Go中函数名就是函数的地址，如同slice, map一样:
```go
var c func(a ...interface{}) (n int, err error)
c = fmt.Println
```

指针函数，返回指针值的函数:
```go
// 2个地址输出是一样的
func getArr() *int[4] {
  arr := [3]int {1, 2, 3}
  fmt.Printf("%p\n", &arr)
  return &arr
}

arr := getArr()
fmt.Printf("%p\n", &arr)
``` 

# Struct
Usually struct has [struct tag](https://stackoverflow.com/questions/55544525/what-is-the-string-after-the-name-and-type-in-a-struct-in-golang).
```go
type Person struct {
  name string
  age int
  sex string
  address string
}

// 几种创建方式
// p1 is an empty struct object with default values
var p1 Person
// same as p1
// 可以拿到外面赋值， 比如p2.age = 34
p2 := Person{}

p3 := Person (name: "XXX", age: 23, sex: "female", address: "YYY")
p4 := Person {
  name: "XXX", 
  age: 23, 
  sex: "female", 
  address: "YYY"
}
// order matters
p3 := Person ("XXX", 23, "female", "YYY")
```

结构体是值类型的，和array一样，结构体名不是它的地址。
```go
// 这是内容复制
p4 := p1

// 这是地址赋值
var pp1 *Person
pp1 = &p1
// 访问字段，以下都可以
pp1.name
(*pp1).name
```

`make`只能创建map, slice, channel等
`make` vs `new`: https://www.godesignpatterns.com/2014/04/new-vs-make.html
`new` returns pointer:
```go
// they are the same
var p1 Person
pp1 := &p1

pp1 := &Persion{}
pp1 := new(Persion)
```
`new`可以创建任意类型空间 和 返回任意类型的指针:
```go
pint := new(int)
```

匿名结构体，和匿名函数类似的定义用法
```go
// 这里定义了一个匿名结构体 同时进行赋值
s2 := struct {
  name string
  age int
}{
  name: "xxx",
  age: 18,
} 
```

匿名字段，字段没有名字，且字段类型不能重复:
```go
type Worker struct {
  // 只能有一个string
  string
  // 只能有一个int
  int
}

w1 := Worker {"xxx", 11}
// 默认使用数据类型作为访问的名字
fmt.Println(w1.string)
fmt.Println(w1.int)
```

结构体嵌套:
```go
type A struct {
  a string
}

type B struct {
  b string
  c A
}

s := B {b: "from B", c: A {a: "from A"}}
// access field
s.b
s.c.a
```

如果对嵌套结构体使用了匿名字段，则相当于融合到了当前结构体，访问的时候不用中间层了。这在OOP 中使用到了:
```go
type A struct {
  a string
}

type B struct {
  b string
  A
}

// 注意这里匿名字段初始化A: A{a: "from A"}
s := B{b: "from B", A: A{a: "from A"}}
// access field
s.b
// 中间层被省略了
s.a
```


# Type
`type` 除了定义结构体，接口，还可以定义全新的类型 或者 别名:
[function type and value](https://yourbasic.org/golang/function-pointer-type-declaration/)
```go
// myint, mystr 就是全新的类型了 
type myint int
type mystr string

// 不能和原类型相互赋值！
var myint = 23
var mystr = "hello"

// myint, mystr 只是一个别名，注意和上面的区别
type myint = int
type mystr = string

// 可以和原类型相互赋值！
var a int = 10
var b myint = a
```

Go语言实现函数式编程的时候，如果函数复杂，可以用`type` 简化，这里的例子是作为返回值，也可以作为参数:
```go
// 把func(string, string) string 整体别名叫做myfun
type myfun func(string, string) string

// myfun 作为fun1 的返回值类型，就不用写一大堆了
func fun1() myfun {
  // 返回一个函数
  return func(a, b string) string {
    return a + b
  }
}

// 调用, res得到一个函数定义
res := fun1()
// "100100"
fmt.Println(res("100", "100"))
```

还需要注意的是，一个package中定义的别名，不能在其他包中为它添加method.

# Error
Go中不要把错误 和 异常弄混了，这个还要单独看其他文章区分一下使用环境。

Go中错误也是一种数据类型`error`, 惯例是`error` 在函数中返回值的最后一个位置, error其实是一个接口:
```go
type error interface {
    Error() string
}
```
使用:
```go
import {
  "os"
  "log"
  "fmt"
}

func main() {
  // 很多包中的函数都会有错误信息返回，特别是文件，网络操作
  f, err := os.Open("./test.txt")
  if err != nil {
    log.Fatal(err)
    // 得到具体的错误类型，输出其他字段
    // *os.PathError 是这个函数返回的错误类型
    if ins, ok := err.(*os.PathError); ok {
      // 错误中的字段
      fmt.Println(ins.Op)
      fmt.Println(ins.Path)
      fmt.Println(ins.Err)
      // 如果错误实现了自己的方法，调用方法也行
    }
  }
  // pass
}
```

自己如何创建错误呢, 通过`errors.New`这个package提供的函数 或者`fmt.Errorf`:
```go
import {
  "errors"
  "fmt"
}

func main() {
  err1 := errors.New("my error")
  fmt.Println(err1)
  // 是 *errors.errorString 类型
  fmt.Printf("%T\n", err1)

  err2 := fmt.Errorf("my error is %d", 100)
  fmt.Println(err2)
  // 是 *errors.errorString 类型
  fmt.Printf("%T\n", err2)
}
```
以上只是简单的例子，复杂的error 是通过结构体实现的，里面包含了错误的具体信息，然后实现了`error` 这个接口.


# Panic and Recover
和`defer` 一起使用较多，遇到`panic`，函数后续会停止执行，然后逆序执行所有已经遇到的当前层的`defer`函数，没遇到的不会执行，因为中断了，最后往上层抛出异常，
`defer` 函数中可能包含`recover` 来恢复`panic`。

```go
func A() {
  fmt.Println("from A")
}

func B() {
  fmt.Println("from B")
  defer fmt.Println("from B defer 1")
  for i := 0; i <= 10; i++ {
    fmt.Println("i = ", i)
    if i == 5 {
      // panic throw
      panic("panic happened")
    }
  }
  // will not execute this defer
  defer fmt.Println("from B defer 2")
}

func main() {
  // 捕捉恢复了panic
  defer func() {
    if msg := recover(); msg != nil {
      fmt.Println("from recover")
    }
  }

  A()
  defer fmt.Println("from main defer 1")
  B()
  // 这后面不会被执行了
  defer fmt.Println("from main defer 2")
  fmt.Println("from main end")
}
```

哪些场景适合使用panic呢?
- 空指针
- 下标越界
- 除数为0
- 分支没有出现
- 错误输入]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>Go Unittest by testify</title>
    <url>/2023/03/01/golang-unittest-testify/</url>
    <content><![CDATA[
For complex testing, the [testify](https://github.com/stretchr/testify) module
is a big life saver, it has 4 main packages: assert, require, mock and suite.
Their usages are quite straightforward from the `testify` 
[README](https://github.com/stretchr/testify/blob/master/README.md) file.

There is a series of [tutorial](https://www.youtube.com/watch?v=Su6zn1_blw0)
about how to use testify, pretty helpful.

Here in this blog I highlight the parts that are important to me. 

# Mock Objects Generation
Regarding `mock`, to create mocking objects for external dependencies, we use
[`mockery`](https://github.com/vektra/mockery/blob/master/docs/index.md) to
generate mock objects from `interface`.

> Thus to write testable code in Go, thinking about how to use interface
properly.

[Install](https://github.com/vektra/mockery/blob/master/docs/installation.md#getting-started)
mockery binary:
```bash
# Check current release tag and use it
# The mockery binary will be downloaded to $(go env GOPATH)/bin
go install github.com/vektra/mockery/v2@v2.20.0
```

Mock objects generation command:
```bash
# If you are using mockery binary downloaded, run it from the repo with absolute
# path.
# This will create a "mocks" folder in current directory and generate a mock
# file named as HelloWorld.go.
/<path to mockery parent folder>/mockery --dir student --name HelloWorld
```
Other flags please see `mockery --help`.

Or using docker:
```bash
docker run --rm -v $(pwd):/src \
-w /src \
vektra/mockery:v2.20.0 --dir student --name HelloWorld
```

Then in your test go file, import the `mocks` package and use it.

# Suite
The [suite](https://pkg.go.dev/github.com/stretchr/testify/suite) helps the testing
lifecycle management, for example, it provides setup/teardown stage for test
cases.

For example the basic suite structure:
```go
import (
  "context"
  "example.com/mocks"

  "github.com/stretchr/testify/suite"
)

type ExploreWorldSuite struct {
  suite.Suite
  // from mocks package
  hw  *mocks.HelloWorld
  // from current package
  std *Student
  // other utilities
  ctx context.Context
}

// Reset for every test case
func (s *ExploreWorldSuite) SetupTest() {
  // The HelloWorld.go mock file contains NewHelloWorld func
  s.hw = mocks.NewHelloWorld(s.T())
  s.ctx = context.TODO()
  std = &Student {
    Id:   1991,
    Name: "cheng",
  }
}

// All methods that begin with "Test" are run as tests within a suite.
func (s *ExploreWorldSuite) TestSayHello() {
  s.hw.On("Say", mock.Anything, "World").Return(nil).Once()

  // FirstTimeMeet calls Say method from HelloWorld interface
  got := s.std.FirstTimeMeet(s.ctx)

  // There are lots more helper methods, use properly
  s.Require().True(got)
  // This can be helpful if the calling func does not have return or obvious
  // side effect
  s.hw.AssertExpectations(s.T())
}

// In order for 'go test' to run this suite, we need to create a normal test
// function and pass our suite to suite.Run
func TestExploreWorldSuite(t *testing.T) {
  suite.Run(t, new(ExploreWorldSuite))
}
```

The commonly used assertions:
```go
s.Require().Nil(err)
s.Require().NotNil(err)
// although not recommended to rely on error message
s.Require().Contains(err.Error(), "xxxxxx")
s.Require().EqualError(err, "xxxxxx")

s.Require().Len(rcs, 5)
s.Require().EqualValues(pt, "xxxxxx")
```

# Run Test
To run  suite, using the same `go test` command:
```bash
# To run the whole package test suites
go test -v -buildvcs=false -mod=readonly \
example.com/student

# To run specific suite in package
go test -v -buildvcs=false -mod=readonly \
example.com/student \
-run <suite struct name regexp>

# to run test against the vendor folder
# -mod=vendor
go test -v -buildvcs=false -mod=vendor \
example.com/student \
-run <suite struct name regexp>

# to run specified tests in specific suite
# -run and -testify.m have to be after package
go test -v -buildvcs=false -mod=readonly \
example.com/student \
-run <suite struct name regexp> \
-testify.m <test name regexp>

# to run test without prior cache
# -count 1
go test -v -buildvcs=false -mod=readonly \
-count 1 \
example.com/student \
-run <suite struct name regexp> \
-testify.m <test name regexp>
```
]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo Quick Start</title>
    <url>/2019/01/27/hexo-setup/</url>
    <content><![CDATA[
[x] 另一个静态文件转换工具[jekyll](https://jekyllrb.com/), you can use AWS s3 to
host static website or use git page as well.

Several weeks ago I decided to summarize and post what I have learned to online
Blog platform, I struggled several days to choose which platform is better.
Finally I think create my own blog and host it somewhere is a good way to go.

# Hexo and GitHub Pages
First understand what is **GitHub Pages** and **Hexo**.

**[GitHub Pages](https://docs.github.com/en/pages/quickstart)**  is a website
for you and your project that hosted directly from your GitHub repository.
There is no Database to setup and no server to configure, you even don't need
to know HTML, CSS and other web development toolkits. You just follow the
basic git add, commit and push operations and it will automatically build and
deploy your blog site for you.

> NOTE: If by any reason you recreate the github page site, don't forget to
set up the `Pages`'s publish branch the same name as it in hexo `_config.yml`
file, for example, I am using `published` branch.

 **[Hexo](https://hexo.io/)** is a open source blog framework that transfers
 plain text files into static website with beautiful theme. You associate it
 with your GitHub Pages repository, then use
 [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)
 write posts and deploy. Hexo provides basic features for manage the blog site,
 such as *categories*, *tag*, *profile*, etc. There are a lot of extensions and
 customizations for Hexo, more details please refer it's website.

There is a Chinese version explanation, refer this
[link](https://www.yuque.com/skyrin/coding/tm8yf5).

# Set up GitHub Pages
Head over to [GitHub](https://github.com/) and
[create a new repository](https://github.com/new) named *username*.github.io,
where *username* is your username (or organization name) on GitHub.

If the first part of the repository doesn’t exactly match your username, it
won’t work, so make sure to get it right.
![](https://drive.google.com/uc?id=18TZ7ee5zWHLS_aaP-pLC6BrevrAaRpA8)


# Set up Hexo
You need first install
[Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git) and
[Node.js](https://nodejs.org/en/), at the time build this blog I use Node
version `10.15.2`, download and install it:
![](https://drive.google.com/uc?id=1IWR24a6-IJirz1o_f9r7LmceSFUdSoS3)

> CAUTION: Please don't use latest nodejs! Otherwise the `hexo deploy` will fail
with odd issue, I stick to nodejs `v12.22.3`, the pkg installer for Max can be
downloaded [here](https://nodejs.org/en/blog/release/v12.22.3/), or googling
the keyword to find desired pkg.

> NOTE: To uninstall nodejs completely from Max that installed by pkg, see this
[reference](https://stackoverflow.com/a/29409357/5282011). After execution,
please also remove `hexo` softlinks from `/usr/local/bin` if needed.

> NOTE: For Mac users, you may encounter some other problems, please refer this
[link](https://hexo.io/docs/index.html). 

Once all requirements are met, install Hexo by running:
```bash
# If upgrade, using the same command.
sudo npm install -g hexo-cli
```

> NOTE: For Mac users, you need to
[enable root user privilege](https://support.apple.com/en-us/HT204012) first.

The installation process may generate some warnings even errors, do a sanity
check to see if it is good, for example:
```bash
hexo -v

hexo-cli: 4.3.0
os: darwin 19.6.0 10.15.7

node: 12.13.1
v8: 7.7.299.13-node.16
uv: 1.33.1
...
```

# Initializing Blog
```bash
cd ~/Desktop/
## create a directory called `chengdol.blog`
hexo init chengdol.blog
```
Go to `chengdol.blog`:
```bash
# boot local hexo server
hexo server

INFO  Start processing
INFO  Hexo is running at http://localhost:4000 . Press Ctrl+C to stop.
```

If you see this webpage, congratulations! you now have a workable blog with
default setting:
![](https://drive.google.com/uc?id=15FS4S4Lfi33giFDmf7NzeWMTq4N8p65M)

# Customizing Blog
Let's open the `chengdol.blog` directory
![](https://drive.google.com/uc?id=1UkhbU2jxwuw6Kd14YDPcBCc80kSXL5Jr)

The Hexo offical [documentation](https://hexo.io/docs/) contains lots of
configuration setting you can follow, Here I just go through what I have applied.

2021-01-03, 今天添加了一个生成过去规定天数改动或新增博客列表的脚本，这个脚本会自动生成一个置顶
的blog，这样就可以随时复习之前一段时间内改动的内容了。
```bash
# The flag --local generates localhost link which is used for 'hexo server'
# command. To generate link for deployment, don't use --local.
./review_list_generator.sh [--local]
```

# Backup
有时候需要在不同的电脑上写作(Don't forget to set Github SSH public key)，这样同步和备份
就不太方便。可以仅仅将最重要的文章部分备份下来，也就是`source`文件夹下的内容，创建一个private
git repo即可。在top-level的gitignore file中选择忽略:
```bash
.source/.git
```

这样一来就可以在多台电脑上写作并同步了:
```bash
# Deploy on github.
hexo clean && hexo g
hexo deploy
```

# Hexo Plugins
https://hexo.io/plugins/

## Search
由于写的Blog越来越多了，查找的需求就来了，刚好Hexo有search plugin, 就用上了。

This will generate local search DB and create a new search UI in webpage
https://github.com/wzpan/hexo-generator-search

```yaml
search:
  path: search.xml
  field: post
  content: true
  template: ./search.xml
```
不需要对Next theme的_config.yml进行改动，初次安装后需要run `hexo g`去生成数据库。部署的操作不变。

## Markdown Rendering
The defaul markdown renderer is `hexo-renderer-marked`, I have migrated it to
[`hexo-renderer-markdown-it`](https://github.com/hexojs/hexo-renderer-markdown-it/)
for better control and less risk. You need to remove the old renderer and
install new one manually, from its README:
```bash
# First go to the blog root directory.
# Uninstall old.
npm un hexo-renderer-marked --save
# Install new.
npm i hexo-renderer-markdown-it --save
```

The configuration is in root `_config.yml` file:
```yaml
markdown:
  preset: 'default'
  render:
    html: true
    xhtmlOut: false
    langPrefix: 'language-'
    # To follow the 80 columns wide rule, we need to disable the
    # line break rendering.
    breaks: false
    linkify: true
    typographer: true
    quotes: '“”‘’'
```


## Mermaid Diagram
This [Mermaid plugin](https://github.com/webappdevelp/hexo-filter-mermaid-diagrams)
is for Hexo(not for VSCode), install by running:
```bash
# Go to blog root folder first.
npm install --save hexo-filter-mermaid-diagrams
```

After installation, in Hexo `_config.yml` file, append:
```yaml
# mermaid chart
mermaid: ## mermaid url https://github.com/knsv/mermaid
  enable: true  # default true
  version: "7.1.2" # default v7.1.2
  options:  # find more api options from https://github.com/knsv/mermaid/blob/master/src/mermaidAPI.js
    #startOnload: true  // default true
```

Additionally, appends below snippet at `Next` theme `layout/_partials/footer.swig` file.
```js
{% if theme.mermaid.enable %}
  <script src='https://unpkg.com/mermaid@{{ theme.mermaid.version }}/dist/mermaid.min.js'></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({{ JSON.stringify(theme.mermaid.options) }});
    }
  </script>
{% endif %}
```

> NOTE: the swig file location is subject to change for different Next versions.
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>gitpage</tag>
      </tags>
  </entry>
  <entry>
    <title>Go Unittest</title>
    <url>/2023/02/12/golang-unittest/</url>
    <content><![CDATA[
> For how to use testify packages for complex testing, please see
`<<Go Unittest by testify>>`.

Golang has provided built-in testing framework composed of `go test` command and
`testing` package, here is simple example:

https://go.dev/doc/code#Testing

# Table Driven
In go test we usually use `table-driven` style, where test inputs and expected
outputs are listed in a struct and a loop to walk through them, for example:
```go
cases := []struct {
    in, want string
}{
    {"Hello, world", "dlrow ,olleH"},
    {"Hello, 世界", "界世 ,olleH"},
    {"", ""},
}
```

# Folder for Test Data
For large test data that needs to be read from outside, the go tool will ignore
a directory named `testdata`, making it available to hold ancillary data needed
by the tests.

# TestMain
If the test code contains a function:
```go
func TestMain(m *testing.M) {


  // call at end
  m.Run()
  // previously requires the os.Exist
  // https://github.com/golang/go/issues/34129#issuecomment-537598931
}
```
that function will be called instead of running the tests directly, this may
come in handy when you need to do some global set-up/tear-down for your tests,
see this [example](https://medium.com/goingogo/why-use-testmain-for-testing-in-go-dafb52b406bc).


# Run Test
To run test for specific packge:
```bash
# ./... means all packages in this directory (and subdirectories)
go list ./...

# For example the output would be:
example.com/fruit
example.com/world
example.com/zoo

# If you want to run specific tests, such as: "TestHelloWorld", "TestExitWorld"
# in example.com/world package:

# -race: for multi-thread program testing, may not need it
go test -buildvcs=false \
-mod=readonly \
-v  -race\
-run "HelloWorld|ExitWorld" \
example.com/world

# To test all packages in the current or sub directories
go test -buildvcs=false \
-mod=readonly \
-v -race \
./...
```

If you have build `tags` such as "integration", "mock" in your test or dependency files for example:
```go
// +build integration
// +build mock
```
To include these taged test files in testing:
```bash
go test -buildvcs=false \
-mod=readonly \
-v -race \
-tags "integration mock" \  # use space to separate tags
-run "HelloWorld|ExitWorld" \
example.com/world
```

If you don't specify the correct `-tags` then you will get error like:
```
imports example.com/common.git/cassandra/setup:
build constraints exclude all Go files in
/usr/local/xxx/example.com/gcloud/cs-common-gc.git/cassandra/setup
```

The `-buildvcs`, `-mod`, `-race`, `-tags` are build tags from `go help build`.
There is logic to combine and use multiple tags, please see
[Build Tag Boolean Logic](https://www.digitalocean.com/community/tutorials/customizing-go-binaries-with-build-tags#build-tag-boolean-logic).

> NOTE that the go test will always run the test files that don't have any build
tag irrespective of `-tags` specified or not.

Other test tags please see `go help testflag`.

# Test Coverage
For simplified coverage display only, just use `-cover` flag:
```bash
go test -cover ./...
```

To generate coverage profile for detailed bad coverage analysis:
```bash
# Generate simple coverage.out profile
# -covermode=set by default
go test -coverprofile=coverage.out ./...
# Generate coverage with count for each statement.
# flag -race will use covermode=atomic
go test -covermode=count -coverprofile=coverage.out ./...

# Without -o, the output goes to stdout
# Convert to html and you can open via browser
go tool cover -html=coverage.out -o coverage.html
# Coverage for each function
go tool cover -func=coverage.out -o coverage.func
```

If you use `-covermode=count` and then go tool the html will show you the 
intensity of the statements calling count, hover the mouse over the line to see
the actual counts.

More details about `cover` please see this go blog
[<<`The cover story`>>](https://go.dev/blog/cover).

]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>Go Version Management</title>
    <url>/2023/02/10/golang-version-manage/</url>
    <content><![CDATA[
> There is a better alternative, please see `<<VSC Developing inside a Container>>`.

This post talks about how to install multiple go binaries in the same machine
and switch back and forth easily.

You may need a primary go version for daily work. Download and install go for
different operating system from here:
- https://go.dev/doc/install

The instruction will put the download to `/usr/local/go`, for multi-version
install below, they are downloaded to different folder so there is no conflict.

If you need to install other go versions in the same machine:
- https://go.dev/doc/manage-install

First, there are 2 go env variables involved:
- `GOROOT` is for compiler/tools.
- `GOPATH` is for your own go projects and 3rd party libraries
(downloaded with `go get/install`).

You can check their value by:
```bash
go env GOROOT
go env GOPATH
```

The `go install` location is controlled by the `GOBIN` and `GOPATH` environment
variables depends their existence, in the order:

* If `GOBIN` is set, binary is installed at that directory. 
* If `GOPATH` is set, binary is installed at the `bin` subdirectory of the
first path in the `GOPATH` list, otherwise binary is installed to the
bin subdirectory of the default `GOPATH` (`$HOME/go`).

```bash
go install golang.org/dl/go1.10.7@latest
```

Go to `$GOPATH/bin` to find the downloaded object and run:
```bash
go1.10.7 download
```

From output, `1.10.7` version go binary will be downloaded to `$GOPATH/../sdk`
folder.

To use the downloaded version, we can update the `PATH` environment variable:
```bash
export PATH=/usr/local/google/home/<user>/sdk/go1.10.7/bin:$PATH
```

> NOTE that if there are many go versions, must set the target go bin path at
the beginning of `PATH`, the `GOROOT` will be automatically set according to
first go bin in the `PATH`.

Then verifying by running:
```bash
# go version
go version go1.10.7 linux/amd64

# go env GOROOT is updated changed accordingly to 1.10.7 bin path
/usr/local/google/home/<user>/sdk/go1.10.7
```

To recover original setting, just revert the `PATH` environment variable.
]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>Consul Quick Start</title>
    <url>/2020/06/13/infra-consul/</url>
    <content><![CDATA[
# Lab Environment Setup
Consul is easy to install, just a executable binary, put it in `/usr/local/bin`:
https://www.consul.io/downloads

我修改了一下课程的demo，做了一个consul lab cluster via Vagrant:
https://github.com/chengdol/InfraTree/tree/master/vagrant-consul

Glossary:
https://github.com/chengdol/InfraTree/blob/master/vagrant-consul/glossary.md


# Introduction
Challenges in managing services:
- Service discovery
- Failure Detection
- Mutli-Data center
- Service configuration

一个应用服务架构中，一般有API tier增加灵活性，同时提供额外的服务，比如以下应用就可以直接拿来API使用:
- [Mailgun](https://www.mailgun.com/homepage/)
- [Stripe](https://stripe.com/)
- [Loggly](https://www.loggly.com/)
- [Datadog](https://www.datadoghq.com/)

Consul is `distributed`.

These services need to be discovered by each other. 对于越来越复杂的内部组织结构，比如很多internal load balancer, Consul can come and play, 比如提供内部的DNS服务, Service discovery.

Failure Dectection, Consul running lightweight Consul agent (server or client mode) on each of node in your environment. The agent will diagnose all services running locally.

`Reacting configuration` via key/value store, reflecting changes quickly in near real time. 
Multi-Data center aware.

Consul vs Other softwares, see [here](https://www.consul.io/intro/vs).
Especailly Consul vs Istio, see [here](https://www.consul.io/intro/vs/istio).

Consul UI online demo:
https://demo.consul.io

# Monitor Nodes
在这一章的例子中提供了一个很好的建模思路！在vagrant virutal machine中安装docker，然后用container的方式运行一些服务(比如这里的Nginx web and HAProxy LB)，再expose(localhost)这些端口(对machine iptables做了更改)，这样就避免了很多的virtual machine上的安装配置工作。

Start consul server agent:
```bash
# -dev: development agent, server mode will be turned on this agent, for quick start
# in production, don't use -dev

# -advertise: specify one ipv4 interface
# -client: specify client access ip, usually 0.0.0.0
consul agent -dev -bind 0.0.0.0 -advertise 172.20.20.31 -client 127.0.0.1

# log output
==> Starting Consul agent...
           Version: 'v1.8.0'
           Node ID: '95b60a36-f350-8a2b-b1cb-54f7b79657dc'
         Node name: 'consul-server'
        Datacenter: 'dc1' (Segment: '<all>')
            Server: true (Bootstrap: false)
       Client Addr: [127.0.0.1] (HTTP: 8500, HTTPS: -1, gRPC: 8502, DNS: 8600)
      Cluster Addr: 172.20.20.31 (LAN: 8301, WAN: 8302)
           Encrypt: Gossip: false, TLS-Outgoing: false, TLS-Incoming: false, Auto-Encrypt-TLS: false

==> Log data will now stream in as it occurs:
    ...
    2020-06-19T23:59:25.729Z [INFO]  agent.server: New leader elected: payload=consul-server
    ...
```

[ ] 我改动了一下Vagrantfile， 我估计是routing table出了问题，在MacOS host上无法访问private network中的virtual machine via private IP:
https://stackoverflow.com/questions/23497855/unable-to-connect-to-vagrant-private-network-from-host

于是我增加了一个VM `ui` 去显示consul 的UI with port forwarding, but still does not work, from the log the port 8500 is bound with 127.0.0.1:
```bash
  Client Addr: [127.0.0.1] (HTTP: 8500, HTTPS: -1, gRPC: -1, DNS: 8600)
  Cluster Addr: 172.20.20.41 (LAN: 8301, WAN: 8302)
```
首先，我想到了更改Client Addr 为 172.20.20.41，因为这是我在Vagrantfile中设置的private IP:
```bash
consul agent -config-file /vagrant/ui.consul.json -advertise 172.20.20.41 -client 172.20.20.41
```
但还是不行, 主机上`localhost:8500` 无法连接，当然为了确认`-client` flag的使用的正确性，用netstat查看一下是否端口在改interface上。后来我就想到应该是iptables的问题了，没有这个interface上的流量forward出去，那就改成`0.0.0.0`好了(specify "any IPv4 address at all"):
```bash
# /vagrant/ui.consul.json set ui is true
consul agent -config-file /vagrant/ui.consul.json -advertise 172.20.20.41 -client 0.0.0.0

# output
==> Starting Consul agent...
           Version: 'v1.8.0'
           Node ID: '10ccbe63-bef0-3cf6-b24b-e0a53bdef213'
         Node name: 'ui'
        Datacenter: 'dc1' (Segment: '')
            Server: false (Bootstrap: false)
       Client Addr: [0.0.0.0] (HTTP: 8500, HTTPS: -1, gRPC: -1, DNS: 8600)
      Cluster Addr: 172.20.20.41 (LAN: 8301, WAN: 8302)
           Encrypt: Gossip: false, TLS-Outgoing: false, TLS-Incoming: false, Auto-Encrypt-TLS: false

==> Log data will now stream in as it occurs:
...
==> Consul agent running!
...
    2020-06-20T03:38:12.476Z [INFO]  agent: (LAN) joining: lan_addresses=[172.20.20.31]
    2020-06-20T03:38:12.477Z [WARN]  agent.client.manager: No servers available
    2020-06-20T03:38:12.477Z [ERROR] agent.anti_entropy: failed to sync remote state: error="No known Consul servers"
    2020-06-20T03:38:12.480Z [INFO]  agent.client.serf.lan: serf: EventMemberJoin: consul-server 172.20.20.31
    2020-06-20T03:38:12.480Z [INFO]  agent: (LAN) joined: number_of_nodes=1
...
```
或者可以在config json中定义`client_addr`:
```json
{
  "retry_join": ["172.20.20.31"],
  "data_dir": "/tmp/consul",
  "client_addr": "0.0.0.0"
}
```

虽然通过的ui virtual machine暴露的web，但是所有信息都来自consul server! 和k8s nodeport的模式类似。
Can access via HTTP API: https://www.consul.io/api-docs
```bash
http://localhost:8500/v1/catalog/nodes
# format readable
http://localhost:8500/v1/catalog/nodes?pretty
```

DNS query, go to `ui` node, when we run consul agent, the DNS port is 8600:
```bash
# query node
dig @localhost -p 8600 consul-server.node.consul
# query service
dig @localhost -p 8600 consul.service.consul
# query service record, will show you the server port, such as 8300
dig @localhost -p 8600 consul.service.consul SRV
```

The RPC Protocol is deprecated and support was removed in Consul `0.8`. Please use the `HTTP API`, which has support for all features of the RPC Protocol.

## Consul Commands
这里提到了2个有用的commands, 本来是用RPC实现的，但现在改了:
```bash
# can specify target point
# provide debug info
consul info [-http-addr=172.20.20.31:8500]
# get log message, 这样就可以在某一agent上查看任意其他的agent log了
consul monitor [-http-addr=172.20.20.31:8500]
```
Here `172.20.20.31` is consul server, you must start it by `-client 0.0.0.0`, otherwise the port is bound with loopback interface and cannot access.

Other commands:
```bash
# maintain node
# enable maintaince, service will not show in consul DNS
# -service: maintain for a specific service
consul maint -enable -reason "Because..."
consul maint
consul maint -disable

# validate config file
# the config file must complete! cannot separate to several parts!
consul validate [config file]

# show members
consul members

# similar to docker/k8s exec
consul exec uptime
```
Note that `consul exec` is by default disabled:
https://www.consul.io/docs/agent/options.html#disable_remote_exec
这个命令挺危险，就相当于ssh到node执行command line. 比如在node上用的docker container提供服务，则可以exec到node `docker stop xxx`.

BTY, gracefully exit the consul process will not cause warning or error in UI display. If you force kill it, the node will be marked as critical.


# Service Discovery
One way to register service to consul is use **Service definition:**
https://www.consul.io/docs/agent/services
比如register LB service to consul，这样的好处就是前面提到了，consul会根据其他agent反馈的web nginx的情况及时修改HAProxy的config信息，更新配置, 接下来会看到:

Regsiter service does not mean the service is healthy, also need to do healthy check:
For example:
```json
{
  "service": {
    "name": "web",
    "port": 8080,
    "check": {
      "http": "http://localhost:8080",
      "interval": "10s"
    }
  }
}
```

Then launch consul client agent, add one more service config file `web.service.json` for registration, for example, in `web1` node:
```js
consul agent -config-file /vagrant/common.json \
             -advertise 172.20.20.21 \
             -config-file /vagrant/web.service.json
```

Then check the consul UI, you will see the node is good but service is unhealthy because now there is no nginx running, so create nginx in `web1` node:
```bash
/vagrant/setup.web.sh
```

Then refresh the web page, everything is good.

You can `dig` the web service from `ui` node, this is so called **internal** `service discovery`, not facing public. 这些数据对于LB来说可以用来direct traffic, 这就是Consul自带DNS的好处，没有什么额外的设置了，并且还提供了health check，就非常方便了. 并且public facing LB也在Consul中注册了，这样一旦LB goes down，就能被马上监测到。
```bash
dig @localhost -p 8600 web.service.consul SRV
# you will see exactly the number of web service running
```

Except query DNS from `dig`, consul HTTP API also can do it:
```bash
# services list
curl http://localhost:8500/v1/catalog/services?pretty
# service web detail
curl http://localhost:8500/v1/catalog/service/web?pretty
# health check
# see the Status field: passing or critical
curl http://localhost:8500/v1/health/service/web?pretty
```

前面用到了service definition去register service，这只是一种方法，还可以用HTTP API 注册.
这里还有一些自动注册的工具:
https://www.consul.io/downloads_tools
- [docker container registrator](https://github.com/gliderlabs/registrator)
- consul aware app: using HTTP API


# LB Dynamic Config
[HAProxy](http://www.haproxy.org/): The Reliable, High Performance TCP/HTTP Load Balancer.
HAProxy config file `haproxy.cfg` example:
```js
global
    maxconn 4096

defaults
    mode http
    timeout connect 5s
    timeout client 50s
    timeout server 50s

listen http-in
    bind *:80
    server web1 172.20.20.21:8080
    server web2 172.20.20.22:8080
```
`8080` port is where nginx web service from, `bind *:80` is meant to expose port for health check, 意思是外界通过LB上的`80` 端口访问后台web servers, 这也就是为啥consul中LB的health check输出 居然是`welcome to Nginx!`，因为那是后台返回的页面. 

In the demo, we run HAProxy container in `lb` machine. How to verify it is up and running?
In any machine:
```bash
dig @localhost -p 8600 lb.service.consul SRV
# the lb record will show
```

Now let's verify LB is actually working:
```bash
# try several times, LB will cycling through backend servers
# you will see different ip returned
curl http://localhost/ip.html
```

如果这时关掉一个web server，在HAProxy没有enable health check功能的情况下，仍然会把请求发往已经挂掉的server，则用户得到503 error. 这也是很多LB的问题，需要设置自身的health check。但如果用consul的DNS，由于各个server的health check已经集成进去了，consul会返回健康的server进行服务. So we can feed information to LB from consul dynamically.

## Consul Template
Consul template is go template format:
https://github.com/hashicorp/consul-template
这个不仅仅用于config LB, any application with config file can utilize this tool!

**Workflow:**
consul template will listen changes from consul, as changes occur it will be pushed to the consul template daemon (run in `lb` machine). consul template daemon will generate HAProxy new config file from a template for HAProxy, then we tell docker to restart HAProxy (or HAProxy reload config).

This is the `haproxy.ctmpl` file
```s
global
    maxconn 4096

defaults
    mode http
    timeout connect 5s
    timeout client 50s
    timeout server 50s

listen http-in
    bind *:80{{range service "web"}}
    server {{.Node}} {{.Address}}:{{.Port}}{{end}}

    stats enable
    stats uri /haproxy
    stats refresh 5s
```

This part means in the web display HAProxy statistic report! 这个统计图挺直观的，但我这里由于route原因看不到, access from `http://<Load balancer IP>/haproxy`:
```s
    stats enable
    stats uri /haproxy
    stats refresh 5s
```

Next, install consul-template in `lb` machine, run some tests with template file:
```bash
# dry run
consul-template -template /vagrant/provision/haproxy.ctmpl -dry
```
At meanwhile, go to `web1` machine, run `docker stop/start web`, you will see the real time updates in output from consul-template command above.

Then, create consul-template template file `lb.consul-template.hcl`, used to tell consul-template how to do its job.
```bash
consul-template -config /vagrant/provision/lb.consul-template.hcl
# you will see the haproxy.cfg is replaced by new one
```
Then we can provision the daemon run in background in `lb` machine:
```bash
(consul-template -config /vagrant/provision/lb.consul-template.hcl >/dev/null 2>&1)&
```

Open the consul UI, in terminal go to `web1` or `web2` machine, stop/start the docker, see the updates. Also in `lb` machine, run below command to see the LB still works good, it will not return the unhealthy server to you:
```bash
curl http://localhost/ip.html
```

## Other tools
- [Envconsul](https://github.com/hashicorp/envconsul)
  Envconsul provides a convenient way to launch a subprocess with environment variables populated from HashiCorp Consul and Vault.
  前面提到了config file for process, here Envconsul set env variables for process and kick off for us.

- [confd](https://github.com/kelseyhightower/confd)
  confd is a lightweight configuration management tool

- [fabio](https://github.com/fabiolb/fabio)
  fabio is a fast, modern, zero-conf load balancing HTTP(S) and TCP router for deploying applications managed by consul


# Reactive Configuration
One of primary use case is to update app configuration. for example, when services changes inject the changes to consul key/value pairs and have it pushed into our application.

注意key/value不要用来当Database, it's not intended for! 但是运作的方式几乎和`etcd`一样！
https://etcd.io/

Go to Consul UI to add key/value pairs, create a folder path `/prod/portal/haproxy`, then create key/value pair in it:
```s
maxconn 2048
stats enable
timeout-client 50s
timeout-connect 5s
timeout-server 50s
```

SSH to `ui` node, let's read the key/value stored:
```bash
# list all pairs
curl http://localhost:8500/v1/kv/?recurse'&'pretty

# add key/value via HTTP API
# /prod/portal/haproxy is path we created before
curl -X PUT -d '50s' http://localhost:8500/v1/kv/prod/portal/haproxy/timeout-server
# delete
curl -X DELETE http://localhost:8500/v1/kv/prod/portal/haproxy/timeout-server
# get one
curl -X GET http://localhost:8500/v1/kv/prod/portal/haproxy/timeout-server?pretty
curl -X GET http://localhost:8500/v1/kv/prod/portal/haproxy/timeout-server?raw
```
The API will return JSON data, you can use `jq` to parse it.

Update the LB config template `haproxy.ctmpl` as:
```s
global
    maxconn {{key "prod/portal/haproxy/maxconn"}}

defaults
    mode http
    timeout connect {{key "prod/portal/haproxy/timeout-connect"}}
    timeout client {{key "prod/portal/haproxy/timeout-client"}}
    timeout server {{key "prod/portal/haproxy/timeout-server"}}

listen http-in
    bind *:80{{range service "web"}}
    server {{.Node}} {{.Address}}:{{.Port}}{{end}}

    stats {{key "prod/portal/haproxy/stats"}}
    stats uri /haproxy
    stats refresh 5s
```
Then make consul-template process reload without killing it:
```bash
# HUP signal will make consul-tempalte reload
killall -HUP consul-template
```
Then you will see the `haproxy.cfg` file is regenerated!

来谈谈为什么这个key/value setting如此重要:
有时候实现并不知道具体设置参数，在production环境，你可能想real time更新参数，比如这里LB中`maxconn`，实际使用中可能由于machine CPU, memory等因素，不得不调小，你可以用consul maint或其他方式去调节, but that would be a pain and the change will take time to converge across the infrastructure.

Use Key/Value store is really a reactive confiuration!

## Blocking query
https://www.consul.io/api-docs/features/blocking

A blocking query is used to wait for a potential change using `long polling`. Not all endpoints support blocking, but each endpoint uniquely documents its support for blocking queries in the documentation.

Endpoints that support blocking queries return an HTTP header named `X-Consul-Index`. This is a unique identifier representing the current state of the requested resource.

Use `curl -v` to check HEADER info to see if it has `X-Consul-Index`.

这个功能可以用在比如自己的app long polling consul API, 去等待changes happen, reactive listen to changes of consul. 这比周期性的探测节省很多资源。for example:
```bash
curl -v http://localhost:8500/v1/kv/prod/portal/haproxy/stats?index=<X-Consul-Index value in header>'&'wait=40s
```
如果有change发生，每次`X-Consul-Index` value 都会变化.

# Health Check
Gossip pool via `Serf` and Edge triggered updates, peer to peer.
Serf: https://www.serfdom.io/ (在UI中每个node都有Serf health status)

If you kill and start the consul agent in one node, you will see the log something like:
```bash
serf: EventMemberFailed ...
serf: EventMemberJoin ...
```

There are LAN gossip and WAN gossip.

Information disseminated:
- Membership (discovery, joining) - joining the cluster entails only knowing the address of one other node (not required to be a server)
- Failure detection - affords distributed health checks, no need for centralized health checking
- Event broadcast - i.e. leader elected, custom events

## System-Level Check
非常类似于K8s的 liveness probe.

https://www.consul.io/docs/agent/checks.html
One of the primary roles of the agent is management of `system-level` and `application-level` health checks. A health check is considered to be application-level if it is associated with a service. If not associated with a service, the check monitors the health of the entire node.

前面都是用到了service check, 这里增加check node status. For example, disk usage, memory usage, etc.

Update `common.json` config file, this config file will take effect on `lb` and `web` machines, 这部分配置在最近的新版本已经变化了:
```json
{
  "retry_join": [
    "172.20.20.31"
  ],
  "data_dir": "/tmp/consul",
  "client_addr": "0.0.0.0",
  "enable_script_checks": true,
  "checks": [
    {
      "id": "check_cpu_utilization",
      "name": "CPU Utilization",
      "args": ["/vagrant/provision/hc/cpu_utilization.sh"],
      "interval": "10s"
    },
    {
      "id": "check_mem_utilization",
      "name": "MEM Utilization",
      "args": ["/vagrant/provision/hc/mem_utilization.sh"],
      "interval": "10s"
    },
    {
      "id": "check_hdd_utilization",
      "name": "HDD Utilization",
      "args": ["/vagrant/provision/hc/hdd_utilization.sh"],
      "interval": "10s"
    }
  ]
}
```
Let's see the `mem_utilization.sh` file:
```bash
AVAILABLE_RAM=`grep MemAvailable /proc/meminfo | awk '{print $2}'`
TOTAL_RAM=`grep MemTotal /proc/meminfo | awk '{print $2}'`
RAM_UTILIZATION=$(echo "scale = 2; 100-$AVAILABLE_RAM/$TOTAL_RAM*100" | bc)
RAM_UTILIZATION=${RAM_UTILIZATION%.*}

echo "RAM: ${RAM_UTILIZATION}%, ${AVAILABLE_RAM} available of ${TOTAL_RAM} total "

if (( $RAM_UTILIZATION > 95 ));
then
    exit 2
fi

if (( $RAM_UTILIZATION > 70 ));
then
    exit 1
fi

exit 0
```

The system-level health check sections will be displayed in consul UI.
For stress test, install `stress` software in `web1` machine (in the demo code it is added):
```bash
# install
sudo apt-get install stress
```

CPU stress test, then you will see in the consul UI the node is unhealthy and is cycled out from LB:
```bash
stress -c 1
```
Watching the consul UI for `web1`, you will see CPU check failed:
```s
CPU: 100%
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
vagrant   3122 97.2  0.0   7312   100 pts/0    R+   21:26   0:14 stress -c 1
root       822  1.4 10.7 578220 53828 ?        Ssl  21:19   0:06 /usr/bin/docker daemon --raw-logs
vagrant   2121  0.6 11.1 785204 55636 ?        Sl   21:20   0:02 consul agent -config-file /vagrant/config/common.json -config-file /vagrant/config/web.service.json -advertise 172.20.20.21
vagrant   3099  0.2  1.1  23012  5724 pts/0    Ss   21:26   0:00 -bash
root         1  0.1  0.7  33604  3792 ?        Ss   21:19   0:00 /sbin/init
```

Once it recover, node will itself back into the pool. 这个功能非常有用，可以提前预警可能会发生问题的node. 比如某个web server overloaded，检测出unhealthy，则会被LB 移出，待恢复后又会自动加进去！









]]></content>
      <categories>
        <category>Infra</category>
      </categories>
      <tags>
        <tag>infra</tag>
        <tag>consul</tag>
      </tags>
  </entry>
  <entry>
    <title>Python gRPC</title>
    <url>/2022/11/21/grpc-python/</url>
    <content><![CDATA[
For revisit, go to check the Python
[Basic tutorial](https://grpc.io/docs/languages/python/basics/)
and [Sample Code](https://github.com/grpc/grpc/tree/master/examples/python).

The [gRPC](https://grpc.io/) is the open source version of Stubby from Google,
it uses [Protocol Buffer](https://developers.google.com/protocol-buffers/docs/overview)
as both its Interface Definition Language (IDL) and as its underlying message
interchange format.

gRPC clients and servers can run and talk to each other in a variety of
environments and can be written in **any** of gRPC’s supported languages. So,
for example, you can easily create a gRPC server in Java with clients in Go,
Python, or Ruby.

[Core Concepts](https://grpc.io/docs/what-is-grpc/core-concepts/) for gRPC. 
There are regular and stream request/response types, they can be combined in any
form, for example:
```proto
service Greeter {
  rpc SayHello(HelloRequest) returns (HelloResponse) {}
  // Response is using yield.
  rpc LotsOfReplies(HelloRequest) returns (stream HelloResponse) {}
  // Request is using iterator.
  rpc ZLotsOfGreetings(stream HelloRequest) returns (HelloResponse) {}
  rpc BidiHello(stream HelloRequest) returns (stream HelloResponse) {}
}
```

The comments in proto service definition will be translated to docstring in
generated code, for example:
```proto
// import definition.
// https://developers.google.com/protocol-buffers/docs/proto#importing
import "xxx/utility.proto";

// The greeting service definition.
service Greeter {
  // Sends a greeting.
  rpc SayHello (HelloRequest) returns (HelloReply) {}
  // Sends a greeting again.
  rpc SayHelloAgain (HelloRequest) returns (HelloReply) {}
}

// The request message containing the user's name.
message HelloRequest {
  string name = 1;
}
// The response message containing the greetings
message HelloReply {
  string message = 1;
}
```
As above shows, write message and service in .proto file.

[Basic tutorial](https://grpc.io/docs/languages/python/basics/) for Python, run
within the python virtualenv, for example:
```bash
virtualenv -p python3 grpc
# python venv manager if you have installed.
workon grpc
```

Required python modules for gRPC:
```bash
# Install gRPC.
python -m pip install grpcio

# Python’s gRPC tools include the protocol buffer compiler protoc and the
# special plugin for generating server and client code from .proto service
# definitions.
python -m pip install grpcio-tools
```

After finished the proto file definition:
```bash
# Command help
python -m grpc_tools.protoc --help

# This will generate xx_pb2.py and xx_pb2_grpc.py file.
# xx_pb2.py: Contains request and response message classes.
# xx_pb2_grpc.py: Contains client and server and utilities classes:
#  - class xxxStub(object)
#  - class xxxServicer(object)
#  - def add_xxxServicer_to_server(servicer, server)
# The relative path can be used here.
python -m grpc_tools.protoc \
--proto_path=<target proto file folder path> \
--python_out=<Generate Python source file> \
--pyi_out=<Generate Python pyi stub> \
--grpc_python_out=<Generate Python source file> \
<target proto file path>
```

When finishes the basic codelab,
[here](https://github.com/grpc/grpc/tree/master/examples/python)
are many different patterns and usages to create client-server applications.

> TIPS: Using Python `typing` to indicate the parameters type in service method
to make it clear.

> NOTE: In basic tutorial for asynchronous pattern, it uses Python `asyncio` as
implementation.

For gRPC authentication in Python, please see
[ALTS authentication](https://grpc.io/docs/languages/python/alts/).]]></content>
      <categories>
        <category>gRPC</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>grpc</tag>
      </tags>
  </entry>
  <entry>
    <title>Consul Mismatch Key</title>
    <url>/2023/06/27/infra-consul-mismatch-key/</url>
    <content><![CDATA[
I haven't dealt with infra for a while after the team shift. Today, I was
got a ticket and its problem solving process refreshed my memory about
some Consul operations, so I would like to document it here.

The initial issue was the DNS lookup service from our stack malfunction and it
did not return update-to-date IP of some VMs. As we use Consul as discovery
service in our distributed system, so the question came to it.

Check if the VM or service was registered to Consul:
```bash
# List the members of a Consul cluster
# Examine the Status(alive), Type(server/client), etc 
consul members

# List all services
consul catalog services
```

Neither VMs not service was there, went to check if the Consul daemon was good:
```bash
sudo systemctl status consul
```

Consul was running, but I observed some short error messages, to see full:
```bash
sudo journalctl -ex -u consul
```

The error was about Consul encrypt key mismatch:
```
error=
| 3 errors occurred:
|         * Failed to join 172.16.4.137:8301: No installed keys could decrypt the message
|         * Failed to join 172.16.4.153:8301: No installed keys could decrypt the message
|         * Failed to join 172.16.4.139:8301: No installed keys could decrypt the message
```

Checked the Consul `/etc/consul/config.json`(check your corresponding setup)
file and compared with the Consul server config, spotted that the
`enctypt` field did not match, to fix it:

1. Replaced the wrong key with the correct one.
2. Deleted the cached key file `/.../serf/local.keyring`, the root path please
check Consul config JSON file's `data_dir` field.

Then restarted the Consul service and problem got solved:
```bash
sudo systemctl restart consul
```

You can check the DNS again by `dig` with Consul as DNS server:
```bash
dig +short <consul service name>.service.consul @localhost -p 8600
```

Or if the VM `/etc/resolv.conf` was configured with Consul already:
```bash
nslookup <consul service name>
```
]]></content>
      <categories>
        <category>Infra</category>
      </categories>
      <tags>
        <tag>infra</tag>
        <tag>consul</tag>
      </tags>
  </entry>
  <entry>
    <title>HTTP Quick Start</title>
    <url>/2020/08/27/http-learn/</url>
    <content><![CDATA[
最近在做migrating Squid to Envoy的工作，其中涉及到了很多HTTP的内容。趁着这次机会深入学习一下，还有就是一些proxy的内容，已经单独拿出来总结了。

# Introduction
非常不错的Tutorial, 把各部分都讲得很详细:
[MDN web docs: HTTP](https://developer.mozilla.org/en-US/docs/Web/HTTP)

很快速的把基础部分过了一下:
[HTTP Crash Course & Exploration](https://www.youtube.com/watch?v=iYM2zFP3Zn0)
The typical HTTP format:
```bash
METHOD PATH PROTOCOL
HEADERS

BODY
```

HTTP status code:
- 1xx, informational
- 2xx, success
- 3xx, redirect
- 4xx, client error
- 5xx, server error

HTTP/2, faster and more efficient & secure, request and response multiplexing.


## Other Tools
1. [httpbin](https://github.com/postmanlabs/httpbin): A simple HTTP/HTTPS Request & Response Service.
2. [ip4.me](http://ip4.me/): check your public IPv4 address.
3. [noip.com](https://www.noip.com/), free hostame + domain <-> public IP mapping. 如果要配置这个hostname对应到router的public IP, 需要设置router把这个流量转移到自己的笔记本某个端口上。


# CONNECT Method
Connect主要是用在建立Tunnel. Tunneling can allow communication using a protocol that normally wouldn’t be supported on the restricted network. Tunnel 只是一个通道，里面可以支持一些传输协议, 并不是说tunnel 必须是ssl/tls. 举个例子，你通过一个forward proxy 访问一个服务器，使用HTTPS协议，假设Proxy是一个善良的中间人，它并不知道加密后的流量内容是什么，就不可能像HTTP一样去窥探，拆解packet，于是client会发送一个CONNECT HTTP请求，设立一个Tunnel经过proxy和server进行通信。


-->> [When should one use CONNECT](https://stackoverflow.com/questions/11697943/when-should-one-use-connect-and-get-http-methods-at-http-proxy-server)
With SSL(HTTPS), only the two remote end-points understand the requests, and the proxy cannot decipher them. Hence, all it does is open that tunnel using CONNECT, and lets the two end-points (webserver and client) talk to each other directly.

-->> [MDN web docs: CONNECT](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/CONNECT)
Some proxy servers might need authority to create a tunnel. See also the `Proxy-Authorization` header.

For example, the CONNECT method can be used to access websites that use SSL (HTTPS). The client asks an HTTP Proxy server to `tunnel` the TCP connection to the desired destination. The server then proceeds to make the connection on behalf of the client. Once the connection has been established by the server, the Proxy server continues to proxy the TCP stream to and from the client.


这篇文章很不错:
-->> [MDN web docs: proxy servers and tunneling](https://developer.mozilla.org/en-US/docs/Web/HTTP/Proxy_servers_and_tunneling)
There are two types of proxies: forward proxies (or tunnel, or gateway) and reverse proxies (used to control and protect access to a server for load-balancing, authentication, decryption or caching).

Forward proxies can hide the identities of clients whereas reverse proxies can hide the identities of servers.

The HTTP protocol specifies a request method called CONNECT. It starts two-way communications with the requested resource and can be used to open a `tunnel`. This is how a client behind an HTTP proxy can access websites using SSL (i.e. HTTPS, port 443). Note, however, that not all proxy servers support the CONNECT method or limit it to port 443 only.


# Basic Authorization
这里提一下`authz` and `authn`的区别:
- `authz`: authorization，授权, what are allowed to do.
- `authn`: authentication, 鉴权, who you are.

这里是讲了HTTP 基本的`authz`操作.
https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication
HTTP provides a general framework for access control and authentication. This page is an introduction to the HTTP framework for authentication, and shows how to restrict access to your server using the HTTP "Basic" schema.

The Basic authz is not secure, send in plain text, although base64. can be decode for example:
```bash
echo <base64 string> | base64 --decode
```
But if over https, the traffic is encrypted. You can demonstrate it in wireshark locally.
[Is BASIC-Auth secure if done over HTTPS?](https://security.stackexchange.com/questions/988/is-basic-auth-secure-if-done-over-https)




]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title>Packer Quick Start</title>
    <url>/2020/07/07/infra-packer/</url>
    <content><![CDATA[
[x] template [user variable](https://www.packer.io/docs/templates/user-variables.html)
[x] builder [google cloud](https://www.packer.io/docs/builders/googlecompute), authentication, etc.
[x] provisioner [ansible](https://www.packer.io/docs/provisioners/ansible)

Build Images for cloud and on-premise, Packer template is `JSON` format (easily source control).
- variables
- builders: can have multiple builders run parallelly.
- provisioners: run in order, need `only` to specify where to run.
- post-processors: auto post-build tasks, eg: compression.

A machine image is a single static unit that contains a pre-configured operating system and installed software which is used to quickly create new running machines. Machine image formats change for each platform. Some examples include AMIs for EC2, VMDK/VMX files for VMware, OVF exports for VirtualBox, etc.

`-debug` flag in build can help run steps one by one, parallel build in debug mode is running sequentially.

To build Ubuntu VirtualBox image VOF, ISO is download from Ubuntu web site then Packer will launch it to run provisioner. Then use post-processor to compress VOF to tar.gz, or convert to Vagrant box.

What are the differences between Packer and Docker?
- [How docker and packer are different](https://stackoverflow.com/questions/47169353/how-are-docker-and-packer-different-and-which-one-should-i-prefer-when-provisio)

Transition from Packer to Docker is easy, but the docker builder may not efficient as the docker native tool.


Example of packer json file, use ansibe as provisioner on google cloud:
```json
{
  "variables": {
    "project_id": "xxxxxx",
    "source_image": "xxxxxx",
    "subnetwork": "xxxxxx",
    "zone": "xxxxxx",
    "image_name": "xxxxxx"
  },
  "builders": [
    {
      "type": "googlecompute",
      "project_id": "{{user `project_id`}}",
      "source_image": "{{user `source_image`}}",
      "subnetwork": "{{user `subnetwork`}}",
      "ssh_username": "xxxxxx",
      "zone": "{{user `zone`}}",
      "use_internal_ip": true,
      "omit_external_ip": true,
      "image_description": "xxxxxx",
      "image_name": "{{user `image_name`}}"
    }
  ],
  "provisioners": [
    {
      "type": "ansible",
      "max_retries": 0,
      "pause_before": "5s",
      "playbook_file": "setup.yml",
      // acts on target instance
      "extra_arguments": ["--become", "-e ansible_python_interpreter=/usr/bin/python3", "-v"],
      "user": "xxxxxx"
    }
  ]
}
```

Some useful commands:
```bash
# illustrate packer.json file
packer inspect

# validate packer.json syntax
packer validate <file.json>

# build image
# -debug: pause for each step, clear
packer build [-debug] <file.json>
# -force: delete existing artifact then build
packer build -force [-debug] <file.json>
```

When using `-debug` flag, Packer will show you the private pem file in current directory, you can use that pem file ssh to running VM, for example, in google cloud:
```bash
# jenkins is the ssh_username you set in template
ssh -i gce_googlecompute.pem jenkins@172.16.160.49
```


]]></content>
      <categories>
        <category>Infra</category>
      </categories>
      <tags>
        <tag>infra</tag>
        <tag>packer</tag>
      </tags>
  </entry>
  <entry>
    <title>Terraform Deep Dive</title>
    <url>/2021/01/04/infra-terraform-deep/</url>
    <content><![CDATA[
//TODO
This is the follow up of `<<Terraform Quick Start>>`.
[github repo](https://github.com/ned1313/Deep-Dive-Terraform/blob/v2/README.md)

# Working with Existing Resources
```bash
# Configure an AWS profile with proper credentials
# for terraform use
aws configure --profile deep-dive
# Linux or MacOS
export AWS_PROFILE=deep-dive

# After terraform files are in place
# download modules and provider plugin
terraform init
terraform validate
# in collective env better to have plan file
terraform plan -out m3.tfplan
terraform apply "m3.tfplan"
```

you can find useful modules out-of-box in Terrafrom public [provider registery](https://registry.terraform.io/browse/providers). For example, [gcp kubernetes module](https://registry.terraform.io/modules/terraform-google-modules/kubernetes-engine/google/latest):
```ini
module "gke" {
  # specify source
  source                     = "terraform-google-modules/kubernetes-engine/google"
  project_id                 = "<PROJECT ID>"
  name                       = "gke-test-1"
  region                     = "us-central1"
  zones                      = ["us-central1-a", "us-central1-b", "us-central1-f"]
  network                    = "vpc-01"
  subnetwork                 = "us-central1-01"
  ip_range_pods              = "us-central1-01-gke-01-pods"
  ip_range_services          = "us-central1-01-gke-01-services"
  http_load_balancing        = false
  horizontal_pod_autoscaling = true
  network_policy             = true

  node_pools = [
    {
      name               = "default-node-pool"
      machine_type       = "e2-medium"
      node_locations     = "us-central1-b,us-central1-c"
      min_count          = 1
      max_count          = 100
      local_ssd_count    = 0
      disk_size_gb       = 100
      disk_type          = "pd-standard"
      image_type         = "COS"
      auto_repair        = true
      auto_upgrade       = true
      service_account    = "project-service-account@<PROJECT ID>.iam.gserviceaccount.com"
      preemptible        = false
      initial_node_count = 80
    },
  ]

  node_pools_oauth_scopes = {
    all = []

    default-node-pool = [
      "https://www.googleapis.com/auth/cloud-platform",
    ]
  }

  node_pools_labels = {
    all = {}

    default-node-pool = {
      default-node-pool = true
    }
  }

  node_pools_metadata = {
    all = {}

    default-node-pool = {
      node-pool-metadata-custom-value = "my-node-pool"
    }
  }

  node_pools_taints = {
    all = []

    default-node-pool = [
      {
        key    = "default-node-pool"
        value  = true
        effect = "PREFER_NO_SCHEDULE"
      },
    ]
  }

  node_pools_tags = {
    all = []

    default-node-pool = [
      "default-node-pool",
    ]
  }
}
```

Then someone in the team provisioning some resources on AWS without using Terraform, we want to include them under the Terraform control:
```bash
# update terraform config file to include new added resources
edit terraform.tfvars

# identifiers from provider and configuration
#Use the values output by junior_admin.sh script
# xxx: the resource ID 应该在AWS dashboard中可以找到
terraform import --var-file="terraform.tfvars" "module.vpc.aws_route_table.private[2]" "xxx"
terraform import --var-file="terraform.tfvars" "module.vpc.aws_route_table_association.private[2]" "xxx"
terraform import --var-file="terraform.tfvars" "module.vpc.aws_subnet.private[2]" "xxx"
terraform import --var-file="terraform.tfvars" "module.vpc.aws_route_table_association.public[2]" "xxx"
terraform import --var-file="terraform.tfvars" "module.vpc.aws_subnet.public[2]" "xxx"

# adds new resources to the state
# you will see some change items if not consistent before
terraform plan -out m3.tfplan
```

# Managing State]]></content>
      <categories>
        <category>Infra</category>
      </categories>
      <tags>
        <tag>infra</tag>
        <tag>terraform</tag>
      </tags>
  </entry>
  <entry>
    <title>Terraform Quick Start</title>
    <url>/2020/05/16/infra-terraform-start/</url>
    <content><![CDATA[
后来还用到了Python [Terraform package](https://github.com/beelit94/python-terraform), 结合Python Click 以及其他custom Python package 去构造glue code.

# Introduction
To alter a planet for the purpose of sustaining life.

This [article](https://www.ibm.com/cloud/learn/terraform) from IBM can give you a good overview:
- Terraform vs Kubernetes
- Terraform vs Ansible

Removing manual build process, adopting declarative approach to deploy **infrastructure as code**, reusable, idempotent and consistent repeatable deployment.

Use gcloud API or client binary can do the same work as Terraform, so what are the `benefits`:
- cloud agnostic, multi-cloud portable.
- Unified workflow: If you are already deploying infrastructure to Google Cloud with Terraform, your resources can fit into that workflow.
- Full lifecycle management: Terraform doesn't only create resources, it updates, and deletes tracked resources without requiring you to inspect the API to identify those resources.
- Graph of relationships: Terraform understands dependency relationships between resources.

[Terraform 文档](https://www.terraform.io/docs/)非常informative，结构清晰:
Terraform之于cloud infra on public cloud 就相当于 helm之于application on k8s, 大大简化了操作复杂性，自动快速部署，同时做到了复用，versioning等特性。但得去了解cloud provider中提供的resources 的用途，搭配。


## Github Repo
resource files in github:
https://github.com/ned1313/Getting-Started-Terraform


## Composition
Terraform executable: download from web (or build terraform docker image)
Terraform files: using hashicorp configure language DSL
Terraform plugin: interact with provider: AWS, GCP, Azure, etc
Terraform state file: json and don't touch it but you can view it to get deployment detial

You can have multiple terraform files: `.tf`, when run terraform it will stitch them together to form a single configuration. 比如把variables, outputs, resources, tfvars分开。

tfvars file by default named as `terraform.tfvars`, otherwise when run `plan` you need to specify the file path. This `tfvars` file is usually generated from some meta-data configuration, then combining with variable declaration file.


# Commands
To execute terraform command, **build a docker image** and mount cloud auth credentials when start container. BTW, if you run on google compute VM, the SDK will inject the host auth automatucally in container.
If you update the terraform file with different configuration, rerun `init`, `plan` and `apply`.
```bash
# list all commands
terraform --help
terraform version

# create workspace, see below section
terraform workspace

# linter
terraform validate

# show a tree of providers in main the sub-modules
# for example, google, kubernetes, random, null, locals they are all providers
terraform providers

# init will download plugin, for example, aws, gcp or azure..
terraform init

# will show you the diff if you update your terraform file
# load terraform.tfvars by default, if not, need to specify
terraform plan -out plan.tfplan


# will generate a tfstate file
# perform creation as much parallel as possible
# --auto-approve: for script, no interactive
terraform apply "plan.tfplan" [--auto-approve]

# -state: output state to specific file
# when run different env with a single definition file
terraform apply -state=qa-env.tfstate -var environment=qa "plan.tfplan"

# Manually mark a resource as tainted, forcing a destroy and recreate
# on the next plan/apply.
terraform taint <google_compute_instance.vm_instance>

# output terraform state or plan file in a human-readable form
# show what has been created
terraform show

# show output variable value
# useful for scripts to extract outputs from your configuration
terraform output [output name]

# Update variables
terraform refresh

# show objects being managed by state file
terraform state list

# destroy Terraform-managed infrastructure
terraform destroy [--auto-approve]
```

## Syntax
Hashicorp [configuration language](https://www.terraform.io/docs/configuration/index.html), basic block syntax:
```js
block_type label_one [label_two] {
    key = value

    embedded_block {
        key = value
    }
}
```

怎么知道resource的名称呢? find the provider, then search the resources:
https://www.terraform.io/docs/configuration/resources.html
还有random provider，比如产生随机数.

# Provider
Support mutliple providers, all written in Go.
https://www.terraform.io/docs/providers/index.html


# Provisioner
在deploy infrastructure 之后的配置操作，比如使用ansible or shell script as privisioners.

Provisioner can be ran in creation or destruction stage, you can also have multi-provisioner in one resources and they execute in order in resource.

Provisioner can be local or remote:
- file: copy file from local to remove VM instance.
- local-exec: executes a command locally on the machine running Terraform, not the VM instance itself. 
- remote-exec: executes on remote VM instance.

Terraform treats provisioners differently from other arguments. Provisioners only run when a resource is created, adding a provisioner does not force that resource to be destroyed and recreated. Use `terraform taint` to tell Terraform to recreate the instance.


# Functions
https://www.terraform.io/docs/configuration/functions.html

You can experiment with functions in Terraform console, this can help with troubleshooting.
```bash
# first run terraform init
# it will auto load tfvars file and variables
terraform console
> lower("HELLO")
> merge(map1, map2)
> file(path)
> min(2,5,90)
> timestamp()
# modular
> 34 % 2
> cidrsubnet(var.network_address_space, 8, 0)
# lookup value in a map
> lookup(local.common_tags, "Bill", "Unknown")
```

For example:
```bash
variable network_info {
    default = "10.1.0.0/16"
}

# split network range by adding 8 bits, fetch the first one subnet
# 10.1.0.0/24
cidr_block = cidrsubnet(var.network_info, 8, 0)
```


# Resource arguments
https://www.terraform.io/docs/configuration/resources.html#meta-arguments
common ones:
- depends_on: make sure terraform creates things in right order
- count: create similar resources
- for_each: create resources not similar
- provider: which provider should create the resource

For example:
```bash
resource "aws_instance" "web" {
    # index start from 0
    count = 3
    tags {
        Name = "web-${count.index + 1}"
    }

    depends_on = [aws_iam_role_policy.custom_name]
}

resource "aws_s3_bucket" "storage" {
    for_each = {
        food = "public-read"
        cash = "private"
    }
    # access key and value
    bucket = "${each.key}-${var.bucket_suffix}"
    acl = each.value
}
```


# Variables
Other way to use variables rather than specifying in single `.tf` file.

The scenario, we need development, QA(Quality Assurance)/UAT(User Acceptance Testing), production environment, how to implement with one configuration and multiple inputs?

The variable values can be from, precedence from low to high:
- environment variable: `TF_VAR_<var name>`.
- file: `terraform.tfvars` or specify by `-var-file` in terraform command.
- terraform command flags `-var`.

You can override variables and precedence, select value based on environment, for example:
```bash
# specify default value in tf file
variable "env_name" {
  type = string
  default = "development"
}

# or specify in tfvars file
env_name = "uat"

# or specify in command line
terraform plan -var 'env_name=production'
```

Variable types:
- string, the default type if no explicitly specified
- bool: true, false
- number (integer/decimal)
- list (index start from 0)
- map, value type can be number, string and bool

For example:
```bash
variable "project" {
  type = string
}

variable "web_instance_count" {
  type    = number
  default = 1
}

# list
variable "cidrs" { default = [] }

# map
variable "machine_types" {
  # map type and key is string
  type    = map(string)
  default = {
    dev  = "f1-micro"
    test = "n1-highcpu-32"
    prod = "n1-highcpu-32"
  }
}

machine_resource = lookup(var.machine_types, var.environment_name)
```

In terraform, the same syntax `${}` for interpolation as in bash:
```bash
# local variable definition
locals {
  # random_integer is a terraform resource
  tags = "${var.bucket_name_prefix}-${var.environment_tag}-${random_integer.rand.result}"
}

# use
resource "aws_instance" "example" {
  tags = local.tags
}
```


# Workspace
Workspace is the recommended way to working with multiple environments, for example:
- state management
- variables data
- credentials management

State file example, we have dev, QA, prod three environments, put them each into separate folder, when run command, specify the input and output:
```bash
# for dev environment
# -state: where to write state file
# -var-file: load file
terraform plan -state="./dev/dev.state" \
               -var-file="common.tfvars" \
               -var-file="./dev/dev.tfvars"
```

Workspace example, there is a `terraform.workspace` built-in variable can be used to indicate the workspace currently in, then use it in map variable to select right value for different environment. (不用再去分别创建不同的folder for different environment了)
```bash
# create dev workspace and switch to it
# 类似于git branch
terraform workspace new dev
# show workspace
terraform workspace list
terraform plan -out dev.tfplan
terraform apply "dev.tfplan"

# now create QA workspace
terraform workspace new QA

# switch workspace
terraform workspace select dev
```
Special terraform variable to get workspace name
```bash
locals {
  env_name = lower(terraform.workspace)
}
```


# Managing secrets
Hashicorp **Vault** is for this purpose. it can hand over credentials from cloud provider to terraform and set ttl for the secrets.

Or you can use environment variable to specify the credentials, terraform will pick it automatically, but bear in mind to use the right env var name. For example:
```bash
# 注意这个和前面的TF_VAR_<var name> 不一样，这里是secret
export AWS_ACCESS_KEY_ID=xxx
export AWS_SECRET_ACCESS_KEY=xxx
```


# Module
Make code reuse eaiser:
https://www.terraform.io/docs/configuration/modules.html

Terraform registry, similar concept with Helm, Docker:
https://registry.terraform.io/
Using module block to invoke local or remote modules.
1. root module
2. support versioning
3. provider inheritance

Module components:
- variables input
- resources
- output values (calling part will take this in)


# Google Cloud Platform
Good tutorial:
https://learn.hashicorp.com/tutorials/terraform/google-cloud-platform-build

If run `terraform apply` get permission issues, add the service account used to IAM, than grant it roles. Then retry the apply command.

用Terraform 建造的VM instance network 没有ssh allow firewall rule, 要自己添加:
https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-ssh

Terrafrom provisions `GKE` with additional node pool:
https://learn.hashicorp.com/terraform/kubernetes/provision-gke-cluster

## Resources
Commonly use reource types for terraform `resource` block:
- google_compute_network
- google_compute_instance
- google_compute_address
- google_storage_bucket
- google_container_cluster
- google_container_node_pool
]]></content>
      <categories>
        <category>Infra</category>
      </categories>
      <tags>
        <tag>infra</tag>
        <tag>terraform</tag>
      </tags>
  </entry>
  <entry>
    <title>Java Deque</title>
    <url>/2020/04/11/java-deque/</url>
    <content><![CDATA[
关于Deque，之前我的总结是，如果要当作Stack使用，则stick to Stack methods，比如push, pop, peek。如果当作Queue使用，则stick to Queue method，比如add/offer, poll/remove, peek。在实现上，我一般使用的是ArrayDeque, a resizable double-ended array。

今天突然想到一个问题，用Deque实现的Queue或者Stack，在使用enhanced for loop的时候，Java是怎么知道元素弹出的正确顺序呢? 或者如果混用Queue和Stack的方法，peek会弹出什么结果？iterator会给出什么顺序的结果呢？

我们来看看ArrayDeque的源码:
```java
transient int head;
transient int tail;
```
这里有2个pointers, head和tail，当arraydeque是empty的时候，head和tail重叠。

对于push来说，移动的是head，head的值减1再使用，并且用的是modulus circularly decrement。
```java
/**
* Pushes an element onto the stack represented by this deque.  In other
* words, inserts the element at the front of this deque.
*
* <p>This method is equivalent to {@link #addFirst}.
*
* @param e the element to push
* @throws NullPointerException if the specified element is null
*/
public void push(E e) 
{
    addFirst(e);
}

/**
* Inserts the specified element at the front of this deque.
*
* @param e the element to add
* @throws NullPointerException if the specified element is null
*/
public void addFirst(E e) 
{
    if (e == null)
        throw new NullPointerException();
    final Object[] es = elements;
    es[head = dec(head, es.length)] = e;
    if (head == tail)
        grow(1);
}

/**
* Circularly decrements i, mod modulus.
* Precondition and postcondition: 0 <= i < modulus.
*/
static final int dec(int i, int modulus) 
{
    if (--i < 0) i = modulus - 1;
    return i;
}
```

对于add/offer来说，tail的值用了再加1，并且用的是modulus circularl increment。
```java
/**
* Inserts the specified element at the end of this deque.
*
* <p>This method is equivalent to {@link #addLast}.
*
* @param e the element to add
* @return {@code true} (as specified by {@link Collection#add})
* @throws NullPointerException if the specified element is null
*/
public boolean add(E e) 
{
    addLast(e);
    return true;
}

/**
* Inserts the specified element at the end of this deque.
*
* <p>This method is equivalent to {@link #add}.
*
* @param e the element to add
* @throws NullPointerException if the specified element is null
*/
public void addLast(E e) 
{
    if (e == null)
        throw new NullPointerException();
    final Object[] es = elements;
    es[tail] = e;
    if (head == (tail = inc(tail, es.length)))
        grow(1);
}

/**
* Circularly increments i, mod modulus.
* Precondition and postcondition: 0 <= i < modulus.
*/
static final int inc(int i, int modulus) 
{
    if (++i >= modulus) i = 0;
    return i;
}
```

peek总是从head pointer取值
```java
/**
* Retrieves, but does not remove, the head of the queue represented by
* this deque, or returns {@code null} if this deque is empty.
*
* <p>This method is equivalent to {@link #peekFirst}.
*
* @return the head of the queue represented by this deque, or
*         {@code null} if this deque is empty
*/
public E peek() 
{
    return peekFirst();
}
```

iterator总是从head pointer -> tail pointer
```java
/**
* Returns an iterator over the elements in this deque.  The elements
* will be ordered from first (head) to last (tail).  This is the same
* order that elements would be dequeued (via successive calls to
* {@link #remove} or popped (via successive calls to {@link #pop}).
*
* @return an iterator over the elements in this deque
*/
public Iterator<E> iterator() 
{
    return new DeqIterator();
}
```

所以现在情形就很清楚了，来看一个例子。首先按照顺序push [1 2 3], 这时peek是3，然后再add/offer [4 5 6], 这时peek还是3，然后iterator的结果是: [3 2 1 4 5 6]
```java
Deque<Integer> dq = new ArrayDeque<>();
dq.push(1);
dq.push(2);
dq.push(3);
// now peek is 3
dq.add(4);
dq.add(5);
dq.add(6);
// now peek is still 3
for (int ele: dq)
{
    System.out.println(ele);
}
// 3 2 1 4 5 6
```
![](https://drive.google.com/uc?id=1KaVUCoUDu-3o7_hqEyCwKyDEstJc-9_z)]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>Vault Quick Start</title>
    <url>/2020/06/21/infra-vault/</url>
    <content><![CDATA[
Course git repo, this repo has useful Vault commands:
https://github.com/ned1313/Getting-Started-Vault

My Vault vagrant demo:
https://github.com/chengdol/InfraTree/tree/master/vagrant-vault

# Questions
[Vault vs K8s secrets?](https://www.reddit.com/r/devops/comments/ejfzhl/kubernetes_secrets_vs_hashicorp_vault/)
Examples of what Vault can do that k8s secrets cannot:
With Vault you can rotate secrets and have secrets with short TTL
With Vault you can access secrets across namespaces (or outside the k8s cluster)
Vault can provide a PKI for signing certs (enabling for example automation of cert generation for mtls)
Vault can use LDAP, oauth, IAM, etc as identity providers

# Introduction
Vault web site:
https://www.vaultproject.io/

Secure, store and tightly control access to tokens, passwords, certificates, encryption keys for protecting secrets and other sensitive data using a UI, CLI, or HTTP API.
注意API的path，并不是和UI上的path一样！

Vault works well with Consul, for example, set Consul as storage backend.

Start a Vault server:
```bash
## same as consul in development mode, don't use this in production!
## 0.0.0.0:8200, used for vagrant port fordwarding access from host
vault server -dev-listen-address 0.0.0.0:8200 -dev
```
Output as below:
```s
WARNING! dev mode is enabled! In this mode, Vault runs entirely in-memory
and starts unsealed with a single unseal key. The root token is already
authenticated to the CLI, so you can immediately begin using Vault.

You may need to set the following environment variable:

    $ export VAULT_ADDR='http://0.0.0.0:8200'

The unseal key and root token are displayed below in case you want to
seal/unseal the Vault or re-authenticate.

## these two are critical
Unseal Key: pLNBmZQRHvspdy5unZcTjm1jOVQ81Z0pO6ywYHNP1zQ=
Root Token: s.hfnmMkgG7cggWDOmPfHC1jIe

Development mode should NOT be used in production installations!
```
`unseal` key在production mode中用来解除对Vault server的封锁，否则无法login as root.

注意，这里VAULT_ADDR是本地的，vault server当然可以在其他地方，设置对应的地址即可.
```bash
## Vault API access
export VAULT_ADDR='http://0.0.0.0:8200'
export VAULT_TOKEN=s.ttlDcetbJe3uLt0FF5rSidg3

## login to vault server need VAULT_TOKEN to login
vault login
```
Vault web UI access: `http://localhost:8200`

So, just like Consul, there are 3 ways to interact with Vault server: UI, API, CLI (actually running API under the hood). 

`secret` is a pre-existing secret engine folder in Vault storage path, you can see it in UI:
```bash
#Write a secret
vault kv put secret/hg2g answer=42
#For Linux
# marvin.json is a json file
curl --header "X-Vault-Token: $VAULT_TOKEN" --request POST \
 --data @marvin.json $VAULT_ADDR/v1/secret/data/marvin
```
marvin.json is as follow:
```json
{
    "data": {
        "paranoid": true,
        "status": "bored"
    }
}
```

```bash
#Get a secret
vault kv get secret/hg2g
#specify format
vault kv get -format=json secret/hg2g
vault kv get -format=yaml secret/hg2g
#For Linux
#Install jq if necessary
sudo yum install jq -y
curl --header "X-Vault-Token: $VAULT_TOKEN" $VAULT_ADDR/v1/secret/data/marvin | jq

#Put a new secret in and a new value for an existing secret
vault kv put secret/hg2g answer=54 ford=prefect
vault kv get secret/hg2g

#Delete the secrets
vault kv delete secret/hg2g
vault kv get secret/hg2g

#For Linux
curl --header "X-Vault-Token: $VAULT_TOKEN" --request DELETE $VAULT_ADDR/v1/secret/data/marvin
```

# Working with Secrets

Secret lifecycle:
- Create
- Read
- Update
- Delete (soft or hard unrecoverable)
- Destroy

There is version `1` and version `2` of `secret engine server`, version `2` is more recoverable and versioning but less performance then version `1` if you need to scale. `secret` folder 就是默认创建的secret engine, version `2`, 可以去UI 查看configuration.

Demo code about secrets lifecycle:
https://github.com/ned1313/Getting-Started-Vault/blob/master/m3/m3-secretslifecycle.sh

Everytime you update the key, the version increment by 1:
```bash
## pick value by version
vault kv get -version=3 secret/hg2g
## from API
curl -X GET --header "X-Vault-Token: $VAULT_TOKEN" $VAULT_ADDR/v1/secret/data/hg2g?version=3 | jq .data.data
```

If you delete version 3, you still can get version 1 or 2:
```bash
vault kv delete secret/hg2g
vault kv get -version=2 secret/hg2g
## undelete version 3
## -versions not -version, you can undelete multiple: -versions=2,3
vault kv undelete -versions=3 secret/hg2g

## API
#For Linux
curl --header "X-Vault-Token: $VAULT_TOKEN" --request POST \
  $VAULT_ADDR/v1/secret/undelete/hg2g  --data '{"versions": [2]}'
```

Destroy, can no longer undelete:
```bash
#Destroy the secrets
vault kv destroy -versions=1,2 secret/hg2g

#For Linux
## metadata is still there
curl --header "X-Vault-Token: $VAULT_TOKEN" --request POST \
  $VAULT_ADDR/v1/secret/destroy/hg2g --data '{"versions": [1,2]}'

#Remove all data about secrets
vault kv metadata delete secret/hg2g
vault kv get secret/hg2g

#For Linux
curl --header "X-Vault-Token: $VAULT_TOKEN" --request DELETE \
  $VAULT_ADDR/v1/secret/metadata/hg2g
```

## Create New Secret Engine
Demo about secret engine, 我这里就不贴出来了, 如果用API，还有一些配置用的json文件，见这个的上层目录中的比如`dev-b.json`:
https://github.com/ned1313/Getting-Started-Vault/blob/master/m3/m3-secretengine.sh

## Create Mysql DB Secret Engine
Vault official reference:
https://www.vaultproject.io/docs/secrets

Vault在这里类似一个中间层，在user 和 Mysql instance之间，保存和传递credential和请求. 这个demo在AZure上spin up了一个bitnami Mysql instance，和本地的vault mysql secret engine关联，然后通过Vault联系Mysql 产生一个dynamic user，并授予这个临时user权限，我们通过这个临时user就可以操作Mysql 数据库了。

这个dynamic user lifecycle also managed by Vault, as well as the permission the user has. 
https://github.com/ned1313/Getting-Started-Vault/blob/master/m3/m3-mysqlengine.sh

Besides Mysql example, Vault secrets can be used for certificates, for SSH credentials, etc.
Secrets engine make Vault extensible, there are different plugin to handle different needs.


# Controlling Access

Similar to RBAC in K8s, authentication method to control login, policies to control what is able to do, managing client tokens.

## Vault Auth Method
Vault has internal authentication (called `userpass`) and support external, multiple authrntication methods.
https://github.com/ned1313/Getting-Started-Vault/blob/master/m4/m4-basicauth.sh

```bash
## list current auth methods
## by default, we only have token auth method
vault auth list
## enable new auth method
vault auth enable userpass
## create user
vault write auth/userpass/users/arthur password=dent
## list user
vault list auth/userpass/users
```
You will see the changes in UI `Access` section.

Once you create a user/password, then you can run for example:
```bash
## 这里使用的是user/password login，不是用token
vault login -method=userpass username=arthur
## then input password
Password (will be hidden): 
## 这个warning是因为，做实验的时候之前export了root token
WARNING! The VAULT_TOKEN environment variable is set! This takes precedence
over the value set by this command. To use the value set by this command,
unset the VAULT_TOKEN environment variable or set it to the token displayed
below.

Success! You are now authenticated. The token information displayed below
is already stored in the token helper. You do NOT need to run "vault login"
again. Future Vault requests will automatically use this token.

Key                    Value
---                    -----
token                  s.z52FGzn78XynKlxeS0Akt0t7
token_accessor         90LaRNNtsCUEg6yDd9ldRi3v
token_duration         768h
token_renewable        true
token_policies         ["default"]
identity_policies      []
policies               ["default"]
token_meta_username    arthur


## will show you where is the token from, not root token any more
## see `path` field where is the token from
vault token lookup
```

Token represents who you are and what you can do, for example, the user itself cannot update its password via its token:
```bash
## update password
vault write auth/userpass/users/arthur/password password=tricia
## output
Error writing data to auth/userpass/users/arthur/password: Error making API request.

URL: PUT http://0.0.0.0:8200/v1/auth/userpass/users/arthur/password
Code: 403. Errors:

* 1 error occurred:
	* permission denied
```
Can do this after export root token.

Delete auth:
```bash
#Remove account
vault delete auth/userpass/users/arthur
```

这里介绍了2个新概念: `LDAP` and `Active Directory`
[What are the differences between LDAP and Active Directory?](https://stackoverflow.com/questions/663402/what-are-the-differences-between-ldap-and-active-directory)
Active Directory is a database based system that provides authentication, directory, policy, and other services in a Windows environment

LDAP (Lightweight Directory Access Protocol) is an application protocol for querying and modifying items in directory service providers like Active Directory, which supports a form of LDAP.

Short answer: AD is a directory services database, and LDAP is one of the protocols you can use to talk to it.

见下面vault policy章节的例子.
Case: 在外部设置了`AD` as authentication method, enable `LDAP` auth in Vault. Then user login Vault with AD credentials against LDAP talk to AD, AD talk to Vault and determine the policy about what the user can do, then user will get the right token from vault to access the store.

## Vault Policy

Similar to K8s role.
https://github.com/ned1313/Getting-Started-Vault/blob/master/m4/m4-activedirectory.sh

这个例子，远程登录了一个Vault server, login as root, create `devkv` store with key and value pair, then create policy `dev` with HCL file for user to access the `devkv`. 

Enable LDAP auth, configure it with Active Directory remotely. Then assign the `developers` group in LDAP with `dev` policy.

Then user `adent` login vault against `ldap` method. The user is in the `developers` group so it will get token with permission specified by `dev` policy.

## Client Token
https://www.vaultproject.io/docs/concepts/tokens

## Wrapping Response
https://github.com/ned1313/Getting-Started-Vault/blob/master/m4/m4-tokenwrapping.sh


# Operating Vaule Server
主要讲了production Vault server setup.
- Vault server architecture
- Storage backend options
- Vault server operations

这里例子用Consul作为 storage backend(不是说it's not intended for this吗😂), 在Vault 主机上运行有Consul agent 和 远处的Consul server通信 (gossip tcp: 8301, RPC tcp: 8300) ，Consul server可以有多个以实现HA。Vault通过本地的Consul agent port 8500和其交互，外界访问Vault 用默认的port 8200:
https://github.com/ned1313/Getting-Started-Vault/tree/master/m5

可以留意一下这里的Consul配置，之前用的一直是dev mode，这里是production mode了, 把Consul 包装成了一个service, run as daemon, in `consul` folder:
- consul-deploy.sh
- consul.hcl
- server.hcl
- consul.service

注意把 `.hcl` and `.service` 文件内容paste到 `consul-deploy.sh` 创建的文件中。script中的`useradd` command也值得借鉴。`consul keygen` 是用来产生encrypt string的，用在server 和 client的 `.hcl`中。

Consul agent setup:
- consul-deploy-agent.sh
- consul-agent.hcl

好了，这里引出一个有意思的东西: **Create Custom Systemd Service**，之前从来没有这么做过，原来systemd是可以自己配置的！
https://medium.com/@benmorel/creating-a-linux-service-with-systemd-611b5c8b91d6

Vault server Config, also set Vault as systemd service, in `vault` folder:
- vault-deploy.sh
- vault.hcl
- vault.service

在 `.hcl` 中就配置了Consul 为storage backend，看上去本来就有这种特性，所以不需要过多的设置。


## Server Operations
Operator command:
https://www.vaultproject.io/docs/commands/operator

Setup production Vault server, sealed and need to unseal:
https://github.com/ned1313/Getting-Started-Vault/blob/master/m5/m5-serveroperations.sh


# Auditing
Everything is audited, sensitive data is hashed unless you explicitly set false.
https://github.com/ned1313/Getting-Started-Vault/tree/master/m6

Auditing device type:
- File, JSON format
- Syslog
- Socket: TCP/UDP

```bash
## enable auditing device
vault audit enable [type] [-path=valut_path]
## log_raw=true means no encrypt sensitive data
vault audit enable file file_path=/var/log/vault/vault_audit.log log_raw=true
vault audit enable -path=file2 file file_path=/var/log/vault/vault_audit2.log
vault audit enable syslog tag="vault" facility="LOCAL7"

## disable
vault audit disable [vault_path]
## disable file2 created above
vault audit disable file2
## list
vault audit list -detailed
```



]]></content>
      <categories>
        <category>Infra</category>
      </categories>
      <tags>
        <tag>infra</tag>
        <tag>vault</tag>
      </tags>
  </entry>
  <entry>
    <title>Java Exception</title>
    <url>/2020/04/11/java-exception/</url>
    <content><![CDATA[Reference: https://docs.oracle.com/javase/tutorial/essential/exceptions/definition.html

Code that fails to honor the Catch or Specify Requirement will not compile.

Not all exceptions are subject to the Catch or Specify Requirement. To understand why, we need to look at the three basic categories of exceptions, only one of which is subject to the Requirement.

## The Three Kinds of Exceptions
The first kind of exception is the `checked exception`.

Checked exceptions are subject to the `Catch or Specify Requirement`. **All** exceptions are checked exceptions, except for those indicated by Error, RuntimeException, and their subclasses.

The second kind of exception is the `error`. These are exceptional conditions that are external to the application, and that the application usually cannot anticipate or recover from.
Errors are not subject to the Catch or Specify Requirement. Errors are those exceptions indicated by Error and its subclasses.

The third kind of exception is the `runtime exception`.
Runtime exceptions are not subject to the Catch or Specify Requirement. Runtime exceptions are those indicated by RuntimeException and its subclasses.

Errors and runtime exceptions are collectively known as `unchecked exceptions`.

> unchecked exception 也可以被catch，只要match就行，也可以被throws, but we don't have to do that: https://stackoverflow.com/questions/8104407/cant-java-unchecked-exceptions-be-handled-using-try-catch-block

[Better Understanding on Checked Vs. Unchecked Exceptions](https://crunchify.com/better-understanding-on-checked-vs-unchecked-exceptions-how-to-handle-exception-better-way-in-java/)

```java
try
{
    // code that could throw an exception
}
// check in order
catch (IOException | SQLException ex)
{
    logger.log(ex);
    throw ex;
}
catch (IndexOutOfBoundsException e) 
{
    System.err.println("IndexOutOfBoundsException: " + e.getMessage());
}
// The finally block always executes when the try block exits.
finally
{
    if (out != null) { 
        System.out.println("Closing PrintWriter");
        out.close(); 
    } else { 
        System.out.println("PrintWriter not open");
    } 
}
```

`finally` block it allows the programmer to avoid having cleanup code accidentally bypassed by a return, continue, or break. Putting cleanup code in a finally block is always a good practice, even when no exceptions are anticipated.

The `try-with-resources` statement ensures that each resource is closed at the end of the statement. Any object that implements `java.lang.AutoCloseable`, which includes all objects which implement `java.io.Closeable`, can be used as a resource.
```java
static String readFirstLineFromFile(String path) throws IOException 
{
    try (BufferedReader br = new BufferedReader(new FileReader(path))) 
    {
        // try block
        return br.readLine();
    }
}
```

> Note: A try-with-resources statement can have catch and finally blocks just like an ordinary try statement. In a try-with-resources statement, any catch or finally block is run **after** the resources declared have been closed.

## Throw exception
declare throws exception for method
```java
public void writeList() throws IOException {}
```

throw an exception
```java
public void test() {
    if (size == 0) {
        throw new EmptyStackException();
    }
}
```

Create custom exception: https://www.baeldung.com/java-new-custom-exception
```java
// create custom exception
public class IncorrectFileNameException extends Exception { 
    public IncorrectFileNameException(String errorMessage) {
        super(errorMessage);
    }
}
```]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>Java equals Method</title>
    <url>/2019/11/03/java-overwrite-equals/</url>
    <content><![CDATA[
This is the first Java post in my blog, actually I have lots of summaries about Java in recently years, they just get accumulated so I decide to post here.

Also, start from next week, I will dive into API work.

When you define a new class and it will deal with hash thing, don't forget to overwrite the `equals` method, or `compare` method if they need to be sorted in natural order, or implement `Comparator` interface for ordering by other rules.

For example:
```java
class Complex { 
    private double re, im; 

    public Complex(double re, double im) { 
        this.re = re; 
        this.im = im; 
    } 
    // Overriding equals() to compare two Complex objects 
    @Override
    public boolean equals(Object o) { 
        if (o == null) { return false; }
        if (o == this) { return true; } 
        if (!(o instanceof Complex)) { return false; } 
        Complex c = (Complex) o; 
        // Compare the data members and return accordingly  
        return Double.compare(re, c.re) == 0
                && Double.compare(im, c.im) == 0; 
    } 
} 
```
https://stackoverflow.com/questions/16970210/java-treeset-remove-and-contains-not-working

One thing I want to emphasize is `TreeSet`, the object in tree set use its compareTo (or compare) method, so two elements that are deemed equal by this method are, from the standpoint of the set, equal. The behavior of a set is well-defined even if its ordering is inconsistent with equals; it just fails to obey the general contract of the Set interface.

To be more accurate, `TreeSet` is an implementation of `SortedSet`
If you want a `.equals()/.hashCode()` compatible set, use, for instance, a HashSet.

]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>Vagrant Quick Start</title>
    <url>/2020/06/13/infra-vagrant/</url>
    <content><![CDATA[
My Vagrant singleton virtual machine and cluster setup for different usage, see this [git repo](https://github.com/chengdol/InfraTree).

早在2019年使用Ansible (from Book `<<Ansible: Up and Running>>`)的时候就了解到了Vagrant，知道它能非常方便的provisioning virtual machine cluster for testing and demo purpose (比如测试Ansible的功能模块, Jenkins, Nginx).这次系统地学习了，这样在本地部署实验集群也很方便了(还有一个选择是docker compose)。

Ansible playbooks are a great way to specify how to configure a Vagrant machine so new comers on your team can get up and running on day one. Of course you can use other provisioners.

[Vagrant Box Search](https://app.vagrantup.com/boxes/search) 提供诸如 Windows, MacOS等许多操作系统的box image. 还有Openshift, Kubernetes all-in-one box等, 但是可能有的配置过于繁杂，可以自己使用root image build your custom image via Packer.

# Introduction
First read the [Vagrant web site](https://www.vagrantup.com/). 
[Vagrant Blog](https://www.hashicorp.com/blog/category/vagrant) (from HashiCorp). To see what's new in latest version.

It leverages a `declarative` configuration file which describes all your software requirements, packages, operating system configuration, users, and more.

Vagrant also integrates with your existing configuration management tooling like Ansible, Chef, Docker, Puppet or Salt, so you can use the same scripts to configure Vagrant as production.

[Vagrant vs. Other Software](https://www.vagrantup.com/intro/vs)
Compare with VirutalBox CLI tools, Docker and Terraform.


## Install
Installation is easy, Vagrant and VirtualBox, other providers are possible, for example, Docker, VMware.

Download and install VirtualBox:
https://www.virtualbox.org/wiki/Downloads

Download and install Vagrant:
https://www.vagrantup.com/downloads


## Project
The `Vagrantfile` is meant to be committed to `version control` with your project, if you use version control. This way, every person working with that project can benefit from Vagrant without any upfront work.

The syntax of Vagrantfiles is `Ruby`, but knowledge of the Ruby programming language is not necessary to make modifications to the Vagrantfile, since it is mostly simple variable assignment.
```bash
# init project
mkdir vagrant_getting_started
cd vagrant_getting_started

# --minimal: generate minimal Vagrantfile
# hashicorp/bionic64: the box name
# if you have Vagrantfile, no need this
vagrant init hashicorp/bionic64 [--minimal]
```
Of course you can create a `Vagrantfile` manually.

```bash
# download box image
# you don't need to explicitly do this, vagrant will handle download from Vagrantfile
vagrant box add hashicorp/bionic64
```
In the above command, you will notice that boxes are namespaced. Boxes are broken down into two parts - the username and the box name - separated by a slash. In the example above, the username is "hashicorp", and the box is "bionic64".

Editing Vagrantfile, this is a simple example to bring up a jenkins cluster:
```ruby
# -*- mode: ruby -*-
# vi: set ft=ruby :

# image info for all machines
IMAGE_NAME = "generic/centos7"
IMAGE_VERSION = "3.0.10"

# server static ip
SERVER_IP = "192.168.3.2"
# agent static ip, start from 192.168.50.1x
AGENT_IP = "192.168.3.1"

# Vagrantfile for Jenkins
Vagrant.configure("2") do |config|
  # box for virtual machines
  config.vm.box = IMAGE_NAME
  config.vm.box_version = IMAGE_VERSION

  # virtualbox configuration for virtual machines
  config.vm.provider "virtualbox" do |v|
    v.memory = 512
    v.cpus = 1
  end

  # synced folder
  config.vm.synced_folder ".", "/vagrant",
    owner: "root", group: "root"

  # Jenkins server, set as primary
  config.vm.define "server", primary: true do |server|
    server.vm.hostname = "jenkins-server"
    # private network
    # jenkins uses port 8080 in browser
    server.vm.network "private_network", ip: SERVER_IP
    # provisioning
    server.vm.provision :shell, path: "./provision/server.sh", privileged: true
  end

  # agents setup
  (1..2).each do |i|
    config.vm.define "agent#{i}" do |agent|
      agent.vm.hostname = "jenkins-agent#{i}"
      # private network
      agent.vm.network "private_network", ip: "#{AGENT_IP}#{i}"
      # provisioning
      agent.vm.provision :shell, path: "./provision/agent.sh", privileged: true
    end
  end
end
```
使用了private IP后，不再需要port forwarding了，比如VM中有apache server listening on port 80，则可以在host browser上直接访问`http://192.168.2.2`. 查看virtualbox当前VM的网络配置，对应的adapter用的是Host-only adapter.

> 注意，在使用private IP的时候，一般遵循private network的[范围限制](https://en.wikipedia.org/wiki/Private_network#Private_IPv4_address_spaces), 我发现chrome上可能无法访问，可以使用firefox or chrome 匿名模式。或直接使用curl, wget等命令。

还有一点要注意，防火墙firewall是否关闭或设置相应的rules，否则也可能访问不到port:
- [Vagrant's port forwarding not working](https://stackoverflow.com/questions/5984217/vagrants-port-forwarding-not-working)


同一网段的private IP是集群必备，否则不能相互SSH访问。
```ini
# assign private IP
config.vm.network "private_network", ip: "192.168.2.2"
```
This configuration uses a Vagrant private network. The machine can be accessed `only` from the machine that runs Vagrant. You won’t be able to connect to this IP address from another physical machine, even if it’s on the same network as the machine running Vagrant. However, different Vagrant machines **can** connect to each other.

Public network, allow general public access to VM, 但本地测试使用意义不大。
```bash
# when start, it will prompt you select which interface to use
config.vm.network "public_network", ip: "192.173.2.2"
```

SSH agent forwarding, for example, git clone from VM using the private key of host
```ini
config.ssh.forward_agent = true
```

Bring up the environment:
```bash
# validate syntax
vagrant validate
# check machine status
vagrant status
# at your project root directory
vagrant up
# ssh to primary machine in your project
vagrant ssh [vm name]

# when you finish working
# destroy machine
# -f: confirm
vagrant destroy -f
# remove the box
vagrant box remove
```
Note that by default `vagrant ssh` login as user `vagrant`, not `root` user, you can use `sudo` to execute command or run `sudo su -` first.

## SSH config
[Vagrant SSH setting](https://www.vagrantup.com/docs/vagrantfile/ssh_settings#ssh-settings).
[SSH to vagrant machine without running 'vagrant ssh'](https://stackoverflow.com/questions/10864372/how-to-ssh-to-vagrant-without-actually-running-vagrant-ssh): Vagrant will generate key pair for each VM, check by `vagrant ssh-config`:
```bash
# vagrant must be running
vagrant ssh-config

Host default
  HostName 127.0.0.1
  User vagrant
  Port 2222
  UserKnownHostsFile /dev/null
  StrictHostKeyChecking no
  PasswordAuthentication no
  IdentityFile <absolute path>/.vagrant/machines/<vm name>/virtualbox/private_key
  IdentitiesOnly yes
  LogLevel FATAL
```
这个SSH 是如何创建的呢？在创建VM后只有 NAT，host并不能访问VM，vagrant 会自动设置 NAT port forwarding 映射SSH port 22 on VM, 然后生成SSH public-private key等内容，之后使用`vagrant ssh`就可以访问了. You can also change the default ssh port mapping, see this [blog](https://medium.com/@ganpat.bit/how-to-modify-vagrant-vms-default-ssh-port-d3e996d07be2), for example:
```ini
# id: "ssh"
# map host port 13001 to guest port 22
config.vm.network :forwarded_port, guest: 22, host: 13001, id: "ssh"
```

To see the full ssh command vagrant uses, run `ps aux | grep ssh` to check, for example:
```bash
# -q: quiet mode
ssh vagrant@127.0.0.1 \
    -p 2222 \
    -o LogLevel=FATAL \
    -o Compression=yes \
    -o DSAAuthentication=yes \
    -o IdentitiesOnly=yes \
    -o StrictHostKeyChecking=no \
    -o UserKnownHostsFile=/dev/null \
    -i <absolute path>/.vagrant/machines/<vm name>/virtualbox/private_key
```
If you disable insert_key in Vagrantfile:
```ini
config.ssh.insert_key = false
```
Then Vagrant uses insecure default key, if you check verbose ssh command under the `vagrant ssh`, you will see the location of that insecure key:
```bash
ps aux | grep ssh
#-i <user home>/.vagrant.d/insecure_private_key
```

By using `synced folders`, Vagrant will automatically sync your files to and from the guest machine. By default, Vagrant shares your `project directory` (remember, that is the one with the Vagrantfile) to the `/vagrant` directory in your guest machine.

Synced folder will be mapped **before** provisioning would run.

Vagrant also support rsync, primarily in situations where other synced folder mechanisms are not available:
https://www.vagrantup.com/docs/synced-folders/rsync.html

Vagrant has built-in support for `automated provisioning`. Using this feature, Vagrant will automatically install software when you vagrant up so that the guest machine can be repeatably created and ready-to-use.
```bash
# use Ansible provisioner
VAGRANTFILE_API_VERSION = "2"
Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  config.vm.box = "ubuntu/trusty64"
  config.vm.provision "ansible" do |ansible|
    ansible.playbook = "playbook.yml"
  end
end
```

Vagrant will not run it a second time only if you force it.
```bash
# reboot machine and reload provision setting if machine is already running
# if no provision needed, just run vagrant reload
# --provision: force run provision again
vagrant reload --provision
# force rerun provision when machine is running
vagrant provision
```
可以软连接synced folder中的文件或文件夹到需要的地方，这样方便在host machine中编辑.


Methods to teardown:
```bash
# hibernate, save states in disk
vagrant suspend
vagrant resume

# normal power off
vagrant halt
# reclaim all resources
vagrant destroy
# boot again
vagrant up
```
[When to use suspend vs halt](https://stackoverflow.com/questions/42549087/in-vagrant-which-is-better-out-of-halt-and-suspend)


## Multi-Machine 
这个是非常实用的部分，对于测试ansible 以及 jenkins:
https://www.vagrantup.com/docs/multi-machine


## Convert ova to box
Convert a Virtualbox ova to a Vagrant box
https://gist.github.com/chengdol/315d3cbb83cf224c3b34913095b7fff9


# VirtualBox
Start from PluralSight 2014年的Vagrant课程，记录的时候会有一些更新。
Virtualbox command line tool:
```bash
# list VMs
vboxmanage list vms
# show running VMs
vboxmanage list runningvms
```

[Networks types](https://www.virtualbox.org/manual/ch06.html):
[Networks types video](https://www.youtube.com/watch?v=cDF4X7RmV4Q)
- `Not attached`. In this mode, Oracle VM VirtualBox reports to the guest that a network card is present, but that there is no connection. This is as if no Ethernet cable was plugged into the card. 断网模式。

- `Network Address Translation (NAT)`. If all you want is to browse the Web, download files, and view email **inside** the guest, then this default mode should be sufficient for you. 从VM内部可以上网，如果外界或host要访问VM的服务，需要port forwarding. (host is used as a proxy for VM)

- `Host-only networking`. This can be used to create a network containing the host and a set of virtual machines, without the need for the host's physical network interface. Instead, a virtual network interface, similar to a loopback interface, is created on the host, providing connectivity among virtual machines and the host. 可以确保host和VMs 相互通信，但VM不能访问外界。

- `Bridged networking`. This is for more advanced networking needs, such as network simulations and running servers in a guest. When enabled, Oracle VM VirtualBox connects to one of your installed network cards and exchanges network packets directly, circumventing your host operating system's network stack. 这个是最全的访问模式，VM, host, 外界均可互访。Docker, Docker compose的默认网络就是这种类型。

- `Internal networking`. This can be used to create a different kind of software-based network which is visible to selected virtual machines, but not to applications running on the host or to the outside world. 只能选中的VMs之间互访。

- `NAT Network`. A NAT network is a type of internal network that allows outbound connections. 选中的VMs之间互访，加上outbound.]]></content>
      <categories>
        <category>Infra</category>
      </categories>
      <tags>
        <tag>infra</tag>
        <tag>vagrant</tag>
      </tags>
  </entry>
  <entry>
    <title>Java LinkedList</title>
    <url>/2019/11/07/java-linkedlist/</url>
    <content><![CDATA[
About LinkedList operation time complexity:

Adding to either end of a linked list does not require a traversal, as long as you keep a reference to both ends of the list. This is what Java does for its `add` and `addFirst`/`addLast` methods.

Same goes for parameterless `remove` and `removeFirst`/`removeLast` methods - they operate on list ends.

`remove(int)` and `remove(Object)` operations, on the other hand, are not O(1). They requires traversal, so their costs are O(n).
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>Inprogress</title>
    <url>/2019/01/01/inprogress/</url>
    <content><![CDATA[
**This is the blog incubator and TODO list**
保持记忆的熟练的唯一方式就是复习 和 刷题，特别是语言类。
最近在做一些pipeline的工作，需要快速上手，主要涉及到的:
Terraform
Python
Jenkins
gradle  (一种build tool, others like Blaze, Make)
Groovy  (JVM language, similar to Java, for declarative Jenkins pipeline)
现在不清楚的地方是各自文件的语法。
先从groovy，gradle入手，然后jenkins。
[x] 复习过去x天改动过的blog

# CI/CD
[x] travis.yml and inspiration: 大概能看出来是怎么回事了

# Envoy Migration
最近几天发现的缺失:
[x] [SSH config and proxy](https://app.pluralsight.com/library/courses/ssh-telnet-protocol-deep-dive/table-of-contents) config file, proxy jump, ssh-add.
[x] Gitlab, see linkedin learning, gitlab-ci and web dashboard general use.
[x] [Http header](https://app.pluralsight.com/channels/details/6f5b8c32-a2a9-472e-bac2-3f2fc198982b), CONNECT method, CRLF, browser debug
[x] [docker compose](https://app.pluralsight.com/library/courses/docker-web-development/table-of-contents) for testing, why not docker?
[x] [SSL/TLS](https://app.pluralsight.com/library/courses/linux-encryption-security-lpic-3-303/table-of-contents), cloud computing use cases, see udemy
ip route table vs iptables filter/nat table???
[x] curl flags and testing, pluralsight
[x] linux tips weekly, proxy sections
[x] Unix domain socket, websocket, co-routine?
[x] proxy, http proxy, reverse proxy?? ssl/tls?

[x] envoy tcp proxy and how to test? 设置好socket server, client，然后client connect to proxy port, in proxy, it driect to server address and port. 解决了！upstream cluster IP 没设置好。。对于docker 不是127.0.0.1!!
[x] for now, understand: http proxy (connect support), tcp proxy, socks proxy.
[x] shell parameter expansion
variable pattern match: `${1##*/}` `${USAGE%/*}` `${1:1:1}`, etc
[x] proxy的模拟测试环境 -> docker compose + flask + python other packages
[x] pagerduty, for on-call

[ ] socks how to setup in gcloud or aws --> VPN

[ ] ip/iptables, vagrant + envoy docker does not work, linkedin learning

[ ] network traffic school, internally! see my SRE week note.
[ ] 养老金扩展: https://www.transglobalus.com/zh/services_zh/

[ ] monit: is a utility for managing and monitoring processes, for example restart service when it cost too much memory and cpu

[x] [systemd init file](https://www.digitalocean.com/community/tutorials/understanding-systemd-units-and-unit-files)
```bash
## some fields
network-online.target
Environment=
OOMScoreAdjust=-1000 ~ 1000
WantedBy=multi-user.target
Type=forking | simple
```

[x] what are smoke & nightly testing
[x] blue/green deployment

[ ] Linux LPIC-1/2/3 [exams](https://www.lpi.org/)
[ ] Linux LFCE [training](https://app.pluralsight.com/paths/certificate/linux-foundation-certified-engineer-lfce)
[ ] CCNA Cisco network certifiate, see pluralsight my channel `network`

[ ] 重读how linux works book
[ ] rsyslogd, syslog services

[ ] Using golang write tcp, http server.
[ ] container runtime varies, search youtube
[ ] device42
[ ] tower
[ ] packer
[ ] jinja2 template syntax

[x] linux [screen command](https://linuxize.com/post/how-to-use-linux-screen/)
[ ] serverspec vs testinfra, infra instance testing tools
[ ] auditiong user account on linux, how?
[ ] command history fzf plugin
[ ] load average: http://www.brendangregg.com/blog/2017-08-08/linux-load-averages.html


# Python
[x] python click必须掌握，很重要！对于advanced script!! click echo color support: `click.echo(click.style('Some more text', bg='blue', fg='white'))`
这样可以为以后log中重点高亮
[x] python logging 更高级的用法

如果要建造一个package 哪些是必要的流程，以及测试?
[ ] setuptools understand
[ ] pytest
[ ] cookiecutter, creating new project
[ ] pip install -e . (on the fly editing and take effect)

# K8s
kubectl wait command:
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#port-forward

kubectl-images:
https://github.com/chenjiandongx/kubectl-images

kubectl krew:
https://github.com/kubernetes-sigs/krew

[ ] k8s is [deprecating docker in upcoming release](https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/). switch to other common docker runtime: containerd and CRI-O.
There are lots of options out there for this specific use case including things like `kaniko`, `img`, and `buildah`.


# Squid
这个对forward proxy支持挺好, long history proxy -_-|||, old
[ ] squid proxy


# Groovy
掌握基本data type, control structure, function definition 就可以了. 因为目前主要是用在Jenkins中。
和Java 通用的.

Groovy quick start:
https://www.youtube.com/watch?v=B98jc8hdu9g

Books, course and presentations, recommend:
https://groovy-lang.org/learn.html#books
O'relly course

http://groovy-lang.org/syntax.html
alternative java language

`DSL`: domain specific language
`sdkman` very good installation toolkit (also for gradle!)
```bash
## must have jdk installed first
yum install java-1.8.0-openjdk-devel -y
## specify version 2.5.2
sdk install groovy 2.5.2
```

Utilize vagrant VM to setup groovy runtime with SDKMAN, sync folder to work with VSC.
groovy closure:
https://groovy-lang.org/closures.html

```groovy
// each
def names = ["alice", "bob", "michael"];
// using closure
names.each { println it };
```
OOP, the class can be no constructor.

String interpolation:
https://groovy-lang.org/syntax.html#_string_interpolation


# Gradle
don't see latest book,  please refer to official doc
https://gradle.org/guides/
Ant and Maven are also build tools, gradle wrapper, incremental builds.
1. build by convention
2. Groovy DSL
3. supports dependencies
4. support mutli-project build
5. easily customizable

To install Gradle, using `sdkman` tool.

有几个概念搞清楚:
1. gradle wrapper -> gradlew (version control)
2. build.gradle: syntax, plugin
3. gradle.properties
4. extract env variable 

```groovy
apply plugin: 'java'

task {
  doLast {
    // groovy syntax
    println "hello, gradle!"
  }
}
```


# Git 
using use/passowrd instead of ssh
need to setup personal access credential:
https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line

for example,  mypersonal access token
`456f6facec85415b91588f581sfsdfssfe56c0`
I can embed the user/cred to push command:
https://stackoverflow.com/questions/29776439/username-and-password-in-command-for-git-push
```bash
git push 'https://chengdol%40ibm.com:456f6facec85415b91588f58182dd1cfe27e56c0@github.ibm.com/repo/hello.git'
```
use `%40` replace `@`.
`github.ibm.com/repo/hello.git` is the url to your repo

[x] git tag, see my blog
[ ] [github CLI](https://cli.github.com/): `gh`, very similar to `git` command, can make alias to it.

[ ] pluralsight git training in my channel
[ ] unfinished, [fork operation](https://app.pluralsight.com/course-player?clipId=ab0678e7-3784-4f9e-a118-d42fc36adf4d)

[ ] [gitlab on GKE lab](https://googlepluralsight-run.qwiklabs.com/focuses/12920544?parent=lti_session)

[ ] git commands more
git rewrite:
https://jonhnnyweslley.net/blog/how-to-rewrite-git-urls-to-clone-faster-and-push-safer/
https://paste.googleplex.com/6716960572178432
```
git diff --name-only --diff-filter=ACMR HEAD~1 HEAD
```

## deploy keys
https://github.blog/2015-06-16-read-only-deploy-keys/


# Gitbook
https://www.gitbook.com/
这个写东西还可以, but not free

# 承上启下
interpersonal skill, BQ
https://mail.google.com/mail/u/1/#inbox/FMfcgxwHNMbchDhFLQBnSltwkxBJVLbX

As an "intro" project, I would recommend deploying `Kubernetes using Terraform`. You can use any provider such as `GKE`, AKS or `EKS`.

Once the Terraform template is working, extend it to install some applications inside Kubernetes, such as `Vault, Elastic, Cassandra`, etc. using the `Helm and Helmfile`.

You may want to integrate the database applications inside Kubernetes with a `Vault instance such that they would use it for authentication backend`.

You can either build the Helm Charts yourself or use existing ones (perhaps you would want to modify them..)

Further, you can extend the application deployment to emit some metrics and setup monitoring using `Prometheus and Grafana`.

Once you are happy with the runtime stack, introduce some `CI/CD pipelines using Jenkins and/or Gitlab`.

现在能接触到的基础打好
k8s, docker, jenkins (groovy + gradle), ansible, helm
git -> gitlab (with jenkins)

- 常用工具:
vim
visual studio
bash (中文版教程可以), zsh

- composing database layer:
cassandra
elasticsearch (monitoring + log system)
consul
vault

- monitoring:
prometheus
grafana
kibana

- others:
kafka
zookeeper
istio
golang
RabbitMQ
redis
Python

load balancer: nginx, haproxy

- cloud 基本结构，应用，组件:
AWS, ok
GCP, ok
Azure

我猜工作流程:
用terraform 构造k8s 环境 或其他， 用ansible增加或更改配置，然后在k8s中部署应用(prometheus + grafana and elk for logs)，最后集成CI/CD
development -> QA/UAT - > production

## understand 
如果不清楚这些则看不懂配置的含义, 要清楚各个cloud中resource的类型，配置，如何搭配:
instance, vpc, lb, dns, storage, routing, subnet, gateway

terraform -> deploy k8s (EKS or GKS) -> install vault, elasticsearch, cassandra by helm (need modify though)
->
Prometheus
Grafana
Kibana
-> 
CI/CD: jenkins + gitlab

把完全不知道是什么的快速过一遍。
先学terraform -> elasticsearch -> vault, cassandra -> monitoring tools
packer -> gitlab, tower -> helmfile

## steps
1. terrform provision k8s on aws and gcp
2. k8s on fyre to test other components via helm
3. monitoring system: prometheus + grafana, deployed by operator
4. log system: elasticsearch, kibaba, logstash, beats

实验: 在k8s中部署prometheus + elastic stack, by helm or yaml，监控系统状态

5. Hashicorp: consul, vault
6. cassandra












]]></content>
      <categories>
        <category>InPorgress</category>
      </categories>
      <tags>
        <tag>inprogress</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Commands Alias</title>
    <url>/2019/06/15/k8s-alias/</url>
    <content><![CDATA[
It's very tedious to type full command when you operate on K8s cluster, this blog is the summary of alias I adopt in daily work.

Put these alias to `$HOME/.bashrc` or `$HOME/.zshrc`, then sourcing it or next time when you login they will take effect.

Some quick commands used to create resources:
```bash
## create nodeport svc to expose access
## --type default is ClusterIP
## --target-port default is --port
kubectl expose pod <podname> [--type=NodePort] --port=80 [--target-port=80] --name=<svcname>

## run command in pod
kubectl exec -n <namespace> <podname> -- sh -c "commands"

## check log
kubectl logs -f <pod name>

## scale up/down
kubectl scale deploy <deploy name> --replicas=5
```

These are some aliases used to run test:
```bash
## exit will delete the pod automatically
## --restart=Never: create a pod instead of deployment
## praqma/network-multitool: network tools image
alias kbt='kubectl run testpod -it --rm --restart=Never --image=praqma/network-multitool -- /bin/sh'
```

很多都是当时在IBM 时候的命令了，留着做个纪念吧.
```bash
## docker shortcut
alias di='docker images'
alias dp='docker ps -a'
alias dri='docker rmi -f'
alias drp='docker rm -f'

alias k='kubectl'
alias kbn='kubectl get nodes'
## can replace with your working namespace
## pods
alias kbp='kubectl get pods --all-namespaces'
## deployments
alias kbd='kubectl get deploy -n zen | grep -E "xmeta|services"'
## statefulsets
alias kbsts='kubectl get sts -n zen | grep -E "conductor|compute"'
## services
alias kbs='kubectl get svc -n test-1'


## get into pods
kbl()
{
  pod=$1
  ## get namepace
  ns=$(kubectl get pod --all-namespaces | grep $pod | awk {'print $1'})
  kubectl exec -it $pod sh -n $ns
}
### for fixed pod name
alias gocond='kubectl exec -it is-en-conductor-0 bash -n test-1'
alias gocomp0='kubectl exec -it is-engine-compute-0 bash -n test-1'
### for dynamic pod name
goxmeta()
{
  isxmetadockerpod=`kubectl get pods --field-selector=status.phase=Running -n test-1 | grep is-xmetadocker-pod | awk {'print $1'}`
  kubectl exec -it ${isxmetadockerpod} bash -n test-1
}

gosvc()
{
  isservicesdockerpod=`kubectl get pods --field-selector=status.phase=Running -n test-1 | grep is-servicesdocker-pod | awk {'print $1'}`
  kubectl exec -it ${isservicesdockerpod} bash -n test-1
}

## clean pods
alias rmxmeta='kubectl delete svc is-xmetadocker -n test-1; kubectl delete deploy is-xmetadocker-pod -n test-1; rm -rf /mnt/IIS_test-1/Repository/*'
alias rmsvc='kubectl delete svc is-servicesdocker -n test-1; kubectl delete deploy is-servicesdocker-pod -n test-1; rm -rf /mnt/IIS_test-1/Services/*'
alias rmcond='kubectl delete svc is-en-conductor-0 -n test-1; kubectl delete svc en-cond -n test-1; kubectl delete statefulset is-en-conductor -n test-1; rm -rf /mnt/IIS_test-1/Engine/test-1/is-en-conductor-0/'
alias rmcomp='kbc delete svc conductor-0 -n test-1; kbc delete statefulset is-engine-compute -n test-1; rm -rf /mnt/IIS_test-1/Engine/test-1/is-engine-compute*'
```
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes API curl Access</title>
    <url>/2019/11/06/k8s-api/</url>
    <content><![CDATA[
In the situation that issue k8s instructions from inside the container, usually we use `curl` command to do that (if you have kubectl binary in container's execution path, you can use kubectl command as well).

First you need credentials and api server information:
```bash
## MY_POD_NAMESPACE
NAMESPACE=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)
K8S=https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT
CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
```
You can get these all from environment variables, when create the pod, k8s has already injected these information into the containers.

Of course, if the service account you use does not have full privilege, the API access is limited.

then for example, get the detail of current pod:
```bash
POD_NAME="$MY_POD_NAME"
NS="$MY_POD_NAMESPACE"
OUT_FILE=$(mktemp /tmp/pod-schedule.XXXX)

## http_code is the return status code
http_code=$(curl -w  "%{http_code}" -sS  --cacert $CACERT  -H "Content-Type: application/json" -H "Accept: application/json, */*" -H "Authorization: Bearer $TOKEN" "$K8S/api/v1/namespaces/$NS/pods/$POD_NAME" -o $OUT_FILE)

if [[ $http_code -ne 200 ]]; then
    echo "{\"result\": \"Failure\", \"httpReturnCode\":$http_code}" |${JQ} '.'
    exit 1
fi
image=$(cat $OUT_FILE |jq '.spec.containers[] | select(.name=="xxx") | .image')
```

How do I know the curl path to request?
```bash
kubectl get pod -v 10
```
this will show you verbose message (curl under the hood), then you can get the path and use it in your curl command.

Not all kubectl commands are clearly with curl, for example `kubectl exec`, still need some efforts to know.

references:
https://blog.openshift.com/executing-commands-in-pods-using-k8s-api/
https://docs.okd.io/latest/rest_api/api/v1.Pod.html#Post-api-v1-namespaces-namespace-pods-name-exec
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Cert-manager</title>
    <url>/2021/11/30/k8s-cert-manager/</url>
    <content><![CDATA[
Cert-manager [git repo](https://github.com/jetstack/cert-manager), it is recommended to read through the [document](https://cert-manager.io/docs/).

## Install
Using [helm chart](https://cert-manager.io/docs/installation/helm/) to deploy cert-manager in K8s cluster in `cert-manager` namespace(customizing chart if needed), verifying deployment is good, see [here](https://cert-manager.io/docs/installation/verify/)


## Highlight Concepts
Issuers and ClusterIssuers:
https://cert-manager.io/docs/concepts/issuer/

Issuers configuration, especially ACME protocol as we use tarsier CA:
https://cert-manager.io/docs/configuration/

Certficiate resources:
https://cert-manager.io/docs/usage/certificate/


## Usage Case
We use Google tarsier CA and cert-manager to manage, renew certificate for ingresses, see [secure ingress resources](https://cert-manager.io/docs/usage/ingress/). Although it cannot directly work on Anthos MCI(multi-cluster ingress) but the workaround is simple by manually creating `certificate` to associate with target tls secret.

After deployment of cert-manager is done and run correctly, add cert-manager supported `annotations` to target ingress, for example:
```yaml
kind: Ingress
metadata:
  annotations:
    # cluster issuer is cluster scope and deployed by yourself
    cert-manager.io/cluster-issuer: example-cert
    cert-manager.io/duration: 2160h
    cert-manager.io/renew-before: 72h
```

Then cert-manager will automatically create `certificate` resource and start to issue certificate. Note that you can manually create this resource for some scenarios: tls secret is used by multiple ingresses(no need to add annotations to each ingress) or Anthos MCI(does not support cert-manager):
```yaml
apiVersion: cert-manager.io/v1
kind: Certificate
  name: example
  namespace: default
spec:
  dnsNames:
  # this is from ingress host name
  - '*.service.example.google'
  duration: 2160h0m0s
  issuerRef:
    group: cert-manager.io
    kind: ClusterIssuer
    name: example-cert
  renewBefore: 72h0m0s
  # this is from ingress tls name
  secretName: example-tls
```
The `spec.dnsNames` is Subject Alternative Name(SAN) that can have multiples, Common Name(CN) is derived from the first item. Note CN is discouraged from being used and deprecated, see [here](https://support.dnsimple.com/articles/what-is-common-name/#common-name-vs-subject-alternative-name)

So, this actually is a SAN certificate, not CN(common name) certificate(as mentioned [here](https://cert-manager.io/docs/usage/certificate/) CN field is deprecated).

Describing it you can examine the Dns names, condition and validation of the new certificate:
```yaml
Spec:
  Dns Names:
    *.service.example.google
  Duration:  2160h0m0s
  Issuer Ref:
    Group:       cert-manager.io
    Kind:        ClusterIssuer
    Name:        example-cert
  Renew Before:  72h0m0s
  Secret Name:   example-tls
Status:
  Conditions:
    Last Transition Time:  2021-10-08T19:18:51Z
    Message:               Certificate is up to date and has not expired
    Observed Generation:   1
    Reason:                Ready
    Status:                True
    Type:                  Ready
  Not After:               2022-01-06T18:18:43Z
  Not Before:              2021-10-08T18:18:44Z
  Renewal Time:            2022-01-03T18:18:43Z
  Revision:                1
```

The certificate and key are stored in K8s secret resource, once certificate issuing is done, cert-manager will manage this secret by adding specific annotations, for example:
```yaml
kind: Secret
metadata:
  annotations:
    cert-manager.io/alt-names: '*.service.example.google'
    cert-manager.io/certificate-name: example
    cert-manager.io/common-name: '*.service.example.google'
    cert-manager.io/ip-sans: ""
    cert-manager.io/issuer-group: cert-manager.io
    cert-manager.io/issuer-kind: ClusterIssuer
    cert-manager.io/issuer-name: example-cert
```

If no secret exists then the cert-manager will create the secret for you. Note that the old secret will be overridden every time new certificate is issued.

Also note that deletion of certificate will not delete associated secret.

To decode the certificate content, using `base64` and `openssl`, usually there are multiple-level certificates placed, from bottom to top(root CA, intermediate CA to certificate), select one to decode:
```bash
echo <secret encode block> | base64 -d
# select one CERTIFICATE block to decode
openssl x509 -in certificate.crt -text -noout
```
Or decoding online: https://www.sslshopper.com/certificate-decoder.html

In reverse, to encode certificate and used in secret:
```bash
# -w 0: git rid of new line, there may be a % chart at end, drop it.
cat certificate.crt | base64 -w 0
```]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>cert-manager</tag>
      </tags>
  </entry>
  <entry>
    <title>Container Command and Args</title>
    <url>/2019/09/10/k8s-cmd-args-format/</url>
    <content><![CDATA[
> Note, below 2 args formats are tested and worked for shell(sh/bash), other init process may not fit, for example `tini`, you have to use array format and one token a line.

The format of `command` and `args` syntax in kubernetes yaml really makes me crazy, when the entrypoint has long or multiple arguments, can be wrote in multiple lines instead of one line for better view:
```yaml
apiVersion: v1
kind: Pod
metadata:
  ...
spec:
  ...
  container:
  - name: busybox
    command: ["/bin/sh", "-c", "--"]
    args:
    - echo "start";
      while true; do
        echo $(hostname) $(date) >> /html/index.html;
        sleep 10;
      done;
      echo "done!";
```

Another format:
```yaml
apiVersion: v1
kind: Pod
metadata:
  ...
spec:
  containers:
  - name: myapp-container
    image: busybox
    command: ["/bin/sh", "-c", "--"]
    args:
       ["echo \"check something...\";
        if [[ ! -f /root/.link ]]; then
          echo \"file does not exist!\";
          echo \"no!\";
        else
          echo \"file is there!\";
        fi;
        echo \"done!\""]
```

The format is error-prone, every command should end with `;`, for example the `while` and `if` above. I separate them into several lines for good looking, but they actually are one line command.

For one command with multiple options or parameters, can be described as below, no `args` field:
```yaml
  containers:
  - command:
    - /usr/local/bin/etcd
    - --data-dir=/var/etcd/data
    - --name=example-etcd-cluster-65zvs2mt8b
    - --initial-advertise-peer-urls=http://example-etcd-cluster-65zvs2mt8b.example-etcd-cluster.default.svc:2380
    - --listen-peer-urls=http://0.0.0.0:2380
    - --listen-client-urls=http://0.0.0.0:2379
    ...
```

]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Pod/Container Memory/CPU Usage</title>
    <url>/2021/09/19/k8s-container-mem-cpu/</url>
    <content><![CDATA[
`top` command ran in container within the pod shows the host machine overview metrics and container level process metrics. The reason is containers inside pod partially share `/proc` with the host system includes path about a memory and CPU information. The `top` utilizes `/proc/stat`(host machine), `/proc/<pid>/stat`(container process), they are not aware of the `namespace`. 

P.S: [lxcfs](https://github.com/lxc/lxcfs) this FUSE filesystem can create container native `/proc`! Make container more likes a VM.

The two methods below collect data from **different** sources and they are also referring to **different** metrics.
- [k8s pod cpu and memory](https://stackoverflow.com/questions/54531646/checking-kubernetes-pod-cpu-and-memory).
- [kubectl top vs docker stats](https://www.ibm.com/support/pages/kubectl-top-pods-and-docker-stats-show-different-memory-statistics)

For k8s OOMKiller event, using `kubectl top` to predicate and track is more accurate.

# Kubectl Top
K8s OOMkiller uses `container_memory_working_set_bytes`(from **cadviosr** metrics, can also show in prometheus if deployed) as base line to decide the pod kill or not. It is an estimate of how much memory cannot be evicted, the `kubectl top` uses this metrics as well.

After metrics-server is installed:
```bash
# show all containers resource usage insdie a pod
kubectl top pod <pod name> --containers
# show pod resource usage
kubectl top pod
# show node resource usage
kubectl top node
```
In prometheus expression browser, you can get the same value as `kubectl top`:
```bash
# value in Mib
# pod, container are the label name, depends on your case
container_memory_working_set_bytes{pod=~"<pod name>",container=~"<container name>"} / 1024 / 1024
```

> 看看Prom alert是从何处取得 container/pod memory 数据的，注意数据源是来自哪个 metrics，以及使用 grafana 也是如此.


# Docker Stats
`docker stats` memory display collects data from path `/sys/fs/cgroup/memory` with some calculations, see below explanation.

On host machine, display the container stats (CPU and Memory usages)
```bash
# similar to top
docker stats --no-stream <container id>
```

Actually docker CLIs fetch data from [Docker API](https://docs.docker.com/engine/api/), for instance `v1.41` (run `docker version` to know API supported verion), you can get stats data by using curl command:
```bash
curl --unix-socket /var/run/docker.sock "http:/v1.41/containers/<container id>/stats?stream=false" | jq
```
```js
  "memory_stats": {
    "usage": 80388096,
    "max_usage": 148967424,
    "stats": {
      "active_anon": 9678848,
      "active_file": 13766656,
      "cache": 31219712,
      "inactive_file": 17829888
      ...
      "total_inactive_file": 17829888,
```

From this [docker stats description](https://docs.docker.com/engine/reference/commandline/stats/):
On Linux, the Docker CLI reports memory usage by subtracting cache usage from the total memory usage. The API does not perform such a calculation but rather provides the total memory usage and the amount from the cache so that clients can use the data as needed. The cache usage is defined as the value of `total_inactive_file` field in the `memory.stat` file on cgroup v1 hosts.

On Docker `19.03` and older, the cache usage was defined as the value of `cache` field. On cgroup v2 hosts, the cache usage is defined as the value of `inactive_file` field.

memory_stats.usage is from `/sys/fs/cgroup/memory/memory.usage_in_bytes`.
memory_stats.stats.inactive_file is from `/sys/fs/cgroup/memory/memory.stat`.

So here it is:
```bash
80388096 - 17829888 = 62558208 => 59.66s Mib
```
This does perfectly match `docker stats` value in `MEM USAGE` column.

> The `dockershim` is deprecated in k8s!! If `containerd` runtime is used instead, to explore metrics usage you can check `cgroup` in host machine or go into container check `/sys/fs/cgroup/cpu`.

To calculate the container memory usage as `docker stats` in the pod without installing third party tool:
```bash
# memory in Mib: used
cd /sys/fs/cgroup/memory
cat memory.usage_in_bytes | numfmt --to=iec

# memory in Mib: used - inactive(cache)
cd /sys/fs/cgroup/memory
used=$(cat memory.usage_in_bytes)
inactive=$(grep -w inactive_file memory.stat | awk {'print $2'})
# numfmt: readable format
echo $(($used-$inactive)) | numfmt --to=iec
```

To calculate the container cpu usage as `docker stats` in the pod without installing third party tool:
```bash
# cpu, cpuacct dir are softlinks
cd /sys/fs/cgroup/cpu,cpuacct
# cpuacct.stat:
# Reports the total CPU time in nanoseconds 
# spent in user and system mode by all tasks in the cgroup.
utime_start=$(cat cpuacct.stat| grep user | awk '{print $2}')
stime_start=$(cat cpuacct.stat| grep system | awk '{print $2}')
sleep 1
utime_end=$(cat cpuacct.stat| grep user | awk '{print $2}')
stime_end=$(cat cpuacct.stat| grep system | awk '{print $2}')
# getconf CLK_TCK aka sysconf(_SC_CLK_TCK) returns USER_HZ 
# aka CLOCKS_PER_SEC which seems to be always 
# 100 independent of the kernel configuration.
HZ=$(getconf CLK_TCK)
# get container cpu usage
# on top of user/system cpu time
echo $(( (utime_end+stime_end-utime_start-stime_start)*100/HZ/1 )) "%"
# if the outcome is 200%, means 2 cpu usage, so on and so forth
```

# Readings
- [How much is too much? The Linux OOMKiller and “used” memory](https://faun.pub/how-much-is-too-much-the-linux-oomkiller-and-used-memory-d32186f29c9d)
We can see from this experiment that `container_memory_usage_bytes` does account for some filesystem pages that are being cached. We can also see that OOMKiller is tracking `container_memory_working_set_bytes`. This makes sense as shared filesystem cache pages can be evicted from memory at any time. There’s no point in killing the process just for using disk I/O.

- [Kubernetes top vs Linux top](https://stackoverflow.com/questions/51641310/kubernetes-top-vs-linux-top)
kubectl top shows metrics for a given pod. That information is based on reports from `cAdvisor`, which collects real pods resource usage.

- [cAdvisor: container advisor](https://github.com/google/cadvisor)
cAdvisor (Container Advisor, go project) provides container users an understanding of the resource usage and performance characteristics of their running containers.]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Dashboard</title>
    <url>/2019/09/15/k8s-dashboard/</url>
    <content><![CDATA[
//TODO:
[ ] how to setup dashboard can be accessed from outside of the cluster
[ ] cert things?
[ ] user admission, for example: cluster admin vs regular user
[ ] authenticate method: token or kubeconfig

**Kubernetes version 1.13.2**

Dashboard relies on Metrics Server or Heapster to sample and provide running parameters of the cluster.
https://github.com/kubernetes/dashboard

Metrics server:
https://github.com/kubernetes-incubator/metrics-server
other tools:
https://blog.containership.io/kubernetes-metrics-collection-options/

]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes NodePort vs LoadBalancer vs Ingress</title>
    <url>/2019/03/26/k8s-external-access/</url>
    <content><![CDATA[
**Kubernetes version 1.13.2**

This [blog](https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0) talks about the ways that provide external access to kubernetes cluster and their difference.

This [blog](https://www.nginx.com/blog/wait-which-nginx-ingress-controller-kubernetes-am-i-using/) is about `NGINX` ingress controller.

I need to do further demo and research on them.
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>ingress</tag>
      </tags>
  </entry>
  <entry>
    <title>External Provisioner</title>
    <url>/2019/12/10/k8s-external-provisioner/</url>
    <content><![CDATA[
Github [external provisioner](https://github.com/kubernetes-incubator/external-storage).

The external provisioner can be backed by  many type of filesystem, here we focus on `nfs-client`.

> Notice that in this project, you will see `nfs-client` and `nfs` directories, `nfs-client` means we already has a nfs server and use it on client. `nfs` means we don't have a nfs server, but we share other filesystem in nfs way.

Another tool very similar is [Rook](https://rook.io/), see my blog [`Rook Storage Orchestrator`](https://chengdol.github.io/2020/01/20/k8s-rook/).

The `Rook` is more heavy and this project is lightweight.

1. Setup NFS server
2. Install nfs-utils on all worker nodes
See my blog [NFS Server and Client Setup](https://chengdol.github.io/2019/05/27/fs-nfs-setup/), this `Server Setup` chapter.

3. Setup NFS provisioner
This blog give me clear instruction on how to adopt the `nfs-client` provisioner: [openshift dynamic NFS persistent volume using NFS-client-provisioner](https://medium.com/faun/openshift-dynamic-nfs-persistent-volume-using-nfs-client-provisioner-fcbb8c9344e).

**Notces**:
1. I find a bug in this project, when set rbac, you need to specify `-n test-1`, otherwise the role was created in `test-1` but rolebinding is created in default namespace.

2. The NFS provisioner is global scoped.

2. The `NFS_SERVER` env in deployment.yaml can be hostname or IP address.

3. If several pods use the same PVC, they share the same PV.

4. you can customize the storage class if it's available. For example, set the reclaim policy as `retain` instead of `delete`. see [doc](https://kubernetes.io/docs/concepts/storage/storage-classes/).
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
## default is delete
reclaimPolicy: Retain
## allow resize the volume by editing the corresponding PVC object
## cannot shrink
allowVolumeExpansion: true
volumeBindingMode: Immediate
```
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>external provisioner</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes CRD Deletion Hang</title>
    <url>/2022/11/21/k8s-crd-deletion-hang/</url>
    <content><![CDATA[
It is possible that the deletion of k8s
[CRD](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/)
hang for indefinite time due to dependency issue.

If it happens, manually edit the CRD and empty the finalizers value block:
```yaml
  finalizers:
  - xxx.finalizer.networking.gke.io
```

For programmatic way, what I did is running `delete` kubectl command in
background and followed by a kubectl `patch` command to remove the finalizer
block, for example the `patch` command:
```bash
kubectl path <crd type name> <crd resource name> \
{"metadata":{"finalizers":[]}} \
--type merge
```

The background `delete` command can be forcely killed at end or after some
period of time if it does not fulfill.
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>CRD</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Feature Gates</title>
    <url>/2019/08/27/k8s-feature-gates/</url>
    <content><![CDATA[
**Kubernetes version 1.13.2**

Sometimes you will find the demo run in your kubernetes cluster doesn't work exactly or doesn't fit your expectation from official documentation. That maybe a feature gate you didn't enable it or you don't have it.

See this issue, I encountered the exactly the same:
https://github.com/coreos/prometheus-operator/issues/2439

This is the [Feature Gates link](https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/).]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Calico Bandwidth Explore</title>
    <url>/2019/07/30/k8s-calico-bandwidth/</url>
    <content><![CDATA[
**Kubernetes version 1.13.2**

I have talked about setting up pod network in Kubernetes cluster using `Calico` network add-on in this post [`<<Set Up K8s Cluster by Kubeadm>>`](https://chengdol.github.io/2018/12/17/k8s-kubeadm-setup/). Recently I was involved in one issue from performance team, they complained that the network has bottleneck in calico, let's see what happened and learn new things!

## Description
The performance team set up a 6 nodes cluster with 1 master and 5 workers. Each machine has 48 cpu cores, 128GB memory, 2T+ disk and 10000Mb/s network speed.

These are the test cases:
```yaml
10 jobs for each user(8 small jobs, 1 middle job and 1 large job)

8 small jobs(RowGen -> XX -> Peek), 1 job unit
aggregate job                         -- 10 millions records (neally 2.2G)
filter job                            -- 10 millions records (neally 2.2G)
funnel_continuous                     -- 10 millions records (neally 2.2G)
funnel sort                           -- 10 millions records (neally 2.2G)
join job                              -- 10 millions records (neally 2.2G)
lookup                                -- 10 millions records (neally 2.2G)
sort job                              -- 10 millions records (neally 2.2G)
Transformation                        -- 10 millions records (neally 2.2G)

1 middle job, 2 job units
Middle_Job                            -- 50 million records(nearly 11G)

1 Large job, 3 job units
Large job                             -- 10 million records(nearly 2.1GB)
```
They ran concurrent users with N compute pods and found that the bottleneck is in calico network:
![](https://drive.google.com/uc?id=1FMc1eRf_FCG2TI1TRWeLLKE2GJ6qysBf)

BTY, there are still enough resources(CPU, Memory and Disk I/O) to support DataStage scale up for DS concurrent users to run jobs on nodes and pods. But the network bandwidth between pods is not enough to support it.

## iperf3 Command
They use [`iperf3`](https://iperf.fr/), a TCP, UDP, and SCTP network throughput measurement tool, measure memory-to-memory performance access a network, server-client mode.

To install, for example in Centos:
```bash
sudo yum install iperf3 -y
```

The [Usage](https://fasterdata.es.net/performance-testing/network-troubleshooting-tools/iperf/) is simple.
More simple [demos](https://datapacket.com/blog/10gbps-network-bandwidth-test-iperf-tutorial/)

### Node to Node
For command options and flags see [user guide](https://iperf.fr/iperf-doc.php).

This will transfer /large_file in client to /large_file in server, time interval is 40 seconds.
```bash
## server
# -s: server
# -F: read data from network and write to the file
iperf3 -s -F /large_file
## client
# -c: client mode connect to server ip
# -i 1: 1 sec report
# -t 40: transfer time 40 sec
# -F: read from file and write to network
iperf3 -c <server ip> -i 1 -t 40 -F /large_file
```

UDP traffic benchmark:
```bash
## server
# -s: server
# -f M: format Mbytes/sec
# -V: verbose
# -p 5999: port 
iperf3 -s -f M -V -p 5999

## client
# -p 5999: port 
# -c: client mode connect to server ip
# -u: udp
# -b 24M: bandwidth 24Mbits/sec -> 3Mbytes/sec
# -t 600: time 600 sec
# -l 300: packet len 300 bytes
# so IP packet rate is around 3Mbytes/300bytes = 10K/sec
iperf3 -p 5999 -c 10.19.1.15 -u -b 24M -t 600 -l 300
```

### Pod to Pod
The same as `Node to Node`, but wget and build `iperf3` inside pod container and use the container's IP (in container run `hostname -I`), for example, I flood data from `is-en-conductor-0` pod to `is-engine-compute-12` pod, they reside in different host machine.
![](https://drive.google.com/uc?id=1WPt2oBzL0bvq1_vCzF9eUqZXiwNfNzOX)
![](https://drive.google.com/uc?id=1Y8nbhUAR0BBbdEcFeNqSG9g2xk8UjyEo)

## Thinking
After I reproducing the tests, I was thinking `Calico` is a widely used add-on that shouldn't have such obvious bottleneck, otherwise many people will complain and improve it.

Is there any improper configuration?
* Configuring IP-in-IP
  By default, the manifests enable `IP-in-IP` encapsulation across subnets (additional overhead compare to non `IP-in-IP`), if don't need it (when? I am not very clear), disable it in calico manifest yaml file:
  ```yaml
  # Enable IPIP
  - name: CALICO_IPV4POOL_IPIP
          value: "off"
  ```
  See this [`IP-in-IP` issue](https://github.com/projectcalico/calico/issues/922)
  I am using `Calico` version 3.3, [document](https://docs.projectcalico.org/v3.3/usage/configuration/ip-in-ip) about `IP-in-IP`

* Which Network Interface is Used
  Another question I have is which network interface is used for `node-to-node` and `pod-to-pod` test?

  There are several network interfaces in host machine, one is for public IP with `MTU 9000` and in K8s we use private IP interface with `MTU 1500`. This will have impact on `iperf3` testing.

  It shows that `pod-to-pod` test uses `MTU 1500` but `node-to-node` uses `MTU 9000`.
  
  Need to test after enlarging MTU size to see if that change improves network throughput, also remember to update `Calico` manifest yaml, refer this [document](https://docs.projectcalico.org/v3.3/usage/configuration/mtu)
  ```yaml
  # Configure the MTU to use
  veth_mtu: "9000"
  ```

## ethtool Command
The `ethtool` can display speed property of the network interface, for example:
```yaml
# ethtool eno1

Settings for eno1:
        Supported ports: [ TP ]
        Supported link modes:   10baseT/Half 10baseT/Full
                                100baseT/Half 100baseT/Full
                                1000baseT/Half 1000baseT/Full
        Supported pause frame use: No
        Supports auto-negotiation: Yes
        Supported FEC modes: Not reported
        Advertised link modes:  10baseT/Half 10baseT/Full
                                100baseT/Half 100baseT/Full
                                1000baseT/Half 1000baseT/Full
        Advertised pause frame use: Symmetric
        Advertised auto-negotiation: Yes
        Advertised FEC modes: Not reported
        Link partner advertised link modes:  10baseT/Full
                                             100baseT/Full
                                             1000baseT/Full
        Link partner advertised pause frame use: Transmit-only
        Link partner advertised auto-negotiation: Yes
        Link partner advertised FEC modes: Not reported
        Speed: 1000Mb/s
        Duplex: Full
        Port: Twisted Pair
        PHYAD: 1
        Transceiver: internal
        Auto-negotiation: on
        MDI-X: on
        Supports Wake-on: g
        Wake-on: d
        Current message level: 0x000000ff (255)
                               drv probe link timer ifdown ifup rx_err tx_err
        Link detected: yes
```
The output depends on the what the network driver can provide, you may get nothing if the **virtual machine** does not have much data available (for example, in IBM softlayer cluster), refer to this [question](https://serverfault.com/questions/447939/why-is-ethtool-not-showing-me-all-the-properties-for-a-nic) 

In a virtual machine the link speed or duplex mode is usually meaningless, as the network interface is most often just a **virtual link** to the host system, with no actual physical Ethernet layer. The speed is as high as the CPU and memory can handle (or as low, as the connection rate limit is configured), cabling type does no exist, as there is no cable, etc. This virtual interface is bridged or routed to the actual physical network by the host system and only on the host system the physical port parameters can be obtained.

you can check if the network interface is virtual or not:
```bash
ls -l /sys/class/net

# output example
# both virtual: virtio1
lrwxrwxrwx. 1 root 0 Apr 27 16:11 eth0 -> ../../devices/pci0000:00/0000:00:04.0/virtio1/net/eth0
lrwxrwxrwx. 1 root 0 Apr 27 16:11 lo -> ../../devices/virtual/net/lo
```

## Summary
1. Performance test big picture
2. iperf3, node-to-node, pod-to-pod tests
3. ethtool
3. Calico configutation: IP-in-IP, MTU
4. calicoctl, haven't got time to learn

## Other Blogs
[k8s calico flannel cilium 网络性能测试](https://www.jianshu.com/p/cfc4e62ff3ea)
[Benchmark results of Kubernetes network plugins (CNI) over 10Gbit/s network](https://itnext.io/benchmark-results-of-kubernetes-network-plugins-cni-over-10gbit-s-network-36475925a560)
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>calico</tag>
      </tags>
  </entry>
  <entry>
    <title>Force Delete StatefulSet and Pods</title>
    <url>/2019/02/16/k8s-force-delete-statefulset/</url>
    <content><![CDATA[
**Kubernetes version 1.13.2**

## Issue description
I encountered the problem that when delete `statefulset` the execution hangs, for example:
```bash
kubectl delete statefulset is-en-conductor
```

it cannot delete the `statefulset` and associated `pods` at all
```bash
[root@opsft-lb-1 OpenShift] oc get all
NAME                             DESIRED   CURRENT   AGE
statefulsets/is-en-conductor     0         1         5h
statefulsets/is-engine-compute   0         2         5h

NAME                     READY     STATUS        RESTARTS   AGE
po/is-en-conductor-0     0/1       Terminating   2          1h
po/is-engine-compute-0   1/1       Running       0          5h
po/is-engine-compute-1   0/1       Terminating   0          5h
```

## Solution
```bash
kubectl delete statefulsets <statefulset name> --force --grace-period=0 --cascade=false
```

now we only have hanging pods, force delete them
```bash
NAME                     READY     STATUS        RESTARTS   AGE
po/is-en-conductor-0     0/1       Terminating   2          1h
po/is-engine-compute-0   1/1       Running       0          5h
po/is-engine-compute-1   0/1       Terminating   0          5h
```

```bash
kubectl delete pod <pod name> --force --grace-period=0
```]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>statefulset</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes CLI Tips</title>
    <url>/2022/08/20/k8s-cli-tips/</url>
    <content><![CDATA[
Some useful CLI tips in daily uses, references:
- https://learncloudnative.com/blog/2022-05-10-kubectl-tips

### Explain resources with specific fields.
This can help you explore the complete resource definition to find all available
fields.
```bash
k explain ep
k explain pod.spec
k explain deploy.metadata.labels
k explain BackendConfig.spec # For GKE ingress
```

###  Create manual job from cronjob, you can also output as json template, edit
and use it.
```bash
k create job --from=cronjob/<name of cronjob> <manual job name>
```

### Check service account/regular user permission.
```bash
# get sa name
k get sa

k -n <namespace> auth can-i \
--list \
--as system:serviceaccount:<namespace>:<service account name>
```

To check what you can do:
```bash
k auth can-i --list
```


### Force delete pods with no grace period
This only works on pod resource:

```bash
k delete pod xxx --grace-period=0 --force
```


### List endpoints(pod IP:port).
```bash
k get ep -n <namespace>
```

### List events sorted by lastTimestamp.
```bash
# lastTimestamp is added by k8s in resource definition yaml
k get events --sort-by=".lastTimestamp"
```

### Watching from events.
```bash
k get events -w --field-selector=type=Warning -A

# watch events for specific container
# The "involvedObject.fieldPath" is JSON return of events, see
# https://stackoverflow.com/questions/51931113/kubectl-get-events-only-for-a-pod
k get events \
--field-selector involvedObject.fieldPath="spec.containers{<container name>}" \
--sort-by=".lastTimestamp"
```

### Get raw json for APIs.
```bash
k get --raw /apis/apps/v1
# Get metrics
k get --raw /metrics
```

### Wait for pods to be ready.
```bash
k wait --for=condition=ready pod -l foo=bar
```

### List customer env vars from a resource.
```bash
k set env <resource>/<resource-name> --list
```

### List node and its pods mapping.
```bash
# column:value mapping:
# NODE:.spec.nodeName
k get po -o=custom-columns=NODE:.spec.nodeName,NAME:.metadata.name
```

### Create starter YAML for resources.
```bash
kubectl create deploy anyname --image=nginx --dry-run=client -o yaml
```

### Pods sorted by memory usage.
```bash
kubectl top pods -A --sort-by='memory'
```

### To examine logs of previous restarted pod/container:
```bash
# Other useful options:
# -f: streaming logs
# --tail: number of line to display
# --since: return logs newer then a relative duration 5s, 2m, 3h
kubectl logs <pod name> -c <container name> --previous
# if no -c specified, return first container log by default

# --tail 5: only show last 5 lines
kubectl logs <pod name> -c <container name> -f --tail=5
```

### Aggregate(tail/follow) logs from multiple pods into one stream.
[kubetail](https://github.com/johanhaleby/kubetail) is written by bash so can be
used without installing other dependencies, git clone and put the executable to
`$PATH`.
```bash
# Tail all pods from a deployment/sts.
kubetail <deployment name>
# Tail specific container from deploy/sts.
kubetail <deploy name> -c container1 -c container2
# Tail with regex matching.
kubetail "^app1|.*my-demo.*" --regex
```

### Rolling out/back resources
Better than editing resource manually with `kubectl edit` cmd.
The [cheat sheet](https://kubernetes.io/docs/reference/kubectl/cheatsheet/#updating-resources)
```bash
# rolling update with new image
# foo1 is the container name
k set image deploy/foo foo1=image:v2  

# Check the history of deployments including the revision
k rollout history deploy/foo

# Rollback to the previous deployment or specific version
# Undo twice turns it back to the original: 1->2->1 
k rollout undo deploy/foo

# Watch rolling update status until completion
k rollout status -w deploy/foo
# Rolling restart of the "foo" deployment
k rollout restart deploy/foo 
```

How to roll out back to a specific version:
```bash
# first check current version number
k get deploy/foo -o yaml | grep revision

# check rollout history and the revision detail
k rollout history deploy/foo
k rollout history deploy/foo --revision=13

# rollout to the target version
k rollout undo deploy/foo --to-revision=13
```]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Completely Delete K8s Cluster</title>
    <url>/2019/03/02/k8s-delete-cluster/</url>
    <content><![CDATA[
**Kubernetes version 1.13.2**

Today I spend some time to investigate how to remove nodes from the k8s cluster that built by `kubeadm`.

For example, I have a 3 nodes cluster called `k8stest`, I deploy the application in `namespace` `test-1`, each worker node (`k8stest2` and `k8stest3`) holds some pods:
```bash
kubectl get pods -n test-1 -o wide

NAME                                     READY   STATUS    RESTARTS   AGE     IP            NODE                    NOMINATED NODE   READINESS GATES
is-en-conductor-0                        1/1     Running   0          5h40m   192.168.1.2   k8stest3.fyre.ibm.com   <none>           <none>
is-engine-compute-0                      1/1     Running   0          5h39m   192.168.1.3   k8stest3.fyre.ibm.com   <none>           <none>
is-engine-compute-1                      1/1     Running   0          5h38m   192.168.2.4   k8stest2.fyre.ibm.com   <none>           <none>
is-servicesdocker-pod-7b4d9d5c48-vvfn6   1/1     Running   0          5h41m   192.168.2.3   k8stest2.fyre.ibm.com   <none>           <none>
is-xmetadocker-pod-5ff59fff46-tkmqn      1/1     Running   0          5h42m   192.168.2.2   k8stest2.fyre.ibm.com   <none>           <none>

```

### Drain and delete worker nodes
You can use `kubectl drain` to safely evict all of your pods from a node before you perform maintenance on the node (e.g. kernel upgrade, hardware maintenance, etc.). Safe evictions allow the pod’s containers to [gracefully terminate](https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods) and will respect the `PodDisruptionBudgets` you have specified.

The `drain` evicts or deletes all pods except mirror pods (which cannot be deleted through the API server). If there are DaemonSet-managed pods, drain will not proceed without `--ignore-daemonsets`, and regardless it will not delete any DaemonSet-managed pods, because those pods would be immediately replaced by the DaemonSet controller, which ignores unschedulable markings. If there are any pods that are neither mirror pods nor managed by `ReplicationController`, `ReplicaSet`, `DaemonSet`, `StatefulSet` or `Job`, then drain will not delete any pods unless you use `--force`. `--force` will also allow deletion to proceed if the managing resource of one or more pods is missing.

Let's first drain `k8stest2`:
```bash
kubectl drain k8stest2.fyre.ibm.com --delete-local-data --force --ignore-daemonsets

node/k8stest2.fyre.ibm.com cordoned
WARNING: Ignoring DaemonSet-managed pods: calico-node-txjpn, kube-proxy-52njn
pod/is-engine-compute-1 evicted
pod/is-xmetadocker-pod-5ff59fff46-tkmqn evicted
pod/is-servicesdocker-pod-7b4d9d5c48-vvfn6 evicted
node/k8stest2.fyre.ibm.com evicted
```
When `kubectl drain` returns successfully, that indicates that all of the pods (except the ones excluded as described in the previous paragraph) have been safely evicted (respecting the desired graceful termination period, and without violating any application-level disruption SLOs). It is then safe to bring down the node by powering down its physical machine or, if running on a cloud platform, deleting its virtual machine.

Let's ssh to `k8stest2` node and see what happens here, the payloads were gone:
```bash
ssh k8stest2.fyre.ibm.com
docker ps

CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS               NAMES
0fbbb64d93d0        fa6f35a1c14d           "/install-cni.sh"        6 hours ago         Up 6 hours                              k8s_install-cni_calico-node-txjpn_kube-system_4b916269-3d49-11e9-b6b3-00163e01eecc_0
b78013d4f454        427a0694c75c           "start_runit"            6 hours ago         Up 6 hours                              k8s_calico-node_calico-node-txjpn_kube-system_4b916269-3d49-11e9-b6b3-00163e01eecc_0
c6aaf7cbf713        01cfa56edcfc           "/usr/local/bin/kube..."   6 hours ago         Up 6 hours                              k8s_kube-proxy_kube-proxy-52njn_kube-system_4b944a11-3d49-11e9-b6b3-00163e01eecc_0
542bc4662ee4        k8s.gcr.io/pause:3.1   "/pause"                 6 hours ago         Up 6 hours                              k8s_POD_calico-node-txjpn_kube-system_4b916269-3d49-11e9-b6b3-00163e01eecc_0
86ee508f0aa1        k8s.gcr.io/pause:3.1   "/pause"                 6 hours ago         Up 6 hours                              k8s_POD_kube-proxy-52njn_kube-system_4b944a11-3d49-11e9-b6b3-00163e01eecc_0

```

The given node will be marked `unschedulable` to prevent new pods from arriving.
```bash
kubectl get nodes

NAME                    STATUS                     ROLES    AGE     VERSION
k8stest1.fyre.ibm.com   Ready                      master   6h11m   v1.13.2
k8stest2.fyre.ibm.com   Ready,SchedulingDisabled   <none>   5h57m   v1.13.2
k8stest3.fyre.ibm.com   Ready                      <none>   5h57m   v1.13.2
```

Because the dedicated node `k8stest2` was drained, so `is-servicesdocker` and `is-xmetadocker` keep pending:
```bash
NAME                                     READY   STATUS    RESTARTS   AGE     IP            NODE                    NOMINATED NODE   READINESS GATES
is-en-conductor-0                        1/1     Running   0          6h3m    192.168.1.2   k8stest3.fyre.ibm.com   <none>           <none>
is-engine-compute-0                      1/1     Running   0          6h2m    192.168.1.3   k8stest3.fyre.ibm.com   <none>           <none>
is-engine-compute-1                      1/1     Running   0          9m26s   192.168.1.4   k8stest3.fyre.ibm.com   <none>           <none>
is-servicesdocker-pod-7b4d9d5c48-vz7x4   0/1     Pending   0          9m39s   <none>        <none>                  <none>           <none>
is-xmetadocker-pod-5ff59fff46-m4xj2      0/1     Pending   0          9m39s   <none>        <none>                  <none>           <none>

```
Now it's safe to delete node:
```bash
kubectl delete node k8stest2.fyre.ibm.com

node "k8stest2.fyre.ibm.com" deleted
```
```bash
kubectl get nodes

NAME                    STATUS   ROLES    AGE     VERSION
k8stest1.fyre.ibm.com   Ready    master   6h22m   v1.13.2
k8stest3.fyre.ibm.com   Ready    <none>   6h8m    v1.13.2
```
Repeat the steps above for worker node `k8stest3` then only master node survives:
```bash
kubectl get nodes

NAME                    STATUS   ROLES    AGE     VERSION
k8stest1.fyre.ibm.com   Ready    master   6h25m   v1.13.2
```

### Drain master node
It's time to deal with master node:
```bash
kubectl drain k8stest1.fyre.ibm.com --delete-local-data --force --ignore-daemonsets

node/k8stest1.fyre.ibm.com cordoned
WARNING: Ignoring DaemonSet-managed pods: calico-node-vlqh5, kube-proxy-5tfgr
pod/docker-registry-85577757d5-952wq evicted
pod/coredns-86c58d9df4-kwjr8 evicted
pod/coredns-86c58d9df4-4p7g2 evicted
node/k8stest1.fyre.ibm.com evicted
```
Let's see what happens for infrastructure pods, some of them were gone:
```bash
kubectl get pods -n kube-system

NAME                                            READY   STATUS    RESTARTS   AGE
calico-node-vlqh5                               2/2     Running   0          6h31m
coredns-86c58d9df4-5ctw2                        0/1     Pending   0          2m15s
coredns-86c58d9df4-mg8rf                        0/1     Pending   0          2m15s
etcd-k8stest1.fyre.ibm.com                      1/1     Running   0          6h31m
kube-apiserver-k8stest1.fyre.ibm.com            1/1     Running   0          6h31m
kube-controller-manager-k8stest1.fyre.ibm.com   1/1     Running   0          6h30m
kube-proxy-5tfgr                                1/1     Running   0          6h31m
kube-scheduler-k8stest1.fyre.ibm.com            1/1     Running   0          6h31m
```
Note that don't do delete for master node.

### Reset cluster
Run this in every node to revert any changes made  by `kubeadm init` or `kubeadm join`:
```bash
kubeadm reset -f
```
All container were gone and also check if `kubectl` still works?
```bash
docker ps

CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
```
```bash
kubectl get nodes

The connection to the server 9.30.219.224:6443 was refused - did you specify the right host or port?
```

### Delete rpm and files
Finally, we need to delete rpms and remove residue in every node:
```bash
yum erase -y kubeadm.x86_64 kubectl.x86_64 kubelet.x86_64 kubernetes-cni.x86_64 cri-tools socat

```
```bash
## calico
/bin/rm -rf /opt/cni/bin/*
/bin/rm -rf /var/lib/calico
/bin/rm -rf /run/calico
## config
/bin/rm -rf /root/.kube
## etcd
/bin/rm -rf /var/lib/etcd/*
## kubernetes
/bin/rm -rf /etc/kubernetes/
```
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>kubeadm</tag>
        <tag>drain</tag>
      </tags>
  </entry>
  <entry>
    <title>Init Container</title>
    <url>/2019/06/10/k8s-initContainer/</url>
    <content><![CDATA[
**Kubernetes version 1.13.2**

In ICP4D non-root development, I see in each tier the pod contains init container, for example:
```yaml
  ...
      hostname: is-xmetadocker
      hostIPC: true
      initContainers:
      - name: load-data
        image: "mycluster.icp:8500/zen/is-db2xmeta-image:11.7.1-1.0"
        imagePullPolicy: IfNotPresent
        securityContext:
          capabilities:
           add: ["SETFCAP", "SYS_NICE", "IPC_OWNER"]
        command: ['/bin/bash', '-c', '--']
        args: [ " set -x;
                ...
                "
               ]
        env:
        - name: DEDICATED_REPOS_VOLPATH
          value: /mnt/dedicated_vol/Repository
        volumeMounts:
        - name: xmeta-pv-volume
          mountPath: "/mnt/dedicated_vol/Repository"
  ...
```

## Understanding
What is this and what is it for? Let's take a look. First reference to [official documentation](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/).

Init Containers, which are **specialized** Containers that run **before** app Containers and can **contain** utilities or setup scripts not present in an app image.

A Pod can have multiple Containers running apps within it, but it can also have one or more Init Containers, which are run before the app Containers are started.

Init Containers are exactly like regular Containers, except:
* They always run to completion.
* Each one must complete successfully before the next one is started.

If an Init Container fails for a Pod, Kubernetes restarts the Pod repeatedly until the Init Container succeeds. However, if the Pod has a restartPolicy of Never, it is not restarted.

### Differences from Regular Containers
Init Containers support all the fields and features of app Containers, including resource limits, volumes, and security settings. However, the resource requests and limits for an Init Container are handled slightly differently.

Init Containers do not support readiness probes because they must run to completion before the Pod can be ready.

## Usage
Init Containers have separate images from app Containers (actually they can have the same), they have some advantages for start-up related code:

* They can contain and run utilities that are not desirable to include in the app Container image for security reasons.
* They can contain utilities or custom code for setup that is not present in an app image. For example, there is no need to make an image FROM another image just to use a tool like sed, awk, python, or dig during setup.
* The application image builder and deployer roles can work independently without the need to jointly build a single app image.
* They use Linux namespaces so that they have different filesystem views from app Containers. Consequently, they can be given access to Secrets that app Containers are not able to access.
* They run to completion before any app Containers start, whereas app Containers run in parallel, so Init Containers provide an easy way to block or delay the startup of app Containers until some set of preconditions are met.

You can check the logs of init container:
```
kubectl logs <pod name> -c <init container> -n test-1
```

During the startup of a Pod, the Init Containers are started **in order**, after the network and volumes are initialized. Each Container must exit successfully before the next is started.

If the Pod is restarted, all Init Containers must execute again. Init Container code should be idempotent. 
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>initContainer</tag>
      </tags>
  </entry>
  <entry>
    <title>Istio</title>
    <url>/2019/12/05/k8s-istio/</url>
    <content><![CDATA[
//TODO
[ ] read official document
[ ] udemy course


主讲Anthos的，由于service mesh是其中重要组成，所以讲了很多service mesh的内容, 并且讲得还很好。
Qucik Labs and slides are from [PluralSight Anthos special](https://app.pluralsight.com/paths/skills/architecting-hybrid-cloud-infrastructure-with-anthos)
关于service mesh的实验可以回顾一下是如何在GCloud中操作的。slides也可以下载看看。


[`Istio`](https://istio.io/docs/concepts/what-is-istio/) is the implementation of a `service mesh` that improves application resilience as you connect, manage, and secure microservices. It provides operational control and performance insights for a network of containerized applications. It can work across environments(think about Google Anthos)!

Important network functions as below, `service mesh` decouple them from applications:
- authn
- authz
- latency
- fault tolerance
- circuit breaking
- quota
- rate limiting
- load balancing
- logging
- metrics
- distributed tracing
- topology

So summarize there are 3 parts: 
- Traffic control 
- Observability (dashboard: prometheus, grafana, jaeger, kiali)
- Security

`Istio` uses [`envoy`](https://www.envoyproxy.io/) and sidecar pattern in the K8s pods.

Istio main components:
- `Pilot`: control plane manages the distributed proxies across the either environment, push service communication policies, just like a software defined network.
  - service discovery
  - traffic management
  - intelligent routing
  - resiliency
- `Mixer`: collect info and send telemetry, logs and traces to your system of choice (prometheus, influxDB, Stackdriver, etc)
- `Citadel`: policies management, service to service auth[n,z], using mutual TLS, credential management.

How does Istio work, for example, life of a request in the mesh:
1. service A comes up.
2. envoy is deployed with it and fetches service information, routing and configuration policy from Pilot.
3. If Citadel is being used, TLS certs are securely distriuted as well.
4. service A calls service B.
5. client-side envoy intercepts the call.
6. envoy consults config to know how/where to route call to service B.
7. envoy forwards to appropriate instance of service B, the envoy on server side intercepts the request.
8. server-side envoy checks with Mixer to validate the call should be allowed.
9. server-side envoy forwards request to service B for response.
10. envoy forwards response to the original caller, the response is intercepted by envoy on the caller side.
11. envoy reports telemetry to Mixer, which in turn notifies appropriate plugins.
12. client-side envoy forwards response to service A
13. client-side envoy reports telemetry to Mixer, which in turn notifies appropriate plugins.
















]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title>Helm Quick Start</title>
    <url>/2020/04/30/k8s-helm-quick-start/</url>
    <content><![CDATA[

# Concepts
Now, let's understand the basic concepts of Helm:
https://helm.sh/docs/intro/using_helm/

[Official Document](https://helm.sh/docs/)
To install helm in the control node, download the corresponding binary and untar to execution path, or using container and mount necessary k8s credentials.

Package manager analogy:
- helm (charts)
- apt (deb)
- yum (rpm)
- maven (Jar)
- npm (node modules)
- pip (python packages)

**Helm v3.2.0**
> Helm3 does not have Tiller server, see [what's new in Helm 3](https://developer.ibm.com/technologies/containers/blogs/kubernetes-helm-3/)

## Plugins
[十六种实用的 Kubernetes Helm Charts工具](https://www.sohu.com/a/304655993_120019946)

### Tillerless
For `helm2`, Tiller server in cluster may not stable and secure, another workaround is run it locally, it talks to remote k8s cluster via kuebctl config.
- [Tillerless Helm v2 plugin](https://github.com/rimusz/helm-tiller), read this article [Why tillerless is needed](https://rimusz.net/tillerless-helm).

```bash
## install tillerless plugin
helm plugin install https://github.com/rimusz/helm-tiller
```

A good practice is to have helm, helm plugin, kubectl and cloud SDK in one container, for example:
```dockerfile
FROM python:3.7.7-alpine3.11

USER root

## version number
ENV KUBCTL_VER="1.16.11"
ENV KUBCTL="/bin/kubectl"
ENV HELM_VER="v2.17.0"
ENV HELMFILE_VER="v0.119.0"
ENV HELMDIFF_VER="v3.1.1"
ENV GCLOUD_SDK_VER="318.0.0"

# Install fetch deps
RUN apk add --no-cache \
    ca-certificates \
    bash \
    jq \
    git \
    curl \
    unzip \
    tar \
    libgit2 \
    openssl-dev \
    libffi-dev \
    gcc \
    musl-dev \
    python3-dev \
    make \
    openssh \
    tini \
    shadow \
    su-exec \
    vim


RUN curl -L https://storage.googleapis.com/kubernetes-release/release/v${KUBCTL_VER}/bin/linux/amd64/kubectl \
    -o $KUBCTL && chmod 0755 $KUBCTL

# install helm && plugins && helmfile
RUN curl -L https://storage.googleapis.com/kubernetes-helm/helm-${HELM_VER}-linux-amd64.tar.gz | tar xz \
    && cp linux-amd64/helm /bin && rm -rf linux-amd64 \
    \
    && curl -O -L https://github.com/roboll/helmfile/releases/download/${HELMFILE_VER}/helmfile_linux_amd64 \
    && mv helmfile_linux_amd64 /usr/local/bin/helmfile && chmod 755 /usr/local/bin/helmfile


# Install GCloud SDK
RUN curl -L https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-${GCLOUD_SDK_VER}-linux-x86_64.tar.gz | tar zx \
    && mv google-cloud-sdk /usr/local/ && /usr/local/google-cloud-sdk/install.sh -q \
    && /usr/local/google-cloud-sdk/bin/gcloud components install beta --quiet

ENV PATH="/usr/local/google-cloud-sdk/bin:${PATH}"

RUN helm init -c \
    && helm plugin install https://github.com/chartmuseum/helm-push \
    && helm plugin install https://github.com/databus23/helm-diff --version ${HELMDIFF_VER} \
    && helm plugin install https://github.com/rimusz/helm-tiller

#Test
Run kubectl version --client \
    && helm version --client \
    && helm plugin list \
    && helmfile -v \
    && gcloud version

ENTRYPOINT ["/bin/bash","-c", "tail -f /dev/null"]
```
Note that `HELM_VER < 2.17.0` does not work anymore, the default stable repo is gone, so upgrade to `2.17.0` in environment variable.

Then run it:
```bash
# go to tillerless folder that with the dockerfile above
docker build -f tillerless.dockerfile -t tillerless:1.0 .

docker run -d --name=xxx \
## something you want to mount
## gcloud config
-v ~/.config:/root/.config \
## kubectl connect
-v ~/.kube:/root/.kube \
-v $(pwd)/../envoy-proxy:/envoy-proxy \
# default workspace path
-w /envoy-proxy \
## tillerless env vars
## by default tiller uses secret
-e HELM_TILLER_STORAGE=configmap \
-e HELM_HOST=127.0.0.1:44134 \
--entrypoint=/bin/bash \
tillerless:1.0 \
-c "tail -f /dev/null"
```
When first time exec into docker container, run kubectl may not work, try exit out and run kubectl on host and exec log in again.

If switch k8s context, please **stop and restart** tillerless to adopt change.
```bash
## export if they are gone
export HELM_TILLER_STORAGE=configmap
export HELM_HOST=127.0.0.1:44134
## by default tiller namespace is kube-system
helm tiller start [tiller namespace]
helm list
helm install..
helm delete..

exit
```
or
```bash
## export if they are gone
export HELM_TILLER_STORAGE=configmap
export HELM_HOST=127.0.0.1:44134
## by default tiller namespace is kube-system
helm tiller start-ci [tiller namespace]
helm list
helm install..
helm delete..

helm tiller stop
```
or
```bash
helm tiller run <command>
```


# Overview
helm3 does not have default repo, usually we use `https://kubernetes-charts.storage.googleapis.com/` as our stable repo. helm2 can skip this as it has default stable repo.
```bash
## add stable repo to local repo
## 'stable' is your custom repo name
helm repo add stable https://kubernetes-charts.storage.googleapis.com/
## display local repo list
helm repo list
## remove repo 'stable'
helm repo remove stable

## create charts sacffold
helm create <chart name>

## install charts
## Make sure we get the latest list of charts
helm repo update 
helm install stable/mysql --generate-name
helm install <release name> stable/mysql -n <namespace>
helm install <path to unpacked/packed chart>

## show status of your release
helm status <release name>
```

Whenever you install a chart, a new release is created. So one chart can be installed multiple times into the same cluster. Each can be independently managed and upgraded.
```bash
## show deployed release
helm ls -n <namespace>

## uninstall
## with --keep-history, you can check the status of release
## or even undelete it
helm uninstall <release name> [--keep-history] -n <namespace>
```

## Install order
Install in certain order, click to see. Or you can split the chart into different part or using init container.
- [Helm install in certain order](https://stackoverflow.com/questions/51957676/helm-install-in-certain-order)
- using [helmfile](https://github.com/roboll/helmfile)


# Chart file structure
https://helm.sh/docs/topics/charts/#the-chart-file-structure

```bash
<chart name>/
  Chart.yaml          # A YAML file containing information about the chart
  LICENSE             # OPTIONAL: A plain text file containing the license for the chart
  README.md           # OPTIONAL: A human-readable README file
  values.yaml         # The default configuration values for this chart
  values.schema.json  # OPTIONAL: A JSON Schema for imposing a structure on the values.yaml file, values.yaml 必须遵守这个结构, 否则不会通过
  charts/             # other dependent
  requirements.yaml   # other dependent (for helm2)
  crds/               # Custom Resource Definitions
  templates/          # A directory of templates that, when combined with values,
                      # will generate valid Kubernetes manifest files.
     xxx.yaml
     _xx.tpl          # functions
     NOTES.txt        # show description after run helm install 
  templates/NOTES.txt # OPTIONAL: A plain text file containing short usage notes
```

> To drop a dependency into your `charts/` directory, use the `helm pull` command

1. Chart.yaml
`apiVersion`, helm3 is `v2`, helm2 is `v1`
`appVersion`, application verion
`version`, charts version, for example, chart file/structure changed
`keywords` field is used for helm search
`type`, we have application and library chart

# Managing dependencies
Package the charts to archive, you can use `tar` but helm has special command for this purpose:
```bash
## it will create .tgz suffix
## and append chart verion to archive name
## chart version is from Chart.yaml
helm package <chart_name>
```

Publishing chart in repos, chartmuseum (like docker hub..), just like private docker registry, you can create a private chartmuseum in your host (有专门的安装包).
```bash
## go to the dir that contains chart archive
## this will generate a index.yaml file
helm repo index .
## for security can be signed and verified
## for verification, we need provenance file
helm package --sign
helm verify <chart>
## verify when install
helm install --verify ...
```

关于dependency，甚至可以只有charts文件夹，里面放所有的chart archive，外面也不需要templates了。
但这样不好管理版本，还是在Chart.yaml中定义依赖比较好。
在定义中还可以指定版本的范围，用的是semver语法: `~1.2.3`, `^0.3.4`, `1.2-3.4.5`
```bash
## will download dependency charts archive to your charts folder
## according to the definition in Chart.yaml
helm dependency update <chart name>
## list dependency, their version, repo and status
helm dependency list <chart name>
```
You can also use `conditions and tags` to control which dependency is needed or not, for example, in `Chart.yaml` file
```yaml
apiVersion: v2
name: guestbook
appVersion: "2.0"
description: A Helm chart for Guestbook 2.0 
version: 1.2.2
type: application
dependencies:
  - name: backend
    version: ~1.2.2
    repository: http://localhost:8080
    condition: backend.enabled
    tags:
      - api
  - name: frontend
    version: ^1.2.0
    repository: http://localhost:8080
  - name: database
    version: ~1.2.2
    repository: http://localhost:8080
    condition: database.enabled
    tags:
      - api
```
Then in `values.yaml` file:
```yaml
## can be true or false
backend:
  enabled: true
database:
  enabled: true
tags:
  api: true
```

# Using existing charts
Helm web: https://hub.helm.sh/

```bash
## add and remove repo
helm add repo ...
helm remove repo ...
## list repo's name and URL
helm repo list

## search chart you want,for example
## mysql, nfs, mongodb, prometheus, redis, dashboard, wordpress

## for mysql, you need to specify storage provisioner
## see inspect readme or values
helm search [hub | repo] <keyword>

## inspect the chart, like docker inspect
## readme: usage 去网上看更清晰
## values: default config
## chart: Chart.yaml
helm inspect [all | readme | values | chart] <chart name>

## the same as 'helm inspect values'
helm show values

## download chart without dependencies
## for example, looking into the source code
helm fetch <chart name>

## download dependencies specified in Chart.yaml
## specify char name unpacked
helm dependency update <chart name>
```

# Customizig existing charts
if you want to override `child` chart's values.yaml, then in your `partent` chart values.yaml, 这是常用的，比如你有个dependency 是 mongodb chart, 要改它的默认配置:
```yaml
## 'mongodb' is child chart name
mongodb:
  persistence:
    size: 100Mi
```
还可以child chart中的values.yaml override parent的，但很少这样用，用法很tricky.

# Chart template guide
https://helm.sh/docs/chart_template_guide/getting_started/
Helm Chart templates are written in the `Go template language`, with the addition of 50 or so add-on template functions from the [`Sprig library`](http://masterminds.github.io/sprig/) and a few other specialized functions.

## Template and values
https://helm.sh/docs/topics/charts/#templates-and-values

Where are the configuration values from, precdence low to high from top to bottom:
1. values.yaml (default use)
2. other-file.yaml: `helm install -f <other-file.yaml> ...`
3. command: `helm install --set key=val ...`

Helm template built-in objects:
1. Chart.yaml: `.Chart.Name` (use upper case)
2. Release data: `.Release.Name`
3. K8s data: `.Capabilities.KubeVersion`
4. File data: `.Files.Get. conf.ini`
5. Template data: `.Template.Name`

In `values.yaml`:
1. use `_` instead of `-`
2. decimal number wrapped by `""`, `"2.0"`, integer number no need

使用placeholder 是最基本的操作，let's see functions and logic.
1. use functions and pipelines, they are interchangeable
https://helm.sh/docs/chart_template_guide/functions_and_pipelines/
commonly used functions and correspinding pipelines
```bash
      function usage         --       pipeline usage
================================================================
default default_value value  -- value | default default_value
quote value                  -- value | quote
upper value                  -- value | upper
trunc value 20               -- value | trunc 20
trimSuffix "-" value         -- value | trimSuffix "-"
b64enc value                 -- value | b64enc
randAlphaNum 10              -- value | randAlphaNum 10 
toYaml value                 -- value | toYaml
printf format value          -- list value | join "-"
```

2. modify scope using `with` to simpify the directives，就不用写一长串引用了
3. control whitespaces and indent
use `-` to remove whitespace (newline is treated as white space!)
```yaml
{{- with ... -}}
...
{{- end}}
## indent 6 space ahead
{{ indent 6 .Value.tcp }}
```
4. logical operators and flow control
if-else and loop
5. use variables
define the variable
```bash
{{- $defaultPortNum := .Values.defaultPortNum -}}
{{ $defaultPortNum }}
## . means global scope
{{ $.Release.Name}}
```
6. use sub-template
define function in `_helper.tpl` file then use `include`:
```bash
{{ include "fun_name" . | indent 4}}
```

## Debug template
Locally rendering template:
https://helm.sh/docs/helm/helm_template/
https://helm.sh/docs/chart_template_guide/debugging/

> Usually first use the static check then dynamic check.
```bash
## static
## works without k8s cluster
## you can also specify which values yaml file
helm template <chart dir or archive file> [--debug] | less
## for helm2
helm template --tiller-namespace tiller --values ./xxx/values.second.yaml --debug <chart dir or archive file> |less

## dynamic
## real helm install but without commit
## can generate a release name as [release]
helm install [release] <chart> --dry-run --debug 2>&1 | less
```

# Helm commands
https://helm.sh/docs/helm/
```bash
## install with specified release name
helm install [release name] [chart] -n <namespace> --values <path to values yaml>
## check release status
helm list -n <namespace>
## display yaml files
helm get manifest [release] -n <namespace> | less

## check release specification and revision numbers
helm status [release] -n <namespace>

## get all info
## helm2: helm get [release]
helm get all [release] -n <namespace>

## upgrade
helm upgrade [release] [chart] -n <namespace>
## check revision
helm history [release] -n <namespace>
## rollback
## revision number can get from helm history
helm rollback [release] [revision] -n <namespace>

## if abort the helm install, check helm list then uninstall the broken release
## helm2: helm delete --purge [release] 
helm uninstall [release] -n <namespace>
```


# PluralSight Supplement
github: https://github.com/phcollignon/helm3

## Helm context
Helm use the same configuration as kubectl
```bash
## helm env, repos, config, cache info
helm env
## check helm version
helm version --short
## helm uses the same current context
kubectl config view
```

Helm stores release configuration and history in k8s as secrets. In helm3, it is stored in each corresponding namepsace.
```bash
## in your working namespace
kubectl get secret -n <ns>
## helm secret is something like:
sh.helm.release.v1.demomysql.v1      helm.sh/release.v1                    1      110s
```

[Improved Upgrade Strategy: 3-way Strategic Merge Patches](https://helm.sh/docs/faq/#improved-upgrade-strategy-3-way-strategic-merge-patches)
In Helm3, Helm considers the `old manifest`, its `live state`, and the `new manifest` when generating a patch.

In helm2, helm client uses `gRPC` protocol to access Tiller server (in production secure connection is required, set TLS/SSL), then Tiller (need service account with privilege) will call K8s API to instantiate the charts. In helm3, no Tiller no security issue.

















]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>helm</tag>
      </tags>
  </entry>
  <entry>
    <title>Helm Upgrade Failed Due to Deprecated K8s API</title>
    <url>/2023/07/07/k8s-helm-upgrade-fail/</url>
    <content><![CDATA[
`PodSecurityPolicy` was [removed](https://v1-25.docs.kubernetes.io/docs/concepts/security/pod-security-policy/) from K8s since v1.25. 

Our GKE cluster had been forcely upgraded to v1.25 from v1.24 and we forgot to
retire the `PodSecurityPolicy` resources in the chart, so the subsequent helm
upgrade from pipeline failed due to the disappearance of `PodSecurityPolicy`
kind and version `policy/v1beta1`.

Even if I disabled the PodSecurityPolicy from the chart but the helm upgrade
still would reference to the resource from existing release and the "not found"
showed up:

```bash
[2023-07-08T01:12:20.712Z] ARGS:
[2023-07-08T01:12:20.712Z]   0: helm (4 bytes)
[2023-07-08T01:12:20.712Z]   1: upgrade (7 bytes)
[2023-07-08T01:12:20.712Z]   2: --install (9 bytes)
[2023-07-08T01:12:20.712Z]   3: cert-manager (12 bytes)
[2023-07-08T01:12:20.712Z]   4: gcloud-helm/cert-manager (24 bytes)
[2023-07-08T01:12:20.712Z]   5: --version (9 bytes)
[2023-07-08T01:12:20.712Z]   6: 1.0.5 (5 bytes)
[2023-07-08T01:12:20.712Z]   7: --wait (6 bytes)
[2023-07-08T01:12:20.712Z]   8: --create-namespace (18 bytes)
[2023-07-08T01:12:20.712Z]   9: --namespace (11 bytes)
[2023-07-08T01:12:20.712Z]   10: cert-manager (12 bytes)
[2023-07-08T01:12:20.712Z]   11: --values (8 bytes)
[2023-07-08T01:12:20.712Z]   12: /tmp/helmfile4095048642/cert-manager-cert-manager-values-6d48855bcc (67 bytes)
[2023-07-08T01:12:20.712Z]   13: --reset-values (14 bytes)
[2023-07-08T01:12:20.712Z]   14: --history-max (13 bytes)
[2023-07-08T01:12:20.712Z]   15: 10 (2 bytes)
[2023-07-08T01:12:20.712Z] 
[2023-07-08T01:12:20.712Z] ERROR:
[2023-07-08T01:12:20.712Z]   exit status 1
[2023-07-08T01:12:20.712Z] 
[2023-07-08T01:12:20.712Z] EXIT STATUS
[2023-07-08T01:12:20.712Z]   1
[2023-07-08T01:12:20.712Z] 
[2023-07-08T01:12:20.712Z] STDERR:
[2023-07-08T01:12:20.712Z]   Error: UPGRADE FAILED: unable to build kubernetes objects from current release manifest: [resource mapping not found for name: "xxx" namespace: "" from "": no matches for kind "PodSecurityPolicy" in version "policy/v1beta1"
...
[2023-07-08T01:12:20.712Z]   ensure CRDs are installed first]
[2023-07-08T01:12:20.712Z] 
[2023-07-08T01:12:20.712Z] COMBINED OUTPUT:

[2023-07-08T01:12:20.712Z]   Error: UPGRADE FAILED: unable to build kubernetes objects from current release manifest: [resource mapping not found for name: "xxx" namespace: "" from "": no matches for kind "PodSecurityPolicy" in version "policy/v1beta1"
```

The helm uninstall on existing release also failed and got stuck with the same
reason:

```bash
helm uninstall <release name> -n <ns> --no-hooks
```

The [fix](https://github.com/helm/helm/issues/11513#issuecomment-1402880749) is
simple by a helm plugin [`mapkubeapis`](https://github.com/helm/helm-mapkubeapis):
```bash
helm plugin install https://github.com/helm/helm-mapkubeapis
helm mapkubeapis <release name> -n <ns>
```

```bash
helm history <release name> -n <ns>

REVISION	UPDATED                 	STATUS         	CHART             	APP VERSION	DESCRIPTION
1       	Sat Nov 20 00:21:31 2021	superseded     	xxx-1.0.3	v1.4.1     	Install complete
2       	Thu Aug 11 10:30:45 2022	superseded     	xxxr-1.0.5	v1.7.3     	Upgrade complete
3       	Thu Aug 11 10:58:10 2022	superseded     	xxx-1.0.3	v1.4.1     	Upgrade complete
# helm uninstall failed and hung
4       	Mon Aug 15 23:30:33 2022	superseded     	xxx-1.0.5	v1.7.3     	Deletion in progress (or silently failed)
# mapkubeapis helped re-map the deprecated API
5       	Sat Jul  8 01:56:17 2023	deployed       	xxx-1.0.5	v1.7.3     	Kubernetes deprecated API upgrade - DO NOT rollback from this version
```

Then the next helm uninstall/upgrade should be good.
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>helm</tag>
      </tags>
  </entry>
  <entry>
    <title>Helm Tiller Server Deploy</title>
    <url>/2019/07/19/k8s-helm-tiller/</url>
    <content><![CDATA[
**Kubernetes version 1.13.2**

Helm 3 removes the Tiller server, see this [post](https://developer.ibm.com/technologies/containers/blogs/kubernetes-helm-3/)

This blog primarily talks about `Tiller` server, especially creating service account and cluster role binding for it.
还有一种解决办法是`Tillerless`, 将Tiller server 运行在本地的container中，见我的 `<<Helm Quick Start>>`

Zen uses `Helm` to install in ICP4D cluster. `Helm` is a tool for managing Kubernetes charts. `Charts` are packages of pre-configured Kubernetes resources. Think of `Helm` like apt(deb), yum(rpm), homebrew for Kubernetes.

## Resource
[Helm Git Repos](https://github.com/helm/helm)

## Download Helm Installer
Download the Latest release from [Helm Release](https://github.com/helm/helm/releases). For example in Linux, we use:
```
Linux amd64 (checksum / 9f50e69cf5cfa7268b28686728ad0227507a169e52bf59c99ada872ddd9679f0)
```
> `Helm` needs to be put in the control node that already configured with `kubectl`.

Untar the file and you can move `helm` binary to one of the exectuable path, such as `/usr/local/bin`:
```bash
# which helm
/usr/local/bin/helm
```

## Deploy Tiller Server
Helm client needs to talk to `Tiller` server, which will be deploied in the K8s cluster.

Most cloud providers enable a feature called Role-Based Access Control - `RBAC` for short. If your cloud provider enables this feature, you will need to create a service account for Tiller with the right roles and permissions to access resources.

From [here](https://helm.sh/docs/using_helm/#tiller-and-role-based-access-control), you need to create a `cluster role binding` which specifies a role and a `service account` name that have been set up in advance `rbac-config.yaml`:
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
```
Then install `Helm`:
```bash
kubectl create -f rbac-config.yaml
helm init --service-account tiller --history-max 200
```

If you forget to create `service account` and `cluster role binding` before you initiate `Helm`, no worries, create `rbac-config.yaml` objects and patch it by:
```bash
kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'
```
Then check `Tiller` pod is running:
```bash
kubectl get pods -n kube-system -l app=helm

NAME                             READY   STATUS    RESTARTS   AGE
tiller-deploy-845fb7cfc6-rn4nq   1/1     Running   0          20h
```

Now, we are in good shape, actually I can config `SSL/TLS` between `Helm` and `Tiller`, not covered in this blog.

## Uninstall Tiller Server 
There are 2 ways to uninstall:
```
helm reset
helm delete deploy tiller-deploy -n kube-system
```]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>helm</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Job and CronJob</title>
    <url>/2019/08/12/k8s-job-cronjob/</url>
    <content><![CDATA[
**Kubernetes version 1.13.2**

Recently I use Job to run some one time tasks (later switch back to regular pod because we want to control it's start point), it's just like other pod controller in K8s.

## Job
Job setup pod, note that:
hostname
job-name, check
env pass

volume mount

Job type

In upgrade, usage

## CronJob
crontab]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kind</title>
    <url>/2021/01/09/k8s-kind/</url>
    <content><![CDATA[
`Kind` helps you bring up local k8s cluster for testing and POC. Seamlessly
working with kubectl and others: such as prometheus, operator, helmfile, etc.

# Install Kind
The install is easy with `go`(1.17+), see this
[instruction](https://kind.sigs.k8s.io/docs/user/quick-start/#installing-with-go-get--go-install):
```bash
# At the time of writing the kind stable version is 0.18.0, it will place the
# kind binary to $GOBIN(if exist) or $GOPATH
go install sigs.k8s.io/kind@v0.18.0
```

# Basic Workflow

## Cluster Creation
To spin up local k8s cluster:
```bash
# see options and flags
kind create cluster --help

# You can have multiple types of cluster such as dev, stg, prod, etc
# create one node cluster with dev as context name
kind create cluster --name dev

# Different cluster with specific configuration
kind create cluster --name stg --config stg_config.yaml

# Check k8s context
# Note the cluster name is not the same as context name
kubectl config get-contexts
```

## Cluster Configuration
The advanced configuration please see this
[section](https://kind.sigs.k8s.io/docs/user/configuration/).
A simple multi-node cluster, can be used to test for example rolling upgrade:
```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
# 1 control plane node and 3 workers
nodes:
# the control plane node config
- role: control-plane
# the three workers
- role: worker
- role: worker
- role: worker
```

## Load Image
The kind k8s cluster uses `containerd` runtime, you can `docker exec` into node
container and check with `crictl`
[command](https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/):
```bash
# list images
crictl images
# list containers
crictl ps
# list pods
crictl pods
```
To load image into the kind node, for example:
```bash
kind load docker-image busybox [--name dev] [--nodes x,y,z]
```
Then in the node container you will see busybox by `crictl images`.

## Context Switch
To manage clusters:
```bash
# View kind clusters
kind get clusters

# Get cluster kubeconfig detail
kind get kubeconfig -n dev

# Switch context from dev to stg
kubectl config use-contexts kind-stg
```

## Cluster Deletion
```bash
kind delete cluser --name dev
```

## Kind Logging
To check kind logs:
```bash
kind export logs --name dev [./some_folder]
```

# Ingress
For how to set up ingress for kind K8s cluster, please check:
https://kind.sigs.k8s.io/docs/user/ingress/

# Load Balancer
For how to set up LB service type in kind K8s cluster, please check:
https://kind.sigs.k8s.io/docs/user/loadbalancer/]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubelet Complains Orphan Pod</title>
    <url>/2021/05/05/k8s-kubelet-orphan-pod/</url>
    <content><![CDATA[
This error is derived from a P1 incident which itself originated from a emerging Google regional network issue. 3 gke nodes in that region were impacted, the kubelet daemon in each node was not able to update node status and other metrics. The node was changed to `NotReady` and some customer services went down.

After the nodes went back to ready state as the root cause mitigated, from GCP `Log Explorer` one gke node kubelet kept posting error:
```js
E0505 03:59:46.470928    1543 kubelet_volumes.go:154] orphaned pod "7935559a-a41b-4b44-960b-6a31fcab81f8" found, but volume paths are still present on disk : There were a total of 2 errors similar to this. Turn up verbosity to see them. 
```


# Concept
The solution for this error is simple but let's understand some concepts:
Why we have orphan pods, see [kubernetes Garbage Collection](https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/):

Some Kubernetes objects are owners of other objects. For example, a ReplicaSet is the owner of a set of Pods. The owned objects are called dependents of the owner object.

If you delete an object without deleting its dependents automatically, the dependents are said to be orphaned.

In background cascading deletion, Kubernetes deletes the owner object immediately and the garbage collector then deletes the dependents in the background.

The default behavior is to delete the dependents in the background which is the behavior when `--cascade` is omitted or explicitly set to background.


# Solution
As we can see, after the nodes was ready, the old pod controllers were deleted and new ones got redeployed, however, some orphan pods hung there because of pod volume, [this is a known issue](https://github.com/longhorn/longhorn/issues/485)

On host node, `cd` to `/var/lib/kubelet/pods` path and manually deleted them.
```bash
cd /var/lib/kubelet/pods
rm -rf ./7935559a-a41b-4b44-960b-6a31fcab81f8/
```]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Ingress</title>
    <url>/2019/10/21/k8s-ingress/</url>
    <content><![CDATA[
**Kubernetes version 1.13.2**

First understand basis:
- [Ingress vs Load Balancer](https://stackoverflow.com/questions/45079988/ingress-vs-load-balancer)
- [Kubernetes NodePort vs LoadBalancer vs Ingress? When should I use what?](https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0)


This [link](https://docs.microsoft.com/en-us/azure/aks/ingress-tls) show you the instructions about how to setup ingress in an Azure Kubernetes Service (AKS) cluster.
It contains [`NGINX ingress controller`](https://github.com/kubernetes/ingress-nginx) and [`cert-manager project`](https://github.com/jetstack/cert-manager) (used to automatically generate and configure [`Let's Encrypt`](https://letsencrypt.org/) certificates).

First understand what is forward proxy and reverse proxy:
https://www.linuxbabe.com/it-knowledge/differences-between-forward-proxy-and-reverse-proxy

There’re many different kinds of forward proxy such as web proxy, HTTP proxy, SOCKS proxy etc. Please keep mind that using forward proxy to browse the Internet usually slows down your overall Internet speed. Another thing to be aware of is that there’re many free forward proxies which is built by hackers for malicious purpose. If you happen to be using one of these proxies, they will log every activity you do on the Internet.

Nginx can be acting both a web server and a reverse proxy at the same time. HAProxy is another well-known open-source reverse proxy software.

TLS termination proxy:
https://en.wikipedia.org/wiki/TLS_termination_proxy

A TLS termination proxy (or SSL termination proxy) is a proxy server that is used by an institution to handle incoming TLS connections, decrypting the TLS and passing on the unencrypted request to the institution's other servers (it is assumed that the institution's own network is secure so the user's session data does not need to be encrypted on that part of the link). TLS termination proxies are used to reduce the load on the main servers by offloading the cryptographic processing to another machine, and to support servers that do not support SSL, like Varnish.


## Create an ingress controller
To create the ingress controller, use `Helm` to install nginx-ingress (or use yaml). For added redundancy, two replicas of the NGINX ingress controllers are deployed with the `--set controller.replicaCount parameter`.

This is for AKS cluster, for bare-metal it's different, since bare-metal does not have existing loadbalancer (please refer https://kubernetes.github.io/ingress-nginx/):
```bash
helm install stable/nginx-ingress \
    --namespace <the namespace as your application> \
    --set controller.replicaCount=2 \
    --set controller.nodeSelector."beta\.kubernetes\.io/os"=linux \
    --set defaultBackend.nodeSelector."beta\.kubernetes\.io/os"=linux
```

Then go to get the public IP assigned for ingress controller:
```
NAME                                             TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)                      AGE
billowing-kitten-nginx-ingress-controller        LoadBalancer   10.0.182.160   51.145.155.210  80:30920/TCP,443:30426/TCP   20m
billowing-kitten-nginx-ingress-default-backend   ClusterIP      10.0.255.77    <none>          80/TCP                       20m
```

Until, we just set up a ingress controller, no ingress rules are specified.

### Delete
```bash
## find helm release name
helm list
## delete
helm delete --purge <name>
```
## Config DNS name
For the HTTPS certificates to work correctly, configure an FQDN(fully qualified domain name) for the ingress controller IP address.

for Azure it is:
```bash
#!/bin/bash

# Public IP address of your ingress controller
IP="51.145.155.210"

# Name to associate with public IP address
DNSNAME="demo-aks-ingress"

# Get the resource-id of the public ip
PUBLICIPID=$(az network public-ip list --query "[?ipAddress!=null]|[?contains(ipAddress, '$IP')].[id]" --output tsv)

# Update public ip address with DNS name
az network public-ip update --ids $PUBLICIPID --dns-name $DNSNAME
```

## Install cert-manager
The NGINX ingress controller supports TLS termination. 
see here https://github.com/jetstack/cert-manager.
cert-manager is a Kubernetes add-on to automate the management and issuance of TLS certificates from various issuing sources.
It will ensure certificates are valid and up to date periodically, and attempt to renew certificates at an appropriate time before expiry

To install the cert-manager controller in an RBAC-enabled cluster, use the following helm install command (this is not the latest version)
```bash
# Install the CustomResourceDefinition resources separately
kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.8/deploy/manifests/00-crds.yaml

# Create the namespace for cert-manager
kubectl create namespace cert-manager

# Label the cert-manager namespace to disable resource validation
kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true

# Add the Jetstack Helm repository
helm repo add jetstack https://charts.jetstack.io

# Update your local Helm chart repository cache
helm repo update

# Install the cert-manager Helm chart
helm install \
  --name cert-manager \
  --namespace cert-manager \
  --version v0.8.0 \
  jetstack/cert-manager

```

### Create a CA cluster issuer
Create a cluster issuer yaml then run `kubectl apply -f`, more details see:
https://cert-manager.readthedocs.io/en/latest/reference/issuers.html

```yaml
apiVersion: certmanager.k8s.io/v1alpha1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: <your email address>
    privateKeySecretRef:
      name: letsencrypt-prod
    http01: {}
```

### Delete
```bash
helm list
helm delete --purge <name>
kubectl delete -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.8/deploy/manifests/00-crds.yaml
kubectl delete -f cluster-issuer.yaml
kubectl delete ns cert-manager
```

## Create ingress route
The apiVersion may update to stable, usually, if the AKS demo works but your application not, that means there are some miss configurations in the ingress annotations, please adjust according to your situation.
```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: <ingress name>
  namespace: <ns>
  annotations:
    kubernetes.io/ingress.class: nginx
    certmanager.k8s.io/cluster-issuer: letsencrypt-prod
    ## if inside cluster use HTTPS
    nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
spec:
  ## this part is for add ssl/tls to ingress
  tls:
  - hosts:
    - <URL>
    secretName: tls-secret
  ## routing rules
  rules:
  - host: <URL>
    http:
      paths:
      - path: /mbi/sii
        backend:
          serviceName: is-servicesdocker
          servicePort: 9446
```
### Delete
```bash
kubectl delete -f ingress.yaml
```

## Create a certificate object
Next, a certificate resource must be created. The certificate resource defines the desired X.509 certificate. For more information, see https://cert-manager.readthedocs.io/en/latest/reference/certificates.html


Cert-manager has likely automatically created a certificate object for you using ingress-shim, which is automatically deployed with cert-manager since v0.2.2. 
see https://docs.cert-manager.io/en/latest/tasks/issuing-certificates/ingress-shim.html

## Test
If the things are going good, check the URL address in the browser:
```
<URL>/mbi/sii/launchpad
```]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Local Persistent Volume</title>
    <url>/2020/04/02/k8s-local-persistent-volume/</url>
    <content><![CDATA[

## Introduction
[Kubernetes 1.14: Local Persistent Volumes GA](https://kubernetes.io/blog/2019/04/04/kubernetes-1.14-local-persistent-volumes-ga/)
**Recap:**
A local persistent volume represents a local disk directly-attached to a single Kubernetes Node. With the Local Persistent Volume plugin, Kubernetes workloads can now consume high performance local storage using the same volume APIs that app developers have become accustomed to.

一个和hostPath的重要区别:
The biggest difference is that the Kubernetes scheduler understands which node a Local Persistent Volume belongs to. With HostPath volumes, a pod referencing a HostPath volume may be moved by the scheduler to a different node resulting in data loss. But with Local Persistent Volumes, the Kubernetes scheduler ensures that a pod using a Local Persistent Volume is **always** scheduled to the same node.

While HostPath volumes may be referenced via a Persistent Volume Claim (PVC) or directly inline in a pod definition, Local Persistent Volumes can only be referenced via a PVC. This provides additional security benefits since Persistent Volume objects are managed by the administrator, preventing Pods from being able to access any path on the host.

Additional benefits include support for formatting of block devices during mount, and volume ownership using fsGroup.

注意: 实际上emptyDir + fsGroup也可以实现类似hostPath的效果，emptyDir用的是`/sysroot` (RedHat Linux), 比如多个pods 使用emptyDir在同一个Node, 我在各自的emptyDir中touch了一个file: compute-0 和compute-3, 进入Node使用find command就可以看到了:
```bash
/sysroot/ostree/deploy/rhcos/var/lib/kubelet/pods/68e65ed4-4e62-4588-9269-8947dea9dd46/volumes/kubernetes.io~empty-dir/compute-dedicated-scratch/compute-0

/sysroot/ostree/deploy/rhcos/var/lib/kubelet/pods/92b26b37-92f3-4609-83cb-da8cb8727ca2/volumes/kubernetes.io~empty-dir/compute-dedicated-scratch/compute-3
```

还需要注意的是，local storage provisioning 在每个node上只会provision attach的disk个数一样的PV，并且这个PV会被一个PVC占据，尽管PV大小是500G但是PVC只请求5G。（不知道这个以后是否会有改进）

## Steps
> Only test on OCP `4.3` version

[OpenShift persistent storage using local volumes](https://docs.openshift.com/container-platform/4.3/storage/persistent_storage/persistent-storage-local.html)
[Deploy local-storage Operator](https://github.com/openshift/local-storage-operator/blob/master/docs/deploy-with-olm.md)
1. install local storage operator (it is by default set in `local-storage` namespace)
2. provision the local storage
3. create local volume persistentVolumeClaim and attach to pod

After deploy the operator, then
```bash
## get hostname of each worker node
## can use label -l to filter worker node if needed
kbc describe node | grep hostname
```

```yaml
apiVersion: "local.storage.openshift.io/v1"
kind: "LocalVolume"
metadata:
  name: "local-disks"
  namespace: "local-storage"
spec:
  nodeSelector:
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          ## put hostname above here
          - worker0.jc-portworx.os.xx.com
          - worker1.jc-portworx.os.xx.com
          - worker2.jc-portworx.os.xx.com
  storageClassDevices:
    - storageClassName: "local-sc"
      volumeMode: Filesystem
      ## The file system that will be formatted when the local volume is mounted
      fsType: xfs
      devicePaths:
        ## use blkid command to get this devicePath
        - /dev/vdc
```

For example:
```bash
blkid
## for clarity I remove unrelated output
## First colume is the device path
/dev/vdc: LABEL="mdvol" UUID="fdac344f-8d5f-48bd-9101-99cb416bb93d" TYPE="xfs"
```
let's check `/dev/vdc`
```bash
lsblk /dev/vdc

NAME MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vdc  252:32   0  500G  0 disk
```

After apply the CR `LocalVolume`, let's check `local-storage` namespace status, you should see lcoal diskmaker and provisioner pods are up and running, corresponding PVs are ready as well.
```bash
NAME                                          READY   STATUS    RESTARTS   AGE
pod/local-disks-local-diskmaker-6787r         1/1     Running   0          52m
pod/local-disks-local-diskmaker-jvwnq         1/1     Running   0          52m
pod/local-disks-local-diskmaker-lfzq9         1/1     Running   0          52m
pod/local-disks-local-provisioner-fzgs2       1/1     Running   0          52m
pod/local-disks-local-provisioner-mqd86       1/1     Running   0          52m
pod/local-disks-local-provisioner-t2bvz       1/1     Running   0          52m
pod/local-storage-operator-7f8dbfb95c-7brlv   1/1     Running   0          16h

## PV
local-pv-38162728                          500Gi      RWO            Delete           Available                                                     local-sc                7m45s
local-pv-64bcf276                          500Gi      RWO            Delete           Available                                                     local-sc                7m45s
local-pv-bd2d227                           500Gi      RWO            Delete           Available 
```


If things are all set, we can consume the local storage provisioned by local-sc. Here I use `volumeClaimTemplates` instead of create separate PVC (这里应该不能使用分开的PVC，因为PVC的创建和pod位于的node有关，事先并不知道). 

> Notice that if there is one PV per node, then one PVC will consume the whole PV. So if use statefulset with volume claim template, we will only have one pod per node.

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: test-local-sc
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  replicas: 3 # by default is 1
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 2
      securityContext: {}
      serviceAccount: wkc-iis-sa
      serviceAccountName: wkc-iis-sa
      containers:
      - name: nginx
        image: xxx.swg.com/compute-image:b994-11_7_1_1-b191
        securityContext:
          allowPrivilegeEscalation: true
          privileged: false
          readOnlyRootFilesystem: false
          runAsNonRoot: true
          runAsUser: 10032
        command: ['/bin/sh', '-c', 'tail -f /dev/null']
        volumeMounts:
        - name: my-scratch
          mountPath: /opt/xx/Scratch2
  volumeClaimTemplates:
  - metadata:
      name: my-scratch
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: local-sc
      resources:
        requests:
          storage: 5Gi
```

Now let's check `/dev/vdc` again by `lsblk`, you will see it is associated with the pod.














]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Specify Kubernetes API Server IP</title>
    <url>/2019/11/04/k8s-kubeadm-init-apiserver/</url>
    <content><![CDATA[
**Kubernetes version 1.13.2**

When I was working on softlayer cluster, after installing kubernetes the kubectl command get stuck and return time execeeds error.

The issue is the master node has 3 IP address, but only one of them is accessable from client, if not specified, the `kubeadm init` command will choose the default network interface, sometimes it's good but here does not fit.

the solution is use `--apiserver-advertise-address <IP>` in `kubeadm init`, then everything is good.
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>K8s Cluster Setup by Kubeadm</title>
    <url>/2018/12/17/k8s-kubeadm-setup/</url>
    <content><![CDATA[
How to be a contribution beginner on k8s?
https://youtu.be/o68ff5NokR8
1. find issues, search label `good first issue`
2. communication
3. build relationships


**Kubernetes version 1.13.2**
This article mainly talks about setting up k8s cluster by `kubeadm` manually. As far as I know there are no coming changes that will significantly impact the validity of these steps.

## Cluster Info
I have a 3 nodes bare-metal cluster called `myk8s` with **CentOS** version 7.5, the `/etc/hosts` file in each node:
```
172.16.158.44    myk8s1.fyre.ibm.com myk8s1
172.16.171.110   myk8s2.fyre.ibm.com myk8s2
172.16.171.227   myk8s3.fyre.ibm.com myk8s3
```
Let' see the network interface on master node:
```bash
# ifconfig -a

eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 172.16.158.44  netmask 255.255.0.0  broadcast 172.16.255.255
        ether 00:16:3e:01:9e:2c  txqueuelen 1000  (Ethernet)
        RX packets 1617615790  bytes 203050237209 (189.1 GiB)
        RX errors 0  dropped 1  overruns 0  frame 0
        TX packets 436  bytes 50037 (48.8 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

eth1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 9.30.97.218  netmask 255.255.254.0  broadcast 9.30.97.255
        ether 00:20:09:1e:61:da  txqueuelen 1000  (Ethernet)
        RX packets 13350021  bytes 1424223654 (1.3 GiB)
        RX errors 0  dropped 5  overruns 0  frame 0
        TX packets 246436  bytes 45433438 (43.3 MiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
...
```

## Configure
> For **every** node in cluster, following instruction below

### Install utilities
```bash
yum update -y
yum install -y vim
yum install -y git
```

### Disable firewall
Check firewall status and disable it if active
```bash
systemctl status firewalld
```
```bash
systemctl disable firewalld
systemctl stop firewalld
```

### Install kubeadm kubectl and kubelet
> [Install kubeadm](https://kubernetes.io/docs/setup/independent/install-kubeadm/) 

```bash
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kube*
EOF

# Set SELinux in permissive mode (effectively disabling it)
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

systemctl enable --now kubelet
```

Setting SELinux in permissive mode by running `setenforce 0` and `sed ...` effectively disables it. This is required to allow containers to access the host filesystem, which is needed by pod networks for example. You have to do this until SELinux support is improved in the kubelet.

Currently installed:
```
Installed:
  kubeadm.x86_64 0:1.13.3-0               kubectl.x86_64 0:1.13.3-0               kubelet.x86_64 0:1.13.3-0
```

check `/etc/sysctl.conf` file, for example:
```bash
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
net.ipv4.ip_forward = 0
...
```

ensure that these 3 options exist and set to 1, because some users on RHEL/CentOS 7 have reported issues with traffic being routed incorrectly due to iptables being bypassed
```
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
```

if these 3 items not set, edit `net.ipv4.ip_forward = 1` and append `net.bridge.bridge-nf-call-ip6tables = 1` and `net.bridge.bridge-nf-call-iptables = 1` in `sysctl.conf` file

then make sure that the `net.bridge.bridge-nf-call` is enabled, check if `br_netfilter` module is loaded. This can be done by running
```bash
lsmod | grep br_netfilter
```
if not, to load it explicitly call 
```bash
modprobe br_netfilter
```
next run this command to reload setting
```
sysctl --system
```
then you can check the final setting:
```bash
sysctl -a | grep -E "net.bridge|net.ipv4.ip_forward"
```

### Install docker
> [CRI installation in Kubernetes](https://kubernetes.io/docs/setup/cri/#docker)

#### Uninstall old versions
Older versions of Docker were called docker or docker-engine. If these are installed, uninstall them, along with associated dependencies.
```bash
yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-engine
```
> [Official Docker installation guides](https://docs.docker.com/install/linux/docker-ce/centos/)

#### Install Docker CE
currently Docker version `18.06.2` is recommended, but 1.11, 1.12, 1.13 and 17.03 are known to work as well. Keep track of the latest verified Docker version in the Kubernetes release notes
```bash
# Set up the repository
yum install yum-utils device-mapper-persistent-data lvm2

# Add docker repository.
yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

# Install docker ce.
yum update && yum install docker-ce-18.06.2.ce

# Create /etc/docker directory.
mkdir /etc/docker

# Setup daemon.
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF

mkdir -p /etc/systemd/system/docker.service.d

# Restart docker.
systemctl daemon-reload
systemctl restart docker
```
check result
```
[root@myk8s1 ~] docker version
Client:
 Version:           18.06.2-ce
 API version:       1.38
 Go version:        go1.10.3
 Git commit:        6d37f41
 Built:             Sun Feb 10 03:46:03 2019
 OS/Arch:           linux/amd64
 Experimental:      false

Server:
 Engine:
  Version:          18.06.2-ce
  API version:      1.38 (minimum version 1.12)
  Go version:       go1.10.3
  Git commit:       6d37f41
  Built:            Sun Feb 10 03:48:29 2019
  OS/Arch:          linux/amd64
  Experimental:     false

```

### Disable swap
[**why we need to disable swap?** ](https://serverfault.com/questions/881517/why-disable-swap-on-kubernetes)
Swap brings disk IO overhead, as well as breaking cgroups for pod memory control.

in `/etc/fstab` file, comment out swap setting
```
/dev/mapper/centos-swap swap                    swap    defaults        0 0
```
activate new configuration and check
```
swapoff -a
```
```
[root@myk8s3 ~] free -h
              total        used        free      shared  buff/cache   available
Mem:           7.6G        189M        5.7G        136M        1.8G        7.0G
Swap:            0B          0B          0B

```

>  for `worker` nodes in cluster, stop here. Continue steps in `master` node:

## Initialize kubernetes cluster
I will use `Calico` as the container network solution, in master node, run
```
kubeadm init --pod-network-cidr=192.168.0.0/16
```
you can specify the version by using `--kubernetes-version v1.13.3`, otherwise it will pull latest version from Internet.

you can see the output like this
```bash
...
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"

...
Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 9.30.97.218:6443 --token jjkiw2.n478eree0wrr3bmc --discovery-token-ca-cert-hash sha256:79659fb0b3fb0044f382ab5a5e317d4f775e821a61d0df4a401a4cbd8d8c5a7f


```
keep the last command for joining worker node later
```bash
kubeadm join 9.30.97.218:6443 --token jjkiw2.n478eree0wrr3bmc --discovery-token-ca-cert-hash sha256:79659fb0b3fb0044f382ab5a5e317d4f775e821a61d0df4a401a4cbd8d8c5a7f

```
then run following command in master node:
```bash
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
```
now if you run `kubectl version`, you will get something like below:
```
[root@myk8s1 ~] kubectl version
Client Version: version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.3", GitCommit:"721bfa751924da8d1680787490c54b9179b1fed0", GitTreeState:"clean", BuildDate:"2019-02-01T20:08:12Z", GoVersion:"go1.11.5", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.3", GitCommit:"721bfa751924da8d1680787490c54b9179b1fed0", GitTreeState:"clean", BuildDate:"2019-02-01T20:00:57Z", GoVersion:"go1.11.5", Compiler:"gc", Platform:"linux/amd64"}
```

let's check what kind of docker images pulled from network to create the cluster in master
```
[root@myk8s1 ~] docker images
REPOSITORY                           TAG                 IMAGE ID            CREATED             SIZE
k8s.gcr.io/kube-apiserver            v1.13.3             fe242e556a99        2 weeks ago         181MB
k8s.gcr.io/kube-controller-manager   v1.13.3             0482f6400933        2 weeks ago         146MB
k8s.gcr.io/kube-proxy                v1.13.3             98db19758ad4        2 weeks ago         80.3MB
k8s.gcr.io/kube-scheduler            v1.13.3             3a6f709e97a0        2 weeks ago         79.6MB
k8s.gcr.io/coredns                   1.2.6               f59dcacceff4        3 months ago        40MB
k8s.gcr.io/etcd                      3.2.24              3cab8e1b9802        4 months ago        220MB
k8s.gcr.io/pause                     3.1                 da86e6ba6ca1        14 months ago       742kB
```

## Launch cluster network
```bash
# kubectl get pods --all-namespaces
NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE
kube-system   coredns-86c58d9df4-5dfh9                      0/1     Pending   0          9m30s
kube-system   coredns-86c58d9df4-d9bfm                      0/1     Pending   0          9m30s
kube-system   etcd-myk8s1.fyre.ibm.com                      1/1     Running   0          8m52s
kube-system   kube-apiserver-myk8s1.fyre.ibm.com            1/1     Running   0          8m37s
kube-system   kube-controller-manager-myk8s1.fyre.ibm.com   1/1     Running   0          8m34s
kube-system   kube-proxy-wxjx8                              1/1     Running   0          9m31s
kube-system   kube-scheduler-myk8s1.fyre.ibm.com            1/1     Running   0          8m46s
```
you can find some pods are not ready, for example `coredns-86c58d9df4-5dfh9` and `coredns-86c58d9df4-d9bfm`, also the master node
```
[root@myk8s1 ~] kubectl get nodes
NAME                  STATUS     ROLES    AGE   VERSION
myk8s1.fyre.ibm.com   NotReady   master   11m   v1.13.3
```
it's time to set up network, you should first figure out which `Calico` version you need, check kubernetes [release note](https://kubernetes.io/docs/setup/release/notes/), we see currently it support `Calico` version 3.3.1:

![](https://drive.google.com/uc?id=1pl6EV9YwD604U1ItoMKnqlAp0cjwEOAd)

 you can also refer this [link](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network) to install, it's the same setting as below:
![](https://drive.google.com/uc?id=11R3h2G2oknDcN3sY1cTdwUHQCnxwRysn)

```
kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml
kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml

```
after applying `rbac-kdd.yaml` and `calico.yaml`, now you can see 
```bash
# kubectl get pods --all-namespaces
NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE
kube-system   calico-node-4vm2c                             2/2     Running   0          45s
kube-system   coredns-86c58d9df4-5dfh9                      1/1     Running   0          37m
kube-system   coredns-86c58d9df4-d9bfm                      1/1     Running   0          37m
kube-system   etcd-myk8s1.fyre.ibm.com                      1/1     Running   0          36m
kube-system   kube-apiserver-myk8s1.fyre.ibm.com            1/1     Running   0          36m
kube-system   kube-controller-manager-myk8s1.fyre.ibm.com   1/1     Running   0          36m
kube-system   kube-proxy-wxjx8                              1/1     Running   0          37m
kube-system   kube-scheduler-myk8s1.fyre.ibm.com            1/1     Running   0          36m
```
```bash
# kubectl get nodes
NAME                  STATUS   ROLES    AGE   VERSION
myk8s1.fyre.ibm.com   Ready    master   38m   v1.13.3
```

> Note that I encountered the problem that when join the worker nodes, the `calico-node` becomes not ready
```bash
# kubectl get pods --all-namespaces
NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE
kube-system   calico-node-4vm2c                             1/2     Running   0          11m
kube-system   calico-node-zsbjj                             1/2     Running   0          96s
kube-system   coredns-86c58d9df4-5dfh9                      1/1     Running   0          48m
...
```

The reason is my master node has multiple `eth`, I need to specify which one to use in order to be consistent among all nodes.
```
wget https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml
```
delete the previous `Calico` deployment and then edit and apply yaml file again:

![](https://drive.google.com/uc?id=1bPLxrrlLLyFvNn6cBdSIJbWpANy-ht4h)

```
[root@myk8s1 ~] kubectl get pods --all-namespaces
NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE
kube-system   calico-node-dpcsp                             2/2     Running   0          6m15s
kube-system   calico-node-gc5hs                             2/2     Running   0          6m15s
kube-system   coredns-86c58d9df4-5dfh9                      1/1     Running   0          81m
...
```

## Join worker nodes
Join worker nodes is pretty easy, run this command on all worker nodes:
```bash
kubeadm join 9.30.97.218:6443 --token jjkiw2.n478eree0wrr3bmc --discovery-token-ca-cert-hash sha256:79659fb0b3fb0044f382ab5a5e317d4f775e821a61d0df4a401a4cbd8d8c5a7f
```
Check node status
```bash
# kubectl get nodes
NAME                  STATUS   ROLES    AGE   VERSION
myk8s1.fyre.ibm.com   Ready    master   83m   v1.13.3
myk8s2.fyre.ibm.com   Ready    <none>   36m   v1.13.3
myk8s3.fyre.ibm.com   Ready    <none>   33s   v1.13.3
```

By default, tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired, you can create a new token by running the following command on the master node
```
kubeadm token create
```
If you don’t have the value of `--discovery-token-ca-cert-hash`, you can get it by running the following command chain on the master node:
```bash
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | \
   openssl dgst -sha256 -hex | sed 's/^.* //'
```

Now a fresh kubernetes cluster with 1 master and 2 worker nodes is created.



]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>calico</tag>
        <tag>kubeadm</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubectx and Kubens</title>
    <url>/2019/02/17/k8s-kubectx-kubens/</url>
    <content><![CDATA[
I want to introduce you two useful tools for kubernetes development, [github link](https://github.com/ahmetb/kubectx) with detail.

> Note: this is not an official Google product.

# kubectx
`kubectx` helps you switching between clusters back and forth.

## Install
Since kubectx/kubens are written in Bash, you should be able to install them to any POSIX environment that has Bash installed.
```bash
git clone https://github.com/ahmetb/kubectx /opt/kubectx
```

Make sure kubectx script is executable:
```
[root@myk8s1 ~] ls /opt/kubectx/ -ltr
total 40
-rw-r--r-- 1 root root 11357 Feb 17 21:25 LICENSE
-rw-r--r-- 1 root root   968 Feb 17 21:25 CONTRIBUTING.md
-rw-r--r-- 1 root root  7784 Feb 17 21:25 README.md
drwxr-xr-x 2 root root   121 Feb 17 21:25 completion
drwxr-xr-x 2 root root    84 Feb 17 21:25 img
drwxr-xr-x 3 root root   100 Feb 17 21:25 test
-rwxr-xr-x 1 root root  5273 Feb 17 21:25 kubens
-rwxr-xr-x 1 root root  5218 Feb 17 21:25 kubectx
```

Create symlinks to kubectx/kubens from somewhere in your PATH, like /usr/local/bin
```bash
ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx
```

## Usage
You should first understand how to switch among different clusters by using configuration files. please investigate this [link](https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/), actually `kubectx` is built on top of it.

For example, I have one cluster on AWS and one cluster on Fyre, in each cluster there is a `~/.kube/config` file, rename it as `config.aws` and `config.fyre` and put them to another client machine `~/.kube/` folder with  kubectl installed.
```bash
[root@centctl1 .kube]# ls -ltr
total 16
drwxr-xr-x 3 root root   23 Nov 26 16:38 cache
-rw-r--r-- 1 root root 2214 Dec  6 10:16 config.aws
drwxr-xr-x 2 root root   73 Dec  6 10:16 kubens
drwxr-xr-x 3 root root 4096 Feb 17 22:05 http-cache
-rw------- 1 root root 5474 Feb 17 22:22 config.fyre
```

Append config files to environment variable `KUBECONFIG `, you can add export to `.bashrc` file.
```bash
export KUBECONFIG=$KUBECONFIG:$HOME/.kube/config.aws:$HOME/.kube/config.fyre
```

Now if you run `kubectx` you will see there are 2 contexts:
```bash
[root@centctl1 .kube] kubectx
arn:aws:eks:us-west-2:296744932886:cluster/IIS-Test-Cluster
kubernetes-admin@kubernetes
```

Jump to `kubernetes-admin@kubernetes`:
```bash
[root@centctl1 .kube] kubectx kubernetes-admin@kubernetes
Switched to context "kubernetes-admin@kubernetes".
```

Jump back:
```bash
[root@centctl1 .kube] kubectx -
Switched to context "arn:aws:eks:us-west-2:296744932886:cluster/IIS-Test-Cluster".
```

It is the same as you run below commands:
```bash
kubectl config view
kubectl config --kubeconfig=config.fyre use-context kubernetes-admin@kubernetes
```

# kubens
`kubens` helps you switch between Kubernetes namespaces smoothly, so you don't need to add `-n <namespace>` in every command.

## Install
Download the same Github repository as `kubectx`, add symlinks:
```bash
ln -s /opt/kubectx/kubens /usr/local/bin/kubens
```

## Usage
```bash
[root@myk8s1 ~] kubens
default
kube-public
kube-system
[root@myk8s1 ~] kubens kube-system
Context "kubernetes-admin@kubernetes" modified.
Active namespace is "kube-system".
[root@myk8s1 ~] kubens -
Context "kubernetes-admin@kubernetes" modified.
Active namespace is "default".
```
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>K8s No Route to Host</title>
    <url>/2019/08/19/k8s-no-route-to-host/</url>
    <content><![CDATA[
**Kubernetes version 1.13.2**

Today I set up a 4 nodes cluster that 3 nodes belong to the same group and one node from another group. It works fine:
```
NAME                   STATUS   ROLES    AGE     VERSION
dstest1.fyre.xxx.com   Ready    master   4h22m   v1.13.2
dstest2.fyre.xxx.com   Ready    <none>   4h15m   v1.13.2
dstest3.fyre.xxx.com   Ready    <none>   4h15m   v1.13.2
opsf3.fyre.xxx.com     Ready    <none>   4h15m   v1.13.2
```

After I scheduling a pod in `opsf3.fyre.xxx.com` and run `kubectl exec -it`, I get this error:
```
Error from server: error dialing backend: dial tcp 172.16.11.239:10250: connect: no route to host
```

The reason is the firewall is active in `opsf3.fyre.xxx.com` if you check by running:
```
systemctl status firewalld
```
Run below commands to stop and disable it, then thing get works.
```
systemctl stop firewalld
systemctl disable firewalld
```
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Operator Learning</title>
    <url>/2020/01/23/k8s-operator/</url>
    <content><![CDATA[
我有另一篇blog专门记录了Kubernetes Operators这本书的总结:
[Kubernetes Operators](https://chengdol.github.io/2020/06/03/book-k8s-operator/)

根据书中资料总结的demo:
https://github.com/chengdol/k8s-operator-sdk-demo

最近安排我去写一个operator的任务，最开始是helm-based operator, then evolve to Go-based, 挺有意思。How to explain Kubernetes Operators in plain English:
https://enterprisersproject.com/article/2019/2/kubernetes-operators-plain-english

Brief introduction:
https://www.youtube.com/watch?v=DhvYfNMOh6A

首先，从K8s官网上粗略地了解一下什么是:
- [Getting started with the Operator SDK](https://docs.openshift.com/container-platform/4.4/operators/operator_sdk/osdk-getting-started.html)
  This is from Red Hat, excellent!
  
- [Operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/)

  Advanced Operators are designed to handle upgrades seamlessly, react to failures automatically, and not take shortcuts, like skipping a software backup process to save time.

  Complex and stateful applications are where an Operator can shine. The cloud-like capabilities that are encoded into the Operator code can provide an advanced user experience, automating such features as updates, backups and scaling.

- [Introducing Operators: Putting Operational Knowledge into Software](https://coreos.com/blog/introducing-operators.html)
- [Best practices for building Kubernetes Operators and stateful apps](https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-building-kubernetes-operators-and-stateful-apps)

- [OpenShift operator special](https://www.openshift.com/blog/tag/operators)

CNCF:
- [Writing a Kubernetes Operator: the Hard Parts](https://www.youtube.com/watch?v=wMqzAOp15wo)

- [Build Kubernetes Operators from Helm Charts in 5 steps](https://www.openshift.com/blog/build-kubernetes-operators-from-helm-charts-in-5-steps)
  However when it comes to stateful applications there is more to the upgrade process than upgrading the application itself.
  
  Helm Charts often are insufficient for upgrading stateful applications and services (e.g. PostgreSQL or Elasticsearch) which require a complex and controlled upgrade processes beyond upgrading the application version. 

一些中文资料，最近在看书Kuberneter Operators，然后被Go卡主了，准备开始学习:
- [如何在kubernetes中开发自己的Operator](https://studygolang.com/articles/22798?fr=sidebar)
  提到了如何用go去写，但很简略，只能给个大致图像
- [Go 语言中文网](https://studygolang.com/topics)
- [自己动手写Operator](http://dockone.io/article/8733)
- [Kubernetes API 与 Operator，不为人知的开发者战争](https://www.jianshu.com/p/27fc6834479b)
- [Kubernetes Operator 快速入门教程](https://www.jianshu.com/p/628aac3e6758)









]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>operator</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Managed NFS</title>
    <url>/2020/01/21/k8s-nfs-example/</url>
    <content><![CDATA[
## Entry
https://kubernetes.io/docs/concepts/storage/volumes/#nfs

这个demo很有意思，从之前来看，如果需要为K8s设置NFS，则我们需要一个physical NFS server，然后create PV based on that NFS server then PVC claim from PV. 这样需要自己去维护NFS的健康保证high availability.

这个例子完全将NFS交给了K8s来管理，它先提供一块存储去构造PVC(这块存储可能来自provisioner或者其他PV)，然后用这个PVC构造了一个NFS server pod以及给这个pod绑定了一个cluster IP，这样就相当于一个虚拟的physical NFS server node了。当我们需要PV的时候，就从这个NFS server pod中取(本质就是从一个PV中构造另一些PV)。

当然为了满足NFS的特性，这个NFS server的镜像必须要特别的构造，安装了NFS的组件，暴露相关端口，以及在初始化启动程序中配置好`/etc/export`的相关参数。以及当NFS server pod被重新构造时，保证之前的share不受影响。

这样的好处就是完全交给K8s去管理，不用担心NFS高可用性的问题，也不用自己去搭建物理的NFS cluster了，只要提供一块存储，就可以构造成NFS。


## Demo
写这个blog的时候这个demo yaml中有一些错误参数，以我的blog准，这是demo git repository:
https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs

Under provisioner folder, it uses storageclass (internal provisioner) to create a PV, for example, in GCE:
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pv-provisioning-demo
  labels:
    demo: nfs-pv-provisioning
spec:
  accessModes: [ "ReadWriteOnce" ]
  resources:
    requests:
      storage: 20Gi
```

If no internal provisioner is available, you can create an external provisioner (for example: NFS), or just create a PV with hostPath:
```yaml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: nfs-pv-provisioning-demo
  labels:
    demo: nfs-pv-provisioning
spec:
  capacity:
    storage: "20Gi"
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/nfs-server-pv"
```
This `/nfs-server-pv` folder will be created on the host where nfs server pod reside. 

Then it creates a `replicationController` for **NFS server** (just like a physical NFS server):
```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: nfs-server
spec:
  replicas: 1
  selector:
    role: nfs-server
  template:
    metadata:
      labels:
        role: nfs-server
    spec:
      containers:
      - name: nfs-server
        image: k8s.gcr.io/volume-nfs:0.8
        ports:
          - name: nfs
            containerPort: 2049
          - name: mountd
            containerPort: 20048
          - name: rpcbind
            containerPort: 111
        securityContext:
          privileged: true
        volumeMounts:
            ## the nfs exports folder
          - mountPath: /exports
            name: mypvc
      volumes:
        - name: mypvc
          persistentVolumeClaim:
            ## mount the pvc from provisioner
            claimName: nfs-pv-provisioning-demo
```

Notice that the image is dedicated with nfs-utils pre-installed, and it expose some nfs dedicated ports, see the dockerfile:
```yaml
FROM centos
RUN yum -y install /usr/bin/ps nfs-utils && yum clean all
RUN mkdir -p /exports
ADD run_nfs.sh /usr/local/bin/
ADD index.html /tmp/index.html
RUN chmod 644 /tmp/index.html

## expose mountd 20048/tcp and nfsd 2049/tcp and rpcbind 111/tcp
EXPOSE 2049/tcp 20048/tcp 111/tcp 111/udp
## init script to set up this nfs server
ENTRYPOINT ["/usr/local/bin/run_nfs.sh", "/exports"]
```

Then create a service with cluster IP to expose the NFS server pod.
```yaml
  
kind: Service
apiVersion: v1
metadata:
  name: nfs-server
spec:
  ports:
    - name: nfs
      port: 2049
    - name: mountd
      port: 20048
    - name: rpcbind
      port: 111
  selector:
    role: nfs-server
```

Then we can create application PV and PVC from this service:
refer to DNS for [service and pod](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/).
```yaml
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs
spec:
  capacity:
    storage: 1Mi
  accessModes:
    - ReadWriteMany
  nfs:
    ## service name
    ## 这里需要cluster IP，如果有其他DNS配置，可以直接用service name
    server: <nfs server service cluster IP>
    path: "/exports"
```
Then you can create PVC bind this PV for other use.
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>nfs</tag>
      </tags>
  </entry>
  <entry>
    <title>Rebinding Release PV with PVC</title>
    <url>/2021/02/12/k8s-rebind-pv/</url>
    <content><![CDATA[
**Reference**
[Reuse existing Persistent Volume](https://github.com/kubernetes/kubernetes/issues/48609#issuecomment-314066616)

**Background**
Prod env upgrade failed, rolled back leads to historical monitoring data lost. The root cause is old PVC was removed accidently thus the corresponding PV got released, thanksfully the PV reclaim policy is `retain` so the data on disk was still preserved.

这里和以前遇到的不同的地方是使用了storage class, PV 定义中有`claimRef` 去做绑定.

**Question**
How to access the historical data on released PV?
显而易见需要重新bound.

**Solution**
The PV is actually provisioned dynamically by custom storage class `gce-regional-ssd`, in its definition, it is preserved for specific PVC by specifying the [`claimRef`](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reserving-a-persistentvolume) field:
```yaml
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: monitoring-alertmanager
    namespace: monitoring
    resourceVersion: "21865"
    uid: 4036d03f-fe2f-4d6f-bae8-dd67f33ad423
```
Since the PVC `monitoring-alertmanager` is alreay used another PV, to make this one available, kubectl edit to remove the `uid` and `resourceVersion`, modify the `name`, save and quit:
```yaml
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: monitoring-alertmanager-old
    namespace: monitoring
```
For now the PV becomes available only to PVC which is named `monitoring-alertmanager-old`. Or if you set `claimRef` to empty, PV will open to all PVCs.

Then creating that PVC to consume the PV (binding):
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app: monitoring
    component: alertmanager
  name: monitoring-alertmanager-old
  namespace: monitoring
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: gce-regional-ssd
  volumeMode: Filesystem
```
Then mount the PVC to your target resource to access the data.

其实如果describe PV, 可以找到在GCE中具体的Persistent Disk (under Compute Engine), 保险起见可以先snapshot 该persistent disk再做其他操作:
```yaml
Source:
    Type:       GCEPersistentDisk (a Persistent Disk resource in Google Compute Engine)
    PDName:     gke-us-east4-f3e3fedd--pvc-4036d03f-fe2f-4d6f-bae8-dd67f33ad423
    FSType:     ext4
    Partition:  0
    ReadOnly:   false
```
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Patch K8s Objects in Place</title>
    <url>/2020/02/07/k8s-patch-object/</url>
    <content><![CDATA[
For updating components secret or configmap, you can do complete replace:
```bash
kubectl get secret docker-registry-auth -o yaml -n default \
        | sed -e "/htpasswd/c\  htpasswd: ${AUTH_BASE64}" \
        | kubectl replace -f -
kubectl get configmap repl-pod-map -n $NAMESPACE -o yaml 
        | sed -e "/${POD_NAME}/d" 
        | kubectl replace -f-
```

For patching pod controller like deployment and statefulset, see:
https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/

## Strategic merge patch
For example, add one more containers in the pod, give the yaml file `patch-file.yaml`:
```yaml
spec:
  template:
    spec:
      containers:
      - name: patch-demo-ctr-2
        image: redis
```
```bash
# this command use default patch strategy for containers field: merge
kubectl patch deployment patch-demo --patch "$(cat patch-file.yaml)"
```
This will get merged not replaced (the original container is kept), but not all fields have `merge` strategy, some use default strategy that is `replace`, for example: tolerations field.

How to know the patch strategy of each field?
see [API document](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/)
It seems not all fields have specified patch strategy, for example, I don't see that on configmap.

## Json merge patch
A strategic merge patch is different from a JSON merge patch. With a JSON merge patch, if you want to update a list, you have to specify the `entire new list`. And the new list completely replaces the existing list.

The `kubectl patch` command has a `--type` parameter that you can set to one of these values:
1. json (json patch)
2. merge (json merge patch)
3. strategic (default)

> 这里文档有问题，如果用json merge patch on configmap，其实还是做的merge操作(之前的数据被保留)，并不是replace.这个值得注意，比如以下2个commands效果是一样的:
  ```bash
  ## json merge patch
  kubectl patch configmap repl-pod-map -n pines --type merge -p "{\"data\": {\"test1\":\"test1key\"}}"
  ## defaul strategic patch
  kubectl patch configmap repl-pod-map -n pines  -p "{\"data\": {\"test1\":\"test1key\"}}"
  ```
  我猜可能是configmap没有明确的patch strategy定义，但对于其他有明确定义的field，则json merge patch会replace之前的数据。








]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>patch</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Security</title>
    <url>/2019/06/24/k8s-security/</url>
    <content><![CDATA[
**Kubernetes version 1.13.2**

I believe security is an unavoidable topic in Kubernetes or OpenShift, I have encountered lots of SCCs and restricted settings.

https://stackoverflow.com/questions/52700507/k8s-what-is-the-difference-between-security-context-and-security-policy

https://kubernetes-security.info/

]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>security</tag>
      </tags>
  </entry>
  <entry>
    <title>Rook Storage Orchestrator</title>
    <url>/2020/01/20/k8s-rook/</url>
    <content><![CDATA[
https://medium.com/flant-com/to-rook-in-kubernetes-df13465ff553

[Rook](https://rook.io/) turns distributed storage systems into self-managing, self-scaling, self-healing storage services. It automates the tasks of a storage administrator: deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management.

Rook uses the power of the Kubernetes platform to deliver its services: cloud-native container management, scheduling, and orchestration.

Another similar tool is [Kubernetes External Provisioner](https://chengdol.github.io/2019/12/10/k8s-external-provisioner/).

## NFS
Now is **alpha version**
in order to use rool NFS, we must have a NFS provisioner or PV first, then rook will work on top of that, manage the NFS storage and provision it again to other application.

https://rook.io/docs/rook/v1.2/nfs.html
好好理解一下这段话：
The desired volume to export needs to be attached to the NFS server pod via a PVC. Any type of PVC can be attached and exported, such as Host Path, AWS Elastic Block Store, GCP Persistent Disk, CephFS, Ceph RBD, etc. The limitations of these volumes also apply while they are shared by NFS. You can read further about the details and limitations of these volumes in the Kubernetes docs.

> NFS is just a pattern, the file system can be any.

NFS client packages must be installed on all nodes where Kubernetes might run pods with NFS mounted. Install nfs-utils on CentOS nodes or nfs-common on Ubuntu nodes.


## Ceph
rook will create a soft ceph cluster for us.

]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>rook</tag>
      </tags>
  </entry>
  <entry>
    <title>RBAC Authorization for K8s API Access</title>
    <url>/2019/01/27/k8s-role-rbac/</url>
    <content><![CDATA[
**Kubernetes version 1.13.2**

We want to **scale the compute pods by calling k8s API from inside the engine conductor container**, this definitely need to be authorized and we need to grant privilege for this action.

There are some concepts you need to know in order to achieve the goal.

## Service Account
> [what is Service Account](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/)

Processes in containers inside pods can contact the apiserver. When they do, they are authenticated as a particular service account (for example, by default is `default` service account).

Once you create a namespace, for example `test-1`, there is a `default` service account automatically generated.
```bash
kubectl get sa -n test-1
```
```bash
NAME      SECRETS   AGE
default   1         139m
```
let's see what is inside the service account
```bash
kubectl describe sa default -n test-1
```
```bash
Name:                default
Namespace:           test-1
Labels:              <none>
Annotations:         <none>
Image pull secrets:  <none>
Mountable secrets:   default-token-mtv4n
Tokens:              default-token-mtv4n
Events:              <none>
```
Here we see there is a mountable secret `default-token-mtv4n`, that is the credentials to access the apiserver.
```bash
kubectl describe secret default-token-mtv4n -n test-1
```
```bash
Name:         default-token-mtv4n
Namespace:    test-1
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: default
              kubernetes.io/service-account.uid: 387381d3-2272-11e9-91a2-00163e0196e7

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  6 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrd...
```

## ClusterRole
> [what is Role and ClusterRole](https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole)

A `ClusterRole` can be used to grant the same permissions as a `Role`, but because they are `cluster-scoped`, they can also be used to grant access to
- cluster-scoped resources (like nodes)
- non-resource endpoints (like “/healthz”)
- namespaced resources (like pods) across all namespaces

Here we use a cluster role called `cluster-admin`, it's generated by default
```bash
kubectl get clusterrole | grep cluster-admin
```
```bash
cluster-admin                        174m
```
## ClusterRole Binding

> [what is RoleBinding and ClusterRole Binding](https://kubernetes.io/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding)

A role binding grants the permissions defined in a role to a user or set of users. It holds a list of subjects (users, groups, or service accounts), and a reference to the role being granted. Permissions can be granted within a namespace with a `RoleBinding`, or cluster-wide with a `ClusterRoleBinding`.

below we grant service account `default` in namespace `test-1` the `cluster-admin` level privilege.
```yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: <cluster role name>
  labels:
    <key:value>
subjects:
 - kind: ServiceAccount
   name: default
   namespace: <namespace of sa>
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
```
then we can write a script, using `curl` to call K8s API, for example, to scale the number of compute pods:
```bash
http_code=$(curl -w "%{http_code}" -sS  --cacert $CACERT  -XPATCH -H "Content-Type: application/strategic-merge-patch+json" -H "Accept: application/json" -H "Authorization: Bearer $TOKEN" "https://kubernetes.default/apis/apps/v1/namespaces/$NAMESPACE/statefulsets/is-engine-compute" --data "{\"spec\":{\"replicas\":$REP}}" -o $OUT_FILE)
if [[ $http_code -ne 200 ]]; then
    ${JQ} '{ result:.status, code: .code,  message: .message }' $OUT_FILE
    exit 1
fi
```

Where are these `CACERT`, `TOEKN` and `NAMESPACE` from? Actually each container has a default mount point reside in:
```
/var/run/secrets/kubernetes.io/serviceaccount
```
You can see this when you run `kubectl describe pod`. Just like other mount files, there are 3 files, for example:
```
total 0
lrwxrwxrwx 1 root root 13 Sep 14 16:21 ca.crt -> ..data/ca.crt
lrwxrwxrwx 1 root root 16 Sep 14 16:21 namespace -> ..data/namespace
lrwxrwxrwx 1 root root 12 Sep 14 16:21 token -> ..data/token
```

All of them are used in `curl` command above.
```
NAMESPACE=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)
CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
```
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>RBAC</tag>
        <tag>service account</tag>
      </tags>
  </entry>
  <entry>
    <title>Lose Connection to K8s Server</title>
    <url>/2019/06/11/k8s-server-missing/</url>
    <content><![CDATA[
**Kubernetes version 1.13.2**

## Server Login Failed
This morning I find I lose the connection with my icp4d kubernetes server (it was good last night), if I run:
```yaml
# kubectl get pods
error: the server doesn't have a resource type "pods"
```
then:
```yaml
# kubectl version
Client Version: version.Info{Major:"1", Minor:"12", GitVersion:"v1.12.4+icp-ee", GitCommit:"d03f6421b5463042d87aa0211f116ba4848a0d0f", GitTreeState:"clean", BuildDate:"2019-01-17T13:14:09Z", GoVersion:"go1.10.4", Compiler:"gc", Platform:"linux/amd64"}

error: You must be logged in to the server (the server has asked for the client to provide credentials)
```

But it seems kubectl config is good, the token is there:
```yaml
# kubectl config view
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://172.16.3.23:8001
  name: icp-cluster1
- cluster:
    certificate-authority: mycluster/ca.pem
    server: https://172.16.3.23:8001
  name: mycluster
contexts:
- context:
    cluster: icp-cluster1
    user: admin
  name: ""
- context:
    cluster: mycluster
    user: mycluster
  name: mycluster
- context:
    cluster: mycluster
    namespace: zen
    user: mycluster-user
  name: mycluster-context
current-context: mycluster-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    client-certificate: /etc/cfc/conf/kubecfg.crt
    client-key: /etc/cfc/conf/kubecfg.key
- name: mycluster
  user:
    client-certificate: /ibm/InstallPackage/ibm-cp-app/cluster/cfc-certs/kubernetes/kubecfg.crt
    client-key: /ibm/InstallPackage/ibm-cp-app/cluster/cfc-certs/kubernetes/kubecfg.key
- name: mycluster-user
  user:
    token: eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdF9oYXNoIjoiYjVkNDMxZDMwNGZmMGUyYWM0NWJlOTY1NjU5YTQyN2ViOWUwNzE5NCIsInJlYWxtTmFtZSI6ImN1c3RvbVJlYWxtIiwidW5pcXVlU2VjdXJpdHlOYW1lIjoiYWRtaW4iLCJpc3MiOiJodHRwczovL215Y2x1c3Rlci5pY3A6OTQ0My9vaWRjL2VuZHBvaW50L09QIiwiYXVkIjoiYTc1ZTZmZjQ3YzQyZTJhZDA3YjZiMjUzMTVmZTExMTQiLCJleHAiOjE1NjAyMzkwNzEsImlhdCI6MTU2MDIxMDI3MSwic3ViIjoiYWRtaW4iLCJ0ZWFtUm9sZU1hcHBpbmdzIjpbXX0.cwGioosvwjONIllJExWRADicgibShbSl2x05r3hpiMpXQQia_4HDuvfUCNNyvLiFkBfz1xvuoz9JeAkOdRa7QVR0RD8TGVnYyu10S50AQ5b_LjGaTNoxdGJjLLEGkBt5gzJCsZaVw49ttd-lzDV28badpUBtm1cih4-3o-wbM6inJqCqR97ujgImRW0BS0Jj1pbENAEidAquyZscGMje5vyyRc9A67VWWJxZXo0J1fG081yhvaryRWbvinLLSPRm8_eley1GqItUMvRmIpzC-X7xsg4zIvCE8QhPoKrJp2xRFjDwsvCN44wJv9hdkfx3cGxjjOBdg6ofsVkNND5njg
```

I check the docker and kubelet status, all are active:
```bash
systemctl status docker
systemctl status kubelet
```

Then I try to reboot all nodes, and it set up correctly. Don't know how to reproduce this issue, no idea what happened and how to fix it (without rebooting), sadly.

## Server Connection Refused 6443
Similar issue happened again in my `dstest` cluster:
```yaml
# kubectl get pods -n test-1
The connection to the server 9.30.188.95:6443 was refused - did you specify the right host or port?

# kubectl version
Client Version: version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.2", GitCommit:"cff46ab41ff0bb44d8584413b598ad8360ec1def", GitTreeState:"clean", BuildDate:"2019-01-10T23:35:51Z", GoVersion:"go1.11.4", Compiler:"gc", Platform:"linux/amd64"}
The connection to the server 9.30.188.95:6443 was refused - did you specify the right host or port?
```

Check this [required ports list](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports).
```
# netstat -tunlp | grep 6443
tcp6       0      0 :::6443                 :::*                    LISTEN      27047/kube-apiserve
```

> Note, there is no `kube-apiserver` service in `systemctl`, so how to restart it? The `kube-apiserver` is from a static pod, so I think I can restart the container directly by `docker restart <container ID>`:
```yaml
# docker ps -a | grep apiserver
8f661411fa02        177db4b8e93a           "kube-apiserver --au..."   About an hour ago   Up 4 minutes                            k8s_kube-apiserver_kube-apiserver-dstest1.fyre.ibm.com_kube-system_d175f38c007e23cc443d6ba50ba15533_0
0f05946a8a59        k8s.gcr.io/pause:3.1   "/pause"                 About an hour ago   Up About an hour                        k8s_POD_kube-apiserver-dstest1.fyre.ibm.com_kube-system_d175f38c007e23cc443d6ba50ba15533_0
```
Haven't got chance to reproduce this issue, this solution may not work...

In a health cluster:
```yaml
# kbc cluster-info
Kubernetes master is running at https://9.30.188.95:6443
KubeDNS is running at https://9.30.188.95:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

## Server Connection Refused 8080
This issue is similar to 6443 one, but it shows:
```yaml
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

Recall that when we set up K8s cluster by `kubeadm`, we run:
```yaml
...
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
...
Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

I can reproduce this issue if the environment variable `KUBECONFIG` is missing, so try to export it, both ways are fine:
```bash
export KUBECONFIG=$HOME/.kube/config
export KUBECONFIG=/etc/kubernetes/admin.conf
```

A good `/etc/kubernetes` folder has these items:
```yaml
# ls -ltr /etc/kubernetes/
total 36
drwxr-xr-x 3 root root 4096 Jun 20 10:02 pki
-rw------- 1 root root 5447 Jun 20 10:02 admin.conf
-rw------- 1 root root 5539 Jun 20 10:02 kubelet.conf
-rw------- 1 root root 5483 Jun 20 10:02 controller-manager.conf
-rw------- 1 root root 5435 Jun 20 10:02 scheduler.conf
drwxr-xr-x 2 root root  113 Jun 20 10:02 manifests
```
The `manifests` contains yaml files for creating etcd, kube-apiserver and kube-controller-manager, kube-scheduler.


]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Moving Pod to Another Node</title>
    <url>/2022/05/24/k8s-move-pod/</url>
    <content><![CDATA[
Sometimes there is a need to shift pod to another node in order to rebalance the k8s node load, such as context switch, CPU LA, network traffic, etc.

For pods controlled by deployment or statefulset, delete the pod alone will usually result in recreating it in the same node again, there is a simple workaround with cordon:
```bash
# make the node unschedulable for new pods
kubectl cordon <pod host node name>

# delete the target pod, it will be recreated  in another node
kubectl delete pod <pod name>

# revert node status
kubectl uncordon <pod host node name>
```

This approach is much convenient than affinity.]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes sysctl</title>
    <url>/2019/05/07/k8s-sysctl/</url>
    <content><![CDATA[
**Kubernetes version 1.13.2**

In my article [`<<Linux IPC>>`](https://chengdol.github.io/2019/05/01/linux-ipcs/), I mentioned that there is a workaround to set IPC kernel parameters using `sysctl` in Kubernetes cluster if `SYS_RESOURCE` is not allowed.

### Clarification
From the Kubernetes [document](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/), we see:

Sysctls are grouped into safe and unsafe sysctls. This means that setting a safe sysctl for one pod:

* must not have any influence on any other pod on the node
* must not allow to harm the node’s health
* must not allow to gain CPU or memory resources outside of the resource limits of a pod.

By far, most of the namespaced sysctls are not necessarily considered safe (please check latest Kubernetes document to figure out), now it supports:

* kernel.shm_rmid_forced,
* net.ipv4.ip_local_port_range,
* net.ipv4.tcp_syncookies.

This list will be extended in future Kubernetes versions when the kubelet supports better isolation mechanisms.

All `safe` sysctls are enabled by default (you can use it directly without additional configuration in kubelet).

All `unsafe` sysctls are disabled by default and must be allowed manually by the cluster admin on a per-node basis. Pods with disabled unsafe sysctls will be scheduled, but will fail to launch:

![](https://drive.google.com/uc?id=1ROz7A1kQPzeaBtpLiYPyFHjii0jIvJ52)

If you describe the failed pod, you get:
![](https://drive.google.com/uc?id=1T7tUI4jIgGXt_qURYSNME-Qtz2RZc-XI)

A number of sysctls are `namespaced` in today’s Linux kernels. This means that they can be set independently for each pod on a node. **Only** namespaced sysctls are configurable via the pod securityContext within Kubernetes.

The following sysctls are known to be namespaced. This list could change in future versions of the Linux kernel.

* kernel.shm*
* kernel.msg*
* kernel.sem
* fs.mqueue.*
* net.*

Sysctls with no namespace are called `node-level` sysctls. If you need to set them, you must manually configure them on each node’s operating system, or by using a DaemonSet with privileged containers.

As with node-level sysctls it is recommended to use taints and toleration feature or taints on nodes to schedule those pods onto the right nodes.

Use the pod securityContext to configure namespaced sysctls. The securityContext applies to **all** containers in the same pod.

### Configure kubelet
If you need to use **unsafe** sysctls, configure kubelet in target node (configure the node that the unsafe sysctls pod will reside) is a must. Go to edit `10-kubeadm.conf` file in `/etc/systemd/system/kubelet.service.d/`, add
```
Environment="KUBELET_UNSAFE_SYSCTLS=--allowed-unsafe-sysctls='kernel.shm*,kernel.sem,kernel.msg*'"

```
Here I need `kernel.shm*`, `kernel.sem` and `kernel.msg*`.

![](https://drive.google.com/uc?id=11mDU4EZv5p6MJJYahaRblXICFBUY52aD)

then run:
```
systemctl daemon-reload
systemctl restart kubelet
```

verify changes, you can see `--allowed-unsafe-sysctls` is there:
```
ps aux | grep kubelet
```

> A brief digress: the kubelet service unit file is in `/etc/systemd/system/kubelet.service`.

Then you can edit YAML file to add `sysctls` option:

![](https://drive.google.com/uc?id=122vY73xveFwkFrtsipVJb50WTCiwgjUq)

Sometimes you need to disable `hostIPC`, if not you will get this problem:

![](https://drive.google.com/uc?id=1s1sLiMy_FCCu_ClNJlX9r8C3k_Kw6c_E)

After things done, get into the container to check the kernel parameter vaule, for example:
```
sysctl -a | grep -i kernel.sem
```

### Resources
[kubernetes 1.4 new feature: support sysctls](https://blog.csdn.net/horsefoot/article/details/53007177?utm_source=blogxgwz5)
[configure kernel parameters in k8s cluster](https://yq.aliyun.com/articles/603745?utm_content=m_1000003747)]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>sysctl</tag>
      </tags>
  </entry>
  <entry>
    <title>Set systemd as Cgroup Driver</title>
    <url>/2019/03/09/k8s-systemd-cgroup-driver/</url>
    <content><![CDATA[
**Kubernetes version 1.13.2**

We want to use `systemd` as cgroup driver for docker and kubelet, let's see how to achieve that.

First you need to understand what is `systemd` and `cgroup`?
You can refer to [this article](https://linuxaria.com/article/how-to-manage-processes-with-cgroup-on-systemd). 

`systemd` is a suite of system management daemons, libraries, and utilities designed as a central management and configuration platform for the GNU/Linux computer operating system. It provides a system and service manager that runs as PID `1` and starts the rest of the system as alternative to the traditional sysVinit.

systemd organizes processes with `cgroups`, this is a Linux kernel feature to limit, police and account the resource usage of certain processes (actually process groups).

### Configure docker
After you install and start docker, by default it will use `cgroupfs` as the cgroup driver, check by running:
```
docker info | grep Cgroup

Cgroup Driver: cgroupfs
```
Edit `/usr/lib/systemd/system/docker.service` file:
```
ExecStart=/usr/bin/dockerd --exec-opt native.cgroupdriver=systemd
```
Then reload daemon and restart docker
```
systemctl daemon-reload
systemctl restart docker
```
Verify the change
```
docker info | grep Cgroup

Cgroup Driver: systemd
```

### Configure kubelet
Currently, the kubelet cannot automatically detects the cgroup driver used by the CRI runtime, but the value of `--cgroup-driver` must match the cgroup driver used by the CRI runtime to ensure the health of the kubelet.

**Note**: interesting thing is `kubeadm init` now can automatically detect and set kubelet with the same cgroup driver as docker (I use version `1.13.x`). 

There is a file:  `/var/lib/kubelet/kubeadm-flags.env`, that `kubeadm init` and `kubeadm join` generates at runtime, populating the `KUBELET_KUBEADM_ARGS` variable dynamically, in `/etc/systemd/system/kubelet.service.d/10-kubeadm.conf` you can see it:
![](https://drive.google.com/uc?id=1Wn2J8wk_7e36Z-W_x5lgM93nFBkGJ7EV)

you will see `systemd` resides in `/var/lib/kubelet/kubeadm-flags.env`:
```
KUBELET_KUBEADM_ARGS=--cgroup-driver=systemd --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.1

```

Anyway let's see how to do the configuration manually. After install kubelet, go to edit `/etc/systemd/system/kubelet.service.d/10-kubeadm.conf` file, add this line:
```
Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=systemd"
```
Append `$KUBELET_CGROUP_ARGS` at end of `ExecStart=/usr/bin/kubelet` statement:

![](https://drive.google.com/uc?id=1AiZ-y12HevZivQm5TwIYYzY-OX7lfzho)

> Note: in the file `/etc/systemd/system/kubelet.service`, it seems you can also configure here: `ExecStart=/usr/bin/kubelet --cgroup-driver=systemd`, not very clear the difference.


Then when you complete `kubeadm init`,  verify the change:
```
ps aux | grep kubelet

root     19864  4.5  0.6 2326996 104192 ?      Ssl  01:21  32:49 /usr/bin/kubelet
--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf 
--kubeconfig=/etc/kubernetes/kubelet.conf 
--config=/var/lib/kubelet/config.yaml 
--cgroup-driver=systemd 
--network-plugin=cni 
--pod-infra-container-image=k8s.gcr.io/pause:3.1 
--cgroup-driver=systemd 
```
You see, there are 2 `--cgroup-driver=systemd ` options, so I think manually configure kubelet service file is needless.

]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>systemd</tag>
        <tag>cgroup driver</tag>
      </tags>
  </entry>
  <entry>
    <title>K8s NFS Mount Volume Permission</title>
    <url>/2019/08/20/k8s-volumeMount-permission-problem/</url>
    <content><![CDATA[
**Kubernetes version 1.13.2**

This is an interesting issue which involves 4 topices: `Volume`, `Security Context`, `NFS` and `initContainer`.

The issue comes from the `permission denied` error. The process fail to create the file under the mount path, I check the owner and group of that path, they are both `root`.

In the yaml file, I specify the `fsGroup` field as id `9092`, from the official document [here](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod) (The example use id `2000`):
```
Since fsGroup field is specified, all processes of the container are also part of the supplementary group ID 2000. The owner for volume /data/demo and any files created in that volume will be Group ID 2000.
```
so the owner of the volume should be `9092`, but they don't.

I searched online and met the same issue from others:
https://github.com/kubernetes/examples/issues/260

It seems `fsGroup` securityContext **does not** apply to nfs mount especially we run the containers as non-root user we cannot access the mount. This issue may be solved in later version, need to take care.

> !!! Why this happens? Because we use `hostPath`, it by default will create root owned path if `path` does not exist. Here the `NFS` is not the `NFS` way kubernetes use, we use `hostPath` then manually nfs the nodes externally, not by the setting of K8s(need to do experiment).

The workaround is using `initContainers` with `busybox` run as root and `chown` to the nfs mount with expected id, for example:
```
initContainers:
  - name: busybox
    image: xxx.com:5000/busybox:latest
    imagePullPolicy: IfNotPresent
    command: ["sh", "-c", "chown 9092:9092 /mnt"]
    securityContext:
      runAsUser: 0
    volumeMounts:
    - name: <volume name from Volumes>
      mountPath: /mnt
```
then we are good.



]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux LPIC-1 Training</title>
    <url>/2020/04/15/linux-LPIC1-training/</url>
    <content><![CDATA[
//TODO
这篇总结是来自PluralSight上的`LPIC-1`课程的Essential章节。
备注:2020年4月份pluralsight在搞活动，免费注册学习！这次lock down是个机会补补课。

Environment: `CentOS 7 Enterprise Linux` or `RedHat`.

# Essentials
Reading OS data
```bash
# system version
# softlink actually
cat /etc/os-release
cat /etc/system-release
cat /etc/redhat-release

# kernel release number
uname -r
cat /proc/version
```

## Shutdown
Send message to others
```bash
# send to individual user terminal
write dsadm
> xxx

# send to all user in terminals
wall < message.txt
```

Shutdown system and prompt
```bash
# reboot now
shutdown -r now
# halt/poweroff in 10 mins and use wall send message to login users
shutdown -h 10 "The system is going down in 10 min"
# cancel shutdown
shutdown -c
```

Changing runlevels
what is `runlevel` in linux?
https://www.liquidweb.com/kb/linux-runlevels-explained/
比如
runlevel 1 就只能root user且没有network enabled，也叫作rescue.target，可以做一些需要隔离的操作。
runlevel 3 是默认的multi-user + network enabled (多数情况是这个状态)
runlevel 5 是Desktop interface + runlevel 3的组合。
```bash
# show current runlevel
who -r
runlevel

# different systemd daemon can have differet target runlevel
# default runlevel
systemctl get-default
# set default runlevel
systemctl set-default multi-user.target
```
More about systemd, see my systemd blog.

## Manage processes
```bash
# show process on current shell
# use dash is UNIX options
ps -f
# -e means all processes
ps -ef --forest
# -F show full format column
ps -F -p $(pgrep sshd)
# kill all sleep processes
pkill sleep

# BSD options
ps aux
```

`$$` the PID of current running process
```bash
cd /proc/$$

# we can interrogate this directory
# current dir
ls -l cwd
# current exe
ls -l exe
```
`top` 命令的options还记得吗? 比如切换memory显示单位，选择排序的依据CPU/MEM occupied..

## Process priority
if something runs in foreground and prevent you from doing anything, use `ctrl+z` to suspend it (still in memory, not takeing CPU time), then put it in background.
```bash
sleep 10000
^Z
[1]+  Stopped                 sleep 10000

# use job command, `+` means current focus
jobs
[1]+  Stopped                 sleep 10000

# use bg command to put current focus in background
bg
[1]+ sleep 10000 &

# check is running in background
jobs
[1]+  Running                 sleep 10000 &

# use fg will bring current focus to foreground again
```

如果你在一个bash shell中sleep 1000& 然后exit bash shell，则这个sleep process will hand over to init process. can check via `ps -F -p $(pgrep sleep)`, 会发现PPID是`1`了。进入另一个bash shell `jobs` 并不会显示之前bash shell的background process.

```bash
# show PRI(priority) and NI(nice) number
ps -l

F S   UID   PID  PPID  C PRI  NI ADDR SZ WCHAN  TTY          TIME CMD
4 S     0 23785 23781  0  80   0 - 28891 do_wai pts/1    00:00:00 bash
0 S     0 24859 23785  0  80   0 - 26987 hrtime pts/1    00:00:00 sleep
0 S     0 24861 23785  0  80   0 - 26987 hrtime pts/1    00:00:00 sleep
...
```
PRI value for real time is from [60,99] and [100,139] for users, the bigger the better.
NI value is from [-20,19], higher the nicer so less CPU time to take. 在相同PRI 之下，NI 决定了多少资源.

比如说你有一个build task并不urgent, 不想它在后台占用太多资源，可以设置nice value.
```bash
# set nice value to 19
nice -n 19 sleep 1000 &
# reset nice value
renice -n 10 -p <pid>
```
要注意的是只有root可以设置负数nice value和降低nice value. root可以去`vim /etc/security/limits.conf`设置对不同user/group的nice value。

## Monitor linux performance
这个很重要，一般关注网络，硬盘，CPU

List content of the package `procps-ng`, `procps` is the package that has a bunch of small useful utilities that give information about processes using the `/proc` filesystem. The package includes the programs ps, top, vmstat, w, kill, free, slabtop, and skill.
```bash
# see executable files under procps package via rpm
rpm -ql procps-ng | grep "^/usr/bin/"

/usr/bin/free
/usr/bin/pgrep
/usr/bin/pkill
/usr/bin/pmap
...

# check the source package of top command
rpm -qf $(which top)

procps-ng-3.3.10-17.el7_5.2.x86_64
```

Introduce 2 new commands: `pmap` and `pwdx`
```bash
# pmap, show memory map of a process
# for example, current running process
pmap $$
# you can also see shared libary been used by the process

# show current working directory of process
pwdx $$
pwdx $(pgrep sshd)
# actually the output is from /proc/<pid>/cwd, it is a softlink
```

[Load average analysis](http://www.brendangregg.com/blog/2017-08-08/linux-load-averages.html)
```bash
# check how long the system has been running
# load average is not normalized for cpu number 如果你知道CPU有多少个
# 根据load average就能看出是不是很忙, 如果load average的值超出了CPU个数
# 则说明需要queue or wait
# 这个命令其实是从/proc/uptime, /proc/loadavg 来的数据 
uptime
18:53:14 up 39 days,  3:50,  1 user,  load average: 0.00, 0.01, 0.05

# check how many cpu
# the number of cpu is equal to processor number
# but you may have less cores, see /proc/cpuinfo
lscpu

# the same as w
w
 18:59:29 up 12 days, 23:40,  3 users,  load average: 0.04, 0.26, 0.26
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     pts/0    9.160.1.111      08:47    6:46m  0.03s  0.03s -bash
...
```

监控load or output
```bash
# execute a program periodically, showing output fullscreen
# 这里的例子是每隔4秒 运行 uptime
watch -n 4 uptime

# graphic representation of system load average
# 如果此时运行一个tar，会看到loadavg显著变化
tload
```

```bash
# -b 使用batch mode 输出所有process情况
# -n2 运行2回合
top -b -n2 > file.txt

# run 3 time, gap 5 seconds
# reports information about processes, memory, paging, block IO, traps, disks and cpu activity
vmstat 5 3
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 1  0    520  90576   4176 1601932    0    0     4   188   18   19  0  1 93  4  2
 0  0    520  90460   4176 1601956    0    0     0    46  514  348  0  0 98  2  1
 0  0    520  88972   4176 1603692    0    0     0   542  707  589  0  1 97  2  1
```

## sysstat toolkit
The package contains many performance measurement tools.
Install `sysstat` (a bunch of command: `iostat`, `netstat`, etc).
```bash
yum install -y sysstat

# then check executable
rpm -ql | grep "^/usr/bin"

/usr/bin/cifsiostat
/usr/bin/iostat
/usr/bin/mpstat
/usr/bin/nfsiostat-sysstat
/usr/bin/pidstat
/usr/bin/sadf
/usr/bin/sar
/usr/bin/tapestat
```

The config file for sysstat can be found by:
```bash
# -q: query
# -c: config file
rpm -qc sysstat
```
在安装后，其实用的cron在背后操作收集数据, configuration is in file `cat /etc/sysconfig/sysstat`，这里面可以设置记录的周期，默认是28天。
```bash
# cron config for sysstat
cat /etc/cron.d/sysstat 

# Run system activity accounting tool every 10 minutes
*/10 * * * * root /usr/lib64/sa/sa1 1 1
# 0 * * * * root /usr/lib64/sa/sa1 600 6 &
# Generate a daily summary of process accounting at 23:53
53 23 * * * root /usr/lib64/sa/sa2 -A
```
start and enable:
```bash
systemctl start sysstat
systemctl enable sysstat
```

来看看sysstat下的工具命令:
```bash
# show in mega byte
# run 3 times 5 seconds in between
iostat -m 5 3
# others
pidstat
mpstat
```

Let's see `sar`(system activity report), gather statistics and historical data, 通过分析一天的bottleneck(cpu/memory/disk/network/loadavg)可以更好的schedule任务，比如发现某个时间cpu, memory的使用比较多。这里并没有深入讲解怎么解读这些数据，并且你需要了解各个部分数据的含义，以及什么样的数据可能是异常.

`sar`的数据在`/var/log/sa`里面，每天一个文件，周期性覆盖。
```bash
# sar specific processor, cpu 0/cpu1
# check %idle
sar -P 0/1

# default show CPU utilization
# %user: user space stuff
# %system: sytem space stuff
sar -u
# interval 1sec and show 5 times
sar -u ALL 1 5

# show memory utilization
sar -r

# show disk utilization
sar -b

# network activity
sar -n DEV

# load average
# interval 5sec and show 2 times
sar -q 5 2

# 显示sa23这天的文件，从18:00:00到19:00:00 
sar -n DEV -s 18:00:00 -e 19:00:00 -f /var/log/sa/sa23
```
图形化sar数据，可以用ksar:https://www.cyberciti.biz/tips/identifying-linux-bottlenecks-sar-graphs-with-ksar.html

## Log and logrotate
Auditing login events，这个还挺有用的，看哪个user什么时候login了, `w`是查看当前哪些user正在使用中。
```bash
# see user login info
lastlog | grep -v "Never"

Username         Port     From             Latest
root             pts/0    9.65.239.28      Fri Apr 24 17:51:48 -0700 2020
fyre             pts/0                     Fri Apr 24 17:52:00 -0700 2020

# check system reboot info
# The last command reads data from the wtmp log and displays it in a terminal window.
last reboot
# check still login user, the same as `w`
last | grep still
```

Auditing root access，看su/sudo的使用情况，在`/var/log/secure`文件中，这里其实有多个secure文件，有日期区分。
```bash
# there are some secure and auditing files
cd /var/log
# secure file
# 当然有grep也行，把用sudo的事件找出来
awk '/sudo/ { print $5, $6, $14 }' secure
```
我会专门总结一下awk的笔记，这个挺有用的。

`journalctl`是一个常用的system log查询工具。当时查看一些docker的log在里面也能看到。
```bash
# show last 10 lines
journalctl -n 10
# ses real time appending
journalctl -f
# -u: systemd unit
journalctl -u sshd
# timestamp
journalctl --since "10 minutes ago"
journalctl --since "2020-04-26 13:00:00"
```

## Selinux
O'Reilly有过相关的课程，在我工作邮件中连接还在。目前只需要知道什么是selinux，如何打开，关闭它即可。
SELINUX= can take one of these three values:
`enforcing` - SELinux security policy is enforced.
`permissive` - SELinux prints warnings instead of enforcing.
`disabled` - No SELinux policy is loaded.
```bash
# see if selinux is permissive, enforcing or disabled
getenforce
# more clear
sestatus
```
如果最开始是disabled的，则要去config file `/etc/selinux/config` 设置permissive，然后重启。
不能setenforce 去disable，也只能在config文件中disable然后重启机器。
```bash
# setenforce [ Enforcing 1| Permissive 0]
# 成为permissive后就可以用setenforce切换了，但都不是永久的
setenforce 0
setenforce 1
```
显示selinux的labels, flag `Z`对于其他命令也有用。
```bash
# see user selinux config
id -Z
# user, role, type
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
# see files selinux config
/bin/ls -Z
# see process selinux config
ps -Zp $(pgrep sshd)
LABEL                             PID TTY      STAT   TIME COMMAND
system_u:system_r:kernel_t:s0     968 ?        Ss     0:00 /usr/sbin/sshd -D
unconfined_u:unconfined_r:unconfined_t:s0 1196 ? Ss   0:00 sshd: root@pts/0
```



]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>LPIC-1</tag>
      </tags>
  </entry>
  <entry>
    <title>Set up Secure Docker Registry in K8s</title>
    <url>/2020/01/06/k8s-secure-registry/</url>
    <content><![CDATA[
The step to set up secure docker registry service in K8s is different from docker. There are some adjustments and changes to apply.

Toolkits we need to achieve our goal:
1. openssl
2. htpasswd
3. skopeo

## Create SSL/TLS Certificate and Key
Use openssl command to generate certificate and private key for setup secure connection:
```bash
## create certs
mkdir -p /root/registry-certs
## get from master

DOCKER_REGISTRY_URL=blair1.fyre.com
## make a copy of .crt and give suffix .cert
openssl req \
        -newkey rsa:4096 -nodes -x509 -sha256 \
        -keyout /root/registry-certs/tls.key \
        -out /root/registry-certs/tls.cert \
        -days 3650 \
        -subj "/C=US/ST=CA/L=San Jose/O=IBM/OU=Org/CN=${DOCKER_REGISTRY_URL}"

cp /root/registry-certs/tls.cert /root/registry-certs/tls.crt
```

Then copy the crt file to every host under `/etc/docker/certs.d/<${DOCKER_REGISTRY_URL}>:5000` folder for self-signed certificate trust.

> Notice that if the docker daemon json file has enabled the insecure registry, it will not verify the ssl/tls cert! You get docker user account and password, then you can login without certs!

## Create Docker User Info
```bash
##  create auth file
DOCKER_USER=demo
DOCKER_PASSWORD=demo

mkdir -p /tmp/registry-auth
htpasswd -Bbn ${DOCKER_USER} ${DOCKER_PASSWORD} > /tmp/registry-auth/htpasswd
```

## Generate Secret
```bash
## create secrets
## we want to setup docker registry in default namespace
kubectl create secret tls docker-registry-tls  \
    --key=/root/registry-certs/tls.key  \
    --cert=/root/registry-certs/tls.cert \
    -n default

kubetctl create secret generic docker-registry-auth \
    --from-file=htpasswd=/tmp/registry-auth/htpasswd \
    -n default

## Assume the working namespace is test-1
WORKING_NAME_SPACE=test-1
DOCKER_REGISTRY_SERVER="${DOCKER_REGISTRY_URL}:5000"

kubectl create namespace ${NAME_SPACE}
kubectl create secret docker-registry docker-registry-creds \
        --docker-server=$DOCKER_REGISTRY_SERVER \
        --docker-username=$DOCKER_USER \
        --docker-password=$DOCKER_PASSWORD \
        -n ${WORKING_NAME_SPACE}
```

## Bind Image Pull Secret to Service Account
see [document](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account) in K8s.
```bash
## patch creds to default service account in test-1
## assume we use default service account in yaml
kubectl patch serviceaccount default \
        -p '{"imagePullSecrets": [{"name": "docker-registry-creds"}]}' \
        -n ${WORKING_NAME_SPACE}
```
Or you can specify imagePullSecrets in yaml explicitly, for example:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: <your-private-image>
  imagePullSecrets:
  - name: <secret name>
```


## Create Secure Docker Registry
```yaml
## notice the 
##        env field
##        secret mount field

## deletion is enabled
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: docker-registry
  labels:
    app: docker-registry
spec:
  replicas: 1
  selector:
    matchLabels:
      app: docker-registry
  template:
    metadata:
      labels:
        app: docker-registry
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - {key: docker-registry, operator: In, values: ["true"]}
      hostNetwork: true
      tolerations:
      - key: "node-role.kubernetes.io/master"
        operator: "Exists"
        effect: "NoSchedule"
      containers:
      - name: docker-registry
        image: localhost:5000/registry:2.7.1        
        imagePullPolicy: IfNotPresent
        env:
        - name: REGISTRY_STORAGE_DELETE_ENABLED
          value: "true"
        - name: REGISTRY_AUTH
          value: "htpasswd"
        - name: REGISTRY_AUTH_HTPASSWD_REALM
          value: "Registry Realm"
        - name: REGISTRY_AUTH_HTPASSWD_PATH
          value: "/auth/htpasswd"
        - name: REGISTRY_HTTP_TLS_CERTIFICATE
          value: /certs/tls.crt
        - name: REGISTRY_HTTP_TLS_KEY
          value: /certs/tls.key
        ports:
        - name: registry
          containerPort: 5000
          hostPort: 5000
        volumeMounts:
        - name: docker-data
          mountPath: /var/lib/registry
        - name: docker-tls
          mountPath: /certs
          readOnly: true
        - name: docker-auth
          mountPath: /auth
          readOnly: true
      volumes:
      - name: docker-data
        persistentVolumeClaim:
          claimName: registry-pv-claim
      - name: docker-tls
        secret:
          secretName: docker-registry-tls
      - name: docker-auth
        secret:
          secretName: docker-registry-auth
```

So far the secure docker registry in K8s is up and running in default namespace, it's host network true so can be accessed from remote. Later can expose it by ingress.

## Update Docker User Info
See this [post](https://stackoverflow.com/questions/38216278/update-k8s-configmap-or-secret-without-deleting-the-existing-one).
```bash
## create new htpasswd file
DOCKER_USER=demonew
DOCKER_PASSWORD=demonew

mkdir -p /tmp/registry-auth
htpasswd -Bbn ${DOCKER_USER} ${DOCKER_PASSWORD} > /tmp/registry-auth/htpasswd
## then encode base64
AUTH_BASE64=$(cat /tmp/registry-auth/htpasswd | base64 -w 0)
## replace old auth secret
## the change will be populated to registry pod
kubectl get secret docker-registry-auth -o yaml -n default \
        | sed -e "/htpasswd/c\  htpasswd: ${AUTH_BASE64}" \
        | kubectl replace -f -

## replace old docker config creds secret in used working namespaces
NEW_REGISTRY_CREDS=$(kubectl create secret docker-registry docker-registry-creds \
        --docker-server=$DOCKER_REGISTRY_SERVER \
        --docker-username=$DOCKER_USER \
        --docker-password=$DOCKER_PASSWORD \
        -n default \
        -o yaml --dry-run \
        | grep "\.dockerconfigjson" | cut -d":" -f2)

kubectl get secret docker-registry-creds -o yaml -n ${WORKING_NAME_SPACE} \
        | sed -e "/\.dockerconfigjson/c\  .dockerconfigjson: ${NEW_REGISTRY_CREDS}" \
        | kubectl replace -f -
```

## Skopeo Operation
Please refer my skopeo [blog](https://chengdol.github.io/2019/09/26/docker-skopeo/) for more details.
```bash
skopeo copy \
       --dest-creds ${DOCKER_USER}:${DOCKER_PASSWORD} \
       --dest-cert-dir /root/registry-certs \
       docker-archive:/root/busybox.tar.gz \
       docker://${DOCKER_REGISTRY_SERVER}/busybox:latest

skopeo inspect \
       --creds ${DOCKER_USER}:${DOCKER_PASSWORD} \
       --cert-dir /root/registry-certs \
       docker://${DOCKER_REGISTRY_SERVER}/busybox:latest
```]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Leetcode Java Summary</title>
    <url>/2019/07/10/leetcode-java-recall/</url>
    <content><![CDATA[
## TO-DO
0. hiring season, internal reference
1. position: backend developer, DevOps, Infra
2. system design from x code in my laptop? baidu cloud?
3. company series, buy x code or leetcode?
4. resume project chat
5. blog review

# Data Structure
java Integer notation, see this [post](https://stackoverflow.com/questions/17636749/java-why-cant-i-declare-integer-types-using-scientific-notation):
```java
// 10^9
int a = (int)1e9;
int a = (int)Math.pow(10, 9);
int b = 2_000_000_000;
```

## Class features
basic structure and syntax
nested class, inner class
static class
private
public 
protected

## LRU
LRU除了可以使用自定义的double linked list去实现，也可以用Java API `LinkedHashMap`，在constructor中指定参数:
```java
// 设置capacity
int CAPACITY = 10;
// true表示自动实现LRU功能
LinkedHashMap<String, Integer> lru = new LinkedHashMap<>(CAPACITY, 0.75f, true){
  // override
  protected boolean removeEldestEntry​(Map.Entry<String, Integer> eldest)
  {
    return size() > CAPACITY;
  }
}
```
如此一来，可以直接使用put(), get()即可，其他不用操心，当达新增的量超过capacity时会自动删除least recently used item.

此外，如果不使用这个special constructor `new LinkedHashMap<>(CAPACITY, 0.75f, true)`，则linked list中的顺序是insertion order，即最先insert的在前面，最近的在后面，如果用keySet() iterator是按照linked list的顺序出来的，即最先insert的先出来。

经过试验发现，LRU声称的access order，都是insertion order的顺序，只不过LRU实现每次会自动把最新活动的元素重新放到linked list最后， 然后超出capacity时，把least recently used one删除了.

## Pair class
https://docs.oracle.com/javase/8/javafx/api/javafx/util/Pair.html
https://www.baeldung.com/java-pairs
```java
import javafx.util.Pair;
```
https://www.geeksforgeeks.org/pair-class-in-java/

## input method

## Deque
如果把Deque既当stack又当queue, 不要使用他们的native method，用能明确指出操作位置的method:
```java
addLast(x)
removeLast()
peekLast()

addFirst()
removeFirst()
peekFirst()
```
如果只充当stack or queue之一，则可以用native method:
```java
stack: push, pop
queue: add, poll
```



## Segment Tree
https://www.geeksforgeeks.org/segment-tree-set-1-sum-of-given-range/

```java
// get min value >= x
// 返回的都是整数，但是是double type
Math.ceil(double x)
// get max value <= x
// 返回的都是整数，但是是double type
Math.floor(double x)
// log e of x
Math.log(double x)
// calculate log2
Math.log(x) / Math.log(2)
```
 
Complete binary tree number of nodes: `2^height - 1` (height start from `1`)
For segment tree, number of nodes:
```java
// n is the length of array
int h = (int)Math.ceil(Math.log(n) / Math.log(2));
int maxSTSize = 2 * (int)Math.pow(2, h) - 1;
int[] st 
```
Or you can define `class Node` to build segment tree.

## List
* LinkedList is doubly-linked!
* change array to list
```java
List<Integer> list = Arrays.<Integer>asList(1, 2, 3, ...);
```
but this is fixed size list backed by array, you can create a new one:
```java
List<Integer> list = new ArrayList<Integer>(Arrays.<Integer>asList(1,2,3, ...));
```

use slow-fast pointer to find middle of linkedlist or find cycle
```java
ListNode slow = head, fast = head;
while (fast.next != null && fast.next.next != null) {
    slow = slow.next;
    fast = fast.next.next;
}
// when loop break, slow is in middle
```
reverse the linked list:
https://leetcode.com/problems/reorder-list/submissions/
```java
// 1->2->3->4
// 1<-2<-3<-4
ListNode pre = null;
ListNode cur = <node 1>;
while (cur != null)
{
    ListNode next = cur.next;
    cur.next = pre;
    pre = cur;
    cur = next;
}
// 注意这里不要弄错了
ListNode newHead = pre;
```

## PriorityQueue
Priority queue has `contains(object)` and `remove(object)` method, but that will take `O(n)` time.

如果要构造比较整数大小的priority queue
```java
// if use (a, b) -> a - b, this may overflow when encounter MAX MIN
Queue<Integer> pq = new PriorityQueue<>((a, b) -> Integer.compare(a, b));
```

> 注意priorityQueue iterator返回的是random order!

## String
String: immutable
StringBuilder: mutable, not thread safe
StringBuffer: mutable, thread safe

注意`"\t", "\n"`在string中只相当于一个长度！！！

"abcdefg"
substring 是连续的: "cdef"
subsequence可以不连续，但顺序要一致: "abdf"

```java
String str = new String("123");
String str = "123";
// convert number 123456 to string "123"
String str = String.valueOf(123456);
str.length();
// use + to concatenate
str1 + str2
// string compare
str1.compareTo(str2);
```

when do replace
```java
String str = "a.b.c.d";
str.replace(".", ""); // get "abcd"
// don't use str.replace('.',''), cannot be empty char
// there is no regex, just CharSequence type
```

if just want to create a string using other type:
```java
"" + other type
```

caution:
```java
String a = "1.2.3.";
// a.split("\\.") will get length of 3, it will ignore last '.'
String a = ".1.2.3";
// a.split("\\.") will get length of 4, it will not ignore first '.'
String b = "1..2.3";
// b.split("\\.") will get length of 4, it will not ignore consecutive '.'
```

Notice the parameters format
```java
indexOf(int ch, int startIndex)
split(String reg)

// returns the index within this string of
// the last occurrence of the specified character.
int lastIndexOf​(int ch)
```

## StringBuilder
```java
StringBuilder sb = new StringBuilder(String);
sb.append(x);
// insert in head
sb.insert(0, x);
sb.toString();
// return stringbuilder
sb.reverse();
// stringbuilder does not implement equals method!!
sb.equals(another stringbuilder object); // wrong!!
```
more methods:
```java
// return String
// 这里只有一个参数的时候是start index!
substring(int startidx)
// [start end)
substring(int startidx, int endidx) 
append(lot of types)
setLength(int len)
```

## Character
```java
// return Character
Character.valueOf(char)
//return char
c.charValue()

// actually the convert is auto like int and Integer!
char c1 = new Character('a');
Character c1 = 'b';
```
char compare in `if` use `==`
if use +- with char, such as `ch - 'a'`, don't forget `(char)(ch)`

check letter:
```java
Character.isDigit(ch);
Character.toLowerCase(ch);
Character.isLetterOrDigit(ch);
```

Non-ASCII character (> 256):
```java
Character.toString((char)342)
```

## 3 items if expression
```java
// I forget the precedence
// this will first 1 + 2 + 3 then compare with 3!
1 + 2 + 3 == 3? true: false
```

## Enhenced for Loop
```java
// enhanced for loop
for (Character ch : str.toCharArray())
{
  ...
}
```

## Collection
convert list to array
```java
// result is List<int[]> type
// think about it
result.toArray(new int[result.size()][]);
```

convert Integer list to int array (from Java8):
https://stackoverflow.com/questions/718554/how-to-convert-an-arraylist-containing-integers-to-primitive-int-array
```java
int[] arr = list.stream().mapToInt(i -> i).toArray();
```
also from int array to Integer list
https://stackoverflow.com/questions/1073919/how-to-convert-int-into-listinteger-in-java
```java
List<Integer> list = Arrays.stream(ints).boxed().collect(Collectors.toList());
```
这个是Java stream的知识点

## Array
```java
// one dimension
int[] arr = new int[]{1,2,3};
int[] arr = new int[4]; // default value is 0
// two dimensions
int[][] arr = new int[3][4]; // 3 rows 4 columns
int[][] arr = new int[3][]; // still ok, you can init one dimension array later
int[][] arr = new int[][]{{1,2,3}, {4,5}, {6,7,8,9}}; // each one dimension array can have different length!
```
Let's see char array
```java
char[] arr = str.toCharArray();
```

Use sort static method, if you want to use comparator, see my **lambda** section
```java
Arrays.sort(E[] e);
// also for collections
Collections.sort(List<xx> l);
```
> Arrays.sort(int[]) primitive type是不能自己写comparator的！

数组内容字符串化用:
```java
int[] a = new int[]{1,2,3,4,5};
// correct!
Arrays.toString(a);
// wrong! 这个返回id string
a.toString()
```

Shuffle arrays:
Every permutation of array element should equally likely:
https://www.geeksforgeeks.org/shuffle-a-given-array-using-fisher-yates-shuffle-algorithm/
https://introcs.cs.princeton.edu/java/stdlib/StdRandom.java.html

```java
// nums[0] swap with element in range [0, n)
// nums[1] swap with element in range [1, n)
// nums[2] swap with element in range [2, n)
public static int[] shuffle(int[] nums)
{
  for (int i = 0; i < nums.length; i++)
  {
    int j = i + rand.nextInt(nums.length - i);
    // swap
    int tmp = nums[i];
    nums[i] = nums[j];
    nums[j] = tmp;
  }
  return nums;
}
```

## Set
TreeSet operation time complexity, from java TreeSet doc:
**This implementation provides guaranteed log(n) time cost for the basic operations (add, remove and contains).**

## Map
```java
import java.util.*;
```
HashMap<>() not thread safe
HashTable<>(), thread safe but ConcurrentHash...

有时用char[26]（`c - 'a'`） 或者 int[256] (直接用c即可) 可以用来代替hashmap

```java
map.containsKey()
// also has contains value method!
map.containsValue()
// return set
map.keySet()
map.entrySet()
// return collection
map.values()
```

```java
// provide value for new key which is absent 
map.put(key, map.getOrDefault(key, 0) + 1);
// if value is a collection object can use:
map.computeIfAbsent(key, k -> new ArrayList<Stone>()).add(new Stone(x,y));
// remove current key if it's value is val
map.remove(key, val)
```

Treemap/TreeSet (sorted map) perform **all operation** based on it's default/custom comparator! if two key are equal in comparator, should be treated as equal in equals() method! 但如果没有一致，还是按照comparator去做. treemap/treeset.remove(object) 用的是comparator去比较相等！！

range query API:
```java
// treemap
tm.ceilingKey(K key)
tm.ceilingEntry(K key)
tm.floorEntry(K key)
tm.floorKey(K key)

tm.firstKey()
tm.firstEntry()
tm.lastKey()
tm.lastEntry()

tm.pollFirstEntry()
tm.pollLastEntry()
// treeset
ts.ceiling(K key)
ts.floor(K key)

ts.first()
ts.last()
// 可以用来模拟pq
ts.pollFirst()
ts.pollLast()
```

如果要返回map中的任意一个key，比如key是string类型
```java
return (String)map.keySet().iterator().next();
```

this is java **syntax**, I see it in leetcode
https://stackoverflow.com/questions/1958636/what-is-double-brace-initialization-in-java


## Exception Handling
```java
throw new Exception("xxx");
```

## Random Number Generation
```java
import java.util.Random;
Random rand = new Random();
int randomNum = rand.nextInt((max - min) + 1) + min;
// simply can use this:
// return double, need to force convert to int maybe
Math.random(): [0.00,1.00) 
```
## Lambda Expression
for comparator and comparable, let's see how to wirte it, [this post](https://www.mkyong.com/java8/java-8-lambda-comparator-example/)

原来的方法，在constructor中定义comparator:
```java
Queue<Integer> que = new PriorityQueue<>(new Comparator<Integer>(){
      @Override
			public int compare(Integer n1, Integer n2) {
				return Integer.compare(n1, n2);
			} 
});

int[] arr = new int[]{5, 3, 56, 78, 4, 2, 5};
// anonymous class to implement comparator
Arrays.sort(arr, new Comparator<Integer>(){
  public int compare(Integer n1, Integer n2)
  {
    return Integer.compare(n1, n2);
  }
});
```

```java
int[][] arr = {{5,1}, {2,3}};
// sort by the first element of one dimenstional array
Arrays.sort(arr, (int[] a, int[]b) -> a[0] - b[0]);
System.out.println("result= " + Arrays.deepToString(arr));
```

用lambda构造comparator 时可以用到外部的数据结构？
https://leetcode.com/problems/top-k-frequent-elements/solution/
比如这个的官方solution
```java
// init heap 'the less frequent element first'
    PriorityQueue<Integer> heap =
            new PriorityQueue<Integer>((n1, n2) -> count.get(n1) - count.get(n2))
```
其实这里可以使用Map.Entry<> 放入pq中。
count是一个外部定义的hashmap. 这个到底是怎么回事？


## Java Memory Management
java heap space, java stack memory
[Memory Allocation in Java](https://www.journaldev.com/4098/java-heap-space-vs-stack-memory)
[Stack Memory and Heap Space in Java](https://www.baeldung.com/java-stack-heap)

## Java Garbage Collection
you can watch the operting system course video
implemented by DFS with marked bit in each object, scan from stack

### Why use Deque instead of stack
check this post https://leetcode.com/problems/flatten-nested-list-iterator/discuss/80147/Simple-Java-solution-using-a-stack-with-explanation/165585

I just recently interviewed with 16 different companies in the silicon valley (including FAANG except Netflix) for senior software engineering positions. Here's my opinion, based on my experience:

Some interviewers will expect you to know why Deque is better than Stack.

Showing mastery of at least one programming language (in this case, Java) can, at worst, score you extra points in the interview, and at best get you the job and even help you get higher compensation or leveling. Getting the optimal solution is the basic requirement to pass an interview. Additionally, you are being benchmarked against other candidates being asked exactly the same question. Lastly, for those targeting senior level positions, the interviewer will also evaluate your seniority. This is one case where I think most interviewers will expect to see that you've mastered the Java programming language (if you chose to use it in your interview).

Here are a few reasons why Deque is better than Stack:

Object oriented design - Inheritance, abstraction, classes and interfaces: Stack is a class, Deque is an interface. Only one class can be extended, whereas any number of interfaces can be implemented by a single class in Java (multiple inheritance of type). Using the Deque interface removes the dependency on the concrete Stack class and its ancestors and gives you more flexibility, e.g. the freedom to extend a different class or swap out different implementations of Deque (like LinkedList, ArrayDeque).

Inconsistency: Stack extends the Vector class, which allows you to access element by index. This is inconsistent with what a Stack should actually do, which is why the Deque interface is preferred (it does not allow such operations)--its allowed operations are consistent with what a FIFO or LIFO data structure should allow.

Performance: The Vector class that Stack extends is basically the "thread-safe" version of an ArrayList. The synchronizations can potentially cause a significant performance hit to your application. Also, extending other classes with unneeded functionality (as mentioned in #2) bloat your objects, potentially costing a lot of extra memory and performance overhead.

## Tries
`string symbol table`: specialized to string key. Faster than hashing, flexible then BST
recall the structure of tries, very simple! put value in last node (character).
put and search method for tries can be recursion implementations.
一般都用的这个。

`3-way tries`: 这里注意，当前char的值和node char比较，less then go left, larger then go right, then compare, if not match, miss hit. Using recursion to do put and search operation. `TST` is as fast as hashing (for string key) and space efficient. support in-order traverse (obey BST rule)

Implementation: prefix matching, wildcard match, longest prefix.

# Algorithms
[Tushar Roy youtube channel](https://www.youtube.com/user/tusharroy2525/playlists)
princeton algorithms，在youtube or coursera上都有

Time complexity:
https://en.wikipedia.org/wiki/Time_complexity
usually:
```
constant O(1)
logarithmic O(logn)
linear O(n)
quasilinear O(nlogn)
quadratic O(n^2), n to the power of 2
cubic O(n^3), n to the power of 3
polynomial O(n^x + n^y + ..)
exponential O(2^n)
factorial O(n!)
```

`Tilde notation`: ignore lower order terms, negligible
best case, lower bound
worst case, upper bound, `Big O` does this
average case, cost for random input, expected cost
when upper bound equals lower bound, we get optimal algorithms

[tilde notation vs big-O](https://cs.stackexchange.com/questions/52776/difference-between-the-tilde-and-big-o-notations)

## Prime
LC 886 https://leetcode.com/problems/prime-palindrome/
isPrime是如何检查的，函数记住, 并不是检查每个递增+1的除数是否整除！
检查特殊的值就可以了
https://en.wikipedia.org/wiki/Primality_test

## Monotonic Stack
LC 496 Next Greater Element I
LC 503 Next Greater Element II
还有LC 31 next permutation 也是属于monotonic problem, 见这题我的note

Monotonic stack practice:
https://medium.com/@vishnuvardhan623/monotonic-stack-e9dcc4fa8c3e

increasing monotonic stack: from stack top to stack bottom is increasing
decreasing monotonoc stack: reverse order


## General 
bit and boolean can use XOR ^ operator!

## Sliding Window
相关window问题的[template](https://leetcode.com/problems/minimum-window-substring/discuss/26808) and [summary](https://leetcode.com/problems/find-all-anagrams-in-a-string/discuss/92007), good sliding window [demo](https://www.geeksforgeeks.org/window-sliding-technique/)

[basic sliding window problem](https://www.geeksforgeeks.org/window-sliding-technique/)
leetcode 904
find the longest sequence with at most 2 integers

## Quick select
Average `O(n)`, worst case `O(n^2)`, need to shuffle.
if quick select Kth element, then in the range [0, K] is guarantee the K smallest elemets in unsorted array.

Summary of top k problem:
https://leetcode.com/problems/k-closest-points-to-origin/discuss/220235/

## Binary search
how to decide the condition?
lo < hi or lo <= hi?
hi = mid - 1 or hi = mid?
lo = mid + 1 or lo = mid?

when target exists, use 
```java
while (lo <= hi)
```
because it will finally concentrate. lo == hi will be the result:
hi = mid - 1 or hi = mid?
or
lo = mid + 1 or lo = mid?
does not matter.
对于可能不存在的也用lo <= hi，但要保证hi = mid - 1 和 lo = mid + 1

You use while (start <= end) if you are returning the match from inside the loop.
You use while (start < end) if you want to exit out of the loop first, and then use the result of start or end to return the match.

## Backtracking
[`Backtracking`](https://www.geeksforgeeks.org/backtracking-algorithms/#standard) is an algorithmic-technique for solving problems `recursively` by trying to build a solution `incrementally`, one piece at a time, abandons those solutions that fail to satisfy the constraints of the problem at any point of time.

for example:
SudoKo
[N Queens problem](https://www.youtube.com/watch?v=xouin83ebxE), smart at process diagonal row-col and row + col. think recursion as a tree, back and forth.

leetcode [summary](https://leetcode.com/problems/permutations/discuss/18239) backtracking about `Subsets`, `Permutations`, and `Combination Sum`

## Others
x^y how to describe: x to the power of y
x^2: x squard
x^3: x cubed

## Find rectangle
p1[x,y], p2[x,y]
if p1 and p2 are diagonal, check if other 2 points exist, can use hashmap or hashset

## Toggle 1 and 0
use xor 1 to toggle 1 and 0
```java
int a = 1
a ^= 1; // a is 0
a ^= 1; // a is 1
```
calculate power of 2:
```java
Math.pow(2, n); // slow
// or can use shift
1 << n // fast
```

## Union Find
for quick find, all sites in a component must have the same value in id[], directly check `id[p] == id[q]`
for quick union, set the root of pool p to the root of q
then improve quick union, compress the path when seach the root:
```java
private int find(int[] uf, int i)
{
  while (uf[i] != i)
  {
    // path compression
    uf[i] = uf[uf[i]];
    i = uf[i];
  }
}
```

## sort
in-place:
  bobble sort     stable
  selection sort  not stable
  insertion sort  stable
  quick sort      not stable
  heap sort       not stable
need space:
  merge sort      stable

insertion sort对于partial sorted的数组排序很快。
merge sort O(nlogn) 很稳定，可以用merge sort的思路去divide and conquer解决其他问题
quick sort 最坏O(n^2)，比如排好序的数组，所以需要shuffle: `Collections.shuffle(list)`

radix sort不是通过comparison机制实现的, leetcode 164

sort one array based on another array
```java
// sort a based on b
int[] a = new int[]{3,5,7,2,5,78,4,2};
int[] b = new int[]{7,45,2,4,7,78,2,4};

// pair[i][0] = a[i];
// pair[i][1] = b[i];
int[][] pair = new int[a.length][a.length];
Arrays.sort(pair, (a, b) -> a[1] - b[1]);

// or use map <b value: b index>
// the sort b, recreate a using the map mapping
```

## 0/1 knapsack

## Binary Search Tree
need to know different traverse orders:
* *pre-order*: current -> display -> left -> right
* *post-order*: current -> left -> right -> display
* *in-order*: current -> left -> display -> right, this will get ascending order if traverse a BST.
* *out-order*: current -> right -> display -> left, this will get descending order if traverse a BST

pre order list can build BST uniquely.
(post)pre order + in order can build BT uniquely


### threaded binary search tree
LC 99:
https://leetcode.com/problems/recover-binary-search-tree/submissions/
video:
morris traversal inorder to binary tree: 不用递归的方式，实现constant space的一种traverse
https://www.youtube.com/watch?v=wGXB9OWhPTg
https://www.geeksforgeeks.org/inorder-tree-traversal-without-recursion-and-without-stack/


## Undirected graph
vertex and edge matters
degree: num of edge connect to a vertex
graph API design:
```
addEdge()
Iterable adjacent nodes
numEdge()
numVertex()
```

How to repesent vertex and it's edge, here use `adjacency list`
如果发现graph的node是连续的，比如`0 ~ n-1`则用adjacency list比较好，否则用`Map<Integer, List<Integer>>`
```java
// generic array
List<Integer>[] adj = (List<Integer>[]) new List[n];
// it seems we can do now:
List<Integer>[] adj =  new List[n];
TreeMap<Integer, Integer>[] tmap = new TreeMap[n];
// init
for (int i = 0; i < n; i++)
{
  adj[i] = new ArrayList<Integer>();
}
```
actually we can use list of list instead of this format, but array is faster.
[how to create an array of list](https://stackoverflow.com/questions/8559092/create-an-array-of-arraylists)

may contains:
self-loop
parallel edges

### BFS
use `queue` to hold the vertices
add all unmarked vertices to queue adjacent to v and mark them
```java
boolean[] marked;
int[] edgeTo;
int[] distTo;
```
time complexity: O(E + V)

### DFS
for example, find exit for maze graph
mark vertex has visited
recursive (or use `stack` )visit all unmarked vertices w adjancent to v
```java
boolean[] marked;
int[] edgeTo;
```
time complexity: O(E + V)

### Bridge
leetcode: [1192](https://leetcode.com/problems/critical-connections-in-a-network/)
这是一种找bridge的算法: Tarjan algorithm
可以看看这个频道的graph theory playlist:
https://www.youtube.com/watch?v=aZXi1unBdJA
https://www.youtube.com/watch?v=TyWtx7q2D7Y


### Connected components
a connected component is a max pool of connected vertices
answer the question: is v and w connected in contact time.

DFS? yes, run DFS for each unvisited vertex, verteice in same connected component has the same id (this can represent component number!), so, check vertex id will get result.

## Directed graph
also called `digraph`
outdegree, indegree
directed cycle

`DAG`: directed acyclic graph

### Unweighted
**without** weight on edge, the same for DFS and BFS
we use `adjacency list` the same as undirected graph
```java
List<Integer>[] adj;
```
#### topological sorting
for example, `precedence scheduling`
at least one vertex has 0 indegree and should be `directed acyclic graph (DAG)`

`BFS`: 从indegree为0的node开始，压入队列中，每次下一个node的indegree减1，再把下一批indegree为0的node压入队列中，最后如果没有cycle，所有node都应该被explored.

`DFS`也可以，但要注意visited用2种记号表示, 0:没有访问，1:当前访问, 2:完全访问。如果在一次DFS中再次遇到了1，则说明有cycle.当完全访问后，再设置成2.

#### strongly connected component
Strongly connected: there is a path from v to w and w to v as well.
skip this topic

### Weighted
API design:
```java
class DirectedEdge: weight, from, to
class EdgeWeightedDigraph: addEdge(new DirectedEdge(from, to, weight));
distTo(int w);
pathTo(int w);
```
we still use adjacency list representation:
```java
List<DirectedEdge>[] adj = (List<DirectedEdge>[]) new List[n];
// new init array
List<DirectedEdge>[] adj = new List[n];
```

#### Dijkstra
`non-negative weight`
`SPT`: shortest path tree exist in graph from the source vertex

**remember**:
Use minium priority queue, to choose the next vertex cloest to the source vertex!
Then update the path accordingly if it's shorter than before (如何实现这个degrade呢?)
[implementation](https://www.geeksforgeeks.org/dijkstras-shortest-path-algorithm-in-java-using-priorityqueue/)

有几点需要注意：
1. 它没有实现degrade操作，只是又加入了新的元素在pq中，这样旧的就沉到底部了
2. 有一个visited set去控制结束
3. 注意comparator interface是怎么实现的，override了compare method，其实也用不着，在pq初始化的时候可以用lambda去实现。

time complexity of the construct computation: O(ElogV)

#### Bellman-Ford
`no negative cycle`

### A*
A graph traversal and path search algorithm
leetcode 1263


### regular expression
leetcode 819
去掉所有非字典字母，然后split每个word组成数组:
```java
String[] words = p.replaceAll("\\W+" , " ").toLowerCase().split("\\s+");
```






]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>What&#39;s New in CentOS 8</title>
    <url>/2020/07/05/linux-centos8-new/</url>
    <content><![CDATA[
New features:
- Using cockpit web interface
- Using enhanced firewall: nftables
- Managing NFSv4 with nfsconf
- Layered storage management with stratis
- Data de-duplication and compression with VDO (virtual data optimizer)

感觉资源挺分散的，不知道就是不知道😢
RedHat provides lab interactive learning exercise:
- https://lab.redhat.com/

Openshift lab interactive exercise:
- https://learn.openshift.com/

# Fast Ping Test
This is the new shorthand format.
```bash
# last decimal represents 24 bits
# the same as 127.0.0.1
ping 127.1
# 1.0.0.1
ping 1.1
```

# Cocopit Web Console
Available since CentOS 7.5. 相当于一个简化版的桌面. You can check logs, create account, monitor network, start services, and so on.

```bash
# install
sudo yum install -y cockpit-211.3-1.el8.x86_64

# start socket only not cockpit.service
sudo systemctl enable --now cockpit.socket
systemctl status cockpit.socket
# see port opened: 9090
sudo ss -tnlp

# still inactive
systemctl status cockpit.service
```

Set root password, user `vagrant` is privileged, run:
```
sudo passwd root
```

I have the port forwarding for `9090`, view cockpit UI by `localhost:9090`, login as root user with the password you set. After login the `cockpit.service` is now active:
```bash
systemctl status cockpit.service
```

There is a `terminal` in web UI, you can work with it just like working on a normal ssh terminal.

The `dashboard` plugin:
```bash
# see plugins
# you can see yum packages installed and available
yum list cockpit*

yum info cockpit-dashboard
yum install -y cockpit-dashboard
```

With cockpit dashboard plugin installed, you can connect to remote machine (with cockpit installed and cockpit.socket running), dashboard is just like a control plane.

Other plugins like `cockpit-machines` is used to manage virtual guests.

# Enhancing Firewall
[RedHat 8 Getting start with nftables](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_networking/getting-started-with-nftables_configuring-and-managing-networking)
It is the designated **successor** to the iptables, ip6tables, arptables, and ebtables tools. Stick to one command, not using mixed. `firewalld` command can be replaced by nftables.

NFTables `nft` is the default kernel firewall in CentOS 8. Single command for IPV4, IPV6 ARP, and Bridge filters. nftables does not have any predefined tables, tables are created by `firewalld` or rely on our scripts.

First yum install nftables, run as sudo or **root**.

```bash
systemctl disable --now firewalld
reboot
# list all tables, nothing is there.
nft list tables
```

Now start and enable firewalld, the tables will be created:
```bash
systemctl enable --now firewalld
# list all tables
nft list tables

table ip filter
table ip6 filter
table bridge filter
table ip security
table ip raw
table ip mangle
table ip nat
table ip6 security
table ip6 raw
table ip6 mangle
table ip6 nat
table bridge nat
table inet firewalld
table ip firewalld
table ip6 firewalld
```

Some common commands:
```bash
# list all tables
nft list tables

# list tables with specific protocol family
nft list tables ip
# check detail of ip filter
nft list table ip filter
```

Let's see the demo code to build nftables: 
- create chains
- create rules

```bash
# disable firewalld
systemctl disable --now firewalld ; reboot
nft list tables
# inet will work both for ipv4 and ipv6
# create a new table `filter`
nft add table inet filter 

# INPUT is the chain name, not necessarily call it INPUT
# here we add INPUT chain to inet filter table
nft add chain inet filter INPUT \
  # basic chain type: filter, route, nat
  # basic hook types: prerouting, input, forward, output, postrouting, ingress
  # priority 0 ~ 100, 0 is hightest
  { type filter hook input priority 0 \; policy accept \;}
```

Add SSH inbound to our system, set rules:
```bash
# add rule to inet filter table INPUT chain
nft add rule inet filter INPUT iif lo accept 
# allow traffic back to system with specified state
nft add rule inet filter INPUT ct state \
      established,related accept 
nft add rule inet filter INPUT tcp dport 22 accept
# drop everthing that is not explicitly defined
nft add rule inet filter INPUT counter drop 
```

Persisting nftables rules
```bash
# store rules
nft list ruleset > /root/myrules
# clear table
nft flush table inet filter
# delete table
nft delete table inet filter
# restore rules
nft -f /root/myrules 
```

Using systemd service unit:
```bash
# the systemd service unit for nftables use /etc/sysconfig/nftables.conf
nft list ruleset > /etc/sysconfig/nftables.conf
nft flush table inet filter
nft delete table inet filter
systemctl enable --now nftables 
```

# NFSv4
CentOS 8 uses [NFSv4.2](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_file_systems/exporting-nfs-shares_managing-file-systems) as NFS server.
The new tool `nfsconf` writes to the `/etc/nfs.conf`.
Enable and use NFSv4 only and managing inbound TCP connections using firewall. 
SELinux NFS configuration.

Install nfs package for both server and clients:
```bash
yum install -y nfs-utils
```
The default will have NFSv2 disable and NFSv3 and above enabled, we will disable NFSv3 and **have NFSv4 only with TCP port 2049 to be opened**. 这样看来之前项目中的NFS 用的默认设置，并且不是secure的.

we can edit `/etc/nfs.conf` or using `nfsconf` commands:
```bash
nfsconf --set nfsd vers4 y
nfsconf --set nfsd tcp y
# close udp and nfsv3
nfsconf --set nfsd vers3 n
nfsconf --set nfsd udp n
```

Start nfs server daemon:
```bash
systemctl enable --now nfs-server.service
```
Check port opened:
```bash
ss -tlp -4

State  Recv-Q   Send-Q      Local Address:Port       Peer Address:Port                                                              
LISTEN 0        128               0.0.0.0:sunrpc          0.0.0.0:*      users:(("rpcbind",pid=8920,fd=4),("systemd",pid=1,fd=76))  
LISTEN 0        128               0.0.0.0:mountd          0.0.0.0:*      users:(("rpc.mountd",pid=8936,fd=8))                       
LISTEN 0        128               0.0.0.0:ssh             0.0.0.0:*      users:(("sshd",pid=917,fd=5))                              
LISTEN 0        128               0.0.0.0:54425           0.0.0.0:*      users:(("rpc.statd",pid=8925,fd=9))                        
LISTEN 0        64                0.0.0.0:nfs             0.0.0.0:*    
```

We don't need `sunrpc` with NFSv4, mask them both service and socket:
```bash
systemctl mask --now rpc-statd rpcbind.service rpcbind.socket
```

Then we have nfs and mounted port only, only nfs port needs firewalld setting: 
```bash
ss -tl -4

State           Recv-Q           Send-Q                      Local Address:Port                         Peer Address:Port           
LISTEN          0                128                               0.0.0.0:mountd                            0.0.0.0:*              
LISTEN          0                128                               0.0.0.0:ssh                               0.0.0.0:*              
LISTEN          0                64                                0.0.0.0:nfs                               0.0.0.0:* 
```

Let's create some shared files:
```bash
mkdir /share
# copy *.txt under /usr/share/doc to /share
# {} represents the content find finds
# \; is used for find command, escape in bash
find /usr/share/doc -name '*.txt' -exec cp {} /share \;
```
Go to edit `/etc/exports` file
```bash
# Here only rw, in my previous work, we use (rw,insecure,async,no_root_squash)
# 这里其他默认设置够用了
/share *(rw)

# launch
exportfs -rav
# check options applied
exportfs -v

/share          <world>(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,root_squash,no_all_squash)
```
Configure firewall:
```bash
firewall-cmd --add-service=nfs --permanent
```

Then go to the client and mount `/share` folder.
后面讲了SElinux对NFS的支持，目前用不到, 也没明白.

# Storage Management Stratis
Stratis resources:
- https://stratis-storage.github.io/
- [MANAGING LAYERED LOCAL STORAGE WITH STRATIS](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_file_systems/managing-layered-local-storage-with-stratis_managing-file-systems)

视频中的讲解比RedHat的练习更好一些，在mount的时候，用的是`/etc/fstab` persistent configuration.

In creating filesystem, the author chooses `xfs`, so what is the difference comparing to `ext4`?
- https://computingforgeeks.com/ext4-vs-xfs-complete-comparison/


# Virtual Data Optimizer
To make use of block level deduplication, compression, thin-provisioning to save space.
- https://lab.redhat.com/vdo-configure

Example Use Case:
To reduce the amount of operational and storage costs in data centers, we use the deduplication and compression features in VDO to decrease the footprint of data.]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title>Awk Command Daily Work Summary</title>
    <url>/2019/02/28/linux-awk-summary/</url>
    <content><![CDATA[
Designed for data extraction and reporting.

`awk` is its own programming language itself and contains a lot of really good tools, enables a programmer to write tiny but effective programs in the form of statements that define **text patterns** that are to be searched for in **each line** of a document and the action that is to be taken when a **match** is found within a line. 

[Reference from GeeksforGeeks](https://www.geeksforgeeks.org/awk-command-unixlinux-examples/)
[awk in 20 mins](https://ferd.ca/awk-in-20-minutes.html)
WHAT CAN WE DO WITH AWK ?
1. AWK Operations:
(a) Scans a file line by line
(b) Splits each input line into fields
(c) Compares input line/fields to pattern
(d) Performs action(s) on matched lines

2. Useful For:
(a) Transform data files
(b) Produce formatted reports

3. Programming Constructs:
(a) Format output lines
(b) Arithmetic and string operations
(c) Conditionals and loops

日期记录的部分主要平时遇到的零散总结:
\################################################################
\#  &emsp; Date &emsp; &emsp; &emsp; &emsp; &emsp; Description
\#  &emsp; 09/11/2019 &emsp; &emsp; skip first line
\#  &emsp; 02/28/2019 &emsp; &emsp; print last column
\#  &emsp; 02/26/2019 &emsp; &emsp; awk remote execution
\#
\################################################################

### 02/26/2019
When use `awk` in script, may suffer shell unexpected expanding:
```bash
ssh -o StrictHostKeyChecking=no sshrm1 "ifconfig eth0 | grep \"inet\" | awk '{print $2}'"
```
Above will not get right data, instead preceding `\` before `$`
```bash
ssh -o StrictHostKeyChecking=no sshrm1 "ifconfig eth0 | grep \"inet\" | awk '{print \$2}'"
```
Another method is `awk` the return value from `ssh` rather than wrap it in `ssh` command.

### 02/28/2019
Print last column separated by space:
```bash
## NF: count of fields of a line
awk '{print $NF}' <file>
```

### 09/11/2019
Skip the first line:
```bash
## NR: current count of lines
awk 'NR>1 {print $1}' <file>
```
You can use `NR>=2`, `NR<5`, `NR==3`, etc to limit the range.

## Quick Start
```bash
## check version
awk -W version
## looks also works
awk --version
```

`awk` has BEGIN and END block, between is the body:
```bash
## BEGIN and END run only once
## body run as line number times
awk 'BEGIN {print "start..."} {print NR, $0} END {print NR}' /etc/hosts

## BEGIN
start...
## body
1 127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
2 ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
3 172.16.163.83 myk8s1.fyre.ibm.com myk8s1
4 172.16.182.156 myk8s2.fyre.ibm.com myk8s2
5 172.16.182.187 myk8s3.fyre.ibm.com myk8s3
## END
5
```

We can also put the awk option into awk script:
```bash
awk -f file.awk /etc/passwd
```
`file.awk` content:
```bash
## FS is used to specify delimiter to parse line, by default awk use space
BEGIN { FS=":" ; print "User Name:"} 
## $3 > 999 is the condition match
## NR is internal variable of awk
$3 > 999 {print NR, $0; count++ } 
END {print "Total Lines: " NR " Count Lines: " count}
```

Let's see more examples, actually sed may perform the same task but awk is more readable.
```bash
## set "," as delimiter, $1 to uppercase, $2 to lowercase
## toupper and tolower is awk internal functions
awk -F"," '{print toupper($1), tolower($2), $3}' <file>
```

lastlog.awk file to show non-root user login statistics
```bash
## exclude if match these:
!(/Never logged in/ || /^Username/ || /^root/) {
cnt++
## line fields == 8
if (NF == 8)
    printf "%8s %2s %3s %4s\n", $1, $5, $4, $8
else
    printf "%8s %2s %3s %4s\n", $1, $6, $5, $9
}
END {
print "==============================="
print "Total # of user processed: " cnt
}
```


]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title>Leetcode Python Summary</title>
    <url>/2020/08/09/leetcode-python-recall/</url>
    <content><![CDATA[
未来可见的一段时期要switch 到 Python了。
这里主要记录一下用Python 刷题的总结，特别是一些不知道的module, funtions等等，还有就是知道了但没有熟练应用。

对于while, if 等，None, 0 等都视作False，不用再去显示比较了。
```py
divmod(x, y)
"""
returns a pair of numbers consisting of their quotient and remainder.
"""
x and y : x is numerator and y is denominator
x and y must be non complex
```

L1480: 这题学到个新的function accumulate 依次叠加
```py
from itertools import accumulate
itr = accumulate([1,2,3,4,5])
list(itr)
## [1, 3, 6, 10, 15]
```

L617: 如果是BFS, 则要使用queue:
```py
## init
queue = collections.deque([])
## same as len(queue)
## canonical way for all collections (tuples, strings, lists, dicts and all their many subtypes) to check empty or not
while queue:
    pass
```

L1512: collecions.Counter()
```py
## return a dict subclass object
## {object: count, ...}
collections.Counter("sbadbfasdbfab")
```

LC236: 注意这里的这里表达方式
```py
def lowestCommonAncestor(self, root, p, q):
    ## 这个表达很省事
    if root in (None, p, q): return root
    ## generator 
    left, right = (self.lowestCommonAncestor(kid, p, q)
                   for kid in (root.left, root.right))
    return root if left and right else left or right
```


]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Make Big Files</title>
    <url>/2020/12/29/linux-big-file-make/</url>
    <content><![CDATA[
最近在学习Linux storage的过程中，研究了一下几个创建大文件的命令，特别是它们的区别.
可以参考这个提问[Quickly create a large file on a Linux system](https://stackoverflow.com/questions/257844/quickly-create-a-large-file-on-a-linux-system).

```bash
cd /tmp
# slow
dd if=/dev/zero of=./ddfile bs=1G count=3
# fastest
truncate -s 3G ./trunfile
# fast
fallocate -l 3G ./fallfile

# sync, depends on you needs
# if no sync, files may be still in memory
sync

# ls -ltrh
# in logical view, they are all 3GB
-rw-r--r--. 1 root root 3.0G Dec 30 07:01 ddfile
-rw-r--r--. 1 root root 3.0G Dec 30 07:02 trunfile
-rw-r--r--. 1 root root 3.0G Dec 30 07:02 fallfile

# check physical storage
# truncate space almost does not count 
# because of sparse
df -h .

# time test
# several seconds
time /bin/cp ./ddfile ~
time /bin/cp ./fallfile ~

# near instant
time /bin/cp ./trunfile ~
```
注意这里`sync`之后，file cache是仍然存在的，如果要彻底drop file cache, run `echo 3 > /proc/sys/vm/drop_caches`

所以说，在需要实打实disk allocated的场景中，不要使用`truncate`, 比如测试network, IO performance. `dd`最耗时，占用CPU 以及IO最多，`fallocate`比`dd`稍微好些，创建大文件比较快，也确实占用了空间。

其实`dd`也可以创建sparse file, with `seek` option:
```bash
dd if=/dev/zero of=sparse_file bs=1 count=0 seek=3G
# Block is 0
stat sparse_file
Size: 3221225472	Blocks: 0          IO Block: 4096   regular file
```

`dd` can be configured write to disk rather as well:
```bash
dd if=/dev/urandom of=/dev/sdb1 bs=1M count=2048
```

You can also shrink a file with many empty blocks with `fallocate` command:
```bash
dd if=/dev/zero of=./empty bs=1G count=3
# see the block number before and after change
stat ./empty
Size: 3221225472	Blocks: 6291544    IO Block: 4096   regular file

# deallocate empty block
fallocate -d ./empty

stat ./empty
Size: 3221225472	Blocks: 0          IO Block: 4096   regular file
```
]]></content>
      <categories>
        <category>Storage</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>storage</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Clean Memory Cache</title>
    <url>/2019/03/30/linux-clean-memory/</url>
    <content><![CDATA[When deploying DS, I find the compute pod that assigned to the second node is hanging in `CreateContainer` status. I SSH into that node and find node memory is occupied heavily by some other processes so the command response is extremely slow(其实ssh laggy是因为%CPU的原因，ssh需要和其他进程竞争), and the %CPU is also high with the swapping daemon(由于当时没记录，我猜测可能是[kswapd] daemon, 并且 [kswapd] 并不是仅仅针对swap工作，也会做释放缓存的操作，结合%CPU被其占用，估计是在持续做释放缓存的操作，但失败了).

Some informative readings:
- [kswapd](http://www.science.unitn.it/~fiorella/guidelinux/tlk/node39.html)
- [why swapping when there is more than enough free memory](https://unix.stackexchange.com/questions/2658/why-use-swap-when-there-is-more-than-enough-free-space-in-ram)
- [memory runs full over time](https://unix.stackexchange.com/questions/415814/memory-runs-full-over-time-high-buffer-cache-usage-low-available-memory)

The memory usage on bad node:
```js
free
              total        used        free      shared  buff/cache   available
Mem:        8168772      105152      295732     7491216     7767888      273448
Swap:             0           0           0
```
See the `available` size is too low, Comparing with the good node:
```js
free
              total        used        free      shared  buff/cache   available
Mem:        8168772      123504     7041388      270836     1003880     7424612
Swap:             0           0           0
```
You see the shared(Memory used mostly by tmpfs) and buff/cache parts are huge on bad node, I need to flush and clean it. (当时还未理解这些column的具体含义, 特别是shared, buff/cache, available). 还应该用memory leak相关分析工具去查看哪个调用栈导致了内存紧张 or memleak).

马后炮, 应该去调查为什么这个node shared, buff/cache size 异常的高，以及为什么[kswapd]操作失败，原因可能是tmpfs(shared)仍然在被使用，需要调查. 后面介绍了如何释放cache by writing to `drop_caches`，但是我记得当时作用不大，this second comment may help:
[Why can't I release memory cache by /proc/sys/vm/drop_caches](https://serverfault.com/questions/968020/why-cant-i-release-memory-cache-by-proc-sys-vm-drop-caches)
```bash
# -t: type
df -t tmpfs --total -h
# check one of the tmpfs mount status
lsof -nP +L1 /dev/shm | grep DEL
```

The general solution to release cache intentionally, this [post](https://www.blackmoreops.com/2014/10/28/delete-clean-cache-to-free-up-memory-on-your-slow-linux-server-vps/) is good to reference.
If you have to clear the disk cache, this command is safest in enterprise and production, will clear the `PageCache` only:
```bash
# modify kernel behavior by proc file
sync; echo 1 > /proc/sys/vm/drop_caches
```
> What is `sync` command: flush any data buffered in memory out to disk.

More aggressively, Clear `dentries` and `inodes`:
```bash
sync; echo 2 > /proc/sys/vm/drop_caches 
```
Clear `PageCache`, `dentries` and `inodes`:
```bash
# 清理文件页、目录项、Inodes等各种缓存
# do all above
sync; echo 3 > /proc/sys/vm/drop_caches 
```
It is not recommended to use this in production until you know what you are doing, as it will clear `PageCache`, `dentries` and `inodes`. Because just after your run drop_caches, your server will get busy re-populating memory with `inodes` and `dentries`, original Kernel documentation recommends not to run this command outside of a testing or debugging environment. But what if you are a home user or your server is getting too busy and almost filling up it’s memory. You need to be able trade the benefits with the risk.

- what is `dirty cache`?
Dirty Cache refers to data which has `not` yet been committed to the database (or disk), and is currently held in computer memory. In short, the new/old data is available in Memory and it is different to what you have in database/disk.

- what is `clean cache`?
Clean cache refers to data which has been committed to database (or disk) and is currently held in computer memory. This is what we desire where everything is in sync.

- what is `dentries` and `inodes`?
A filesystem is represented in memory using dentries and inodes.  Inodes are the objects that represent the underlying files (and also directories). A dentry is an object with a string name (d_name), a pointer to an inode (d_inode), and a pointer to the parent dentry (d_parent)

- what is `drop_caches`?
Writing to this will cause the kernel to drop `clean caches`, as well as reclaimable slab objects like dentries and inodes.  Once dropped, their memory becomes free. It will not kill any process.

BTW, if you want to clean swap space (actually here we don't have swap enabled):
```bash
swapoff -a && swapon -a
```]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Capability</title>
    <url>/2019/05/13/linux-capability/</url>
    <content><![CDATA[
Starting with kernel 2.2, Linux divides the privileges traditionally associated with superuser into distinct units, known as `capabilities`, which can be independently enabled and disabled. This way the full set of privileges is reduced and decreasing the risks of exploitation.

This story started by removing `SYS_ADMIN` and `SYS_RESOURCE` Linux capabilities from K8s container which hosts DB2. Why we removed them? Because by the time DB2 has to run as root(to tune kernel parameters), so we want to minimize the privilege of the root user by removing some risky Linux capabilities from pod/container.

And a container is really just a process running on the system, separated using cgroups and namespaces in the kernel. This means that capabilities can be assigned to the container in just the same way as with any other process and this is handled by the container runtime when it creates the container.

[How to add/remove Linux capabilities in K8s](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container)

[Secure Your Containers with this One Weird Trick](https://www.redhat.com/en/blog/secure-your-containers-one-weird-trick): The way I describe it is that most people think of root as being all powerful. This isn't the whole picture, the root user with all capabilities is all powerful. 

[Linux capabilities in Kubernetes](https://snyk.io/blog/kubernetes-securitycontext-linux-capabilities/)

For the purpose of performing permission checks, traditional UNIX implementations distinguish two categories of processes: `privileged` processes (whose effective user ID is 0, referred to as superuser or root), and `unprivileged` processes (whose effective UID is nonzero).

## Basic Capability Thing

### Header File
Linux capabilities are defined in a header file with the non-surprising name `capability.h`, in `/usr/include/linux/capability.h`. They're pretty self-explanatory and well commented

### Capability Number
To see the highest capability number for your kernel, use the data from the `/proc` file system.
```bash
# cat /proc/sys/kernel/cap_last_cap
36
```

### Current Capabilities
To see the current capabilities list, run `capsh --print`, for example, as normal user `dsadm`:
```bash
# capsh --print

Current: =
Bounding set =cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,
cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,
cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,
cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,
cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,
cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,
35,36
Securebits: 00/0x0/1'b0
 secure-noroot: no (unlocked)
 secure-no-suid-fixup: no (unlocked)
 secure-keep-caps: no (unlocked)
uid=1002(dsadm)
gid=1002(dsadm)
groups=1002(dsadm)
```
you see the `Current: =` is empty, but if you run as `root` user:
```bash
$ capsh --print

Current: = cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,
cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,
cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,
cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,
cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,
cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,
35,36+ep
Bounding set =cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,
cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,
cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,
cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,
cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,
cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,
35,36
Securebits: 00/0x0/1'b0
 secure-noroot: no (unlocked)
 secure-no-suid-fixup: no (unlocked)
 secure-keep-caps: no (unlocked)
uid=0(root)
gid=0(root)
groups=0(root)
```

To see the capabilities for a particular process, run `cat /proc/<PID>/status | grep -i cap`:
```bash
# cat /proc/1/status | grep -i cap

CapInh: 00000000a884a5fb
CapPrm: 00000000a884a5fb
CapEff: 00000000a884a5fb
CapBnd: 00000000a884a5fb
CapAmb: 0000000000000000
```
This is the bit map for capabilities, the meaning for each is:
* CapInh = Inherited capabilities
* CapPrm – Permitted capabilities
* CapEff = Effective capabilities
* CapBnd = Bounding set
* CapAmb = Ambient capabilities set

> The `CapBnd` defines the upper level of available capabilities. During the time a process runs, no capabilities can be added to this list. Only the capabilities in the bounding set can be added to the inheritable set, which uses the capset() system call. If a capability is dropped from the boundary set, that process or its children can no longer have access to it.

Using the `capsh` utility we can decode them into the capabilities name:
```bash
# capsh --decode=00000000a884a5fb

0x00000000a884a5fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,
cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_ipc_owner,cap_sys_chroot,
cap_sys_nice,cap_mknod,cap_audit_write,cap_setfcap
```
The another easy way is use `getpcaps` utility:
```bash
# getpcaps 1965

Capabilities for `1965': = cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,
cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_ipc_owner,
cap_sys_chroot,cap_sys_nice,cap_mknod,cap_audit_write,cap_setfcap+eip
```

It is also interesting to see the capabilities of a set of processes that have a relationship.
```bash
# getpcaps $(pgrep db2)

Capabilities for `1965': = cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,
cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_ipc_owner,
cap_sys_chroot,cap_sys_nice,cap_mknod,cap_audit_write,cap_setfcap+eip
Capabilities for `2151': = cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,
cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_ipc_owner,
cap_sys_chroot,cap_sys_nice,cap_mknod,cap_audit_write,cap_setfcap+i
Capabilities for `2245': = cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,
cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_ipc_owner,
cap_sys_chroot,cap_sys_nice,cap_mknod,cap_audit_write,cap_setfcap+eip
Capabilities for `2246': = cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,
cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_ipc_owner,
cap_sys_chroot,cap_sys_nice,cap_mknod,cap_audit_write,cap_setfcap+eip
Capabilities for `2247': = cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,
cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_ipc_owner,
cap_sys_chroot,cap_sys_nice,cap_mknod,cap_audit_write,cap_setfcap+eip
Capabilities for `2249': = cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,
cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_ipc_owner,
cap_sys_chroot,cap_sys_nice,cap_mknod,cap_audit_write,cap_setfcap+i
Capabilities for `2614': = cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,
cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_ipc_owner,
cap_sys_chroot,cap_sys_nice,cap_mknod,cap_audit_write,cap_setfcap+i
Capabilities for `4213': = cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,
cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_ipc_owner,
cap_sys_chroot,cap_sys_nice,cap_mknod,cap_audit_write,cap_setfcap+i
Capabilities for `4238': = cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,
cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_ipc_owner,
cap_sys_chroot,cap_sys_nice,cap_mknod,cap_audit_write,cap_setfcap+i
```

### Limit Capability
You can test what happens when a particular capability is dropped by using the `capsh` utility. This is a way to see what capabilities a particular program may need to function correctly. The `capsh` command can run a particular process and restrict the set of available capabilities.
```bash
capsh --print -- -c "/bin/ping -c 1 localhost"
```
After dropping `cap_net_raw`, `ping` not permitted.
```bash
capsh --drop=cap_net_raw --print -- -c "/bin/ping -c 1 localhost"
```

### Capability Meet
List the capabilities I have seen so far:
* CAP_SYS_ADMIN
  Without it, I cannot perform `hostname` command for docker container in K8s.
* CAP_SYS_RESOURCE
  This is for adjust [DB2 kernel parameters](https://www.ibm.com/support/knowledgecenter/en/SSEPGG_11.1.0/com.ibm.db2.luw.qb.server.doc/doc/c0057140.html)

These 3 necessaries are for DB2:
* CAP_SETFCAP
  Set arbitrary capabilities on a file. (actually this is default in unprivileged docker container)
* CAP_SYS_NICE
* CAP_IPC_OWNER
  Bypass permission checks for operations on System V IPC objects.

### My Questions
1. Is capability granted to user or process?
   Frist ensure the env(container) has enough Linux caps.
   Then using command grant process certains caps, but need root privilege to do that, then you can run process as ordinary user.

2. Privilege process bypass all kernel permission check? Does that mean linux
   capabilities are only for non-privilege user or process?
   I think there is a global or default capability set in system to determine what ever processes on system is allowed to do. Then you can fine-tune for unprivileged process.

3. If we have root and normal user both in docker container, so capabilities are applied 
   on root or normal user or both?
   After testing and comparing by `capsh --print` with different user in xmeta container, I think capabilities are applied on all users in K8s environment.

Later I post blogs to talk about `<<Capability in Docker>>`.

## Resources
[Linux Programmer's Manual](http://man7.org/linux/man-pages/man7/capabilities.7.html)
[Linux capabilities 101](https://linux-audit.com/linux-capabilities-101/)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>capability</tag>
      </tags>
  </entry>
  <entry>
    <title>Scheduling Recurring Tasks with cron</title>
    <url>/2019/05/28/linux-cronjob/</url>
    <content><![CDATA[
Let's first see what does `cron` represent from [wiki](https://en.wikipedia.org/wiki/Cron):
> The software utility `cron` is a time-based job scheduler in Unix-like computer operating systems. People who set up and maintain software environments use `cron` to schedule jobs (commands or shell scripts) to run periodically at fixed times, dates, or intervals. 

`cron` is most suitable for scheduling repetitive tasks. For example, it runs log file rotation utilities to ensure that your hard drive doesn’t fill up with old log files. You should know how to use cron because it’s just plain useful.

> Also see [cronjob](https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/) in K8s.

## Install crontab
In CentOS or RedHat, you can run:
```
yum install cronie
```
If you are not sure, try `yum provides crontab` to see which package will provide this service.

To check if `cron` service is running or not:
```
systemctl status crond
```
If inactive, enable and restart it. 

## crontab File
Cron is driven by a `crontab(cron table)` file, a configuration file that specifies shell commands to run periodically on a given schedule. 

The program running through cron is called a `cron job`. To install a `cron job`, you’ll create an entry line in your `crontab` file, usually by running the `crontab` command.

Each user can have his or her own crontab file, which means that every system may have multiple crontabs, usually found in `/var/spool/cron/` folder. the crontab command installs, lists, edits, and removes a user’s crontab.

## crontab Commands
For example, run as `root`, I want to set a recurring task for user `dsadm`:
```
crontab -u dsadm -e
```
Then edit like this:
```
00 21 * * * /home/dsadm/test.sh > /tmp/cron-log 2>&1
```
This means on everyday at 9:00PM, user `dsadm` will run `test.sh` and redirect output to `/tmp/cron-log` file. You can also put the entries into a file and run:
```
crontab -u dsadm <entry file>
```

The meaning of the entry is:
```
# ┌───────────── minute (0 - 59)
# │ ┌───────────── hour (0 - 23)
# │ │ ┌───────────── day of the month (1 - 31)
# │ │ │ ┌───────────── month (1 - 12)
# │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday;
# │ │ │ │ │                                   7 is also Sunday on some systems)
# │ │ │ │ │
# │ │ │ │ │
# * * * * * command to execute
```
> A `*` in any field means to match every value.

Now, if you check `/var/spool/cron` directory, the `dsadm` crontab file is created there.

To list the `dsadm` cron job:
```
crontab -u dsadm -l
```
To remove `dsadm` cron job:
```
crontab -u dsadm -r
```

## Run as Non-Root
If you want to run `crontab` as `dsadm`, you must set the cron permission:
* `/etc/cron.allow` - If this file exists, it must contain your username for you to use cron jobs.
* `/etc/cron.deny` - If the cron.allow file does not exist but the `/etc/cron.deny` file does exist then, to use cron jobs, you must not be listed in the `/etc/cron.deny` file. 

So, if you put `dsadm` in `/etc/cron.allow` file, then you can use `crontab` directly.

## System crontab File
Linux distributions normally have an `/etc/crontab` file. You can also edit here, but the format is a little bit difference:
```
# Example of job definition:
# .---------------- minute (0 - 59)
# |  .------------- hour (0 - 23)
# |  |  .---------- day of month (1 - 31)
# |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...
# |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
# |  |  |  |  |
# *  *  *  *  * user-name  command to be executed
```
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>cronjob</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Commands Collect</title>
    <url>/2020/03/19/linux-cmd-collect/</url>
    <content><![CDATA[
这些都是我工作至今或多或少用到过的，单纯地陈列这些命令没什么用，关键是要在合适的场景选择合适的工具去解决问题，这里就是给自己提个醒儿，也是出于好奇统计一下自己的知识储备。

## a
`awk` (data processing and extracting), `alias`, `at`(schedule job one time), `arp`(check layer2 to layer3 mapping), `ab`(apach HTTP server benchmark tool), `apropos`, `aspell`, `ack`(grep like search for source code)

## b
`base64`(encode/decode), `bg`, `basename`(last name), `blkid`, `bzip2`, `bc`

## c
`cat`, `cd`, `cp`, `cut` ,`curl` ,`chown` ,`chmod` ,`chgrp` ,`cron` ,`clear`, `cal`(calendar), `comm`

## d
`df`(disk space check), `dirname`(path prefix), `du`(occupied space check), `diff`, `date`, `dd`(convert and copy), `dnsdomainname`, `dig`(dns lookup utility), `disown`(remove job from current job list)

## e
`echo`, `exit`, `export`, `env`, `exportfs`, `ethtool`(physical card), `eval`, `expand`(tab to spaces)

## f
`file`, `free`, `find`, `fg`, `firewall-cmd`, `fallocate`(fast allocate space), `fuser`, `fsck`, `fold`, `fmt`

## g
`gzip`, `grep`, `git`, `gitk`, `getfacl`, `getent`(look up /etc/hosts)

## h
`hostname`, `host`, `htpasswd`, `history`, `hostnamectl`(permenant hostname), `hdparm d`

## i
`ip`(so powerful), `ifconfig`(obsolete), `id`, `iperf3`, `iptables`, `iostat`, `ifdown`, `ifup`, `install`(copy files and set attributes)

## j
`jq`, `jobs`, `journalctl`, `join`

## k
`kill`

## l
`ls`, `less`, `ln`, `lsblk`, `lvdisplay`, `lscpu`, `lastlog`, `last`(reboot), `losetup`, `lsof`(list open files), `loginctl`, `lvm`, `lvreate`, `lvextend`, `lvresize`, `lvreduce`, `lvemove`, `lvdisplay`, `locate`

## m
`mv`, `mount`, `more`, `man`, `mkdir`, `mktemp`, `mdadm`, `make`, `mkfifo`(named pipe), `mpstat`

## n
`nc`, `netstat`, `nslookup`, `nmap`, `nice`(process priority), `nmcli`, `nohup`, `nl`, `numfmt`(unit convert)

## o
`openssl`

## p
`pwd`, `ping`, `ps`, `perf`, `pvdisplay`, `pmap`, `pwdx`, `pv`(throttle the progress of data through a pipe), `pvcreate`, `pvscan`, `paste`, `patch`, `printf`, `pidstat`

## q
so far no!

## r
`rm`, `rmdir`, `route`, `rsync`, `readlink`, `runlevel`, `renice`, `rpm`, `rev`(reverse seq), `reset`

## s
`ssh`, `scp`, `sftp`, `strace`, `sudo`, `su`, `sed`(stream editor), `setfacl`,
`sleep`, `stat`, `systemctl`, `shutdown`, `sar`(system activity report), `ss`(similar to netstat), `stress`, `stress-ng`, `sort`, `seq`, `sync`, `sshfs`, `screen`, `script`, `set`, `split`, `strings`(print printable chars in files)

## t
`tar`, `tcpdump`(wireshark), `tcpreplay`, `top`(powerful!), `trap`, `touch`, `tee`, `tail`, `tree`, `tracepath`, `traceroute`, `tload`, `tc`(traffic control), `tac`(reversion of `cat`), `truncate`, `tr`(translate), `timeout`, `tmux`

## u
`uniq`, `uname`, `umount`, `unlink`, `uuidgen`, `uptime`, `udevadm`, `unalias`

## v
`vim`, `vmstat`, `vgcreate`, `vgextend`, `vgreduce`, `vgremove`, `vgdisplay`

## w
`w`, `who`, `wget`, `wc`, `wall`, `write`, `watch`, `wipefs`

## x
`xargs`

## y
`yum`, `yes`

## z
`zip`]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>linux-change-homedir</title>
    <url>/2019/07/15/linux-change-homedir/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Linux Check Disk Space</title>
    <url>/2019/08/11/linux-disk-check/</url>
    <content><![CDATA[
这里补充一下，常用的关于disk排查的命令就是`df`, `du`, `dd`, `lsof`, `lsblk`, `blkid`, `mount`, `fdisk`, `mkfs`, `sync`, `lvm`. More about Linux Storage, see my blog `<<Linux Storage System>>`.

I get error messages when run `docker load` command, this is caused by disk space run out. How do I know the disk utilization?

Shows the amount of disk space used and available on Linux file systems.
```bash
# disk space report tool, check mount point and fs
# -h: readable
# -T: file system type
$ df [-hT]

Filesystem             Size  Used Avail Use% Mounted on
/dev/mapper/rhel-root  241G  219G   23G  91% /
devtmpfs               3.9G     0  3.9G   0% /dev
tmpfs                  3.9G     0  3.9G   0% /dev/shm
tmpfs                  3.9G  8.6M  3.9G   1% /run
tmpfs                  3.9G     0  3.9G   0% /sys/fs/cgroup
/dev/vda1             1014M  208M  807M  21% /boot
tmpfs                  783M     0  783M   0% /run/user/0

# check mounted filesystem of directory
df -h <directory>
# inode usage of directory
df -i <directory>
```

Shows total size of the directory and it's subdirectories
```bash
# du: estimate file space usage

# 统计当前目录下文件大小，并按照大小排序
# -s: display only a total
# -m: --block-size=1M
# *: shell globs
du -sm * | sort -nr
# -BG: blocksize as G
du -s -BG * | sort -nr

# current directory total size
du -sm .
```

`lsof` 在这方面主要就是检查哪些文件正在被什么user, process使用:
```bash
# see what is interacting with this file
# -l: show user id instead of user name
lsof -l /path/to/file/or/directory

# see files have been opened from a directory
# +D: directory option
lsof +D /var/log

# files opened by process name, wild match by beginning characters
lsof -c ssh
lsof -c init

# files opened by process id
# ^: exclude
lsof -p 5644,^3122

# files opened by user
lsof -u root,1011
lsof -u 1000

# ^: excluding root
lsof +D /home -u ^root

# -a: and operation
# default is or operation
lsof -u mary -c ssh -a
```

当然在network上`lsof`也很强大:
[An lsof primer](https://danielmiessler.com/study/lsof/)
[How to use lsof command](https://www.howtogeek.com/426031/how-to-use-the-linux-lsof-command/)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Check Linux Distributions</title>
    <url>/2019/03/27/linux-distros/</url>
    <content><![CDATA[
I encounter a problem that check which OS is running in my docker container, or extend it as how to check which OS am I using?
```
cat /etc/os-release

NAME="Red Hat Enterprise Linux Server"
VERSION="7.5 (Maipo)"
ID="rhel"
ID_LIKE="fedora"
...
```

```
 hostnamectl

 Static hostname: example.com
         Icon name: computer-vm
           Chassis: vm
        Machine ID: e57cfe9136e9430587366e04f14195e1
           Boot ID: 6ebe05de8b7f43c0bfa36d2c62b702de
    Virtualization: kvm
  Operating System: Red Hat Enterprise Linux Server 7.5 (Maipo)
       CPE OS Name: cpe:/o:redhat:enterprise_linux:7.5:GA:server
            Kernel: Linux 3.10.0-862.14.4.el7.x86_64
      Architecture: x86-64
```

For docker image, you can use `docker image inspect` command:
```
docker image inspect <image name>:<tag> | grep -i "base image"

"org.label-schema.schema-version": "= 1.0  
org.label-schema.name=CentOS Base Image  
org.label-schema.vendor=CentOS
...
```

]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux DNS Exploration</title>
    <url>/2019/11/04/linux-dns/</url>
    <content><![CDATA[
DNS is the phonebook of internet.
[Comparison of DNS server software](https://en.wikipedia.org/wiki/Comparison_of_DNS_server_software), be aware of alternatives.
[Cloudflare DNS course](https://www.cloudflare.com/learning/dns/what-is-dns/) 内容很不错。

# Issue
这个问题很有意思，最开始我并没有意识到这其实是个DNS问题，后来随着逐步深入排查，解决了一些有干扰的边边角角的错误，才发现。

问题的开始是当集群中docker registry 已经正常运行的时候，docker push 以及 docker pull不能正常工作，retry 超时。当时的push URL 是以hostname 为主的，比如：
```bash
dal12-3m-3w-testcluster-03master-00.demo.ibmcloud.com:5000/is-realtime-busybox:latest
```

如果以上docker push 操作在docker registry pod的宿主机上进行，还是不行，但把地址改成localhost 就可以了, 或则在其他机器上用host VM的public IP:
```bash
localhost:5000/is-realtime-busybox:latest
```
这让我首先意识到是域名解析的问题，我的第一反应是查看各个节点上的`/etc/hosts`文件，完全没问题, `ping`命令也OK，很奇怪。

让我们来再仔细的检查一下域名配置：
参考这篇[文章](https://www.tecmint.com/setup-local-dns-using-etc-hosts-file-in-linux/), 查看`/etc/nsswitch.conf`可知域名查询时的顺序, 值得注意的是，有的malicious scripting或病毒可能会更改你的nsswitch.conf文件。
```bash
#hosts:     db files nisplus nis dns
hosts:      files dns
```
files就是指`/etc/hosts`, dns 指DNS server，说明确实是先看local file `/etc/hosts`的。

查看`/etc/resolv.conf`，这个就是DNS server的地址了，貌似也没啥问题。
```
nameserver 10.0.80.11
nameserver 10.0.80.12
```

我猜想有的命令可能不会使用local DNS file `/etc/hosts`，试了试`host` command，果然如此：
[Why does the host command not resolve entries in /etc/hosts?](https://serverfault.com/questions/498500/why-does-the-host-command-not-resolve-entries-in-etc-hosts), 看来docker push/pull 也是如此。
这个答案还告诉了我另一个命令`getent`，对于查询`/etc/hosts`挺方便的。
```bash
getent hosts halos1
```
You will find that `dig` and `nslookup` behave the same way as `host`, the purpose of all of these commands is to do DNS lookups, not to look in files such as `/etc/hosts`.

后来我让别人把master node的域名和IP加入到集群访问的DNS Server中，问题就解决了！

所以，下次遇到类似问题，除了检查本地DNS配置和文件，还要用`host` command试一下，看看外部DNS Server是否工作正常，最重要的是，有的命令不会使用`/etc/hosts`去查询。


# resolv.conf
The file is a plain-text file usually created by the network administrator or by applications that manage the configuration tasks of the system. The file is either maintained manually, or rewriting by DHCP server. If wants to customize this file, need to `disable` resolved serivce.

The process of determining IP addresses from domain names is called `resolving`.

[resolv.con file content explanation](https://www.shellhacks.com/setup-dns-resolution-resolvconf-example/), or see `man resolv.conf`:
```ini
# local domain name suffix
# obsolete only for search directive
domain service.consul
# Which Domain to search
search service.consul node.consul
# DNS server IP, up to 3
# ipv4 or ipv6
# query in order
nameserver 127.0.0.1 
```
So if we lookup hostname xxx, the DNS will try to resolve xxx.service.consul followed by xxx.node.consul on localhost DNS server.


# BIND
`/etc/hosts` file is not enough as internet keep growing, in 1984 7 Top level domains got created.
DNS [record type](https://en.wikipedia.org/wiki/List_of_DNS_record_types), for example [`A` record](https://www.cloudflare.com/learning/dns/dns-records/dns-a-record/)

[`BIND`](https://www.isc.org/bind/) DNS: is an acronym for Berkeley Internet Name Domain. [Install DNS server using bind 9 on centOS 7](https://www.unixmen.com/dns-server-setup-using-bind-9-on-centos-7-linux/), the package is called bind but service is called named:
```bash
yum updates
yum install -y bind bind-utils

# list files in bind package
# -q: query
# -l: list option under -q
rpm -ql bind

/etc/logrotate.d/named
/etc/named
/etc/named.conf
/etc/named.iscdlv.key
/etc/named.rfc1912.zones
/etc/named.root.key
/etc/rndc.conf
/etc/rndc.key
/etc/rwtab.d/named
/etc/sysconfig/named
...

# if have firewall open 53 port
firewall-cmd --permanent --add-port=53/tcp
firewall-cmd --permanent --add-port=53/udp
firewall-cmd --reload

# enable and start the dns service
systemctl enable named
systemctl start named

# you will see 53 and 953 port
# 953 is for connecting with rdnc command 
netstat -tnlp

# query
dig @localhost www.google.com
# check dns version
named -v
```
DNS information is stored in text file called `zones`

`rdnc` command is used to control the `named` service:
```bash
# see dns version
# system resources: CPU, threads, zones
# up and running state
rndc status
# reload config
rndc reload
```

Let's see the named systemd unit file, similar to zookeeper's, also have an eye on `ExecStartPre` using bash:
```ini
# systemctl cat named
[Unit]
Description=Berkeley Internet Name Domain (DNS)
Wants=nss-lookup.target
Wants=named-setup-rndc.service
Before=nss-lookup.target
After=network.target
After=named-setup-rndc.service

[Service]
Type=forking
Environment=NAMEDCONF=/etc/named.conf
EnvironmentFile=-/etc/sysconfig/named
Environment=KRB5_KTNAME=/etc/named.keytab
PIDFile=c

# check zone files
ExecStartPre=/bin/bash -c 'if [ ! "$DISABLE_ZONE_CHECKING" == "yes" ]; then /usr/sbin/named-checkconf -z "$NAMEDCONF"; else echo "Checking of zone files is disabled"; fi'
ExecStart=/usr/sbin/named -u named -c ${NAMEDCONF} $OPTIONS
ExecReload=/bin/sh -c '/usr/sbin/rndc reload > /dev/null 2>&1 || /bin/kill -HUP $MAINPID'
ExecStop=/bin/sh -c '/usr/sbin/rndc stop > /dev/null 2>&1 || /bin/kill -TERM $MAINPID'
PrivateTmp=true

[Install]
WantedBy=multi-user.target
```
As you see, `/etc/named.conf` is the config file.


## Zone
[What is a DNS zone and zone file?](https://www.cloudflare.com/learning/dns/glossary/dns-zone/)
A zone file is a plain text file stored in a DNS server that contains an actual representation of the zone and contains all the records for every domain within the zone. 

For example, a local configuration:
```js
zone "example.com" IN {
  type master;
  file "db.example";
  allow-update { none; };
};

zone "2.0.10.in-addr.arpa" IN {
  type master;
  file "db.10.0.2";
  allow-update { none; };
};
```
Run syntax checks on configuration and zone files:
```bash
# no parameters needed
sudo named-checkconf -v
# or
# check zone file syntax
sudo named-checkzone <zone name> <paht to zone file>
```
Then you can create `db.example` file accordingly, for example:
```js
$TTL 3h
$ORIGIN example.com.
example.com. IN SOA master.example.com root.example.com. (
  2020012323 ; Serial
  8h ; Refresh
  4h ; Retry
  1w ; Expire
  1h ; Negative TTL
)
example.com. IN NS master.example.com.
master      IN  A 10.0.2.4
gw          IN  A 10.0.2.1
mail        IN  A 10.0.2.2
$GENERATE 101-200 student-$ IN A 10.0.2.$
; Alias
ns1         IN  CNAME master.example.com.

; Mail Servers
nexample.com. IN  MX 5 mail.example.com.
```
```bash
# check syntax
named-checkzone example.com db.example
```


# Dnsmasq
dnsmasq 是最常用的 DNS 缓存服务之一，还经常作为 DHCP 服务来使用。它的安装和配置都比较简单，性能也可以满足绝大多数应用程序对 DNS 缓存的需求.

[Want Faster, Easier-to-Manage DNS? Use Dnsmasq](https://medium.com/linode-cube/want-faster-easier-to-manage-dns-use-dnsmasq-a02517234d5f)
Dnsmasq (short for DNS masquerade) is a lightweight, easy to configure DNS forwarder, designed to provide DNS (and optionally DHCP and TFTP) services to a small-scale network. It can serve the names of local machines which are not in the global DNS.

Dnsmasq accepts DNS queries and either answers them from a small, local `cache` or forwards them to a real, recursive DNS server. It loads the contents of `/etc/hosts`, so that local host names which do not appear in the global DNS can be resolved.

By default, Dnsmasq will use the DNS servers setup in your `/etc/resolv.conf` file.

Dnsmasq will only access the first three sites listed in the resolv.conf file. I usually add one of the Google Public DNS servers, `8.8.8.8` or `8.8.4.4` and one of Cisco’s OpenDNS servers, `208.67.222.222` or `208.67.220.220`, and `1.1.1.1` operated by cloudflare to the default DNS site

While, you’re in the resolv.conf file, go ahead and add `127.0.0.1` localhost as the first line. This enables Dnsmasq to cache DNS queries for queries from the local machine.
```bash
yum install -y dnsmasq
# dnsmasq does not create its own un-privileged user and group
groupadd -r dnsmasq && useradd -rg dnsmasq dnsmasq
# add the user and group in conf file
# Config file: `/etc/dnsmasq.conf`


# if have firewall open 53 port
firewall-cmd --permanent --add-port=53/tcp
firewall-cmd --permanent --add-port=53/udp
firewall-cmd --reload

systemctl enable dnsmasq
systemctl start dnsmasq
```

config options for example:
```ini
listen-address=127.0.0.1,10.0.2.15
port=53
domain-needed
bogus-priv
# no read /etc/hosts
no-hosts
dns-forward-max=100
cache-size=500
# no continue polling to update cahce
no-poll
# specify resolv file location
resolv-file=/etc/resolv.conf
# upstream dns server ip
server=
```


]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>dns</tag>
      </tags>
  </entry>
  <entry>
    <title>Double Level Softlink</title>
    <url>/2019/09/27/linux-double-softlink/</url>
    <content><![CDATA[
This blog shows how double level softlink can be a good workaround for some situations.

We need to link persistent data to a directory under `/` in a container or pod, for example: `/data`, this folder is owned by `demo`, `demo` is also the start user of the container.

在pod中有个`/data` folder, owned by `demo`，我们想persist这个folder的内容. 在这个pod中有个mount point `/mnt`. 于是想把`/data` map到 `/mnt/data`.
```bash
# ln -s <target> <link name>
# 这样相当于/mnt/data 是个link
ln -s /data /mnt/data
```
这样是不对的，从pod外部的storage provisioner看 `/min/data`仅仅是个borken link.

The correct way is first remove `/data` then `ln -s /mnt/data /data` (`/data`变成了快捷方式，所以写入`/data`的内容实际上被写入了`/mnt/data`), but `demo` is a non-root user without super privilege, it cannot remove `/data` (`/` is owned by root).

Let's see how double level softlink can help:
1. first in docker build time remove `/data`: `rm -rf /data`
2. create a intermediary: `mkdir -p /home/demo/data && chown demo:demo /home/demo/data`
3. link: `ln -s /home/demo/data /data`

then commit the changes into image.

when container start, in the entrypoint:
1. first remove `/home/demo/data`: `rm -rf /home/demo/data`, this will make link to `/data` break.
2. create another link: `ln -s /mnt/data /home/demo/data`, now link connected and fixed.

So finally the link structure is:
```
/mnt/data -> /home/demo/data -> /data
```
`/home/demo/data` is a agent between persistent mount`/mnt/data` and `/data`.

]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>ip Command</title>
    <url>/2020/02/04/linux-ip-command/</url>
    <content><![CDATA[
//TODO
[ ] `ip` vs `iptables` 各自用在什么地方?
my question: https://unix.stackexchange.com/questions/608560/what-is-the-difference-between-iptables-and-ip-route-table


`ip` command in Linux is present in the `iproute` package which is used for performing several network administration tasks. IP stands for Internet Protocol.

`ifconfig` is obsolete, `ip` is preferred.

It can perform several tasks like configuring and modifying the default and static routing, setting up tunnel over IP, listing IP addresses and property information, modifying the status of the interface, assigning, deleting and setting up IP addresses and routes.

Explanations:
[Linux ip Command with Examples](https://linuxize.com/post/linux-ip-command/)
[Linux manual page: ip](http://man7.org/linux/man-pages/man8/ip.8.html)

Most frequently used subcommands:
- link (l) - Display and modify network interfaces.
- address (a) - Display and modify IP Addresses.
- route (r) - Display and alter the routing table.
- neigh (n) - Display and manipulate neighbor objects (ARP table).
- netns - deal with network namespace

The configurations set with the ip command are **not** persistent. After a system restart, all changes are lost. For permanent settings, you need to edit the distro-specific configuration files or add the commands to a startup script.

## address
Show all IP address associated with all interfaces that are available
```bash
## ip address
ip addr
ip a
## only show ipv4
ip -4 a
## show specified device
ip addr show eth0
## abbr
ip a s eth0
## assign an IP address to an interface. (need append netmask)
## can assign multiple ip address to an interface
## not persistent
ip addr add 192.168.1.50/24 dev eth0
## delete an assigned IP address to an interface. (need append netmask)
ip addr del 192.168.1.50/24 dev eth0
```

## link
It is used to display `link layer` information, like MAC address, it will fetch characteristics of the link layer devices currently available. Any networking device which has a driver loaded can be classified as an available device.
```bash
## show statistic of device with human readable format
## 显示有多少error, drop packets来看是不是网络由问题
## double -s will show error details
ip -s -s -h link
ip -s -h link show eth0
## bring up
ip link set eth0 up
## bring down
ip link set eth0 down
```

## route
This command helps you to see the route packets your network will take as set in your **kernel routing table**. The first entry is the default route.
Other commands perform the same: `route`, `netstat -r`
```bash
ip route
## route for a specific network
ip route list 9.30.204.0/22
## add a route to 192.168.121.0/24 via the gateway at 192.168.121.1
ip route add 192.168.121.0/24 via 192.168.121.1
## add a route to 192.168.121.0/24 that can be reached on device eth0.
ip route add 192.168.121.0/24 dev eth0
```

## neigh

## netns


]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux CPU Info</title>
    <url>/2019/11/14/linux-cpuinfo/</url>
    <content><![CDATA[
Check CPU information on Linux, just like check memory by watching `/proc/meminfo`
```bash
cat /proc/cpuinfo

## each processor has a dedicated description
processor       : 0
vendor_id       : GenuineIntel
cpu family      : 6
model           : 61
model name      : Intel Core Processor (Broadwell, IBRS)
stepping        : 2
microcode       : 0x1
cpu MHz         : 2199.996
cache size      : 4096 KB
physical id     : 0
siblings        : 1
core id         : 0
cpu cores       : 1
apicid          : 0
initial apicid  : 0
fpu             : yes
fpu_exception   : yes
cpuid level     : 13
wp              : yes
flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl eagerfpu pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt spec_ctrl
bogomips        : 4399.99
clflush size    : 64
cache_alignment : 64
address sizes   : 40 bits physical, 48 bits virtual
```

Count number of processing units
```bash
cat /proc/cpuinfo | grep processor | wc -l
```

To get actualy number of cores
```bash
cat /proc/cpuinfo | grep 'core id'
```

> Note: The number of processors shown by `/proc/cpuinfo` might not be the actual number of cores on the processor. For example a processor with 2 cores and hyperthreading would be reported as a processor with 4 cores.]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Zombie and Orphan Process</title>
    <url>/2019/06/05/linux-defunct-process/</url>
    <content><![CDATA[
这里记录了一下 orphan 和 zombie process 的区别和表现形式，以及在 bash 中怎么制造它们和处理 zombie.

Today after killing a running process, it didn't get removed but was marked as `<defunct>` (run `ps` can see it). What's this?

[Zombie vs Orphan process](https://en.wikipedia.org/wiki/Zombie_process)
On Unix and Unix-like computer operating systems, a `zombie process` or `defunct process` is a process that has completed execution but still has an entry in the `process table`. This entry is still needed to allow the parent process to read its child's exit status.

`Zombie processes` should not be confused with `orphan processes`: an orphan process is a process that is still **executing**, but whose parent has **died**. When the parent dies, the orphaned child process is adopted by **init** (process ID 1). When orphan processes die, they do not remain as zombie processes; instead, they are waited on by init. The result is that a process that is both a zombie and an orphan will be reaped automatically.

To create orphan in bash:
```bash
# this subshell spawns a new process sleep in background
# and print the child pid then die immediately
(sleep 10 & echo orphan pid $!)
# now the ppid is took over by init process
ps axo pid,ppid,comm | grep <orphan pid>
```
Notice that `&` is background sign, don't append `;` after it.

To make zombie in bash:
```bash
(sleep 1 & echo zombie pid $!; exec /bin/sleep 60) &
# check <defunct> mark
ps axo pid,ppid,comm | grep <zombie pid>
```
Analysis: in subshell we first spawn `sleep 1` on background and output the child pid, then we `exec` to replace the parent shell, so `sleep 1` will not be reaped by the original parent, neither of init process because its parent does not die, thus become a zombie. Eventually they both are reaped by init.

There is no harm in letting such processes be unless there are many of them. Processes that stay zombies for a long time are generally an error and cause a resource leak, but the only resource they occupy is the process table entry – process ID.

To check the maximum number of process can be created:
```bash
cat /proc/sys/kernel/pid_max
```

Usually zombie process will last a very short time, it will be reaped by its parent or init as mentioned above. But if not, need to reap them manually.

I checked the PPID of that `defunct` process, it's not PID 1 but other shell process. By killing its **parent** process, the zombie will be handed over to init and reaped. Just check the PPID:
```bash
ps axo pid,ppid | grep defunct
# or
ps -ef | grep defunct
```
You can also use this command to verify defunct process is gone.

> Notice you cannot kill zombie because by `kill -9` since it is already dead!

]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>process</tag>
      </tags>
  </entry>
  <entry>
    <title>cURL Use Case Summary</title>
    <url>/2019/05/23/linux-curl-summary/</url>
    <content><![CDATA[
`cURL` stands for `Client URL`, it is a command-line tool for getting or sending
data including files using URL syntax. Since `cURL` uses `libcurl`, it supports
a range of common network protocols, currently including HTTP, HTTPS, FTP, FTPS,
SCP, SFTP, TFTP, LDAP, DAP, DICT, TELNET, FILE, IMAP, POP3, SMTP and RTSP.

Here is a open-source book: [Everything curl](https://ec.haxx.se/)

About `curl` proxy environment variables(see man for more details):
- `http_proxy`(only be lower case) to specify http proxy.
- `https_proxy` to specify https proxy (curl to proxy over ssl/tls).
- `NO_PROXY` or `--noproxy '*'` to skip proxy use for all hosts, or a list for
some hosts.

When using `curl` download files, make sure it actually downloads it rather than
the HTML page, you can check the file type via `file` command, for example: 
```bash
curl -Ss <URL> -o xxx.tar.gz
# Check if it is a real tar gz file
file xxx.tar.gz | grep gzip
```

If the URL is wrong, `curl` will download the HTML page instead. `wget` will
show 404 error and fail.

# Date and Note
```
05/23/2019 download file
06/16/2019 http request verbose
06/22/2020 redirect, PUT/GET/DELETE
09/02/2020 check headers only
09/03/2020 use specific forward proxy
09/06/2020 resume download
09/07/2020 limit rate
09/08/2020 fetch headers only
09/09/2020 proxy tunnel
10/25/2022 name resolve tricks
04/07/2022 retry
04/01/2023 post request with json payload
04/03/2023 trace and see request body
```

## 05/23/2019
If the file server needs user name/password (usually will prompt when you open
in browser).
```bash
# -O: downloads the file and saves it with the same name as in the URL.
# -u: specify "user:password"
# -k: explicitly allows curl to perform 'insecure' SSL connections and transfers.
# -L: allow redirect
# -o: custom file name
# -s: slient all output
# -S: used with slient, output error message
USER="username"
PASSWD="passwd"
USER_PWD="$USER:$PASSWD"
STREAMURL="https://xxx/target.tar.gz"
# Downlaod file with user/password.
curl -k -u ${USER_PWD} -LO ${STREAMURL}

# Download file with custom name /tmp/archive.tgz and slient.
curl -sS <url> -o /tmp/archive.tgz
```

If you don't turn off the certificate check(`-k`), you will get error message
and fail:
```
curl: (60) Peer's Certificate has expired.
More details here: http://curl.haxx.se/docs/sslcerts.html

curl performs SSL certificate verification by default, using a "bundle"
 of Certificate Authority (CA) public keys (CA certs). If the default
 bundle file isn't adequate, you can specify an alternate file
 using the --cacert option.
If this HTTPS server uses a certificate signed by a CA represented in
 the bundle, the certificate verification probably failed due to a
 problem with the certificate (it might be expired, or the name might
 not match the domain name in the URL).
If you'd like to turn off curl's verification of the certificate, use
 the -k (or --insecure) option.
```

## 06/16/2019
When I was working on
[Docker Registry API](https://chengdol.github.io/2019/06/10/docker-registry-api/),
I primarily used `curl` to do the job.
```bash
# -v: Makes the fetching more verbose/talkative. Mostly useful for debugging. A
# line starting with `>` means `header data` sent by curl, `<` means `header
# data` received by curl that is hidden in normal cases, and a line starting
# with `*` means additional info provided by curl.

# -X: (HTTP) Specifies a custom request method to use when communicating with
# the HTTP server.
curl -v -k -XGET http://localhost:5000/v2/_catalog
```

## 06/22/2020
```bash
# -L: redirect
# -i: include response header info to output
curl -iL http://...

# -v: verbose, to see better error message
curl -v http://...

# -d|--data: value to be put
# -X, --request
curl -X PUT -d '50s' http://localhost:8500/v1/kv/prod/portal/haproxy/timeout-server
curl -X DELETE http://localhost:8500/v1/kv/prod/portal/haproxy/timeout-server
curl -X GET http://localhost:8500/v1/kv/prod/portal/haproxy/timeout-server?pretty
```

## 09/02/2020
Add additional headers, only show header info:
```bash
# -H,--header: add header
# -L: redirect
# -I: fetch headers only
# -v: verbose, to see better error message
# -s: hide progress bar, slient
# > /dev/null: hide output, show only the -v output
curl --header "Host: chengdol.github.io" \
--header "..." \
-L -Ivs http://185.199.110.153 > /dev/null
```

## 09/03/2020
Using HTTP proxy (forward) with proxy authentication, learned from Envoy.
```bash
# -x, --proxy: use the specific forward proxy
curl -v -x <proxy:port> http://www.example.com
# the same as
http_proxy=<proxy:port> curl -v -x http://www.example.com

# -U, --proxy-user: user/password for proxy itself.
# this option overrides the existing proxy environment variable 
curl -v -U <user:password> -x <proxy:port>  http://www.example.com
```
Default to use basic authentication scheme, Some proxies will require another
authentication scheme (and the headers that are returned when you get a `407`
response will tell you which).
```bash
## --proxy-anyauth: ask curl to use any method the proxy wants
curl -U <user:password> --proxy-anyauth -x myproxy:80 http://example.com 
```

## 09/06/2020
Resume the download:
```bash
curl <url> -o archive
# then break and resume
# -C -: automatrically resume from the break point
curl <url> -C - -o archive
```

## 09/07/2020
Download/upload speed limit if you have a limit bandwidth.
```bash
# limit rate as 1m/second, for example: 10k, 1g
curl <url> -O --limit-rate 1m
```

## 09/08/2020
Only fetch header information, no body:
```bash
# -I,--head: (HTTP FTP FILE) Fetch the headers only!
# notice that -i is to have response header to output, they are different
curl -I <url>
```

## 09/09/2020
Non-HTTP protocols over HTTP proxy
Most HTTP proxies allow clients to "tunnel through" it to a server on the other
side. That's exactly what's done every time you use HTTPS through the HTTP proxy.
```bash
# -p, --proxytunnel: make curl tunnel through the proxy
# used with -x, --proxy options
# here tunnel ftp protocol
curl -p -x http://proxy.example.com:80 ftp://ftp.example.com/file.txt
```

## 10/25/2020
The curl with 
[name resolve tricks](https://everything.curl.dev/usingcurl/connections/name)
and another [post](https://daniel.haxx.se/blog/2018/04/05/curl-another-host/)
about it.

This feature is primarily used for HTTP server development and testing the
server locally to mimic real world situations.

```bash
# First start a simple http server within python virtualenv:
python3 -m http.server
Serving HTTP on :: port 8000 (http://[::]:8000/) ...
```

```bash
# Override the default Host header otherwise the Host header value
# will be localhost:8000
# This is not enough for HTTPS server due to SNI, see next --resolve.
curl  -vI -H "Host: www.example.com:80" http://localhost:8000
# The header info:
> HEAD / HTTP/1.1
> Host: www.example.com:80
> User-Agent: curl/7.84.0
> Accept: */*

# www.myfakelist.com does not exist and dns it to 127.0.0.1
# port 8000 must be the same.
curl -vI --resolve www.myfakelist.com:8000:127.0.0.1 http://www.myfakelist.com:8000
# The header info:
> GET / HTTP/1.1
> Host: www.myfakelist.com:8000
> User-Agent: curl/7.84.0
> Accept: */*
# works for HTTPS server as well
curl -kvI --resolve www.myfakelist.com:443:127.0.0.1 https://www.myfakelist.com

# www.myfakelist.com does not exist and map both the name:port
# to localhost:8000 points to the fake python http server.
curl -vI --connect-to www.myfakelist.com:80:localhost:8000 http://www.myfakelist.com
# The header info:
> GET / HTTP/1.1
> Host: www.myfakelist.com
> User-Agent: curl/7.84.0
> Accept: */*
# works for HTTPS server as well
curl -kv --connect-to www.myfakelist.com:443:127.0.0.1:443 https://www.myfakelist.com
```


## 04/07/2022
Retry until reaching condition limits:
```bash
# --retry: worked on a timeout, an FTP 4xx response code or an HTTP 408, 429,
# 500, 502, 503 or 504 response code.
# --retry-all-errors: retry on any errors. If you want to retry on all response
# codes that indicate HTTP errors (4xx and 5xx) then combine with `-f`, `--fail`.
# --retry-delay: if not set, retries it will double the waiting time until it
# reaches 10 minutes.
# --retry-max-time: given total time limit for retries.
# --retry-connrefused: connect refused retry.
# -m, --max-time: timtout for whole operation.
# --connect-timeout: timeout for curl's connection to take.
curl -kI \
--max-time 3.55 \
--connect-timeout 2.12 \
--retry-all-errors \
--retry 3 \
--retry-delay 1 \
--retry-max-time 10 \
https://no-op

# output
curl: (6) Could not resolve host: no-op
Warning: Problem : timeout. Will retry in 2 seconds. 3 retries left.
curl: (6) Could not resolve host: no-op
Warning: Problem : timeout. Will retry in 2 seconds. 2 retries left.
curl: (6) Could not resolve host: no-op
Warning: Problem : timeout. Will retry in 2 seconds. 1 retries left.
curl: (6) Could not resolve host: no-op
```

## 04/01/2023
For example, the POST request is like:
```bash
curl -vs -X POST \
--http1.1 \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $TOKEN" \
-d @payload.json \
https://xxx.com/v1/do-something
```

Regardless of the format in `payload.json`, `curl` will always remove the line
returns and one-line the JSON content and send it, which is different from
Postman who will send payload with line returns (which could be identified as
security issue by firewall rules).

## 04/03/2023
To check the request payload sent, use `--trace-ascii`:
```bash
curl -vs -X POST \
--http1.1 \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $TOKEN" \
-d @payload.json \
https://xxx.com/v1/do-something \
--trace-ascii /dev/stdout
```
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>curl</tag>
      </tags>
  </entry>
  <entry>
    <title>GNU Make</title>
    <url>/2022/02/17/linux-makefile/</url>
    <content><![CDATA[
[Makefile tutorial](https://makefiletutorial.com/)
[GNU Make website](https://www.gnu.org/software/make/)

Make function used:
[shell](https://www.gnu.org/software/make/manual/html_node/Shell-Function.html)

We are using Makefile for each repo to help:

1. Download/sync dependencies.
2. Run unit tests.
3. Build binary.
4. Package binary and build docker image.
5. Push tagged docker image to repository.

Thus the Makefile can be easily integrated into CICD pipeline or run manually
for regular development, for example:

```bash
# download dependencies
make vendor

# run unit tests
make test

# build, package and publish are 3 phonies defined in Makefile
make build package publish serivce=example version=chengdol-example-0.0.1
```
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>make</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Kernel Parameters</title>
    <url>/2019/05/01/linux-kernel-param/</url>
    <content><![CDATA[
## Prequel
Recently I was dealing with Linux kernel parameters, which are new to me and in my case they are the key to performance of the Database (DB2). 

In DB2 [Kernel parameter requirements (Linux)](https://www.ibm.com/support/knowledgecenter/en/SSEPGG_11.1.0/com.ibm.db2.luw.qb.server.doc/doc/c0057140.html), the database manager uses a formula to automatically tune kernel parameter settings and eliminate the need for manual updates to these settings.

So the DB2 relies on good kernel parameters used for IPC to perform well.

When instances are started, if an interprocess communication (IPC) kernel parameter is below the enforced minimum value, the database manager updates it to the enforced minimum value.

There are severl Linux IPC kernel parameters need to be adjusted for DB2:
```js
kernel.shmmni (SHMMNI)
kernel.shmmax (SHMMAX)
kernel.shmall (SHMALL)
kernel.sem (SEMMNI)
kernel.sem (SEMMSL)
kernel.sem (SEMMNS)
kernel.sem (SEMOPM)
kernel.msgmni (MSGMNI)
kernel.msgmax (MSGMAX)
kernel.msgmnb (MSGMNB)
```

By the time the case was we had to run DB2 as root in container(becuase it would tune kernel parameters), so to minimize the root user privilege we decide to remove some of Linux capibilities: `SYS_RESOURCE` and `SYS_ADMIN`, but these removed caps may impact the kernel parameters tuning, so we ran test suite to expose failures and errors on xmeta pods. 后来想想，当时应该查看一下 DB2 进程的 cap 用到了哪些, 比如用`pscap` or `getpcaps` command.

For example, if you check `SYS_RESOURCE` [manual](http://man7.org/linux/man-pages/man7/capabilities.7.html), you can see:
```js
CAP_SYS_RESOURCE

  * raise msg_qbytes limit for a System V message queue above
    the limit in /proc/sys/kernel/msgmnb (see msgop(2) and
    msgctl(2));
  * use F_SETPIPE_SZ to increase the capacity of a pipe above
    the limit specified by /proc/sys/fs/pipe-max-size;
  * override /proc/sys/fs/mqueue/queues_max limit when creating
    POSIX message queues (see mq_overview(7));
```
Without granting `SYS_RESOURCE`, `msgmnb` (maybe also other kernel parameters) cannot be changed properly (actually I doubt this after checking having and not having `SYS_RESOURCE` result).

## About IPC
Let's first understand what is IPC? 

[IPC Mechanisms](http://en.tldp.org/LDP/tlk/ipc/ipc.html)
[IPC Mechanisms on Linux - Introduction](http://www.chandrashekar.info/articles/linux-system-programming/introduction-to-linux-ipc-mechanims.html)

This post seems on dying, forwards it here (after I go through it, I can still remember some tech words from 402 Operating System, but I forget the detail).

`Inter-Process-Communication` (or IPC for short) are mechanisms provided by the kernel to allow processes to communicate with each other. On modern systems, IPCs form the web that bind together each process within a large scale software architecture.

The Linux kernel provides the following IPC mechanisms:

1. Signals
2. Anonymous Pipes
3. Named Pipes or FIFOs
4. SysV Message Queues
5. POSIX Message Queues
6. SysV Shared memory
7. POSIX Shared memory
8. SysV semaphores
9. POSIX semaphores
10. FUTEX locks
11. File-backed and anonymous shared memory using mmap
12. UNIX Domain Sockets
13. Netlink Sockets
14. Network Sockets
15. Inotify mechanisms
16. FUSE subsystem
17. D-Bus subsystem

While the above list seems quite a lot, each IPC mechanism from the list describe above, is tailored to work better for a particular use-case scenario.

* SIGNALS
  Signals are the cheapest forms of IPC provided by Linux. Their primary use is to notify processes of change in states or events that occur within the kernel or other processes. We use signals in real world to convey messages with least overhead - think of hand and body gestures. For example, in a crowded gathering, we raise a hand to gain attention, wave hand at a friend to greet and so on.

  On Linux, the kernel notifies a process when an event or state change occurs by interrupting the process's normal flow of execution and invoking one of the signal handler functinos registered by the process or by the invoking one of the default signal dispositions supplied by the kernel, for the said event.

* ANONYMOUS PIPES
  Anonymous pipes (or simply pipes, for short) provide a mechanism for one process to stream data to another. A pipe has two ends associated with a pair of file descriptors - making it a one-to-one messaging or communication mechanism. One end of the pipe is the read-end which is associated with a file-descriptor that can only be read, and the other end is the write-end which is associated with a file descriptor that can only be written. This design means that pipes are essentially half-duplex.

  Anonymous pipes can be setup and used only between processes that share parent-child relationship. Generally the parent process creates a pipe and then forks child processes. Each child process gets access to the pipe created by the parent process via the file descriptors that get duplicated into their address space. This allows the parent to communicate with its children, or the children to communicate with each other using the shared pipe.

  Pipes are generally used to implement Producer-Consumer design amongst processes - where one or more processes would produce data and stream them on one end of the pipe, while other processes would consume the data stream from the other end of the pipe.

* NAMED PIPES OR FIFO
  Named pipes (or FIFO) are variants of pipe that allow communication between processes that are not related to each other. The processes communicate using named pipes by opening a special file known as a FIFO file. One process opens the FIFO file from writing while the other process opens the same file for reading. Thus any data written by the former process gets streamed through a pipe to the latter process. The FIFO file on disk acts as the contract between the two processes that wish to communicate.

* MESSAGE QUEUES
  Message Queues are synonymous to mailboxes. One process writes a message packet on the message queue and exits. Another process can access the message packet from the same message queue at a latter point in time. The advantage of message queues over pipes/FIFOs are that the sender (or writer) processes do not have to wait for the receiver (or reader) processes to connect. Think of communication using pipes as similar to two people communicating over phone, while message queues are similar to two people communicating using mail or other messaging services.

  There are two standard specifications for message queues.

  * SysV message queues.
  The AT&T SysV message queues support message channeling. Each message packet sent by senders carry a message number. The receivers can either choose to receive message that match a particular message number, or receive all other messages excluding a particular message number or all messages.

  * POSIX message queues.
  The POSIX message queues support message priorities. Each message packet sent by the senders carry a priority number along with the message payload. The messages get ordered based on the priority number in the message queue. When the receiver tries to read a message at a later point in time, the messages with higher priority numbers get delivered first. POSIX message queues also support asynchronous message delivery using threads or signal based notification.

  Linux support both of the above standards for message queues.

* SHARED MEMORY
  As the name implies, this IPC mechanism allows one process to share a region of memory in its address space with another. This allows two or more processes to communicate data more efficiently amongst themselves with minimal kernel intervention.

  There are two standard specifications for Shared memory.

  * SysV Shared memory. Many applications even today use this mechanism for historical reasons. It follows some of the artifacts of SysV IPC semantics.

  * POSIX Shared memory. The POSIX specifications provide a more elegant approach towards implementing shared memory interface. On Linux, POSIX Shared memory is actually implemented by using files backed by RAM-based filesystem. I recommend using this mechanism over the SysV semantics due to a more elegant file based semantics.

* SEMAPHORES
  Semaphores are locking and synchronization mechanism used most widely when processes share resources. Linux supports both SysV semaphores and POSIX semaphores. POSIX semaphores provide a more simpler and elegant implementation and thus is most widely used when compared to SysV semaphores on Linux.

* FUTEXES
  Futexes are high-performance low-overhead locking mechanisms provided by the kernel. Direct use of futexes is highly discouraged in system programs. Futexes are used internally by POSIX threading API for condition variables and its mutex implementations.

* UNIX DOMAIN SOCKETS
  UNIX Domain Sockets provide a mechanism for implementing applications that communicate using the Client-Server architecture. They support both stream and datagram oriented communication, are full-duplex and support a variety of options. They are very widely used for developing many large-scale frameworks.

* NETLINK SOCKETS
  Netlink sockets are similar to UNIX Domain Sockets in its API semantics - but used mainly for two purposes:

  For communication between a process in user-space to a thread in kernel-space
  For communication amongst processes in user-space using broadcast mode.

* NETWORK SOCKETS
  Based on the same API semantics like UNIX Domain Sockets, Network Sockets API provide mechanisms for communication between processes that run on different hosts on a network. Linux has rich support for features and various protocol stacks for using network sockets API. For all kinds of network programming and distributed programming - network socket APIs form the core interface.


* INOTIFY MECHANISMS
  The Inotify API on Linux provides a method for processes to know of any changes on a monitored file or a directory asynchronously. By adding a file to inotify watch-list, a process will be notified by the kernel on any changes to the file like open, read, write, changes to file stat, deleting a file and so on.

* FUSE SUBSYSTEM
  FUSE provides a method to implement a fully functional filesystem in user-space. Various operations on the mounted FUSE filesystem would trigger functions registered by the user-space filesystem handler process. This technique can also be used as an IPC mechanism to implement Client-Server architecture without using socket API semantics.

* D-BUS SUBSYSTEM
  D-Bus is a high-level IPC mechanism built generally on top of socket API that provides a mechanism for multiple processes to communicate with each other using various messaging patterns. D-Bus is a standards specification for processes communicating with each other and very widely used today by GUI implementations on Linux following Freedesktop.org specifications.

## IPC Related Commands
you can use `ipcs` command to show IPC facilities information: shared memory segments, message queues, and semaphore arrays.

```js
ipcs -l

------ Shared Memory Limits --------
max number of segments = 4096               // SHMMNI	
max seg size (kbytes) = 32768               // SHMMAX
max total shared memory (kbytes) = 8388608  // SHMALL
min seg size (bytes) = 1

------ Semaphore Limits --------
max number of arrays = 1024                 // SEMMNI
max semaphores per array = 250              // SEMMSL
max semaphores system wide = 256000         // SEMMNS
max ops per semop call = 32                 // SEMOPM
semaphore max value = 32767

------ Messages: Limits --------
max queues system wide = 1024               // MSGMNI
max size of message (bytes) = 65536         // MSGMAX
default max size of queue (bytes) = 65536   // MSGMNB
```

Also, you can use `sysctl` command to view kernel parameters:
```ini
#sysctl -a | grep -i shmmni
kernel.shmmni = 4096
```
or
```ini
#sysctl kernel.shmmni
kernel.shmmni = 4096
```

## Modify Kernel Parameters
From this post [Db2 Modify Kernel Parameters](https://www.ibm.com/support/knowledgecenter/en/SSEPGG_11.1.0/com.ibm.db2.luw.qb.server.doc/doc/t0008238.html).

Modify the kernel parameters that you have to adjust by editing the `/etc/sysctl.conf` file. If this file does not exist, create it. The following lines are examples of what must be placed into the file:
```yaml
kernel.shmmni=4096
kernel.shmmax=17179869184
kernel.shmall=8388608
#kernel.sem=<SEMMSL> <SEMMNS> <SEMOPM> <SEMMNI>
kernel.sem=4096 1024000 250 4096
kernel.msgmni=16384
kernel.msgmax=65536
kernel.msgmnb=65536
```
Reload settings from the default file `/etc/sysctl.conf`:
```bash
sysctl -p
```
For RedHat The rc.sysinit initialization script reads the `/etc/sysctl.conf` file automatically after each reboot.

You can also make the change inpermanent, for example:
```yaml
sysctl -w kernel.shmmni=4096
sysctl -w kernel.sem="4096 1024000 250 4096"
```
Or directly wirte it to `procfs` file:
```bash
echo "4096 1024000 250 4096" > /proc/sys/kernel/sem
```
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Multiplexer with VNC, Screen and Tmux</title>
    <url>/2019/05/29/linux-multiplexer/</url>
    <content><![CDATA[
Overall, tmux is the best among all selections here, but you should know how do
others work in case tmux is unavailable. 你还需要理解VNC, Screen, tmux 的原理是什么?
为什么使用它们则SSH 断开之后进程仍可以继续运行呢? 哪里在收集和记录状态。

# Run in Background
The most straightforward way is put long running task in background, redirect
the stdout/stderr to files or `/dev/null`, for example:
```bash
./task.sh &> log.$(date "+%Y-%m-%d") &
# then working on it by jobs, bg, fg and ctrl+z commands
```
Note that ctrl+z also works when editting Vim file, it will suspend editing, put
it in background and bring you back to terminal.

To detach a job from the current `jobs` list, using `disown %<job id>` command.
The detched job will still be running but cannot bring it back to `jobs`.

# Nohup
Why the work gets lost when SSH session drops is because the signal SIGHUP will
be sent to the foreground process. If we let the process ignore this signal, then
it can keep running when remote SSH connection is gone.
```bash
# nohup will redirect the stdin from /dev/null
# stdout/err to FILE $HOME/nohup.out
nohup [bash|sh] <script> &
# similar to
# &> is the modern version of 2>&1
<script> </dev/null  &>/dev/null  &
```
You can use `jobs`, `bg` and `fg` command to operate it.

# VNC
之前用VNC的目的之一是为了keep SSH long running task on remote machine , prevent lost
of work, the alternative is `screen` and `tmux`command(best).

`Virtual Network Computing (VNC)` is a graphical desktop-sharing system that
uses the Remote Frame Buffer protocol (RFB) to remotely control another computer.
It transmits the keyboard and mouse events from one computer to another, relaying
the graphical-screen updates back in the other direction, over a network.

Popular uses for this technology include remote technical support and accessing
files on one's work computer from one's home computer, or vice versa.

I usually use it to do remote development on Fyre, I have a Fyre central control
machine with VNC installed, after vnc to that machine I use it to SSH to other
machines within the same internal network(usually connections will not break),
you can also install IDE in VNC to do general programming work rather than
developing locally.

> Note, there are other remote terminal tools such as `Termius`. 

## Install VNC Server
The settings are varies on different Linux distros.
```bash
yum update -y

# seems no need this:
yum install open-vm-tools

# if you don’t have desktop installed, note linux has many 
# different desktop system themes, only install core packages 
# of KDE or GNOME, KDE is better
yum groupinstall 'X Window System' 'KDE'
yum groupinstall 'X Window System' 'GNOME'

yum list | grep tiger
yum -y install tigervnc-server 

# Fyre firewalld default is inactive, no need to deal with firewall
# if it’s enable, may need to config
systemctl status firewalld
firewall-cmd --permanent --zone=public --add-service vnc-server 
firewall-cmd --reload

# start vnc
# select yes then create password: 123456
# first time it will open port 1
# then you will get the address and port to login:
# centctl1.fyre.ibm.com:1
vncserver 
```
Note you will get new vnc session everytime you run `vncserver`, check with
```bash
ps aux | grep vnc

root      2682  0.0  3.0 284860 116840 ?       Sl   May27   1:38 /usr/bin/Xvnc :1 -auth /root/.Xauthority -desktop mycentctl1.fyre.ibm.com:1 (root) -fp catalogue:/etc/X11/fontpath.d -geometry 1024x768 -pn -rfbauth /root/.vnc/passwd -rfbport 5901 -rfbwait 30000
root      9199  1.4  1.6 231660 62388 pts/3    Sl   08:59   0:00 /usr/bin/Xvnc :2 -auth /root/.Xauthority -desktop mycentctl1.fyre.ibm.com:2 (root) -fp catalogue:/etc/X11/fontpath.d -geometry 1024x768 -pn -rfbauth /root/.vnc/passwd -rfbport 5902 -rfbwait 30000
```
you can kill it by running:
```bash
vncserver -kill :2
```
If the VNC is broken due to Fyre maintenance, open a new vnc session again by
`vncserver`.

## Install VNC viewer
Install VNC viewer on your local laptop, then connect by for example:
```bash
centctl1.fyre.ibm.com:1
```

## Copy and Paste
If you want to copy from local to viewer, sometimes it's malfunction, kill the
`klipper` process:
```bash
ps aux | grep klipper
kill -9 <PID of klipper>
```
You can adjust shortcuts in VNC viewer for copy and paste:
Settings -> Configure Shortcuts -> copy / paste

## Other Settings
Other settings useful:
Settings -> Edit Current Profile -> Mouse -> copy on select / Trim trailing space

Adjust font size the themes:
Settings -> Manage Profiles -> Edit Profile -> Appearabce -> Black on Random Light
-> check Vary the background color for each tab

Also change the text size under Appearance.

## Screen resolution
If the screen open by VNC viewer is small, you can change the resolution, run:
```bash
xrandr -s 1920x1080
```
on the terminal in your remote machine.


# Screen
Create virtual terminal lives beyond your terminal session.

[How To Use Linux Screen](https://linuxize.com/post/how-to-use-linux-screen/)
Screen or GNU Screen is a terminal multiplexer. In other words, it means that
you can start a screen session and then open any number of windows (virtual
terminals) inside that session. Processes running in Screen will continue to run
when their window is not visible even if you get disconnected, the work or
progress will not get lost! 所以说可以不用`&` 后台运行了。

[screen command cheat sheet](https://gist.github.com/jctosta/af918e1618682638aa82),
or see help in screen:  `crtl+a ?`, hit enter to quit. you can use either `screen`
or `ctrl+a`(key binding).

Note that install and run `screen` on target machine, VNC 应该是同样的.
```bash
apt update
apt install -y screen
# or
yum install -y screen

# start a screen session with description
# and enter into it
screen -S remote_playbook
```

You can create multiple windows in one session
```
Ctrl+a c         Create a new window (with shell)
Ctrl+a p         Go back to previous screen
Ctrl+a n         Go to next screen
Ctrl+a Ctrl+a    Toggle between the current and previous region or screen

Ctrl+a A         Rename the current window

Ctrl+a "         List all window
Ctrl+a 0/1/2/3   Switch to window 0 (by number )


Ctrl+a S         Split current region horizontally into two regions
Ctrl+a |         Split current region vertically into two regions
Ctrl+a tab       Switch the input focus to the next region
Ctrl+a Q         Close all regions but the current one
Ctrl+a X         Close the current region

Ctrl+a k         Kill current window or `exit`
Ctrl+a \         Kill all windows
```

```bash
# list running sessions
screen -ls

# detach session
screen -d [session name]
# detach current session
Ctrl+a d

# attach session
screen -r [session name]
# attach to a running session
screen -x

# detach and logout all sessions
# will not terminate session
Ctrl+a D D

# quit and terminate current session
Ctrl+a :quit
# or kill all window on sessions
Ctrl+a k
```

How do I know I am in screen and which session:
```bash
# show session id if you are in screen
# for example: 3384204.screen-es-upgrade
echo $STY 

# return with screen prefix if you are in screen
# for example: screen.xterm-256color
echo $TERM

# press ctrl+a with t
# otherwise nothing will show
# for example: 22:26:35 Mar 04 chengdolcent 0.36 0.28 0.2
```

# Tmux
Tmux 相比Screen 有terminal底部的提示，不容易和host terminal搞混.

Note that tmux has 3 terms(same as Screen): session -> window -> pane. you can
have multiple sessions, each session can have mutliple windows and each window
can have several panes. Usually rename the session and window with meanful
symbol.

```bash
# version
tmux -V
# start a session, notice the bottom line for window status
tmux
tmux new -s <session name>

# rename session, by default no meanful name
Ctrl+b $

# list tmux sessions
tmux ls

# switch list sessions
# use arrow key to expand each session and select
Ctrl+b s

# detch tmux session
Ctrl+b d
# attch again
tmux [attach|a] -t <session name>
# attach most recent one
tmux [a]ttach|a]

# kill all sessions but current
tmux kill-session -a
# kill all sessions but my session
tmux kill-session -t <my session>
```
In the bottom line, `*` means current window, you will see the windows list and
name here.

Some key bindings for window:
```bash
Ctrl+b ?         help, press `q` to quit

Ctrl+b c         create new window
Ctrl+b ,         rename window
Ctrl+b w         list windows

Ctrl+b <id>      switch to other window 
Ctrl+b n         go to next window
Ctrl+b p         go to previous window

Ctrl+b &         kill and close the window

Ctrl+b t         clock display, press any key goes back
```

Add panes to windows, active one has the green color.
```bash
Ctrl+b "         horizontal split
Ctrl+b %         vertical split
Ctrl+b o         switch pane
Ctrl+b arrow     switch pane by arrow key
Ctrl+b ;         switch between most recently panes
Ctrl+b x         close pane
```

You can also adjust the pane size or even promote the pane to a its own window.
```bash
Ctrl+b [esc + arrow]    连续多次esc + arrow resize the pane
Ctrl+b !                promote window
```

Tmux also has command mode to accomplish the same task by key bindings, similar
to Vim. 比如说monitor-activity设置，在有输出变化时会提示高亮. You can pre-set tmux
config: `.tmux.conf`, there are a lot examples on Internet.
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>vnc</tag>
        <tag>screen</tag>
        <tag>tmux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Network Interface</title>
    <url>/2019/11/04/linux-network-interface/</url>
    <content><![CDATA[
Sometimes I need to know IP of specified network interface, for example `eth0`.

There are several ways to do it:

* `ifconfig` command, but you need to yum install `net-tools.x86_64` if it does
not present:
```bash
ifconfig eth0 | grep "inet" | awk '{print $2}'
```

2. `ip` command, this command is pre-installed in most Linux distros.
```bash
ip addr show eth0 | grep "inet\b" | awk '{print $2}' | cut -d/ -f1
```
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>jq/yq Cheat Sheet</title>
    <url>/2021/11/12/linux-jq/</url>
    <content><![CDATA[
Summary of some `jq` use cases and caveats daily.
Quick reference: 
- [cheat sheet](https://lzone.de/cheat-sheet/jq)

Need to read through some common use cases:
- [official tutorial](https://stedolan.github.io/jq/tutorial/)
- [official manual](https://stedolan.github.io/jq/manual/)

[`yq`](https://github.com/mikefarah/yq) is another open source tool relies on `jq` to query yaml file, tt has most common operations available and syntactically like `jq`.


1. Must quote the expression 
For example:
```bash
# -r: raw string
k get pod banzai-vault-0 -o json | jq -r '.spec.containers[].name'
```

2. Filter out null value
For example, I want to get the configMap name from a deployment, the volumes may have multiple subitems and one of them is config map, we need to filter out `null`:
```bash
kubectl get deploy nwiaas -o json | jq -r '.spec.template.spec.volumes[].configMap.name | select(. != null)'
```
This is a simple case to introduce [`select` function](https://stedolan.github.io/jq/manual/#select(boolean_expression))


2. Replace field values
Using `|=` operator, see [here](https://stedolan.github.io/jq/manual/#Assignment).

For example, I want to create one job from cronjob but with different args:
```bash
# --dry-run=client: print without sending object
kubectl create job \
--from=cronjob/<cronjob name> \
<job name> \
--dry-run=client \
--output json > original.json

# use jq docker
docker run -it --rm \
-v $(pwd)/original.json:/original.json \
--entrypoint=/bin/bash \
stedolan/jq \
-c "cat /original.json |jq -r '.spec.template.spec.containers[].args |= [\"python\", \"make.py\", \"do_something\"]' | cat" \
| kubectl apply -f -
```

Here I use `cat` at end to remove color supplement, otherwise kubectl apply will fail.

3. Loop and extract specified values
For example, the Elasticsearch API returns a list of map object and each map has the same number of fields, I want to extract few of the fields and output with specific format.

```bash
# [.index,.shard,.prirep,.node]: generate [] array format and put items in
curl -s "http://localhost:9200/_cat/shards/.ds-*?h=index,shard,prirep,state,node&format=json" | \
jq -r '.[] | [.index,.shard,.prirep,.node] | @csv' | sed 's/\"//g'
```
It will output [`csv`](https://stedolan.github.io/jq/manual/v1.6/#Basicfilters) style string, for example:
```bash
"xxx","yyy","zzz","qqq"
```

Or format as a table:
```bash
# [.index,.shard,.prirep,.node]: generate an array
# join(":|:"): join array elements
# column -s : -t: use `:` as separator and make table
curl -s "http://localhost:9200/_cat/shards/.ds-*?h=index,shard,prirep,state,node&format=json" | \
jq -r '.[] | [.index, .shard, .prirep, .node] | join(":|:")' | column -s : -t | sort
```
The output is like:
```bash
.ds-k8s-nodes-filebeat-2022.04.28-000020  |  0  |  p  |  172.16.0.172
.ds-k8s-nodes-filebeat-2022.04.28-000020  |  1  |  p  |  172.16.0.164
.ds-k8s-nodes-filebeat-2022.04.28-000020  |  2  |  p  |  172.16.0.247
```


]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>jq</tag>
        <tag>yq</tag>
      </tags>
  </entry>
  <entry>
    <title>nc/netcat Command</title>
    <url>/2020/10/25/linux-netcat-nc-summary/</url>
    <content><![CDATA[
I almost forget this command, but recently I used it as a TCP client to test Envoy TCP proxy. `nc` can also be a TCP server that listening on port and waiting for connection.

# Install
```bash
# install nc (netcat)
# on centos
yum install -y nc
# on ubuntu
apt install -y netcat
```
> Note that some nc version may support different features and options, please read man first!!

# Usage Example
Test networking between 2 machines is good:
```bash
# on one machine, set up listener
# default is tcp
# -l: listen
# -k: keep connection
nc -lk 1555

# on another machine
# talk to listener
echo "from client" | nc <ip> 1555
```
I used to set up a UDP client to test logstash input UDP plugin and pipeline.

# Proxy
Connection via proxy, see `man nc`:
```bash
# https connect
nc -x10.2.3.4:8080 -Xconnect host.example.com 42

# proxy authentication
# -P: proxy user
nc -x10.2.3.4:8080 -Xconnect -Pruser host.example.com 42
```

# Port Scan
Port scanning to know which ports are open and running services on target machine:
```bash
# -v: verbose
# -z: Zero-I/O mode, report connection status only
# -w: timeout second

# scan port 22 and 8080
nc -v -w 2 -z 127.0.0.1 22 8080

# range scan
nc -v -w 2 -z 127.0.0.1 1-10004

# -n: don't perform DNS resolution
nc -v -w 2 -n -z 8.8.8.8 53
# Connection to 8.8.8.8 53 port [tcp/domain] succeeded
```

# Transfer
Data transfer, also see `man nc`
```bash
# content will be put to filename.out
nc -l 1234 > filename.out

# feed it with filename.in
# -N: disconnet when finish
nc -N host.example.com 1234 < filename.in
```

For folder transfer:
```bash
# note there is a - after tar command, used as input
# after done you will see the folder
nc -v -l 1234 | tar zxf -

# - here is used as output
# -N: close connection when is done
tar czf - folder | nc -N -v <ip> 1234
```
Other ways to transfer files: scp, sftp, python http server.

# Server Client
Client/Server model, a chat server, can talk in either way:

TCP server and client
```bash
# server 
# -l: listening
# -vv: verbose
# -p: listening on port 3000
nc -lk -vv -p 3000
# client
# -p: use port 6666 to connect to 3000
nc localhost -p 6666 3000
```

UDP server and client
```bash
# server
nc -u -lk localhost 515
# client
nc -u localhost 515
```

Actually you can use it in script:
```bash
#!/bin/bash
# it well block until get the message
message=$(nc -l -p 1234)

# in another script, interesting
echo hi > /dev/tcp/localhost/1234
```

# Backdoor
Execute command on remote via backdoor opened by `nc`, see nc's manual
```bash
# server side, mk a named pipe
rm -f /tmp/f; mkfifo /tmp/f
# -i: interactive shell
# cat pipe's content sent by client to interactive shell
# then redirect the output to pipe to show it on client side
cat /tmp/f | /bin/sh -i 2>&1 | nc -l 0.0.0.0 1234 > /tmp/f
# remove after done
rm -f /tmp/f

# client side
nc <server ip> 1234
# prompt a interactive shell 
# then run command on remote server
```]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>nc</tag>
        <tag>netcat</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Network Timeout</title>
    <url>/2022/01/15/linux-network-timeout/</url>
    <content><![CDATA[
Today is a frustrating day and I am scheduled for oncall. There is one related to network that the requests from one side intermittently timeout, for example:
```bash
failed to create dial connection with read/write 10s timeout: dial tcp: i/o timeout
```

It turns out the root cause is network packet loss. GCP has network performance dashboard to help you monitor and spot packet loss and latency statistics, very helpful.

I am also educated by this [blog](https://www.alibabacloud.com/blog/how-to-get-to-the-bottom-of-network-timeout-issues_595248), it is worth to read through.


]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Network Port Watching</title>
    <url>/2019/11/04/linux-network-port-watching/</url>
    <content><![CDATA[
This issue is from a machine without `net-tools.x86_64 : Basic networking tools` pre-installed, so `netstat` command does not exist. When watching docker registry pod setup, ansible runs `netstat -tunlp | grep 5000` to check 5000 port, failed.

Is there other way around to check if the 5000 port is up or not? Yes.
```yaml
- name: Wait for docker registry to come up
  any_errors_fatal: true
  shell: |
    declare -a array=($(cat /proc/net/tcp6 | cut -d":" -f"3"|cut -d" " -f"1"))
    for port in ${array[@]};
    do
      # $((0x$port)) is 16 based, will output 10 based
      val=$(echo $((0x$port)) | grep 5000)
      if ! [[ "X${val}" == "X" ]]; then
        break
      fi
    done
    echo ${val} | grep 5000
  register: docker_registry
  until: docker_registry.rc == 0
  retries: "{{ 10 }}"
```

Here I watch `/proc/net/tcp6` kernel file to see what ipv6 port is running (docker registry port is in ipv6 scope here). for ipv4, use `/proc/net/tcp`. 

This file is not plain text, after use `cut` to extra the port field, then convert the number to decimal. see https://www.kernel.org/doc/Documentation/networking/proc_net_tcp.txt










]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux getfacl and setfacl Commands</title>
    <url>/2019/09/25/linux-getfacl-setfacl/</url>
    <content><![CDATA[
Today I learn a new method to operate on permission of files and directories, usually I use `chmod` and `chown`.

One thing you need to be clear is if for example `/etc` is owned by root, and `/etc/xxx` is owned by `demo` (non-root) user, `demo` cannot remove `/etc/xxx` because of permission deny, but `demo` can create soft link from `/etc/xxx` and do all other operations inside `/etc/xxx`.

What if `demo` want to remove `/etc/xxx` without changing permissiond of `/etc` by `chmod` or `chown` and without `sudo`? `setfacl` is a good choice.

> Note that docker will not allow commit the change of any permission of `/` directory into image.

Each file and directory in a Linux filesystem is created with `Access Control Lists (ACLs)`. The permissions can be set using the `setfacl` utility. In order to know the access permissions of a file or directory we use `getfacl`.

For example:
```
# getfacl /etc

getfacl: Removing leading '/' from absolute path names
# file: etc/
# owner: root
# group: root
user::rwx
group::r-x
other::r-x
```

then we add `demo` full permission to `/etc`
```bash
## run as root
setfacl -m u:demo:rwx /etc
```
check again:
```
# getfacl /etc

getfacl: Removing leading '/' from absolute path names
# file: etc
# owner: root
# group: root
user::rwx
user:demo:rwx
group::r-x
mask::rwx
other::r-x
```

I have this question:
[Difference between chmod vs ACL](https://unix.stackexchange.com/questions/364517/difference-between-chmod-vs-acl)

Under Linux, `ls -l` puts a `+` at the end of the permissions characters to indicate that `ACL` are present. If `ACL` are presenting then the basic permissions do not tell the full story: ACL override POSIX permissions:
```
# ls -l /etc

drwxrwxr-x+ 89 root root 8192 Sep 25 16:24
```
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Offline Package Installation I</title>
    <url>/2019/02/25/linux-offline-package-install-1/</url>
    <content><![CDATA[
When I was working on the upgrade DS k8s installer issue, I ran into the problem that I need to install `ansible`, `docker` and `kubeadm` offline. In the production environment, we may not have internet access, that means we need to prepare the rpms and dependencies needed and create a self-contained installer.

### Download missing rpms without installing

> Note: This method is (by-design) sensitive to the existence of already-installed packages. It will **only** download missing dependencies you need for that particular box, not all rpms.

First let's install the `yum-plugin-downloadonly`:
```bash
yum install -y yum-plugin-downloadonly
```
```
yum install --downloadonly --downloaddir=<directory> <package:version>
```
For example, I want to get missing rpms for *vim* editor, reside them in `/root/vim` folder
```bash
mkdir -p /root/vim
yum install --downloadonly --downloaddir=/root/vim vim
```
![](https://drive.google.com/uc?id=1l5sohiuz020QXowIfKddAtIlUyjJWmFl)

List the target folder:

![](https://drive.google.com/uc?id=1o9fPX-boaTBYl7r6y2uzDFePAsLa9G9B)

Another way is using `yumdownloader` that is from `yum-utils`. The difference is if the package is already installed completely, `yumdownloader` will download the outermost level rpm but `--downloadonly` will do nothing.
```bash
yum install -y yum-utils
```
```bash
yumdownloader --resolve --destdir=/root/vim vim
```
![](https://drive.google.com/uc?id=1I_QfnLAWpj-CNKnfZAvF4yKjZ6U8xHET)

### Download all rpms without installing
#### yum & yumdownloader
Usually what we really want is to resolve all dependencies and download them, even though some required rpms have already installed in box, `yumdownloader` or `yum --downloadonly` with `--installroot` option is the solution.

Keep in mind that `yumdownloader` will use your yum database when resolving dependencies.

For example if you download bash, which needs glibc, it will resolve glibc and skip it, since it is installed. **If you want to download all dependencies**, use a different `installroot` instead.
```bash
mkdir -p /root/vim
mkdir -p /root/new_root
yumdownloader --installroot=/root/new_root --destdir=/root/vim/ --resolve vim
```
> This is what I need for a self-contained offline installer.

Let's check how many `vim` related rpms are here, way too many then what we get from the first section.
```bash
ls -ltr /root/vim | wc -l
57
```

#### repotrack
This method can also resolve and download all dependencies, `repotrack` is from `yum-utils`, it will down all the dependencies for any architecture by default.
```bash
mkdir -p /root/vim
repotrack -p /root/vim vim-enhanced
```
if you check `/root/vim`, there are some `i686` rpms, once you delete them and count again, `57` the same as we use `yumdownloader ` above.

> Note: actually `repotrack` has `-a` option to specify arch, but I am not able to use it, when I specify `x86_64`, it still downloads `i686`.

### Install local rpms

Now the problem is how to install these rpms in correct order, install them one by one is obviously infeasible, the method that can resolve their dependencies and install automatically is welcome, both command like:
```bash
yum --disablerepo=* --skip-broken install -y /root/vim/*.rpm
```
and
```bash
rpm --force -ivh /root/vim/*.rpm
```
may work but it's not a good way, you may encounter rpm version upgrade issue and duplicate problem. Now from my knowledge create a local yum repository is clean and elegant, please refer my blog `Set up and Use Local Yum Repository`.

]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>yum</tag>
        <tag>rpm</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Networking Summary</title>
    <url>/2019/11/29/linux-networking-summary/</url>
    <content><![CDATA[
//TODO
[ ] https://www.youtube.com/watch?v=kQYQ_3ayz8w&list=PLvadQtO-ihXt5k8XME2iv0cKpKhcYqe7i&index=5

常用的关于networking 检查的commands: `ss`, `lsof`, `netstat`, `ifconfig`, `hostname`, `ip`, `route`, `iptables`, `nc`, `ping`, `arp`, `curl`, `wget`, `host`, `nslookup`, `dig`.

这篇总结主要是来自PluralSight上的`LPIC-1`课程的Network chapter，以及`LFCE` Advanced Networking training. 后来加入了一些iptables的内容, from Youtube.
Environment: `CentOS 7 Enterprise Linux` or `RedHat`.

**Frequently Asked Question:**
What is going on when you hit URL in browser?
- [from medium](https://medium.com/@maneesha.wijesinghe1/what-happens-when-you-type-an-url-in-the-browser-and-press-enter-bb0aa2449c1a)
- [from Quora](https://www.quora.com/What-are-the-series-of-steps-that-happen-when-a-URL-is-requested-from-the-address-field-of-a-browser)

About domain name: `www.microsoft.com.`:
- root domain: `.`
- top-level domain: `com`
- second-level domain: `microsoft`
- third-level domain: `www`

以上是最基本的流程，如果使用了HTTPS，还可以描述一下TLS handshakes的过程, 再比如中间有proxy则会Tunnel，有load balancer则可能有TLS termination等等。


## Ip vs Ifconfig
`ifconfig` is obsolete, use `ip` instead.
我专门有一篇写的ip command.

`ipv4`: `32`  bits long, dotted decimal
`ipv6`: `128` bits long, quad hex

## Hostname
```bash
# show full hostname
hostname -f

# node hostname
uname -n

# query and change the system hostname and related settings
hostnamectl

   Static hostname: halos1.fyre.xxx.com
         Icon name: computer-vm
           Chassis: vm
        Machine ID: f7bbe4af93974cbfa5c55b68c011d41c
           Boot ID: 4e30e7107fa441a9b3ad70d0b784782d
    Virtualization: kvm
  Operating System: Red Hat Enterprise Linux Server 7.6 (Maipo)
       CPE OS Name: cpe:/o:redhat:enterprise_linux:7.6:GA:server
            Kernel: Linux 3.10.0-957.10.1.el7.x86_64
      Architecture: x86-64

# show domain name
# The chances are unless we have a web server running on our computer, we will not have any dns domain
# name. By default, there is no web server running on a system and hence there is no result when we
# type “dnsdomainname” on the terminal and hit enter.
dnsdomainname
```

```bash
# this will not be persistent
# the static hostname is still unchanged but transient hostname is xxx.example.com
# you can see transient name by hostnamectl
hostname xxx.examplel.com

# this will be persistent in
# /etc/hostname
hostnamectl set-hostname xxx.example.com

# set pretty hostname which includes '
# /etc/machine-info
hostnamectl set-hostname "xxx'ok.example.com"
```

Notice that the order we add in `/etc/hosts` file is important!
把fully qualified hostname放第一个，然后aliases，否则在一些场景会出问题！
```bash
# /etc/hosts
<ip> <fully qualified domain name: FQDN> <aliases>
```

除了local hosts file, 来看看DNS设置, 我有一篇blog讲到了这个。
`dig` command (DNS lookup utility)，用来check response and checking hostname from DNS server.
```bash
# use default dns server
# -t A: type A record
dig www.pluralsight.com -t A
# use specified dns server, for example, google dns server 8.8.8.8
dig www.pluralsight.com @8.8.8.8 -t A
```
Output:
```yaml
<<>> DiG 9.9.4-RedHat-9.9.4-61.el7_5.1 <<>> www.pluralsight.com @8.8.8.8
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 14726
;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 512
;; QUESTION SECTION:
;www.pluralsight.com.           IN      A

# 59,186,186 is TTL (second, keep changing)
;; ANSWER SECTION:
www.pluralsight.com.    59      IN      CNAME   www.pluralsight.com.cdn.cloudflare.net.
www.pluralsight.com.cdn.cloudflare.net. 186 IN A 104.19.162.127
www.pluralsight.com.cdn.cloudflare.net. 186 IN A 104.19.161.127

# server now is 8.8.8.8
;; Query time: 60 msec
;; SERVER: 8.8.8.8#53(8.8.8.8)
;; WHEN: Sun Apr 12 13:03:48 PDT 2020
;; MSG SIZE  rcvd: 132
```

Add short format `+short` to return the IP address only:
```bash
# only show resolved output
dig +short www.pluralsight.com @8.8.8.8
```

[How to check dns record TTL](https://www.cyberciti.biz/faq/how-to-see-time-to-live-ttl-for-a-dns-record/): You can set TTL for the DNS record that defines how long a resolver supposed to cache the DNS query before the query expires. TTL typically used to reduce the load on your authoritative name servers and to speed up DNS queries for clients.
```bash
# A is type, check loacl dns resolver
dig A google.com

# other type
# AAAA: ipv6
dig AAAA google.com
# canonical name
dig cname google.com

# get authoritative dns server
# NS: name server
dig +short NS google.com
# check by authoritative dns server
dig A google.com @ns1.google.com.

# onyl show ttl
dig +nocmd +noall +answer +ttlid A google.com
# human-readable
dig +nocmd +noall +answer +ttlunits A google.com
```

## Network services
04/12/2020 目前我只是查看配置，没有去设置过。

Display and set IP address
```bash
ip -4 addr
ip addr show eth0
# not persist
ip addr add 192.168.1.50/24 dev eth0
```

没太明白这些配置的具体用法。
Network Manager tool, 这个tool也不是万能的，有的地方不适用, can be used to set persistent change so we will not lost it.
```bash
# check status
systemctl status NetworkManager
# if not active, start it
systemctl start NetworkManager

# nmcli command
# command-line tool for controlling NetworkManager
# show all connections
nmcli connection show
# pretty format
nmcli -p connection show eth0

# terminal graph interface
nmtui
# then edit a connection, select network interface
# config ipv4 ip address/gateway.
systemctl restart network
```

Traditional network service, more flexible and common.
```bash
systemctl status network
```
The network configuration is read from scripts under `/etc/sysconfig/network-scripts/`.
```
ifcfg-eth0  ifcfg-eth1  ifcfg-lo ...
```
这些文件里面都写好了配置，more details see this link:
https://www.computernetworkingnotes.com/rhce-study-guide/network-configuration-files-in-linux-explained.html
```
TYPE=Ethernet
BOOTPROTO=dhcp
NAME=eth0
DEVICE=eth0
ONBOOT=yes
...
```
After editing the ifcfg-xx file, bring down and up that interface:
```bash
ifdown eth0
ifup eth0
```

## Routing
[ ] IP tables vs routing tables 有啥区别，使用场景? see this [question](https://superuser.com/questions/419659/iptables-vs-route) and diagram in comment.

Display routing tables 路由表
```bash
# see below
ip r
# route and netstat 每个column的意思更清楚一些
# -n: displays the results as IP addresses only and does not attempt to perform a DNS lookup
netstat -rn
# -e: display as netstat format
route -n [-ee]
```

Explain host routing table (因为这不是一个router), the column name explaination can see `man route`，比如Flags字母的含义。
The order in the routing table does not matter, the longer prefix always takes priority.
```bash
# 简而言之，路由表就是找,到哪里,出口在哪以及下一跳是谁

# Destination 表示destination `network name` or `host name`
# 是用来和要出去的packet destination IP 和 (Genmask)mask 作用之后得到的结果对比的
# 如果match了，则通过Iface(interface)送出去
# 如果和mask作用后有多个match, 则去match最长的那个destination

# 0.0.0.0在Destination中表示默认网关，network mask也是0.0.0.0, 任何一个IP和0.0.0.0 与操作，最后
# 就是0.0.0.0了，所以没有match的IP都去了default gateway了

# Gateway: gateway address, 比如192.168.0.1，如果是0.0.0.0，表示unspecified 或者`没有`, 有时
# 用 * 表示没有.
# this assumes that the network is locally connected, as there is no intermediate hop.
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.0.1     0.0.0.0         UG    100    0        0 ens4
# 注意这里是个host IP了，不是network name
192.168.0.1     0.0.0.0         255.255.255.255 UH    100    0        0 ens4
192.168.9.0     0.0.0.0         255.255.255.0   U     0      0        0 docker0
```

对比一下`ip r` command, 显示不太一样: 
```bash
# proto [type]: routing protocol identifier of this route
# scope link: 表示在设备的网段内通过此链接允许通信

# default gateway
default via 192.168.0.1 dev ens4 proto dhcp metric 100
192.168.0.1 dev ens4 proto dhcp scope link metric 100
# docker0
192.168.9.0/24 dev docker0 proto kernel scope link src 192.168.9.1
```

Adding routes, 把所有的找不到routing的traffic全部转到192.168.56.104上去，通过eth0, 比如当前的machine无法访问外网，而192.168.56.104却可以, 但之后192.168.56.104也需要配置成router。
```bash
# this command is not persistent
# default can be formatted as 192.168.1.0/24
ip route add default via 192.168.56.104 dev eth0
```
如果需要make it persist, need to edit `/etc/sysconfig/network-scripts/` corresponding file eth0, 或者自己添加script，然后重启network `systemctl restart network`.

Configuring a linux system as router:
```bash
# now let's configure machine 192.168.56.104 as a router
vim /etc/sysctl.conf
# add this line to enable ipv4 forward
net.ipv4.ip_forward=1
# reload
sysctl -p
```

当时在做项目的时候需要去DataStage Ops Console查看performance, 但Openshift worker node外界无法直接访问，只能通过infra node的routing才行，于是先用nodePort expose service, 再设置infra node到对应worker node port的映射，最后对外用MASQUERADE。
```bash
# this is operating on nat iptables
# run in infra node
# DNAT: destination nat
iptables -t nat -A PREROUTING -p tcp --dport 32160 -j DNAT --to-destination <worker private IP>:32160
iptables -t nat -A POSTROUTING -j MASQUERADE
iptables -t nat -nvL
```

Allowing access to the internet via NAT, so traffic can get back to private network.
> 注意routing这部分还没有涉及到firewall, firewall is inactive

```bash
# -t nat: working on nat table
# -A POSTROUTING: appending to post routing chain
# -o eth0: outbound via eth0, eth0 connects to internet
# -j MASQUERADE: jump to MASQUERADE rule

# not persistent, see iptables section below
iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE
```
then if you check `iptables -t nat -nvL` will see the postrouting rule with new line added.


## Firewall
其实很多linux是靠iptables去实现firewall的功能的，见下一节，firewalld service背后改动的也是iptables.

Implement packet filtering (iptables and firewalld both can do this)
firewall `zone`: represent a concept to manage incoming traffic more transparently. The zones are connected to networking interfaces or assigned a range of source addresses. You manage firewall rules for each zone independently.

配置命令类似于kubectl/oc的形式。
```bash
systemctl start firewalld

# show default zone
firewall-cmd --get-default-zone
# show active zones, will see interfaces apply to it
firewall-cmd --get-active-zones
# show available zones
firewall-cmd --get-zones

# permanently remove interface eth0 from public zone
firewall-cmd --permanent --zone=public --remove-interface=eth0
# permanently add eth0 to external zone
firewall-cmd --permanent --zone=external --add-interface=eth0
# permanently add eth1 to internal zone
firewall-cmd --permanent --zone=internal --add-interface=eth1

# change default zone
firewall-cmd --set-default-zone=external
# after updating, restart to take effect
systemctl restart firewalld
```
后面主要讲了firewall的配置，可以对不同的zone添加或删除services, ports等，service的默认配置文件在`/usr/lib/firewalld/services`目录，但是自己创建的service文件在`/etc/firewalld/services/`。

## Iptables
用iptables也可以实现firewall的功能via filter table.

There are currently five independent tables:
- `filter`: This is the default table (if no `-t` option is passed),It contains the built-in chains `INPUT` (for packets destined to local sockets),`FORWARD` (for packets being routed through the box), and `OUTPUT` (for locally-generated packets).
- `nat`: This table is consulted when a packet that creates a new connection is encountered.  It consists of three built-ins: `PREROUTING` (forltering  packets  as  soon  as  they  come in), `OUTPUT` (for altering locally-generated packets before routing), and `POSTROUTING` (foraltering packets as they are about to go out). IPv6 NAT support is available since kernel 3.7.
- `mangle`: This table is used for specialized packet alteration.
- `raw`: This  table  is used mainly for configuring exemptions from connection tracking in combination with the NOTRACK target.
- `security`: This table is used for Mandatory Access Control (MAC) networking rules

```bash
# list 3 basic chain in filter table: INPUT, FORWARD, OUTPUT
# INPUT: traffic comes in firewall
# FORWARD: traffic pass through firewall
# OUTPUT: traffic leaving firewall
iptables [-t filter] -L

# policy ACCEPT: default policy is ACCEPT if no specific rules
# other policies: DROP, REJECT(will send ICMP rejecter to sender)
# by default, most system won't have any rules
Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
```

Change default policies。
注意， 可以自己添加rules去加功能，但不要轻易去更改default policy ACCEPT。否则出了意外都不能连接上了。
```bash
# set default policy to DROP
# accept any traffic for INPUT and OUTPUT

# rules 类似于switch中的case，从上到下match，顺序很重要！
# -A: append
iptables -A INPUT -j ACCEPT
iptables -A OUTPUT -j ACCEPT
# 这里设置为DROP是因为上面新加了ACCEPT
iptables -P INPUT DROP
iptables -P OUTPUT DROP
iptables -P FORWARD DROP

# accept any loopback traffic
# loopback traffic never leaves machine
# -i: in-interface
# -o: out-interface
iptables -A INPUT -i lo -j ACCEPT
iptables -A OUTPUT -o lo -j ACCEPT

# -v: verbose, 
# -n: numberic data
# --line-numbers: show rules index
iptables -nvL --line-numbers

# keep current traffic, for example, current ssh connection
iptables -A INPUT -j ACCEPT -m conntrack  --ctstate ESTABLISHED,RELATED
iptables -A OUTPUT -j ACCEPT -m conntrack  --ctstate ESTABLISHED,RELATED

# remove rule by index from --line-numbers
# -D: delete rule
# 这里就把之前ACCEPT去掉了，但链接并不会断开，因为有conntrack with established
iptables -D INPUT 1
iptables -D OUTPUT 1

# 目前为止，没有新的流量可以进来或出去
# add filter rules to iptables firewall for inbound and outbound traffic
# others can ping me
iptables -A INPUT -j ACCEPT -p icmp --icmp-type 8
# I can ping others
iptables -A OUTPUT -j ACCEPT -p icmp --icmp-type 8
# others can ssh in
# add comment
iptables -A INPUT -j ACCEPT -p tcp --dport 22 -m comment --comment "allow ssh from all"

# I can access others
# 当时这里理解有点问题，为什么不需要INPUT 80 port呢？
iptables -A OUTPUT -j ACCEPT -p tcp --dport 80
iptables -A OUTPUT -j ACCEPT -p tcp --dport 443
# DNS
iptables -A OUTPUT -j ACCEPT -p tcp --dport 53
iptables -A OUTPUT -j ACCEPT -p udp --dport 53
# NTP
iptables -A OUTPUT -j ACCEPT -p tcp --dport 123
```

```bash
# save current config
# can edit in this output file
iptables-save > orgset
iptables-restore < orgset

# drop if not match 
# 这个放最后，否则一来就drop了，但如果设置了default DROP则不需要了
iptables -A INPUT -j DROP
# not acting as a router
iptables -A FORWARD -j DROP

# -I: insert
# 把这个rule加到INPUT chain的第一行
iptables -I INPUT 1 -p tcp --dport 80 -j ACCEPT

# clear rules in all chains
iptables -F [chain name]
```

来看看iptables service的使用，变成systemctl service的形式了，使用上更正规一些。 
```bash
yum install -y iptables-services
```
在`/etc/sysconfig`目录下，有`iptables` and `iptables-config` files, If set these two values as `yes`, then iptables will save the config automatically in `iptables` file, easy to maintain.
```bash
# Save current firewall rules on stop.
#   Value: yes|no,  default: no
# Saves all firewall rules to /etc/sysconfig/iptables if firewall gets stopped
# (e.g. on system shutdown).
IPTABLES_SAVE_ON_STOP="yes"

# Save current firewall rules on restart.
#   Value: yes|no,  default: no
# Saves all firewall rules to /etc/sysconfig/iptables if firewall gets
# restarted.
IPTABLES_SAVE_ON_RESTART="yes"
```

## Monitoring Network
Measure network performance, bottleneck
```bash
# 可以查看途径的IP，比如VPN看路径是不是正确的
tracepath www.google.com
```
`traceroute` vs `tracepath`: 
https://askubuntu.com/questions/114264/what-are-the-significant-differences-between-tracepath-and-traceroute
some option of `traceroute` need root privilege, and has more features then `tracepath`.


Display network status
```bash
# 显示有多少error, drop packets来看是不是网络有问题
ip -s -h link
ip -s -h link show eth0
```

`netstat` command can also do the same thing.
```bash
netstat -i

Kernel Interface table
Iface             MTU    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
eth0             1500 16008695      0      5 0       8446165      0      0      0 BMRU
eth1             1500   461914      0     12 0         35082      0      0      0 BMRU
lo              65536   277761      0      0 0        277761      0      0      0 LRU
```

还介绍了一下`sysstat` command，需要yum安装，安装之后它会收集每日的系统历史数据供查看。这也是一个很重要的系统监控工具。
还有一个command `nmap`, 用来scan ports:
```bash
yum install -y nmap
# check what ports in your system is opening
nmap scanme.nmap.org
# list interface and routes information
nmap -iflist
```

Can use `ss` command (similar to `netstat`) to show listening tcp ports:
```bash
# show listening ipv4 tcp sockets in numeric format
ss -ltn -4

# *:* means listening from any address and any port
State      Recv-Q Send-Q                  Local Address:Port                                 Peer Address:Port              
LISTEN     0      64                                  *:2049                                            *:*                  
LISTEN     0      128                                 *:36168                                           *:*                  
LISTEN     0      128                                 *:111                                             *:* 

# list current active connections
ss -t

# 9.30.166.179:ssh is my Mac IP, it ssh to current host
# 这里State is ESTAB, 如果握手没回应，则会显示SYN-SENT
State      Recv-Q Send-Q                Local Address:Port                                 Peer Address:Port                
ESTAB      0      128                    9.30.166.179:ssh                                  9.160.91.147:62991                
ESTAB      0      0                      9.30.166.179:54556                               54.183.140.32:https
```

# Network Basic
这里主要是通过做实验，把基本概念过了一遍。用Vitual Box 设置实验环境，在虚拟机中安装使用wireshark, tcpdump很清晰，没有其他干扰信息。设置实验环境时，可以有1主2从，主机可以访问外界(Adapter1 设置NAT, Adapter2/3 设置Internal Network)，从机可以访问主机，间接实现外部访问(各自的Adapter1 设置Internal Network连接主机的Internal Network). 然后可以进行各种ip, route, iptables的实验了。

`Network topology`: LAN, WAN (bus, star, ring, full mesh)
`Network devices`: adapter, switch, router, firewall
`OSI` model

subnetting: a logically grouped collection of devices on the same network
subnet mask: network portion / host portion
special address: 
  network address (all 0 in host portion)
  broadcast (all 1 in host portion)
  loopback 127.0.0.1
classful subnet: class A/B/C, they are inefficient

`VLSM`: variable length subet mask, for example x.x.x/25
`NAT`: one to one, many to one map
`ARP`: address resolution protocol (IP -> MAC), broadcast on bus to see who has MAC for a particular IP
`DNS`: map hostname to IP, UDP protocol

`IP packet`: can be fragmented and reassembled by router and host. fragments其实很影响throughput，因为每个IP packet都有header。还要注意有的IP加密 (VPN)会额外增加IP packet的长度，造成fragments.
`TTL`: time to live in IP header, this is how `traceroute` works

`Routing Table`:
static: path defined by admin
dynamic: path programmatically defined, routing protocol [software Quagga on Linux](https://en.wikipedia.org/wiki/Quagga_(software))

`TCP`:
  connection oriented: three way handshake
  connection establishment/termination
  data transfer
  ports: system can have more than one IP, ports are only unique per IP
         well know port: 0-1024
  flow control: maintained by receiver
  congestion control: the sender slow down
  error detection and retransmission

`UDP`:
  send it and forget it
  DNS (dig, host commands)
  VoIP

1. setup http service on server host
```bash
yum install -y httpd
# if firewall is on
firewall-cmd --permanent --add-port=80/tcp
firewall-cmd --reload
# set page content
echo "hello world" > /var/www/html/index.html
systemctl enable httpd
systemctl start httpd
```

2. get the web page from other host
```bash
wget http://<ip or hostname>/index.html
```

3. install [tcpdump](https://danielmiessler.com/study/tcpdump/) wireshark on other host
```bash
yum install -y tcpdump wireshark wireshark-gnome
# if you have desktop in linux, start wireshark
wireshark & 
```

Check the arp cache
```bash
# '?' means stale
arp -a
ip neighbor
# delete arp cache
arp -d 192.168.1.1
```

specify size of the data and ping total number:
```bash
# -c 1: ping once
# -s 1472: 1472 bytes long (this is not total length of IP, it will append header)
# so maybe exceed 1500 MTU and then packet will be fragmented
ping -c 1 -s 1472 192.168.1.1
# -t set TTL
ping -c 2 -t 5 192.168.0.1
```

Create a large file to transfer:
```bash
# fast allocate file
# -l5G: length of file is 5G
fallocate -l5G test.bin
# then using scp to copy from network
scp ...
# you can check wireshark to see the tcp window scaling graph
# will see slow start and speed up
```

Traffic control setting
用来模拟网络不好的情况, 如用scp在传输文件，设置tc bad performance，然后恢复，会发现transmission rate提高了。可以查看wireshark window scaling graph 和 IO graph.
[Linux 下 TC 命令原理及详解](https://blog.csdn.net/pansaky/article/details/88801249)
```bash
tc qdisc add dev eth1 root netem delay 3000m loss 5%
# remove the above policy
tc qdisc del dev eth1 root
```
let's see the statistic:
After performance recover, TCP congestion window size enlarge quickly:
![](https://drive.google.com/uc?id=1DqpLc4_K5ZSjSSSBKIqUEYuSOrXWdra_)

This is IO graph, shows TCP window size and update points:
![](https://drive.google.com/uc?id=1soVs4XwDeH6A6D9CG6ZK4WS_jSAnVxPI)
![](https://drive.google.com/uc?id=1QRlirLbMDaA2p_XgEPnvA1fZn98qLJ_M)


# Network Troubleshooting
`Network is not reachable`. For example, cannot ping through.
```bash
# check subnet and gateway, then
ip route
# check interface, state DOWN? NO-CARRIER? then
ip addr
# check MAC mapping in layer 2, then
arp -a
# layer1 is ok? link detected no?
# 注意虚拟机是没有这个统计的！真实网卡才有，之前遇到过这个情景了
# port speed 也可以查看
ethtool eth0
```
`No route to host`，比如在scp的时候，这时去host server上看一下port是不是打开的
```bash
ss -lnt4
```
wireshark看一下client端的情况，发现可能是firewall issue! 端口被屏蔽了。










]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>network</tag>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title>Offline Package Installation II</title>
    <url>/2019/02/26/linux-offline-package-install-2/</url>
    <content><![CDATA[
Now, let's practice what we have learned from `Offline package Installation I`. For example, I want to install `docker` and `kubeadm` etc offline in the target machine.

### Docker

> Note: here we only download actual dependencies need for installation, not all rpms if we use `--installroot` option


I want to install `Docker 18.06.3` (currently kubeadm now properly recognizes `Docker 18.09.0` and newer, but still treats `18.06` as the default supported version). You should perform below steps on a machine that hasn't installed docker yet.

> Note: [install from a package](https://docs.docker.com/install/linux/docker-ce/centos/#install-from-a-package), rpms list in this link are not complete, they are in top level but can be used to upgrade version.

#### Uninstall old version
```bash
yum remove docker \
           docker-client \
           docker-client-latest \
           docker-common \
           docker-latest \
           docker-latest-logrotate \
           docker-logrotate \
           docker-engine
```
The contents of `/var/lib/docker/`, including images, containers, volumes, and networks, are preserved. The Docker CE package is now called `docker-ce`.

#### Set up docker repository
Before you install Docker CE for the first time on a new host machine, you need to set up the Docker repository. Afterward, you can install and update Docker from the repository.
```bash
yum install -y yum-utils \
  device-mapper-persistent-data \
  lvm2
```
Use the following command to set up the stable repository, `yum-utils` contains `yum-config-manager`:
```bash
yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo
```
#### List docker version
List and sort the versions available in your repo. This example sorts results by version number, highest to lowest, and is truncated:
```bash
yum list docker-ce --showduplicates | sort -r
```
```bash
Loaded plugins: product-id, search-disabled-repos
docker-ce.x86_64            3:18.09.2-3.el7                     docker-ce-stable
docker-ce.x86_64            3:18.09.1-3.el7                     docker-ce-stable
docker-ce.x86_64            3:18.09.0-3.el7                     docker-ce-stable
docker-ce.x86_64            18.06.3.ce-3.el7                    docker-ce-stable
docker-ce.x86_64            18.06.2.ce-3.el7                    docker-ce-stable
docker-ce.x86_64            18.06.1.ce-3.el7                    docker-ce-stable
docker-ce.x86_64            18.06.0.ce-3.el7                    docker-ce-stable
docker-ce.x86_64            18.03.1.ce-1.el7.centos             docker-ce-stable
docker-ce.x86_64            18.03.0.ce-1.el7.centos             docker-ce-stable
docker-ce.x86_64            17.12.1.ce-1.el7.centos             docker-ce-stable
...
```
#### Download docker rpms
Install a specific version by its fully qualified package name, which is the package name (`docker-ce`) plus the version string (2nd column) starting at the first colon (:), up to the first hyphen, separated by a hyphen (-). For example, `docker-ce-18.06.3.ce`.
```bash
mkdir -p /root/docker-18.06.3-rpms
yum install --downloadonly --downloaddir=/root/docker-18.06.3-rpms docker-ce-18.06.3.ce
```
list the rpms in the target folder:
```
audit-2.8.4-4.el7.x86_64.rpm               libselinux-utils-2.5-14.1.el7.x86_64.rpm
audit-libs-2.8.4-4.el7.x86_64.rpm          libsemanage-2.5-14.el7.x86_64.rpm
audit-libs-python-2.8.4-4.el7.x86_64.rpm   libsemanage-python-2.5-14.el7.x86_64.rpm
checkpolicy-2.5-8.el7.x86_64.rpm           libsepol-2.5-10.el7.x86_64.rpm
container-selinux-2.68-1.el7.noarch.rpm    libtool-ltdl-2.4.2-22.el7_3.x86_64.rpm
docker-ce-18.06.3.ce-3.el7.x86_64.rpm      policycoreutils-2.5-29.el7_6.1.x86_64.rpm
libcgroup-0.41-20.el7.x86_64.rpm           policycoreutils-python-2.5-29.el7_6.1.x86_64.rpm
libseccomp-2.3.1-3.el7.x86_64.rpm          python-IPy-0.75-6.el7.noarch.rpm
libselinux-2.5-14.1.el7.x86_64.rpm         setools-libs-3.3.8-4.el7.x86_64.rpm
libselinux-python-2.5-14.1.el7.x86_64.rpm
```

Note that the required components may be changed in later version, such as `18.09.2`, there are 2 more packages `docker-ce-cli-18.09.2` and `containerd.io`.
```bash
mkdir -p /root/docker-18.09.2-rpms
yum install --downloadonly --downloaddir=/root/docker-18.09.2-rpms docker-ce-18.09.2 docker-ce-cli-18.09.2 containerd.io

```

#### Install docker rpms
now install docker `18.06.3` offline by running:
```bash
yum --disablerepo=* -y install /root/docker-18.06.3-rpms/*.rpm
```
> Note: please refer my blog `Set up and Use Local Yum Repository` if you want to create and use local yum repository
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>linux</tag>
        <tag>yum</tag>
        <tag>rpm</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Paste Command</title>
    <url>/2023/12/17/linux-paste/</url>
    <content><![CDATA[
When working on update target field in Cassandra in batch, the `IN` operator is
used with comma separated primary keys.

To get a list of primary keys is simple, for example, you can `CAPTURE` them
to a text file and find them out by bash pipeline if needed, then we need to put
them into a single line with comma as separator.

For example, the file `test.txt` conatins primary keys:
```bash
aaaaa
bbbbb
ccccc
ddddd
```

There are multi-way to do it, one is using `paste` command:
```bash
cat test.txt | paste -s -d, -
```

The output is:
```
aaaaa,bbbbb,ccccc,ddddd
```

Using `tr` command can do the same with an additional tail comma, for example:
```bash
cat test.txt | tr '\n' ','
# aaaaa,bbbbb,ccccc,ddddd,
```

]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux PAM Module Configuration</title>
    <url>/2019/04/01/linux-pam/</url>
    <content><![CDATA[
When I was working on non-root set up worker containers or pods, in order to grant the non-root user su password-less privilege, I got into PAM module in RHEL. Let's spend time to understand it.

**Pluggable authentication modules (PAMs)** are a common framework for **authentication** and **authorization**. 

There are many programs on your system that use PAM modules like su, passwd, ssh and login and other services. PAM main focus is to authenticate your users.

PAM or Pluggable Authentication Modules are the management layer that sits between Linux applications and the Linux native authentication system.

> For full details, please refer:
[USING PLUGGABLE AUTHENTICATION MODULES (PAM)](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system-level_authentication_guide/pluggable_authentication_modules)
[CHINESE VERSION](https://www.cnblogs.com/kevingrace/p/8671964.html)

Each PAM-aware application or service has a file in the `/etc/pam.d/` directory. Each file in this directory has the same name as the service to which it controls access. For example, the `login` program defines its service name as login and installs the `/etc/pam.d/login` PAM configuration file.

What I have done is add a non-root user, for example `demo`, to `wheel` group (usually `wheel` group is pre-existing), operate this as root user:
```
usermod -a -G wheel demo
```

Why `wheel` group? What is `wheel` stand for in computing?
In computing, the term `wheel` refers to a user account with a wheel bit, a system setting
that provides additional special system privileges that empower a user to execute restricted commands that ordinary user accounts cannot access.

Modern Unix systems generally use user groups as a security protocol to control access privileges. The `wheel` group is a special user group used on some Unix systems to control access to the su or sudo command, which allows a user to masquerade as another user (usually the super user).

By default it permits root access to the system if the applicant user is a member of the `wheel` group.

Check `demo` group information:
```bash
id demo

uid=1010(demo) gid=1010(demo) groups=1010(demo),10(wheel)
```
You can also go to `/etc/group` file to check group members of `wheel`:
```bash
wheel:x:10:demo
```
Then go to edit `/etc/pam.d/su` file to uncomment this directive:
```bash
# Uncomment the following line to implicitly trust users in the "wheel" group.
auth           sufficient      pam_wheel.so trust use_uid
```

What is `pam_wheel.so`?
The `pam_wheel` PAM module is used to enforce the so-called `wheel` group. By default it permits root access(su - ) to the system if the applicant user is a member of the wheel group. If no group with this name exist, the module is using the group with the group-ID 0.

Now the user `demo` can su to other users (include root) without password. You can run command as another user:
```
su - <another> -c "<command>"
```

If you also want to have `sudo` password-less privilege for `wheel` group user, you need to edit `/etc/sudoers` file by `visudo` as root like this:
```bash
## Allows people in group wheel to run all commands
#%wheel       ALL=(ALL)       ALL

## Same thing without a password
%wheel        ALL=(ALL)       NOPASSWD: ALL
```

### PAM file format
Each PAM configuration file, such as `/etc/pam.d/su` contains a group of directives that define the module (the authentication configuration area) and any controls or arguments with it.

The directives all have a simple syntax that identifies the module purpose (interface) and the configuration settings for the module.
```
module_interface	control_flag	module_name module_arguments
```
* **module_name** — such as `pam_wheel.so`
* **auth** — This module interface authenticates users. For example, it requests and verifies the validity of a password. Modules with this interface can also set credentials, such as group memberships.

All PAM modules generate a success or failure result when called. `Control flags` tell PAM what to do with the result. Modules can be listed (stacked) in a particular order, and the control flags determine how important the success or failure of a particular module is to the overall goal of authenticating the user to the service.

* **sufficient** — The module result is ignored if it fails. However, if the result of a module flagged sufficient is successful and no previous modules flagged required have failed, then no other results are required and the user is authenticated to the service.

Let's see an example:
```bash
[root@MyServer ~]# cat /etc/pam.d/setup

auth       sufficient	pam_rootok.so
auth       include	system-auth
account    required	pam_permit.so
session	   required	pam_permit.so
```
Here the modules are stacking, from up to bottom, verify each directive one by one,
* *auth sufficient pam_rootok.so* — This line uses the `pam_rootok.so` module to check whether the current user is root, by verifying that their UID is 0. If this test succeeds, no other modules are consulted and the command is executed. If this test fails, the next module is consulted.

* *auth include system-auth* — This line includes the content of the /etc/pam.d/system-auth module and processes this content for authentication.


PAM uses `arguments` to pass information to a pluggable module during authentication for some modules.
* **trust**: The `pam_wheel` module will return PAM_SUCCESS instead of PAM_IGNORE if the user is a member of the wheel group

* **use_uid**: The check for wheel membership will be done against the current uid instead of the original one

]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>pam</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Networking Tunable</title>
    <url>/2021/07/17/linux-network-tunable/</url>
    <content><![CDATA[
Summary from book `<<System Performance 2rd>>`, Network Chapter.

Tuning depends on the network workload characterization, the available tunables
also vary between versions of OS.

Check TCP settings by:
```bash
sysctl -a | grep -i tcp
```
Write changes to the file `/etc/sysctl.conf`, reload the settings via
`sysctl -p` command and it will take effect without rebooting.

# Production Example
Socket and TCP buffer in bytes, may need to be set to 16M or higher to support
full-speed 10GbE connections:
```ini
net.core.rmem_max = 16777216
net.core.wmem_max = 16777216
```

Enable auto-tuning TCP receive buffer:
```ini
net.ipv4.tcp_moderate_rcvbuf = 1
```

TCP read and write buffer auto-tunning in bytes (min, default, max):
```ini
net.ipv4.tcp_rmem = 4096 87380 16777216
net.ipv4.tcp_wmem = 4096 87380 16777216
```

TCP backlog, for half-open connections:
```ini
net.ipv4.tcp_max_syn_backlog = 4096
```

Listen backlog, for passing connections to accept:
```ini
net.core.somaxconn = 1024
```
Above two attributes with bigger value can better handle burst of load.

Device backlog queue length, per CPU (for example for 10Gbe NICs):
```ini
net.core.netdev_max_backlog = 10000
```

TCP congestion control, Linux supports pluggable congestion control algorithms.
```bash
# Check available ones.
sysctl net.ipv4.tcp_available_congestion_control

# Load and enable available algorithm `tcp_htcp` to use
modprobe tcp_htcp

# Check again and see `tcp_htcp` is in list
sysctl net.ipv4.tcp_available_congestion_control

# Set cubic as default in config file
net.ipv4.tcp_congestion_control = cubic
```

Other TCP options:
```ini
# Improve performance over high latency network
net.ipv4.tcp_sack = 1
net.ipv4.tcp_fack = 1
net.ipv4.tcp_tw_reuse = 1

# Other settings
net.ipv4.ip_local_port_range = 10240 65535
net.ipv4.tcp_abort_on_overflow = 1
net.ipv4.tcp_slow_start_after_idle = 0
net.ipv4.tcp_syn_retries = 2
```

# Queuing Disciplines
It is for network packets scheduling, manipluating, filtering and shaping,
for example, control the packet loss rate.
```ini
# Check default 
sysctl net.core.default_qdisc
# Set as fq_codel
net.core.default_qdisc = fq_codel
```
Many Linux distros have already switched to `fq_codel` as the default, it
provides good performance in most cases.]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title>nologin User</title>
    <url>/2020/09/22/linux-nologin/</url>
    <content><![CDATA[
Previously I had a post talked about the `login and non-login shell`, please note that is a different concept with `nologin` user here.

Take `envoy` image as an example, pull it and launch the container:
```bash
docker pull envoyproxy/envoy-dev:latest

docker run -d \
--name test \
--entrypoint=/bin/bash \
envoyproxy/envoy-dev:latest \
-c "tail -f /dev/null"
```

Get into the container by (note this is not a `login` operation! see below my question):
```bash
docker exec -it test bash
```

Check `/etc/passwd` file, the `envoy` is a nologin user:
```bash
envoy:x:101:101::/home/envoy:/usr/sbin/nologin
```

If you run `su - envoy` from any other users (even you enter the login password), you get error:
```bash
# su - envoy
This account is currently not available
```

From nologin man page, the description is clear: nologin displays a message that an account is not available and exits non-zero. It is intended as a replacement shell field to **deny login access** to an account. If the file `/etc/nologin.txt` exists, nologin displays its contents to the user instead of the default message. The exit code returned by nologin is always `1`.

Sometimes you will also see `/bin/false` is used:
```
syslog:x:101:104::/home/syslog:/bin/false
```

They both have the same purpose, but `nologin` is preferred since it give you a friendly message. `ssh`, `scp` and other login services will not work if the user is `nologin` type on target machine.

BTW, You still can execute command as a nologin user:
```bash
sudo -u <nologin user name> bash -c "ls -ltr /tmp"
## or launch a shell
sudo -u <nologin user name> bash
```

Then I have a question here: [Why docker exec command can launch shell with nologin user?](https://unix.stackexchange.com/questions/610851/why-docker-exec-command-can-launch-shell-with-nologin-user). It turns out `docker exec` is not `login` action! It just starts a process in that PID namespace and it's PPID is 1.


# References
[Does /usr/sbin/nologin as a login shell serve a security purpose?](https://unix.stackexchange.com/questions/155139/does-usr-sbin-nologin-as-a-login-shell-serve-a-security-purpose)
[https://serverfault.com/questions/519215/what-is-the-difference-between-sbin-nologin-and-bin-false](https://serverfault.com/questions/519215/what-is-the-difference-between-sbin-nologin-and-bin-false)
[https://serverfault.com/questions/333321/executing-a-command-as-a-nologin-user](https://serverfault.com/questions/333321/executing-a-command-as-a-nologin-user)
[Don't sshd your container](https://jpetazzo.github.io/2014/06/23/docker-ssh-considered-evil/), this is a old post at early stage of docker and before `docker exec`, it uses `nsenter` to get a shell into container namespace.]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>linux</tag>
        <tag>nologin</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Performance Tuning</title>
    <url>/2021/01/23/linux-performance/</url>
    <content><![CDATA[
最近一周在on-call，这里整理一下Linkedin Learn: Linux performance的内容. 这里主要关注4个方面: CPU, Memory, Disk and FileSystem IO, Network. 其实很多时候，alerts or incidents are derived from performance issues, of course we have insight checks for services themselves. 建议阅读 `<<How linux works>>`这本书，里面都讲到了。

此外，极客时间的课程`<<Linux性能优化实战>>`非常好，受益匪浅。我还阅读了书籍`<<Systems Performance>>`.

# From 极客时间
[性能领域大师布伦丹·格雷格 personal website](http://www.brendangregg.com/)

For more comprehensive cheat sheet, please switch to `<<On-Call System Performance>>`.
## CPU
Understand the meaning of `uptime` command(check man). Load average is **not** CPU usage. 平均负载是指单位时间内，系统处于可运行状态(R)和不可中断状态(D)的平均进程数，也就是平均活跃进程数(可以这么理解，但源码它实际上是活跃进程数的指数衰减平均值)，它和 CPU 使用率并**没有**直接关系. (有时需要对比系统的运行时间，有的故障可能是系统重启导致的)
```bash
# check CPU number
lscpu
# check load average
uptime
w
# or using top with 1 to show each cpu
# and load average
top -b -n 1 | head
```

关于不可中断状态，当一个进程向磁盘读写数据时，为了保证数据的一致性，在得到磁盘回复前，它是不能被其他进程或者中断打断的，这个时候的进程就处于不可中断状态。如果此时的进程被打断了，就容易出现磁盘数据与进程数据不一致的问题. 不可中断状态实际上是系统对进程和硬件设备的一种保护机制.

综合`top`, `ps` 中查看进程 status code, such as `SLsl`，很有意义，可以了解进程的组成等，比如多线程.
```bash
man ps
# PROCESS STATE CODES
D    uninterruptible sleep (usually IO)
R    running or runnable (on run queue)
S    interruptible sleep (waiting for an event to complete)
T    stopped by job control signal
t    stopped by debugger during the tracing
W    paging (not valid since the 2.6.xx kernel)
X    dead (should never be seen)
Z    defunct ("zombie") process, terminated but not reaped by its parent
I    idle
# For BSD formats and when the stat keyword is used, additional characters may be displayed:
<    high-priority (not nice to other users)
N    low-priority (nice to other users)
L    has pages locked into memory (for real-time and custom IO)
s    is a session leader (会话是指共享同一个控制终端的一个或多个进程组)
l    is multi-threaded (using CLONE_THREAD, like NPTL pthreads do)
+    is in the foreground process group (进程组表示一组相互关联的进程，比如每个子进程都是父进程所在组的成员)
```

比如当平均负载为2时, 意味着什么呢? When number of CPU is larger, equal or less then 2. Check `lscpu` or `grep 'model name' /proc/cpuinfo | wc -l` to see the logical CPU size.

三个不同时间间隔的平均值，其实给我们提供了，分析系统负载趋势的数据来源，让我们能更全面、更立体地理解目前的负载状况. 当平均负载高于 CPU 数量 70% 的时候，你就应该分析排查负载高的问题了。一旦负载过高，就可能导致进程响应变慢，进而影响服务的正常功能. 最推荐的方法，还是把系统的平均负载监控起来(prometheus + grafana)，然后根据更多的历史数据，判断负载的变化趋势。当发现负载有明显升高趋势时，比如说负载翻倍了，你再去做分析和调查。

当发现负载高的时候，你可以使用 `iostat`, `mpstat`, `pidstat` 等工具，辅助分析负载的来源.
```bash
# stress cpu with 1 process
stress --cpu 1 --timeout 600
# stress io
# stress -i 1 --timeout 600 does not work well
# because VM sync buffer is small
stress-ng --io 1 --hdd 1 --timeout 600
# stress cpu with 8 processes
stress -c 8 --timeout 600

# check uptime updates
# -d: highlight the successive difference
watch -d "uptime"

# check all cpus status
# 判断cpu usage 升高是由于iowait 还是 computing
mpstat -P ALL 1

# check which process cause cpu usage high
# -u: cpu status
pidstat -u 5 2
# -d: io status
# -p: pid
pidstat -d -p 12345 5 2
```
在查看pidstat 时，可能没有%wait column，则需要升级版本到11.5.5 (centos 8)，见[sysstat git repo](https://github.com/sysstat/sysstat). 或者如果在prod VM上无安装，直接把编译好的文件丢到系统上就可以运行。

理解 CPU 上下文切换: 分为进程上下文切换、线程上下文切换和中断上下文切换。

注意，`系统调用`过程中，并不会涉及到虚拟内存等进程用户态的资源，也不会切换进程。这跟我们通常所说的进程上下文切换是不一样的。进程上下文切换，是指从一个进程切换到另一个进程运行。而系统调用过程中一直是同一个进程在运行。所以，系统调用过程通常称为特权模式切换，而不是上下文切换。但实际上，系统调用过程中，CPU 的上下文切换还是无法避免的。

线程与进程最大的区别在于，线程是调度的基本单位，而进程则是资源拥有的基本单位。说白了，所谓内核中的任务调度，实际上的调度对象是线程；而进程只是给线程提供了虚拟内存、全局变量等资源。

跟进程上下文不同，中断上下文切换并不涉及到进程的用户态。所以，即便中断过程打断了一个正处在用户态的进程，也不需要保存和恢复这个进程的虚拟内存、全局变量等用户态资源。中断上下文，其实只包括内核态中断服务程序执行所必需的状态，包括 CPU 寄存器、内核堆栈、硬件中断参数等。

`vmstat` 是一个常用的系统性能分析工具，主要用来分析系统的内存使用情况，也常用来分析 CPU 上下文切换和中断的次数。
`sysbench` 是一个多线程的基准测试工具，一般用来评估不同系统参数下的数据库负载情况。
```bash
# benchmark tool
sysbench --threads=10 --max-time=300 threads run

# 系统整体的性能
# focus on in, cs, r, b, check man for description
# 注意r 的个数是否远超CPU 个数
# in 太多也是个问题
# us sy 看cpu 主要是被用户 还是 内核 占据
# -w: wide display
# -S: unit m(mb)
# 2: profile interval
vmstat -w -S m 2

procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 2  0      0 230548     24 4068628    0    0     0    33 16436 29165 25  2 73  0  0
 1  0      0 226888     24 4069028    0    0     0    43 15443 27179 39  2 59  0  0
 3  0      0 225588     24 4070544    0    0     0   523 20873 38865 36  2 61  0  0

# 查看process 的性能
# only show at least one column non-zero item 
# -w: show context switch
# -u: show cpu stat
pidstat -w -u 2
# -t: thread level, helpful!
# -p: pid
# check both process and its thread context switch status
pidstat -wt -p 14233 2

# analyze interrupt, good point to check proc file
# focus on the most frequently changing part, 然后查看指代什么行为
watch -d 'cat /proc/interrupts'
# RES 重调度中断
```
如果系统的上下文切换次数比较稳定，那么从数百到一万以内，都应该算是正常的。但当上下文切换次数超过一万次，或者切换次数出现数量级的增长时，就很可能已经出现了性能问题。

这里比较一下2个测压工具:
`stress` 基于多进程的，会fork多个进程，导致进程上下文切换，导致cpu %us开销很高；
`sysbench` 基于多线程的，会创建多个线程，单一进程基于内核线程切换，导致cpu %sy的内核开销很高。

一些思路:
登录到服务器，现在系统负载怎么样。高的话有三种情况，首先是cpu使用率，其次是io使用率，之后就是两者都高。
cpu 使用率高，可能确实是使用率高，也的可能实际处理不高而是进程太多切换上下文频繁，也可能是进程内线程的上下文切换频繁。
io 使用率高，说明 io 请求比较大，可能是文件io，网络io.

这里你一定要记得，当碰到无法解释的 CPU 使用率问题时，先要检查一下是不是短时应用在捣鬼。短时应用的运行时间比较短，很难在 top 或者 ps 这类展示系统概要和进程快照的工具中发现，你需要使用记录事件的工具来配合诊断，比如 `execsnoop` 或者 `perf top`.

当 iowait 升高时，进程很可能因为得不到硬件的响应，而长时间处于不可中断状态。从 ps 或者 top 命令的输出中，你可以发现它们都处于 D 状态，也就是不可中断状态. D 状态的进程会导致平均负载升高， I 状态的进程却不会.

正常情况下，不可中断状态在很短时间内就会结束。所以，短时的不可中断状态进程，我们一般可以忽略. 如果系统或硬件发生了故障，进程可能会在不可中断状态D 保持很久，甚至导致系统中出现大量不可中断进程。这时，你就得注意下，系统是不是出现了 I/O 等性能问题。看僵尸进程，当一个进程创建了子进程后，它应该通过系统调用 wait() 或者 waitpid() 等待子进程结束，回收子进程的资源；而子进程在结束时，会向它的父进程发送 SIGCHLD 信号，所以，父进程还可以注册 `SIGCHLD` 信号的处理函数，异步回收资源。如果父进程没有处理子进程的终止，那么子进程就会一直处于僵尸状态。大量的僵尸进程会用尽 PID 进程号，导致新进程不能创建，所以这种情况一定要避免。

```bash
# versatile tool for generating system resource statistics
dstat
```

strace 正是最常用的跟踪进程系统调用的工具, 但僵尸进程都是已经退出的进程，所以就没法儿继续分析它的系统调用:
```bash
# -f: trace threads generated
strace -p <pid> [-f]
```
[strace hanging at futex, FUTEX_WAIT](https://meenakshi02.wordpress.com/2011/02/02/strace-hanging-at-futex/): The process is multi-threaded, you are tracing the original parent thread, and it’s doing nothing but waiting for some other threads to finish.

If top, pidstat 这类工具已经不能给出更多的信息了。这时，我们就应该求助那些基于事件记录的动态追踪工具了。
```bash
# wait for 15 seconds
# crtl+c quit
perf record -g
# check
pert report
```

iowait 高不一定代表 I/O 有性能瓶颈。当系统中只有 I/O 类型的进程在运行时，iowait 也会很高，但实际上，磁盘的读写远没有达到性能瓶颈的程度。碰到 iowait 升高时，需要先用 dstat, pidstat 等工具，确认是不是磁盘 I/O 的问题，然后再找是哪些进程导致了 I/O。

其实除了 iowait，`软中断(softirq)`导致 CPU 使用率升高也是最常见的一种性能问题。

为了解决中断处理程序执行过长和中断丢失的问题，Linux 将中断处理过程分成了两个阶段，也就是上半部和下半部：
上半部用来快速处理中断，它在中断禁止模式下运行，主要处理跟硬件紧密相关的或时间敏感的工作。
下半部用来延迟处理上半部未完成的工作，通常以内核线程的方式运行。

上半部直接处理硬件请求，也就是我们常说的硬中断，特点是快速执行；而下半部则是由内核触发，也就是我们常说的软中断，特点是延迟执行。

上半部会打断 CPU 正在执行的任务，然后立即执行中断处理程序。而下半部以内核线程的方式执行，并且每个 CPU 都对应一个软中断内核线程，名字为 “ksoftirqd/CPU编号”，比如说， 0 号 CPU 对应的软中断内核线程的名字就是 `ksoftirqd/0`。

经常听同事说大量的网络小包会导致性能问题，一直不太理解，从今天的课程来看，大量的小网络包会导致频繁的硬中断和软中断呢.软中断问题在大流量网络中最为常见. 

注意cpu 使用率 和 load average 没有直接关系，这个在最开始提到过了，它们各自定义不一样。
```bash
# test tool
# -S参数表示设置TCP协议的SYN（同步序列号），-p表示目的端口为80
# -i u100表示每隔100微秒发送一个网络帧
hping3 -S -p 80 -i u100 192.168.0.30

# check cpu %si and %hi
# load average may be low
top

# check softirq frequencies and category
# usually NET_RX ratio change a lot
watch -d cat /proc/softirqs

# check network rx/tx packet vs KBS per second
# -n DEV 表示显示网络收发的报告，间隔1秒输出一组数据
# can calculate bytes/packet
sar -n DEV 1

# -i eth0 只抓取eth0网卡
# -n不解析协议名和主机名
# tcp port 80表示只抓取tcp协议并且端口号为80的网络帧
tcpdump -i eth0 -n tcp port 80
```
在tcpdump的输出中，观察`Flag[x]` 类型, 比如`Flag[S]` 表示SYN 包。可以找到source IP 并且通过防火墙隔离。

[ ] cassandra perf analysis find cause?
[x] why cassandra Sleep with high CPU usage? -> multi-threads are running
[x] why process is `S` but some threads are `R`, parent is doing nothing but waiting children threads.


倪老师您好，我在网上看到一个关于iowait指标的解释，非常形象，但不确定是否准确，帮忙鉴别一下，谢谢，
链接 http://linuxperf.com/?p=33

sar -w 或者 sar -w 1 也能直观的看到每秒生成线程或者进程的数量。Brendan Gregg 确实是这个领域的大师，贡献了很多的技术理念和实践经验。他的《性能之巅》可以和本课对比着看，会有更多的理解.

## Memory
Understanding concepts:
- [virtual memory](https://en.wikipedia.org/wiki/Virtual_memory#Usage)
- [MMU and page table](https://en.wikipedia.org/wiki/Memory_management_unit)
- [kernal part in process virtual memory](https://unix.stackexchange.com/questions/472223/whats-the-use-of-having-a-kernel-part-in-the-virtual-memory-space-of-linux-proc)
- [page](https://en.wikipedia.org/wiki/Page_(computer_memory))
- [paging fault](https://en.wikipedia.org/wiki/Page_fault)

这里查了一下tmpfs的解释和作用:
- [what is tmpfs](https://en.wikipedia.org/wiki/Tmpfs)
- [tmpfs 详解](https://segmentfault.com/a/1190000014737366)
It is intended to appear as a mounted file system, but data is stored in volatile memory instead of a persistent storage device. A similar construction is a RAM disk, which appears as a virtual disk drive and hosts a disk file system. tmpfs is meant only for ephemeral files
```bash
# resize mounted tmpfs size
# only temporary
mount -o remount,size=300M tmpfs /dev/shm
# -T: see file system type
df -hT

# create new tmpfs mounted
mkdir /data
# -t: type
# -o: options
# second tmpfs: device name
mount -t tmpfs -o size=100M tmpfs /data
umount /data
```
tmpfs用途还是较广的，Linux中可以把一些程序的临时文件放置在tmpfs中，利用tmpfs比硬盘速度快的特点来提升系统性能.

需要理解`free`, `top`, `ps` 中关于memory 部分的含义, for example: buffers, cached, shared.

`/proc` 是 Linux 内核提供的一种特殊文件系统，是用户跟内核交互的接口。比方说，用户可以从 /proc 中查询内核的运行状态和配置选项，查询进程的运行状态、统计数据等，当然，你也可以通过 /proc 来修改内核的配置.
```bash
# generate files
dd if=/dev/urandom of=/tmp/file bs=1M count=500
# check bi/bo and mem
vmstat 1 20
```
在读写普通文件时，会经过文件系统，由文件系统负责与磁盘交互；而读写磁盘或者分区时，就会跳过文件系统，也就是所谓的“裸I/O“。这两种读写方式所使用的缓存是不同的，也就是文中所讲的 Cache 和 Buffer 区别。

我的分析步骤：使用`top` 和 `ps` 查询系统中大量占用内存的进程，使用`cat /proc/[pid]/status`和`pmap -x <pid>`查看某个进程使用内存的情况和动态变化。

查看缓存的实际效果使用`缓存命中率`: 是指直接通过缓存获取数据的请求次数，占所有数据请求次数的百分比。
```bash
# read file
dd if=/tmp/file of=/dev/null bs=1M count=500

yum install -y bcc-tools
# need to manually export
export PATH=$PATH:/usr/share/bcc/tools
# check overall cache hit rate
cachestat 1 3
# check process cache hit rate
# similar to top command mechanism
# 3: 3 seconds update
cachetop 3
```
Investigate file cache size by [pcstat](https://github.com/tobert/pcstat)
```bash
# install go first
# use go to fetch and install pcstat, see github page
pcstat /bin/ls
```
但同时也要注意，如果我们把 dd 当成测试文件系统性能的工具，由于缓存的存在，就会导致测试结果严重失真。所以测试前要查看文件在缓存中的大小，先清理一下缓存:
```bash
echo 3 > /proc/sys/vm/drop_caches
```
不过要注意，Buffers 和 Cache 都是操作系统来管理的，应用程序并不能直接控制这些缓存的内容和生命周期。所以，在应用程序开发中，一般要用专门的缓存组件，来进一步提升性能。比如，程序内部可以使用堆或者栈明确声明内存空间，来存储需要缓存的数据。再或者，使用 Redis 这类外部缓存服务，优化数据的访问效率。

`dd`命令也支持直接IO的 有选项oflag和iflag 所以dd也可以用来绕过cache buff做测试, 注意直接IO和裸IO 不一样，直接IO是跳过Buffer，裸IO是跳过文件系统(还是有buffer的)。

还要注意的是，就算是第一次读，也会有缓存命中，因为系统会预读一部分到内存中，直接IO虽然跳过了buffer, 但cache hit rate is 100%，根据缓存命中次数(每次命中就是one page 4K size)，计算出在当时时间间隔中的命中大小: HITS * 1024 * 4K, 在除以时间间隔，就可知速率K/s.

`dd` command `bs(block size)` option [value](https://unix.stackexchange.com/questions/9432/is-there-a-way-to-determine-the-optimal-value-for-the-bs-parameter-to-dd), 4K is fine, for large storage hard drive, 1M is good to go, `bs` vaule deponds on your RAM size, it is the size that processed of each operation.

Memory leak, 理解内存的分配与回收，哪些虚拟内存段容易发生泄漏问题: heap and 内存映射段(包括动态链接库和共享内存，其中共享内存由程序动态分配和管理)。所以，如果程序在分配后忘了回收，就会导致跟堆内存类似的泄漏问题。
```bash
# -r: mem statistic
# -S: swap statistic
# 3: refresh rate second
sar -r -S 3
# in bcc-tools with cachestat and cachetop
memleak -a -p $(pidof app_name)
```
memleak的输出结果中addr 表示分配的地址，然后后面会给出哪个stack请求的分配，列出了相关调用。
还有一个常见的内存debug tool: `valgrind`.

文件页(file-backed page) 和 匿名页(anonymous page)的区别和回收.

了解swap的机制。`watermark` 也就是阈值。
事实上不仅 hadoop，包括 ES 在内绝大部分 Java 的应用都建议关 swap，这个和 JVM 的 gc 有关，它在 gc 的时候会遍历所有用到的堆的内存，如果这部分内存是被 swap 出去了，遍历的时候就会有磁盘IO。

[Swapping, memory limits, and cgroups](https://jvns.ca/blog/2017/02/17/mystery-swap/).
开启swapping 会使cgroup memory limit失效。

用`smem --sort swap`命令可以直接将进程按照swap使用量排序显示.


记录被OOM杀掉的进程:
```bash
dmesg | grep -E "kill|oom|out of memory"
```

[ ] cgroup mem vs proc mem data? https://www.cnblogs.com/muahao/p/9593869.html

## I/O
在 Linux 中一切皆文件。不仅普通的文件和目录，就连块设备、套接字、管道等，也都要通过统一的文件系统来管理。
为了方便管理，Linux 文件系统为每个文件都分配两个数据结构，索引节点（index node）和目录项（directory entry）.

索引节点，简称为 inode，用来记录文件的元数据，比如 inode 编号、文件大小、访问权限、修改日期、数据的位置等。索引节点和文件一一对应，它跟文件内容一样，都会被持久化存储到磁盘中。所以记住，索引节点同样占用磁盘空间(`df -i <dir>`)。

目录项，简称为 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的关联关系。多个关联的目录项，就构成了文件系统的目录结构。不过，不同于索引节点，目录项是由内核维护的一个内存数据结构，所以通常也被叫做目录项缓存。

实际上，磁盘读写的最小单位是扇区，然而扇区只有 512B 大小，如果每次都读写这么小的单位，效率一定很低。所以，文件系统又把连续的扇区组成了逻辑块，然后每次都以逻辑块为最小单元，来管理数据。常见的逻辑块大小为 4KB，也就是由连续的 8 个扇区组成。

目录项、索引节点、逻辑块以及超级块，构成了 Linux 文件系统的四大基本要素。不过，为了支持各种不同的文件系统，Linux 内核在用户进程和文件系统的中间，又引入了一个抽象层，也就是虚拟文件系统 `VFS(Virtual File System)`。

VFS 定义了一组所有文件系统都支持的数据结构和标准接口。这样，用户进程和内核中的其他子系统，只需要跟 VFS 提供的统一接口进行交互就可以了，而不需要再关心底层各种文件系统的实现细节。

文件读写方式的各种差异，导致 I/O 的分类多种多样。最常见的有，缓冲与非缓冲 I/O、直接与非直接 I/O、阻塞与非阻塞 I/O、同步与异步 I/O 等。
```bash
# check dentry and inode cache
cat /proc/slabinfo | grep -E '^#|dentry|inode'
# dispaly kernel slab cache real time
slabtop
```

衡量磁盘IO性能, 须要提到五个常见指标，也就是我们经常用到的，使用率、饱和度、IOPS、吞吐量以及响应时间.
在数据库、大量小文件等这类随机读写比较多的场景中，IOPS 更能反映系统的整体性能；而在多媒体等顺序读写较多的场景中，吞吐量才更能反映系统的整体性能。
```bash
# -d: Display the device utilization report
# -x: Display extended statistics
iostat -d -x 2

# process io statistic
pidstat -d 1

# sort by io
iotop

# trace system calls
strace -p <pid>
# -f: show threads system calls
strace -f -p <pid>
# -T: 显示系统调用的时长
# -tt: 显示跟踪时间
strace -f -T -tt -p <pid>

# -t: show threads
# -a: show command
pstree -t -a -p <pid>

# check open files
# note use pid not tid(please use its parent pid)
lsof -p <pid>
```
你可以用 iostat 获得磁盘的 I/O 情况，也可以用 pidstat、iotop 等观察进程的 I/O 情况。
使用率是从时间角度衡量I/O，但是磁盘还可以支持并行写，所以即使使用率100%，有可能还可以接收新的I/O（不饱和）

很有意思的思路:
一般来说，生产系统的应用程序，应该有动态调整日志级别的功能。继续查看源码，你会发现，这个程序也可以调整日志级别。如果你给它发送 SIGUSR1 信号，就可以把日志调整为 INFO 级；发送 SIGUSR2 信号，则会调整为 WARNING 级

在排查应用程序问题时，我们可能需要，在线上环境临时开启应用程序的调试日志。有时候，事后一不小心就忘了调回去。没把线上的日志调高到警告级别，可能会导致 CPU 使用率、磁盘 I/O 等一系列的性能问题，严重时，甚至会影响到同一台服务器上运行的其他应用程序。


```bash
# File reads and writes by filename and process. Top for files
filetop

# Trace open() syscalls. Uses Linux eBPF/bcc
opensnoop
```

MySQL 的 MyISAM 引擎，主要依赖系统缓存加速磁盘 I/O 的访问。可如果系统中还有其他应用同时运行， MyISAM 引擎很难充分利用系统缓存。缓存可能会被其他应用程序占用，甚至被清理掉。所以，不建议把应用程序的性能优化完全建立在系统缓存上。最好能在应用程序的内部分配内存，构建完全自主控制的缓存；或者使用第三方的缓存应用，比如 Memcached、Redis 等。

学习了redis的一些特性和配置。

为了更客观合理地评估优化效果，我们首先应该对磁盘和文件系统进行基准测试，得到文件系统或者磁盘 I/O 的极限性能。fio 文件系统和磁盘 I/O 性能基准测试工具, fio  is  a tool that will spawn a number of threads or processes doing a particular type of I/O action as specified by the user.  The typical use of fio is to write a job file matching the I/O load one wants to simulate:

> Note that 用磁盘路径测试写，会破坏这个磁盘中的文件系统，所以在使用前，你一定要事先做好数据备份。
```bash
# 随机读
fio -name=randread -direct=1 -iodepth=64 -rw=randread -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb

# 随机写
fio -name=randwrite -direct=1 -iodepth=64 -rw=randwrite -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb

# 顺序读
fio -name=read -direct=1 -iodepth=64 -rw=read -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb

# 顺序写
fio -name=write -direct=1 -iodepth=64 -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb 
```
fio 支持 I/O 的重放。借助前面提到过的 blktrace，再配合上 fio，就可以实现对应用程序 I/O 模式的基准测试。你需要先用 blktrace ，记录磁盘设备的 I/O 访问情况；然后使用 fio ，重放 blktrace 的记录。我们就通过 blktrace+fio 的组合使用，得到了应用程序 I/O 模式的基准测试报告.

还谈到了磁盘优化的几个方面，比较深入了，不太理解.
平常没机会从系统层面优化磁盘性能参数。
能做的就是减少磁盘写入，以及错峰操作磁盘。
比如在凌晨或业务低谷时，压缩备份日志，减少对正常业务的影响。

[ ] [If threads share the same PID, how can they be identified?](https://stackoverflow.com/questions/9305992/if-threads-share-the-same-pid-how-can-they-be-identified)
[ ] [Makefile launches docker container](https://github.com/feiskyer/linux-perf-examples/blob/master/mysql-slow/Makefile)
[ ] [nsenter](https://github.com/jpetazzo/nsenter) 这个和docker exec 有关系，和我之前提到的理解错误的docker login 问题有联系. 可以看一下readme. 认真研究一下这个工具.

## Network
本质上是一种进程间通信方式，特别是跨系统的进程间通信，必须要通过网络才能进行。随着高并发、分布式、云计算、微服务等技术的普及，网络的性能也变得越来越重要。

网络接口配置的最大传输单元（MTU），就规定了最大的 IP 包大小。在我们最常用的以太网中，MTU 默认值是 1500（这也是 Linux 的默认值).
```bash
# check virtual NI
ls -l /sys/class/net
```
实际上，我们通常用带宽、吞吐量、延时、PPS（Packet Per Second）等指标衡量网络的性能。除了这些指标，网络的可用性（网络能否正常通信）、并发连接数（TCP 连接数量）、丢包率（丢包百分比）、重传率（重新传输的网络包比例）等也是常用的性能指标。

而对 TCP 或者 Web 服务来说，更多会用并发连接数和每秒请求数（QPS，Query per Second）等指标，

查看网络配置:
```bash
# -s: statistic, show RX and TX
ip -s a s
# multiple -s, show more info
ip -s -s a s
```

查看套接字socket:
```bash
# -l 表示只显示监听套接字
# -t 表示只显示 TCP 套接字
# -n 表示显示数字地址和端口(而不是名字)
# -p 表示显示进程信息
netstat -nlp | head
ss -ltnp | head

# 协议栈protocol stack statistic
netstat -s
ss -s
```

查看throughput and PPS
```bash
# check bandwidth
ethtool eth0 | grep -i speed
# -n: network
# DEV EDEV TCP UDP ICMP, etc
sar -n DEV 1
```
查看delay:
```bash
ping
```
Linux 内核自带的高性能网络测试工具 `pktgen`, 需要加载内核模块. 用来测试PPS.
TCP/UDP 性能测试: `iperf3`.
HTTP 性能: `ab`, `webbench`, 如果需要mock 负载，使用`wrk`.

DNS 不仅方便了人们访问不同的互联网服务，更为很多应用提供了，动态服务发现和全局负载均衡（Global Server Load Balance，GSLB）的机制。这样，DNS 就可以选择离用户最近的 IP 来提供服务。

注意查询IP指定的name server不同，得到的IP也可能不同。

`dig` can show the recurring query steps:
```bash
# +trace: enable trace
# +nodnssec: 表示禁止DNS安全扩展
dig +trace +nodnssec time.geekbang.org
# check time use Queryrime field
dig time.geekbang.org
```

nslookup debug mode, used when lookup failed:
```bash
nslookup -debug <hostname>
time nslookup xxx
```
在应用程序的开发过程中，我们必须考虑到 DNS 解析可能带来的性能问题，掌握常见的优化方法.
碰到dns问题最多的就是劫持，现在公网都是强制https，内部用powerdns(open source).

在实际分析网络性能时，先用 tcpdump 抓包，后用 Wireshark 分析，也是一种常用的方法。
```bash
# 禁止接收从DNS服务器发送过来并包含googleusercontent的包
iptables -I INPUT -p udp --sport 53 -m string --string googleusercontent --algo bm -j DROP

# packet catch
tcpdump -nn udp port 53 or host 35.190.27.188

# -n: forbid ptr
ping -n -c2 google.com

# ptr 查询35.190.27.188的域名
nslookup -type=PTR 35.190.27.188 8.8.8.8
```
实际上，根据 IP 地址反查域名、根据端口号反查协议名称，是很多网络工具默认的行为，而这往往会导致性能工具的工作缓慢.


[ ] [Linux ring buffer with dmesg](https://www.computerhope.com/unix/dmesg.htm)
[ ] [DMA ring](https://stackoverflow.com/questions/47450231/what-is-the-relationship-of-dma-ring-buffer-and-tx-rx-ring-for-a-network-card)
[ ] [conntrack module](https://blog.cloudflare.com/conntrack-tales-one-thousand-and-one-flows/)
[ ] [Diagnose NAT issue](https://mp.weixin.qq.com/s/VYBs8iqf0HsNg9WAxktzYQ)
[ ] [酷壳](https://coolshell.cn/about)
[ ] [tcpdump man](https://www.tcpdump.org/manpages/tcpdump.1.html)
[ ] [wireshark doc](https://www.wireshark.org/docs/)
[ ] [level-triggered vs edge-triggered](https://www.quora.com/What-is-the-difference-between-the-edge-trigger-and-the-level-trigger-in-epoll)
[ ] [thundering herd 惊群](https://www.zhihu.com/question/22756773)
[ ] 不懂的很多。。可以从go network programming开始学习? 网络学习吃力的同学，先去把林沛满老师两本Wireshark分析网络看完
[ ] hping3 a network penetration test tool, security auditiing, fw test

[ ] blog [动态追踪技术漫谈](https://blog.openresty.com.cn/cn/dynamic-tracing/#%E4%BB%80%E4%B9%88%E6%98%AF%E5%8A%A8%E6%80%81%E8%BF%BD%E8%B8%AA)

# From LinkedIn Learning
`time` in shell is a keyword, it is also a command, they have different output.
```bash
yum install -y time

# use time yum installed
# also show major and minor page faults
$(which time) sleep 1
0.00user 0.00system 0:01.00elapsed 0%CPU (0avgtext+0avgdata 644maxresident)k
0inputs+0outputs (0major+205minor)pagefaults 0swaps

# use shell keyword time
time sleep 1
# user and sys is cpu time on user and system space
# user + sys could be larger than real time if your program
# uses multi-cores
real	0m1.002s
user	0m0.000s
sys	0m0.001s
```

If you want to test performance of 2 similar commands, put each in a loop and `time` it as a whole to see the differences, or using `strace -c -o /tmp/result.out ./script` and `head` the first several lines of the result to see what system calls cost much.

`/proc` is mounted at boot time, see mount detail by `mount | grep proc` command. proc files provide kernel info, printing the contents of the proc file causes the corresponding function in the kernel to be called to produce fresh value. we can also write to proc file to change kernal parameters. `/proc` is a pseudo filesystem, so `ls -l` the length may be 0.

`/proc` files under `/proc/sys` represent kernel variables.

## CPU
Packages for performance tools, yum install them and `rpm -ql` to see utilities contained:
- sysstat: iostat, mpstat, nfsiostat-sysstat, pidstat, **sar**
- procps-ng: free, pmap, ps, since, tload, **top**, uptime, vmstat, watch
- perf: `sudo perf record find / -xdev -name core >/dev/null 2>&1; sudo perf report`

主要说了sar, top, cpuinfo 以及scheduling priority and nice value. (具体记录在其他blog中，搜一下关键字)

`Throughput`, important for server, fewer context switch and longer time silces. throughput 和 responsiveness 需要权衡，好的throughput 意味着尽可能run 单一的任务，这样context switch就少. `Responsiveness`, important for interactive or control system, requires quick context switch and shorter time slices and less page faults.

Can configure the kernel to preemptible or not, preemption means context switch.

Linux kernel has 3 choices for the kind of `preemption` it employs.
- None: no preemption
- Voluntary: kernel checks frequently for placement (the most commonly choice)
- Preempt: schedule preempts unless kernel is in a critical section.

## Memory
`/proc/meminfo` file, the `MemAvailable` value is space for program without swapping (includes reclaimable cache and buffer), important.
```yaml
MemTotal:       32779460 kB
MemFree:          239788 kB
MemAvailable:    3996032 kB
Buffers:              20 kB
Cached:          5797312 kB
SwapCached:            0 kB
Active:          4435144 kB
...
```

`htop` command is similar to `top` but with colorful display. From the options you will sort by different categories, e.g. %CPU, $MEM, etc.

`Translation lookaside buffer`(TLB), use for virtual address mapped to physical addresses. Linux supportss having huge pages, can use `sysctl` config at runtime or during bootstrap (/etc/sysctl.d).

`Page faults`, a process uses an address that is not mapped or even not RAM resident. minor or major page faults, minor is not a big deal, major has disk I/O involved, much slower. Linux is on-demand page system. (how linux works 这本书也提到了)


## Disk
`atop` command is also helpful.

后面谈到了如何测试不同filesystem 的performance, 可以用dd 生成大文件用作loop device, 格式化为不用的filesystem，然后mount，随后进行大量的文件或文件夹创建，记录时间，在对文件和文件夹进行操作，记录时间.]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>performance</tag>
      </tags>
  </entry>
  <entry>
    <title>SELinux</title>
    <url>/2019/06/23/linux-selinux/</url>
    <content><![CDATA[
I have got chance to learn something about `SELinux` (Security-Enhanced Linux). This is a online training from O'REILLY.

The Linux operating system was never designed with overall security in mind, and that’s exactly where `SELinux` comes in. Using SELinux adds 21st century security to the Linux operating system. It is key to providing access control and is also an important topic in the Red Hat RHCSA, CompTIA Linux+ and Linux Foundation LFCS exams.

[Security-Enhanced Linux](https://en.wikipedia.org/wiki/Security-Enhanced_Linux)
I am using a `CentOS` machine in this training.

SELinux implements Mandatory Security. All syscalls are **denied** by default,
unless specifically enabled
* All objects (files, ports, processes) are provided with a security label (the
context)
* User, role and type part in the context
* Type part is the most important
* The SELinux policy contains rules where you can see which source context
has access to which target context

To check if SELinux status, dsiabled or enforcing
[Enable SELinux](https://howto.lintel.in/enable-disable-selinux-centos/)

`Z` flag is the magic to show SELinux information
```
ls -Z /boot
netstat -Ztunlp
ps auxZ
```

看到22:00，没来得及看完😂，唉。。。这个topic对于目前的我，有点用不上，晦涩。不过这个配置有时会被特别提起，disable or permissive.











]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Sync Up Files with rsync</title>
    <url>/2019/05/28/linux-rsync/</url>
    <content><![CDATA[
This command is awesome for backup with complicated file structure and frequently modifications. It's more elegant and smart(skip unchanged files) than using portable storage or `scp` to transfer the files.

> Note that `SSHFS` monut may also fit the needs.

# Introduction
Let's see what is `rsync` from [wiki](https://en.wikipedia.org/wiki/Rsync):
`rsync` is a utility for efficiently transferring and synchronizing files between a computer and an external hard drive and across networked computers by comparing the modification times and sizes of files.

`rsync` will use **SSH** to connect. Once connected, it will invoke the remote host's `rsync` and then the two programs will determine what parts of the local file need to be transferred so that the remote file matches the local one.

`rsync` can also operate in a daemon mode, serving and receiving files in the native rsync protocol (using the `rsync://` syntax). Here I only talks SSH way.

[How to Exclude Files and Directories with Rsync](https://linuxize.com/post/how-to-exclude-files-and-directories-with-rsync/)
[Rsync Command in Linux with Examples](https://linuxize.com/post/how-to-use-rsync-for-local-and-remote-data-transfer-and-synchronization/)

# Usage
To get `rsync` working between two hosts, the `rsync` program must be installed on both the source and destination, and you’ll need a way to access one machine from the other.

Copy files to remote home or from remote to local
```bash
rsync files remote:
rsync files user@remote:
rsync user@remote:source dest
```
If `rsync` isn’t in the remote path but is on the system, use `--rsync-path=path` to manually specify its location. Unless you supply extra options, `rsync` copies **only** files. You will see:
```bash
skipping directory xxx
```

To transfer entire directory hierarchies, complete with symbolic links, permissions, modes, and devices, use the `-a` option.
```bash
# -n: dry-run, this is vital when you are not sure.
# -P: show progress bar
# -v: verbose mode
# -z: compress during transfer
# -a: archive mode, equals -rlptgoD
# here rsync a file and a dir
rsync -n -P -vza file dir user@remote:<path>

# -q: quiet
# -e: choose a different remote shell
# for example remote ssh uses a port other than 22
rsync -q -e "ssh -p 2322" file user@remote:<path>
```

To make an exact replica of the source directory, you must delete files in the destination directory that do not exist in the source directory:
```bash
# --delete: delete extraneous files from dest dirs
rsync -v --delete -a dir user@remote:
```
Please use `-n` dry-run to see what will be deleted before performing command.

Be particular careful with tailing slash after dir:
```bash
# dir vs dir/
rsync -a dir/ user@remote:dest
```
This will copy all files under dir to dest folder in remote instead of copy dir into dest.

You can also `--exclude/--include=PATTERN` and `--exclude-from/--include-from=PATTERN_FILE`in command.

To speed operation, `rsync` uses a quick check to determine whether any files on the transfer source are already on the destination. The quick check uses a combination of the file size and its last-modified date.

When the files on the source side are not identical to the files on the destination side, `rsync` transfers the source files and overwrites any files that exist on the remote side. The default behavior may be inadequate, though, because you may need additional reassurance that files are indeed the same before skipping over them in transfers, or you may want to put in some extra safeguards:

* `--checksum`(abbreviation: `-c`) Compute checksums (mostly unique signatures) of the files to see if they’re the same. This consumes additional I/O and CPU resources during transfers, but if you’re dealing with sensitive data or files that often have uniform sizes, this option is a must. (This will focus on file content, not date stamp)

* `--ignore-existing` Doesn’t clobber files already on the target side.

* `--backup` (abbreviation: `-b`) Doesn’t clobber files already on the target but rather renames these existing files by adding a ~ suffix to their names before transferring the new files.

* `--suffix=s` Changes the suffix used with –backup from `~` to `s`.

* `--update` (abbreviation: `-u`) Doesn’t clobber any file on the target that has a later date than the corresponding file on the source.

For example, sync my code repo in local host to remote for testing and developing, after verifying, sync back to local host to check in:
```bash
# forward sync source proj folder itself to dest
# result in remote: /home/chengdol/proj
rsync -vza \
    ./proj \
    remote_user@remote:/home/chengdol

# then coding and editing

# backward sync remote proj folder itself to current directory
# result ./proj
rsync -vza \
    # exclude folder inside remote proj
    --exclude .terraform \
    --exclude output \
    --exclude utils/__pycache__ \
    --exclude deployment/__pycache__ \
    remote_user@remote:/home/chengdol/proj \
    .

# you will see the incremental transferred files, as well as the .git changes
```]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>rsync</tag>
      </tags>
  </entry>
  <entry>
    <title>Special Permission Bits</title>
    <url>/2019/05/11/linux-setuid-sticky-bit/</url>
    <content><![CDATA[
I have seen something like this in permission bits, What do these `s/S` and `t/T` stand for? 
```bash
# s bit
-r-sr-sr-x 1 root     db2iadm1   115555 Mar 21 03:19 db2start
# s/t/T bit
drwxrwsr-t 1 db2inst1 db2iadm1 212 May 10 17:16 sqllib
drwxr-x--T 1 wasadmin dstage    21 Mar 21 03:56 usr
```

# Basic Permission Bits
- `Read` - a readable permission allows the contents of the file to be viewed. A read permission on a directory allows you to list the contents of a directory.
- `Write` - a write permission on a file allows you to modify the contents of that file. For a directory, the write permission allows you to edit the contents of a directory (e.g. add/delete files).
- `Execute` - for a file, the executable permission allows you to run the file and execute a program or script. For a directory, the execute permission allows you to change to a different directory and make it your current working directory.

# Setuid and Setgid Bits
Note that `setuid` and `setgid` have an effect **only** on binary executable files and not on scripts (e.g., Bash, Perl, Python), see [wiki](https://en.wikipedia.org/wiki/Setuid).

Assigning the `setuid` bit to binaries, most often this is given to a few programs owned by the superuser. When an ordinary user runs a program that is setuid root, the program runs with the effective privileges of the superuser. `Linux capabilities` is a great alternative to reduce the usage of setuid.

Capabilities break up root privileges in smaller units, so root access is no longer needed. Most of the binaries that have a setuid flag, can be changed to use capabilities instead.

## Apply on File
When the `setuid` or `setgid` attributes are set on an executable file, then any users able to execute the file will automatically execute the file with the privileges of the file's owner (usually root) and/or the file's group, depending upon the flags set. This may pose potential security risks in some cases and executables should be properly evaluated before set.

For example the `passwd` file:
```bash
# ordinary user can run this binary effectively as root
# this is a binary executable
-rwsr-xr-x. 1 root root 27832 Jan 29  2014 /usr/bin/passwd
```
set setuid on binary executable:
```bash
chmod u+s <executable>
# remove setuid on file:
chmod u-s <executable>
```

This [post](https://unix.stackexchange.com/questions/364/allow-setuid-on-shell-scripts) explains why `setuid/setgid` is usually ignored on shell script.


Another thing is about `ping`, it is a binary executable, for now, it may not have `setuid`:
```bash
-rwxr-xr-x 1 root root 81608 Feb  5 04:37 /usr/bin/ping
```
Instead, it may have capability see [hardening Linux binaries by removing setuid](https://linux-audit.com/linux-capabilities-hardening-linux-binaries-by-removing-setuid/):
```bash
# getcap /usr/bin/ping
/usr/bin/ping cap_net_raw=ep
```
我也总结了一篇关于 Linux capabilities 的博客.

Or not at all, see this[ping without SETUID and Capabilities](https://stackoverflow.com/questions/63177554/linux-why-am-i-able-to-use-ping-if-neither-setuid-nor-capabilities-are-set): Creating (normal) ICMP packets does not require special permissions anymore.

## Apply on Directory
Setting the `setgid` permission on a directory causes new files and subdirectories created within it to inherit its `group ID`, rather than the primary group ID of the user who created the file (the owner ID is **never** affected, only the group ID).

set setgid on directory:
```bash
chmod g+s dir
# remove setgid on directory:
chmod g-s dir
```

The `setuid` permission set on a directory is ignored on most UNIX and Linux systems.However FreeBSD can be configured to interpret `setuid` in a manner similar to setgid, in which case it forces all files and sub-directories created in a directory to be owned by that directory's owner - a simple form of inheritance.

Note that both the `setuid` and `setgit` bits have **no effect** if the executable bit is not set. if executable bit is not set, `s` changes to `S`.

## Chown Removes Setuid
If you run `chown` on a `setuid` script, you will find that the `s` is gone. This is a reasonable design, otherwise the `s` will apply to the new owner, a big security hole.


The `chown` command sometimes clears the set-user-ID or set-group-ID permission bits.  This behavior depends on the policy and functionality of the underlying `chown` system call, which may make system-dependent file mode modifications outside the control of the `chown` command. For example, the `chown` command might not affect those bits when
invoked by a user with appropriate privileges, or when the bits signify some function other than executable permission (e.g., mandatory locking).  When in doubt, check the underlying system behavior.

Note that if you want to have setuid and owner no change when copy, for example last time deal with `sqllib`, please perserve when you do copy like: 
```bash
/bin/cp -rfp /home/dfdcdc/sqllib /tmp
```

# Sticky Bit
When set on a file or directory, the `sticky bit`, or `+t` mode, means that only the `owner` (or `root`) can delete the file (or files under the directory), **regardless of** which users have write access to this file or directory by way of group membership or ownership! This is often used to control access to a shared directory, such as `/tmp`.

This is useful when a file or directory is owned by a group through which a number of users share write access to a given set of files.

Set sticky bit:
```bash
chmod +t script.sh
```
Remove sticky bit, note that to change the sticky bit, you need to be either root or the file owner. The root user will be able to delete files regardless of the status of the sticky bit.
```bash
chmod -t script.sh
```

Sometimes you see `T` instead of `t`, usually `t` sits with all fields `x`, but if the executable bit is not set then the `t` is flagged up as a capital, for example:
```bash
touch file
chmod u=rwx,go=rx file    # "-rwxr-xr-x 1 roaima 0 Sep 10 23:13 file"
chmod +t file             # "-rwxr-xr-t 1 roaima 0 Sep 10 23:13 file"
chmod o-x file            # "-rwxr-xr-T 1 roaima 0 Sep 10 23:13 file"
chmod u=rwx,go=,+t file   # "-rwx-----T 1 roaima 0 Sep 10 23:13 file"
```
Now if a user is not in that group, it cannot even enter the directory.

# Resources
[wiki setuid and setgid](https://en.wikipedia.org/wiki/Setuid#SGID)
[how to set `T` bit](https://unix.stackexchange.com/questions/228925/how-do-you-set-the-t-bit)
[chown remove setuid](https://unix.stackexchange.com/questions/53665/chown-removes-setuid-bit-bug-or-feature)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>setuid</tag>
        <tag>setgid</tag>
        <tag>sticky</tag>
      </tags>
  </entry>
  <entry>
    <title>/proc/$$ and /proc/self</title>
    <url>/2020/02/03/linux-proc-$$-self/</url>
    <content><![CDATA[
`$$` is a special bash variable (special parameter `$` with a preceding expansion mark `$`) that gets expanded to the `pid of the shell`.

`/proc/self` is a real symbolic link to the /proc/ subdirectory of the process that is `currently making the call`.

When you do `ls /proc/$$` the shell expands it to `ls /proc/pid-of-bash` and that is what you see, the contents of the shell process.

But when you do `ls /proc/self` you see the contents of the short lived `ls` process. If you write code which uses `/proc/self` that code will see its own pid, namely the process making the system call with `/proc/self` as part of the pathname in one of its arguments. 

The `$$` is not limited to this usage, you can write echo $$ to see the bash pid; you can use it to kill yourself, etc.
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>sed Command Daily Work Summary</title>
    <url>/2019/05/30/linux-sed-summary/</url>
    <content><![CDATA[
Normally we use a interactive text editor, like `vim`. The command `sed` is one of the most commonly used `command line editor` in Linux world. `sed` stands for the `stream editor`, it uses the roles supplied to edit a stream of data on the fly.

**Note**: Be careful that `sed` will break the softlink and create a file with the same name! for example:
```bash
ln -s /tmp/source.txt /tmp/link.txt
sed -i -e "s#aaa#bbb#" /tmp/link.txt
```
then the softlink is gone, a new file named link.txt is created instead.
so use `readlink` first to get resolved symbolic links, then use `sed` on it.


First you need to understand how `sed` works with text:
1. Reads **one** data line at a time from the input
2. Matches that data with the supplied editor commands
3. Changes data in the stream as specified in the commands
4. Outputs the new data to STDOUT


2个对于输出很有帮助的flags, 可以加在其他功能后面:
```bash
# -n: quiet
# /p: print the edited line
# 这样组合就只会输出改动的部分
sed -n -e 's/root/toor/p' file 
```

## Substitution
I have a text file `sedtxt`:
```
Using Java print hello world and hello tree.
Using Java print hello world and hello tree.
Using Java print hello world and hello tree.
Using Java print hello world and hello tree.
```

If I want to substite all `hello` with `hi` in-place:
```bash
sed -i -e 's/hello/hi/g' sedtxt
```
`-i`: does in-place substitution in file `sedtxt`, create backup file automatically by using `-i.bak`
`-e`: followed by commands
`s/x/y/flags`: substitute option, `/` is the delimiter, can be other chars; `g` represents that replace in all occurrences (global).

Note that if not set `g`, it will replace first occurrence in **each** line, what if I want to replace the first occurrence in a file, the workaround could be limit the scaning range:
```bash
sed '0,/Apple/{s/Apple/Banana/}' input_filename
```
the explain see [here](https://stackoverflow.com/questions/148451/how-to-use-sed-to-replace-only-the-first-occurrence-in-a-file)

If I want to substitute the **second** `hello` with `goodbye` in each line:
```bash
sed -e 's/hello/goodbye/2' sedtxt
```
```
Using Java print hello world and goodbye tree.
Using Java print hello world and goodbye tree.
Using Java print hello world and goodbye tree.
Using Java print hello world and goodbye tree.
```

If I want to substitute `hello` with `hi` and `Java` with `Python`:
```bash
sed -e 's/hello/hi/g' -e 's/Java/Pyhton/g' sedtxt
# or
sed -e 's/hello/hi/g; s/Java/Python/g' sedtxt
```
```
Using Pyhton print hi world and hi tree.
Using Pyhton print hi world and hi tree.
Using Pyhton print hi world and hi tree.
Using Pyhton print hi world and hi tree.
```

If I want to print only the matching lines, convenient for debugging:
```bash
sed -n -e 's/hello/hi/gp' sedtxt
```
```
Using Java print hi world and hi tree.
Using Java print hi world and hi tree.
Using Java print hi world and hi tree.
Using Java print hi world and hi tree.
```
`-n`: quiet output
`p`: substitute flag to print matching line

> Note, combine with `grep` to debug is good

### Using Address
The `sed` editor assigns the first line in the text stream as line number 1 and continues sequentially for each new line.

only replace 2rd line:
```bash
sed -e '2s/hello/hi/g' sedtxt
```
```bash
Using Java print hello world and hello tree.
Using Java print hi world and hi tree.
Using Java print hello world and hello tree.
Using Java print hello world and hello tree.
```

range substitution, `'1,$s/hello/hi/g'` means from top to bottom.
```bash
sed -e '2,3s/hello/hi/g' sedtxt
```
```
Using Java print hello world and hello tree.
Using Java print hi world and hi tree.
Using Java print hi world and hi tree.
Using Java print hello world and hello tree.
```

can also use text pattern to filter lines, this will apply one line contains `print` word.
```bash
sed -e '/print/s#and#or#' sedtxt
```
```
Using Java print hello world or hello tree.
Using Java print hello world or hello tree.
Using Java print hello world or hello tree.
Using Java print hello world or hello tree.
```

## Deletion
[Delete consecutive lines after match](https://stackoverflow.com/questions/38497896/delete-4-consecutive-lines-after-a-match-in-a-file)

I have a text file `seddel`:
```
The 1st line is 1
The 2rd line is 2
The 3rd line is 3
The 4th line is 4
```

Delete line 2 to end:
```bash
sed -e '2,$d' seddel
```
```
The 1st line is 1
```
can also use pattern matching, delete `3rd`
```bash
sed -e '/3rd/d' seddel
```
```
The 1st line is 1
The 2rd line is 2
The 4th line is 4
```

You can combine 2 address syntax, this will start from first line and until match `3rd`, replace `3rd` in the range with `NAN`:
```bash
sed -e '1,/3rd/{s/3rd/NAN/g}' seddel
```
```
The 1st line is 1
The 2rd line is 2
The NAN line is 3
The 4th line is 4
```

Delete commented and empty line
```bash
sed -e '/^#/d; /^$/d' <file>
```

## Insertion and Appending
The insert command (`i`) adds a new line **before** the specified line.
The append command (`a`) adds a new line **after** the specified line.

I have a text file `sedins`:
```
The 1st line is 1
The 2rd line is 2
The 3rd line is 3
The 4th line is 4
```

Insert at first line:
```bash
sed -e '1iNew line coming!' sedins
```
```
New line coming!
The 1st line is 1
The 2rd line is 2
The 3rd line is 3
The 4th line is 4
```

Append at 3rd line:
```bash
sed -e '2aNew line coming!' sedins
# using regexp
sed -e '/The 2rd line is 2/a New line coming!' sedins
```
```
The 1st line is 1
The 2rd line is 2
New line coming!
The 3rd line is 3
The 4th line is 4
```
### Insert with white spaces
For example, When developing non-root, I want to add `runAsUser: 1000` right after `securityContext:` with correct alignment:
```bash
sed -i -e '/securityContext/a\         runAsUser: 1000' xxx.yml
```
Only to escape the first space. `sed` can automatically recognize the rest of the spaces.
```
...
       securityContext:
         runAsUser: 1000
         privileged: false
...
```

## Changing
The change command allows you to change the contents of an entire line of text in the data stream.

I have a text file `sedch`:
```
The 1st line is 1
The 2rd line is 2
The 3rd line is 3
The 4th line is 4
```

change the second line:
```bash
sed -e '2cNONE' sedch
## or
sed -e '/2rd/cNONE' sedch
```
```
The 1st line is 1
NONE
The 3rd line is 3
The 4th line is 4
```

## Transforming chars
The transform command (`y`) is the only sed editor command that operates on a single character. 

I have a text file `sedtrans`:
```
The 1st line is 1
The 2rd line is 2
The 3rd line is 3
The 4th line is 4
```

The transform command performs a one-to-one mapping of the inchars and the outchars values.
```bash
sed -e 'y/1234/5678/' sedtrans
```
```
The 5st line is 5
The 6rd line is 6
The 7rd line is 7
The 8th line is 8
```

The transform command is a **global** command; that is, it performs the transformation on any character found in the text line automatically, without regard to the occurrence.
You can't limit the transformation to a specific occurrence of the character.

## sed files in directory and subdirectories recursively
Actually we can find all files by `find` then exec `sed`, see this [post](https://stackoverflow.com/questions/6758963/find-and-replace-with-sed-in-directory-and-sub-directories):
```bash
find <dir> -type f -name "*sh" -exec sed -i -e "s|${old}|${new}|g" {} \;
```
> Note that `-type f` is necessary, otherwise will pass directory name to `sed`.]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>sed</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Signals and Trap</title>
    <url>/2019/08/13/linux-signal-trap/</url>
    <content><![CDATA[
When we design a large, complicated script, it is important to consider what happens if the user logs off or shuts down the computer while the script is running. When such an event occurs, a signal will be sent to all affected processes. In turn, the programs representing those processes can perform actions to ensure a proper and orderly termination of the program.

Signals are software interrupts sent to a program to indicate that an important event has occurred. The events can vary from user requests to illegal memory access errors.

To list the signal supported on Linux system:
```bash
## list all signal and corresponding number
kill -l

 1) SIGHUP	 2) SIGINT	 3) SIGQUIT	 4) SIGILL	 5) SIGTRAP
 6) SIGABRT	 7) SIGBUS	 8) SIGFPE	 9) SIGKILL	10) SIGUSR1
11) SIGSEGV	12) SIGUSR2	13) SIGPIPE	14) SIGALRM	15) SIGTERM
16) SIGSTKFLT	17) SIGCHLD	18) SIGCONT	19) SIGSTOP	20) SIGTSTP
21) SIGTTIN	22) SIGTTOU	23) SIGURG	24) SIGXCPU	25) SIGXFSZ
26) SIGVTALRM	27) SIGPROF	28) SIGWINCH	29) SIGIO	30) SIGPWR
31) SIGSYS	34) SIGRTMIN	35) SIGRTMIN+1	36) SIGRTMIN+2	37) SIGRTMIN+3
38) SIGRTMIN+4	39) SIGRTMIN+5	40) SIGRTMIN+6	41) SIGRTMIN+7	42) SIGRTMIN+8
43) SIGRTMIN+9	44) SIGRTMIN+10	45) SIGRTMIN+11	46) SIGRTMIN+12	47) SIGRTMIN+13
48) SIGRTMIN+14	49) SIGRTMIN+15	50) SIGRTMAX-14	51) SIGRTMAX-13	52) SIGRTMAX-12
53) SIGRTMAX-11	54) SIGRTMAX-10	55) SIGRTMAX-9	56) SIGRTMAX-8	57) SIGRTMAX-7
58) SIGRTMAX-6	59) SIGRTMAX-5	60) SIGRTMAX-4	61) SIGRTMAX-3	62) SIGRTMAX-2
63) SIGRTMAX-1	64) SIGRTMAX

## output signal name or number
kill -l 9
kill -l usr1
```

Kill process in command line:
```bash
kill -9 <pid>
kill -s 9 <pid>
kill -SIGKILL <pid>
```

Some commonly use signals:
- **SIGHUP**: Many daemons will `reload` their configuration instead of exiting when receiving this signal, see here for [explanation](https://stackoverflow.com/questions/19052354/sighup-for-reloading-configuration)
- **SIGINT**: Issued if the user sends an interrupt signal (Ctrl + C), usually will terminate a program
- **SIGQUIT**: Issued if the user sends a quit signal (Ctrl + D), it will dump core
- **SIGKILL**: If a process gets this signal it must quit immediately and will not perform any clean-up operations, this one cannot be ignored or trapped! (这里提一下，force kill会导致程序，比如Python中finally clauses 和 exit handler 无法工作)
- **SIGTERM**: Software termination signal (sent by kill by default)
- **SIGSTOP**: Stop process (Ctrl + Z), for example, stop fg mode, see SIGCONT below
- **SIGCONT**: Continue if stopped, for example, after stop fg mode by (Ctrl + Z), switch to bg will trigger this signal
- **SIGSTP**: Stop typed at terminal
- **SIGCHLD**: When a child process stops or terminates, SIGCHLD is sent to the parent process. (这个可以用来trap回收subprocess的资源)

For signal default behavior, see `man 7 signal`!!

> Note that process cannot define handler for `SIGKILL` and `SIGSTOP`, the default behavior is in use.

> Note that non-init process cannot ignore `SIGKILL` and `SIGSTOP`, they are used for root user and kernel for process management. But, remember, process in `D` state is uninterruptable even with `SIGKILL` and `SIGSTOP`.

> Note that you cannot kill init process(PID 1) by `kill -9 1`, this is the exception as described in `man 2 kill`: "The only signals that can be sent to process ID 1, the init process, are those for which init has explicitly installed signal handlers. This is done to assure the system is not brought down accidentally." See here for [hint](https://www.quora.com/Is-it-possible-to-kill-the-init-process-in-Linux-by-the-kill-9-command).

About trap the signals in script, for example:
```bash
## Set a Trap for Signals for graceful shutdown
declare -a SIGNALS_TRAPPED=(INT TERM)
shudown_hook() {
  ...
  (/opt/servers/stop.sh)
  echo "Shutdown and Cleanup Successful..."
  exit 0
}
trap 'shudown_hook' " ${SIGNALS_TRAPPED[@]}"
```
The general format is `trap command signals`, for example:
```bash
trap "rm -rf /tmp/peek.txt; exit 0" 1 2
```

To ignore signals, for example:
```bash
trap "" 2 3 15
```

If main process ignore a signal, all subshells also ignore that signal. However, if you specify an action to be taken on the receipt of a signal, all subshells will still take the default action on receipt of that signal.

Reset the traps to default:
```bash
trap 2 3 15
```
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>signal</tag>
      </tags>
  </entry>
  <entry>
    <title>Symbolic Link Operations</title>
    <url>/2019/04/13/linux-soft-link/</url>
    <content><![CDATA[
首先了解softlink 解决了hardlink什么问题:
- Hard links cannot span physical devices or partitions.
- Hard links cannot reference directories, only files.

Same hard links are spotted by inode value, if a file has mutlipel hard links, then delete all of them in any order would delete the file. You can use `stat <file>` to check inode and hard links if a file.

A symbolic link, also known as a **symlink** or a **soft link**, is a special kind of file (entry) that points to the actual file or directory on a disk (like a shortcut in Windows).

Symbolic links are used all the time to link libraries and often used to link files and folders on mounted NFS (Network File System) shares.

Generally, the `ln` syntax is similar to the `cp` or `mv` syntax:
```bash
cp source destination
ln -s source destination
```
But if the destination is an directory, soft link will be created inside destination directory.

# Create Softlink
For example, create a symbolic link to a file and directory:
`-s`: soft link
`-n`: treat LINK_NAME as a normal file if it is a symbolic link to a directory. 意思是如果LINK_NAME存在并且是一个文件夹softlink，那么还是当做一个普通文件对待，这样就不会再文件夹softlink内部去构造软连接了，一般配合`-f`使用去replace已经存在的软连接。
`-f`: remove existing link, otherwise if the link is exist, get error like this:
```bash
# you need to remove the link first
# but with -f, no need
ln: failed to create symbolic link ‘xxxx’: File exists
```

Link path can be absolute or relative, relative is more desirable since the path may be changed.
Softlink size is the length of the link path, see in `ls -l`:
```bash 
# link a file
ln -nfs <path to file> <path to link>
# link a directory
ln -nfs <path to dir> <path to link>
```

# Delete Softlink
There are 2 ways to undo or delete the soft link:
```bash
unlink <link name>
rm [-rf] <link name>
```
Note that, the `rm` command just removes the link. It will not delete the origin.

# Broken Softlink
Find broken symbolic link:
```bash
find . -xtype l
```
If you want to delete in one go:
```bash
find . -xtype l -delete
```
A bit of explanation:
`-xtype l` tests for links that are broken (it is the opposite of `-type`)
`-delete` deletes the files directly, no need for further bothering with `xargs` or `-exec`

# Ownership of Softlink
[How to change ownership from symbolic links?](https://unix.stackexchange.com/questions/218557/how-to-change-ownership-from-symbolic-links)
[Change permissions for a symbolic link](https://unix.stackexchange.com/questions/87200/change-permissions-for-a-symbolic-link)

On most system, softlink ownership does not matter, usually the link permission is `777`, for example: `lrwxrwxrwx`, when using, the softlink origin's permission will be checked.

How to change the ownership or permission of softlink, `chown/chmod -h <link>`, if no `-h`, the chown/chmod will change the ownership of the link origin.

# Resources:
[SymLink – HowTo: Create a Symbolic Link – Linux](https://www.shellhacks.com/symlink-create-symbolic-link-linux/)
[How can I find broken symlinks](https://unix.stackexchange.com/questions/34248/how-can-i-find-broken-symlinks)
[How to delete broken symlinks in one go?](https://unix.stackexchange.com/questions/314974/how-to-delete-broken-symlinks-in-one-go)


]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>soft link</tag>
      </tags>
  </entry>
  <entry>
    <title>Solve Conflicts in RPM installation</title>
    <url>/2019/03/02/linux-solve-rpm-conflicts/</url>
    <content><![CDATA[
### Problem

I want to offline install some rpms for an application, I put all dependencies for that application in a dedicated directory. The problem is it will cause conflicts with the old installed ones, I also want to **keep old existing** rpms because they may needed by other packages. For example, I offline install `bind-utils` use command:
```bash
yum --disablerepo=* install -y ./bind-utils/*.rpm
```
Error output:
```bash
...
Error: Package: 1:openssl-1.0.2k-12.el7.x86_64 (@anaconda/7.5)
           Requires: openssl-libs(x86-64) = 1:1.0.2k-12.el7
           Removing: 1:openssl-libs-1.0.2k-12.el7.x86_64 (@anaconda/7.5)
               openssl-libs(x86-64) = 1:1.0.2k-12.el7
           Updated By: 1:openssl-libs-1.0.2k-16.el7.x86_64 (/openssl-libs-1.0.2k-16.el7.x86_64)
               openssl-libs(x86-64) = 1:1.0.2k-16.el7
...
You could try using --skip-broken to work around the problem
** Found 1 pre-existing rpmdb problem(s), 'yum check' output follows:
mokutil-15-1.el7.x86_64 is a duplicate with mokutil-12-1.el7.x86_64
```
This error shows that `yum` try to update old rpm with new one but this breaks the dependency chain. Option `--skip-broken` won't work here, it will skip the dependency-problem rpm which include exactly what I need:
```
# skipped
bind-utils.x86_64 32:9.9.4-73.el7_6
```
Then I try to use:
```bash
rpm -ivh ./bind-utils/*.rpm
```
still bad with conflicts:
```
...
file /usr/lib64/openssl/engines/libcapi.so from install of openssl-libs-1:1.0.2k-16.el7.x86_64 conflicts with file from package openssl-libs-1:1.0.2k-12.el7.x86_64
...
```


### Solution

After doing research I find some `rpm` options may help:
```
rpm {-i|--install} [install-options] PACKAGE_FILE ...

       This installs a new package.

       The general form of an rpm upgrade command is

rpm {-U|--upgrade} [install-options] PACKAGE_FILE ...

       This upgrades or installs the package currently installed to a newer version.  This is the same as  install,
       except all other version(s) of the package are removed after the new package is installed.

rpm {-F|--freshen} [install-options] PACKAGE_FILE ...

       This will upgrade packages, but only ones for which an earlier version is installed.
...
--force
              Same as using --replacepkgs, --replacefiles, and --oldpackage.
--replacepkgs
              Install the packages even if some of them are already installed on this system.
--replacefiles
              Install the packages even if they replace files from other, already installed, packages.
--oldpackage
              Allow an upgrade to replace a newer package with an older one.

```
Let's add `--force` flag and try again, this works and the old rpms are still there:
```bash
rpm --force -ivh ./bind-utils/*.rpm
```
```bash
rpm -qa | grep openssl-libs
openssl-libs-1.0.2k-12.el7.x86_64
openssl-libs-1.0.2k-16.el7.x86_64
```
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>rpm</tag>
      </tags>
  </entry>
  <entry>
    <title>Grant User sudo Privilege</title>
    <url>/2019/07/15/linux-sudo-privilege/</url>
    <content><![CDATA[
Let's see how to grant a regular user `sudo` privilege, this is summaried from process to use regular user instead of root. Use of `sudo` does not require access to the superuser’s password. If Authenticating, using sudo requires the user’s own password. 

Say, there is a regular user named `guest`, the simplest way is to edit sudoers file solely, run as root user:
```bash
visudo
```
Append this line after root user field
```bash
# root    ALL=(ALL)       ALL
# 如果需要仅仅开放某些命令，这里可以进行调整
guest ALL=(ALL)       ALL
```
Then user `guest` can run `sudo` free.

Another way is adding `guest` user to `wheel` group then give this special group password-less `sudo` privilege.

why uses `wheel`? Because sometimes I also need grant `su` password-less to the user, `wheel` group is easy for both. See my blog [`<<Linux PAM Module Configuration>>`](https://chengdol.github.io/2019/04/01/linux-pam/)

Run as root user, add `guest` to `wheel` group:
```bash
usermod -a -G wheel guest
```
Then edit sudoers file
```bash
visudo
```
Comment and uncomment like this:
```bash
## Allows people in group wheel to run all commands
#%wheel ALL=(ALL)       ALL

## Same thing without a password
%wheel  ALL=(ALL)       NOPASSWD: ALL
```]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Setup SSH Pub Key Authentication</title>
    <url>/2019/10/30/linux-ssh-passwordless-setup/</url>
    <content><![CDATA[
More information about ssh and scp can refer this [post](https://chengdol.github.io/2019/02/21/linux-ssh-scp-summary/).

This is about how to set up SSH public key authentication, after that you will not prompt to input password for ssh connection.

We need to setup ssh passwordless in softlayer cluster, otherwise our Datastage installer wouldn't work. Now the master node in cluster uses `/ibm/unicorn_rsa` as the key to ssh, we can generate a new key and utilize it to communicate.

```bash
## "yes" will overwrite existing rsa key
## -t specify the type of key to create
## -N provides the new passphrase
## -f specifies the filename of the key file
echo "yes" | ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa

## then append the id_rsa.pub content to authorized_keys in each node
declare -a nodes=($(cat /etc/hosts | grep -i ibmcloud | awk {'print $2'}))
key=$(cat ~/.ssh/id_rsa.pub)
for node in "${nodes[@]}"
do
  echo "[INFO] copy ssh public to ${node}"
  ssh -i /ibm/unicorn_rsa -o StrictHostKeyChecking=no ${node} "echo ${key} >> ~/.ssh/authorized_keys"
done
```

Notice that:
1. the `~/.ssh/authorized_keys` permission on target machine should be `644` or `600` and file owner should be the right user
2. public key authentication on target machine must be allowed `PubKeyAuthentication yes`



]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Storage System</title>
    <url>/2020/12/25/linux-storage/</url>
    <content><![CDATA[
[x] ordinary file can also be formatted and mounted
[x] terraform mount disk `/dev/sdb`, why this name?
[x] do experiment, using vagrant mount extra disk
[x] blkid, lsblk 使用场景
[x] fstab mount 设置

Vagrant demo please see [vagrant-storage](https://github.com/chengdol/InfraTree/tree/master/vagrant-storage). This is for VirtualBox provider, at the time of writting this is a experimental [feature](https://www.vagrantup.com/docs/disks/usage).

After launch the vagrant machine:
```bash
# up
VAGRANT_EXPERIMENTAL="disks" vagrant up
# down
vagrant destroy -f
```

```bash
# check block device, their name, type, size and mount point
# sdb and sdc are the additional disks added in Vagrantfile
lsblk
NAME                    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
sda                       8:0    0  40G  0 disk
├─sda1                    8:1    0   1G  0 part /boot
└─sda2                    8:2    0  31G  0 part
  ├─centos_centos7-root 253:0    0  29G  0 lvm  /
  └─centos_centos7-swap 253:1    0   2G  0 lvm  [SWAP]
sdb                       8:16   0   2G  0 disk
sdc                       8:32   0   2G  0 disk

# /dev/sda is not used up, let's add one more partition of 3GB
fdisk /dev/sda
Command (m for help): n
Partition type:
   # we have sda1 sda2 2 primart already
   p   primary (2 primary, 0 extended, 2 free)
   e   extended
Select (default p):
Using default response p
Partition number (3,4, default 3):
First sector (67108864-83886079, default 67108864):
Using default value 67108864
Last sector, +sectors or +size{K,M,G} (67108864-83886079, default 83886079): +3GB
Partition 3 of type Linux and of size 2.8 GiB is set

Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.

WARNING: Re-reading the partition table failed with error 16: Device or resource busy.
The kernel still uses the old table. The new table will be used at
the next reboot or after you run partprobe(8) or kpartx(8)
Syncing disks.

# re-read partition table for /dev/sda
partprobe -s /dev/sda

# now see lsblk, sda3
NAME                    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                       8:0    0   40G  0 disk
├─sda1                    8:1    0    1G  0 part /boot
├─sda2                    8:2    0   31G  0 part
│ ├─centos_centos7-root 253:0    0   29G  0 lvm  /
│ └─centos_centos7-swap 253:1    0    2G  0 lvm  [SWAP]
└─sda3                    8:3    0  2.8G  0 part
sdb                       8:16   0    2G  0 disk
sdc                       8:32   0    2G  0 disk

# format sda3
mkfs -t ext4 /dev/sda3

# see type
blkid | grep sda3
/dev/sda3: UUID="7d4a365c-1639-41de-a7c7-ebe79ea2830c" TYPE="ext4"

# mount to /data3 folder
mkdir /data3
mount /dev/sda3 /data3
mount | grep sda3
# check lsblk
# now see lsblk
NAME                    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                       8:0    0   40G  0 disk
├─sda1                    8:1    0    1G  0 part /boot
├─sda2                    8:2    0   31G  0 part
│ ├─centos_centos7-root 253:0    0   29G  0 lvm  /
│ └─centos_centos7-swap 253:1    0    2G  0 lvm  [SWAP]
└─sda3                    8:3    0  2.8G  0 part /data3
sdb                       8:16   0    2G  0 disk
sdc                       8:32   0    2G  0 disk

# check space and inode usage
cd /data3
df -k .
df -i .
```
If you copy a big file to `/data3`, its use% may not change because flush does not happen, run `sync` to flush file system buffer.

```bash
# format whole disk sdb without partition
# use btrfs and mount with compress
mkfs -t btrfs /dev/sdb

mkdir /datab
mount /dev/sdb -o compress /datab

mount | grep sdb
/dev/sdb on /datab type btrfs (rw,relatime,seclabel,compress=zlib,space_cache,subvolid=5,subvol=/)

# check lsblk
NAME                    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                       8:0    0   40G  0 disk
├─sda1                    8:1    0    1G  0 part /boot
├─sda2                    8:2    0   31G  0 part
│ ├─centos_centos7-root 253:0    0   29G  0 lvm  /
│ └─centos_centos7-swap 253:1    0    2G  0 lvm  [SWAP]
└─sda3                    8:3    0  2.8G  0 part /data3
sdb                       8:16   0    2G  0 disk /datab
sdc                       8:32   0    2G  0 disk
```
For creating logical volumes, please see LVM section.
For creating loop device, please see Loop Device section.


From Linkedin Learn [Linux: Storage Systems](https://www.linkedin.com/learning/linux-storage-systems?trk=course_title&upsellOrderOrigin=default_guest_learning).

首先需要理解加入一个新的disk到系统之后，发生了什么，以及需要做什么工作才能使用这个新加入的disk。可以参考一下[这篇文章](https://www.techotopia.com/index.php/Adding_a_New_Disk_Drive_to_an_Ubuntu_Linux_System). 主要用到了`fdisk`(partition), `mkfs`(make filesystem), `mount or fstab`(make accessable)

新加入的disk 的device file 如何命名的, Name conventions of device file, see [here](https://en.wikipedia.org/wiki/Device_file#Naming_conventions), 比如`/dev/sda`, `/dev/sdb`, etc.

总的来看，可以用这样的顺序去观察block storage system:
- lsblk, blkid 查看block device的大致状态，filesystem, disk, partition, mount point等
- /etc/fstab 查看是否persistent 以及 mount option 是否合适
- mount 查看一下defaults mount option 具体内容是什么
- df -k/-i 查看mount point space使用情况
- dd 测试I/O performance (或者结合iperf3如果是NFS 之类的分布式存储)

# Partition
`lsblk` and `blkid` are used to identify block storages (linux also has character device).
有一点要注意，使用`lsblk`的时候，比如:
```js
NAME                    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
sda                       8:0    0  32G  0 disk
├─sda1                    8:1    0   1G  0 part /boot
└─sda2                    8:2    0  31G  0 part
  ├─centos_centos7-root 253:0    0  29G  0 lvm  /
  └─centos_centos7-swap 253:1    0   2G  0 lvm  [SWAP]
```
区分`TYPE`的不同类型(也可以从缩进结构看出), `MOUNTPOINT`中有值的部分才会在`mount` command中显示。在`/etc/fstab`中也会看到对应的entries.

Check storage partition:
```bash
# list partitions and see type of a disk
# `p` to see part 
# `n` to add new part
# `d` to delete part
# `w` to confirm
# `q` to quit
# /dev/sda is disk type
fdisk /dev/sda

# or
ls /dev/sda*
```

# Formating
Formatting partition or disk is to make a filesystem on it.
```bash
# format as ext4 type to /dev/sdb1 partition
# -t: type
# -f: force reformat
mkfs -t ext4 -f /dev/sdb1
mkfs -t xfs -f /dev/sdb1
```

# Mounting
Mounting is associating a filesystem with a directory, mornally we would mount an empty directory, otherwise the content in existing directory will be hidden.
```bash
mount /dev/sdb1 /data

# non-disk filesystem, system will do special mounts for you
# -t: type
mount -t proc proc /proc
mount -t sysfs sysfs /sys
mount -t debugfs debugfs /sys/kernel/debug
# NFS
mount server:dir /nfs-data
```

The persistent mounts are in `/etc/fstab` file, If you type `mount` command on shell, you will see several mount points, like above `proc`, `sysfs`, `debugfs`. These are special mounts not in `/etc/fstab`, `systemd` mounts them on boot automatically.

For the mount options specific to file system type, see man page `fstab` and `mount`.

You can unmount by `umount` command, filesystem cannot be unmounted while in use, for example files are open, process has dir in it. Can check by `lsof` command.
```bash
umount /data
```


# Filesystem Types
Commonly used ones: `ext2/3/4`, `xfs`, `btrfs`. they have different properties.
```bash
man 5 filesystemd
man 5 ext4
man 5 xfs
man 5 btrfs
```

Note that sometime you cannot create file because of no space left on device, but when you check `df -k .`, it is not full, check `df -i .` may help, you may use up the inode capacity even if there are lots of space remain.


# LVM
logical volume manager, a layer above physical partitions that allows multiple partitions to appear as one. Provides for growing a filesystem by adding physical space, can stripe and mirror.

There are 3 levels of abstraction, one onto another:
- physical volumes: disk or disk partitions
- volume groups: collections of physical volumes
- logical volumes: partitioning up a volume group

Assume we have new disk partitions /dev/sdc1, /dev/sdc2, /dev/sdc3:
```bash
lvm
# show all physical volume
> pvs -a
# find lv on physical volume 
> pvck /dev/sda2
# show logical volume group
> vgs

# create physical volume
pvcreate /dev/sdc1
pvcreate /dev/sdc2
pvcreate /dev/sdc3

# list physical volume
# you can see volume group name
pvscan

# display detail
pvdisplay /dev/sdc1
```

Let's create volume group:
```bash
# -c n: cluster no
# vg1: group name
vgcreate -c n vg1 /dev/sdc1 /dev/sdc2
pvscan
# add /dev/sdc3 to group
vgextend vg1 /dev/sdc3
pvscan
# remove unused pv
vgreduce vg1 /dev/sdc3
pvscan
# remove group
vgremove vg1
pvscan
```

Then create logical volume:
```bash
# create a logical volume 600M from group vg1
# -L: size
# -n: lv name
lvcreate -L 600M -n apple vg1
# format and mount it
mkfs -t ext4 /dev/vg1/apple
mkdir /apple && mount /dev/vg1/apple /apple
```
```js
// apple will across 2 physocal volume
sdc                       8:32   0    2G  0 disk
├─sdc1                    8:33   0  500M  0 part
│ └─vg1-apple           253:2    0  600M  0 lvm  /apple
├─sdc2                    8:34   0  500M  0 part
│ └─vg1-apple           253:2    0  600M  0 lvm  /apple
└─sdc3                    8:35   0  500M  0 part
```
```bash
umount /apple
# extend
# 注意不能超过所在group的总大小
# it will also extend filesystem automatically
lvextend -L +200M -r /dev/vg1/apple

# shrink, first resuze filesystem
# then lv
fsadm -e resize /dev/vg1/apple 300M
lvresize --size 300M /dev/vg1/apple 

# remove and reduce for logical volume
lvremove
lvreduce
```

现在就明白了，在Vagrantfile demo中，最开始已经有了2个logical volume: root and swap:
```js
NAME                    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                       8:0    0   40G  0 disk
├─sda1                    8:1    0    1G  0 part /boot
├─sda2                    8:2    0   31G  0 part
│ ├─centos_centos7-root 253:0    0   29G  0 lvm  /
│ └─centos_centos7-swap 253:1    0    2G  0 lvm  [SWAP]
```
可以查看这2个lv 的信息, 这表示/dev/sda2是一个physical volume，centos_centos7 是volume group name.
对于logical volume的每一个抽象层，都有对应的command:
```bash
# show physical volume info
pvdisplay
# show lv info
lvdisplay
# show volume group info
vgdisplay
```

# Swapping
Swapping 对于很多应用和服务都不太好，影响性能，所以注意是否需要关闭。
Usually swapping is set in `/etc/fstab` file, for example, in Vagrant machine:
```js
// as mentioned, /dev/mapper/centos_centos7-swap
// is a logical volume
/dev/mapper/centos_centos7-swap swap                    swap    defaults        0 0
```
To disable it, just comment out and run `swapoff /dev/mapper/centos_centos7-swap` or `swapoff -a`

A partition or file can be configured as swap space:
```bash
# a partition
mkswap /dev/sdc2
# on and off, not persistent
swapon /dev/sdc2
swapoff /dev/sdc2

# check swap components
swapon -s
```

Uses a file as swap space (loop device也是类似的情况)
```bash
# generate 1G file
# or using truncate or fallocate
dd if=/dev/zero of=/tmp/myswap bs=1G count=1
chown root:root /tmp/myswap
chmod 0600 /tmp/myswap
# enable swap
mkswap /tmp/myswap
swapon /tmp/myswap
# off
swapoff /tmp/myswap
```
Then if you check `free -h`, swap space gets extended 1G.


# Loop Device
这里不得不说到[Pseudo-devices](https://en.wikipedia.org/wiki/Device_file#Pseudo-devices), 也就是常用的`/dev/null`, `/dev/zero`, `/dev/full`, `/dev/random`, etc. 它们都是character based devices.

[What is loop device wiki](https://en.wikipedia.org/wiki/Loop_device), is a pseudo-device that makes a file accessible as block device. files of this kind are often used for CD, ISO images, etc. So after mounting them you can access the content.

In my blog `<<What's New in CentOS 8>>`, I have mentioned the commands:
```bash
# shrink or extend file to specific size
# much faster than 'dd' command
truncate -s 1g /tmp/loop.img
# you can make partition on it or skip this step
# for example, make 2 partitions 200M and 400M each
fdisk /tmp/loop.img

# create loopback device from a file, will associate file with /dev/loop0
# -f: find unused loop device
# -P: force kernel to scan partition table on newly create loop device
losetup -fP /tmp/loop.img
# now from lsblk, you will see the partitions under /dev/loop0

# format
mkfs -t ext4 /dev/loop0p1
mkfs -t xfs /dev/loop0p2

# mount it
mkdir /loop{0,1}
mount /dev/loop0p1 /loop0
mount /dev/loop0p2 /loop1
mount | grep loop0
# unmount 
umount /loop0
umount /loop1
```

Or without losetup command:
```bash
truncate -s 1g /tmp/loop.img2
mkdir /loop2
# must first format the file
mkfs -t xfs /tmp/loop.img2
# -o: option is loop
# asociate /dev/loop1 to the file
# no need losetup command
mount -o loop /tmp/loop.img2 /loop2

# unmount 
umount /loop2
```

So let's see
```js
// see first unused loop device
// losetup -f
/dev/loop2

// losetup -a
/dev/loop0: [64768]:16777282 (/tmp/loop.img)
/dev/loop1: [64768]:16777842 (/tmp/loop.img2)

// lsblk, only show loop part
NAME                    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
loop0                     7:0    0    1G  0 loop
├─loop0p1               259:0    0  200M  0 loop /loop0
└─loop0p2               259:1    0  400M  0 loop /loop1
loop1                     7:1    0    1G  0 loop /loop2

// blkid
/dev/loop0p1: UUID="293db313-904b-40a7-9e05-de2ea6f7e12a" TYPE="ext4"
/dev/loop0p2: UUID="2d382bc4-8323-45d1-927b-17bbd1e8880d" TYPE="xfs"
/dev/loop0: PTTYPE="dos"
/dev/loop1: UUID="40282d5f-1d4e-495c-a480-78470237f8e2" TYPE="xfs"
```

# RAID Partitioning
Here is software RAID, combining multiple disks to improve performance and/or reliability, we can have [striping](https://en.wikipedia.org/wiki/Data_striping), redundancy features, etc.
```bash
# level1: mirror
# level5: redundancy
mdadm --create --verbose /dev/md/myraid --level=5 --raid-devices=3 /dev/sdd{1,2,3}
mkfs -t ext4 /dev/md/myraid
mkdir /mydir && mount /dev/md/myraid /mydir
# check
lsblk -o name,size,fstype,type
# cancel
umount /mydir
mdadm --stop  /dev/md/myraid
```

# SSHFS
Filesystem client based on ssh. Fuse-based in user space, not privileged. Communication securely over SSH. Using standard SSH port (you can specify other ports).

类似于NFS mount, 通过SSH实现，这种方式还是挺方便，比如需要在bastion host中运行程序，可以把在develop host上的code repo 同步挂在到bastion中，就不用scp了.
```bash
# on client install
sudo apt-get install sshfs
# or
yum install -y fuse-sshfs

mkdir /sshdir
# mount remote root home directory
# the connection may be flakely
sshfs [user]@<hostname or ip>:[dir] /sshdir [options]

# check on /sshdir side host
cd /sshdir && df -h .
mount | grep ssh

# unmount
fusermount -u /sshdir
```]]></content>
      <categories>
        <category>Storage</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>storage</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux System Admin</title>
    <url>/2020/12/25/linux-system-admin/</url>
    <content><![CDATA[
这个课程的收获就是boot stages, kernel的升级以及linux logging的种类，使用。特别是journald，来自systemd。我从其他文章中补充了一些内容，包括loginctl.


# Boot
Linux booting process:
- firmware stage (BIOS or UEFI)
- boot loader stage (grub2)
- kernel stage (ramdisk -> root filesystem)
- initialization stage (systemd)

`/boot` directory is about kernel.
grub configuration file:
```bash
# providing boot menu and excuting kernel
# -N: show line number
sudo less -N /boot/grub2/grub.cfg
```
讲了一下如何定制grub 的kernel 菜单选项 to add custom boot entry，grub 的菜单会在开机时的图形界面显示。可以在开机时更改kernel line加入systemd rescue or emergency target, refer here:
- [CentOs boot into rescur or emernency mode](https://www.thegeekdiary.com/centos-rhel-7-how-to-boot-into-rescue-mode-or-emergency-mode/)
- [Ubuntu boot into rescur or emernency mode](https://ostechnix.com/how-to-boot-into-rescue-mode-or-emergency-mode-in-ubuntu-18-04/)


# Kernel
Upgrade kernel version for CentOS:
```bash
# uname -r
sudo yum list installed kernel-*
# see new kernel available
sudo yum list available kernel

# update
sudo yum update -y kernel
# then reboot and check kernel version
sudo reboot
```
The above steps usually cannot help much because the lack of latest version in official repo. We need third-party repo, see this artical for help:
[How to Upgrade Linux Kernel in CentOS 7](https://phoenixnap.com/kb/how-to-upgrade-kernel-centos)

Because we use SSH session to upgrade the kernal, so we are not able to select the kernel version on boot menu, we can do it by configuring the grub2:
```bash
# check kernel index list, index starts from 0
sudo awk -F\' '$1=="menuentry " {print $2}' /etc/grub2.cfg

CentOS Linux (5.4.125-1.el7.elrepo.x86_64) 7 (Core)
CentOS Linux (3.10.0-1160.31.1.el7.x86_64) 7 (Core)
CentOS Linux (3.10.0-1160.25.1.el7.x86_64) 7 (Core)
CentOS Linux (0-rescue-adbe471f40421bfbf841690042db23fd) 7 (Core)
```
Switch to `5.4.125-1` version:
```bash
# set kernel index 0
sudo grub2-set-default 0
# reconfig boot loader code
sudo grub2-mkconfig -o /boot/grub2/grub.cfg
sudo reboot
```
After rebooting, check the kernel version:
```bash
uname -r
```
Switch back to old kernel version is easy:
```bash
# # set kernel index 2, see above index list
sudo grub2-set-default 2
# reconfig boot loader code
sudo grub2-mkconfig -o /boot/grub2/grub.cfg
sudo reboot
```


注意kernel version 和 OS version不一样，比如查看CentOS OS version:
```bash
cat /etc/centos-release
# or
rpm -qa centos-release
```

# Linux Logging
Linux has 2 logging systems，这 2 个logging systems can run parallelly, or you can use journal alone.
- rsyslog (persistent logs, can log `remotely`)
- journald (nonpersistent by default)

[syslog vs rsyslog vs syslog-ng](https://serverfault.com/questions/692309/what-is-the-difference-between-syslog-rsyslog-and-syslog-ng): Basically, they are all the same, in the way they all permit the logging of data from different types of systems in a central repository, each project trying to improve the previous one with more reliability and functionalities.

Different logs for differnet purpose, some for failed jobs, some for cron jobs, etc. The rsyslog is a daemon:
```bash
systemctl status rsyslog
```

`/etc/rsyslog.conf` is the configuration file, see section under `#### RULES ####`. For example, anything beyond mail, authpriv and cron is logged in `/var/log/messages`, in the below file, `cron.*` means messages of all priorities will be logged (debug, info, notice, warn, err, crit, alert, emerg), `cron.warn` will log warn and above:
```bash
#### RULES ####

# Log all kernel messages to the console.
# Logging much else clutters up the screen.
#kern.*                                                 /dev/console

# Log anything (except mail) of level info or higher.
# Don't log private authentication messages!
*.info;mail.none;authpriv.none;cron.none                /var/log/messages

# The authpriv file has restricted access.
authpriv.*                                              /var/log/secure

# Log all the mail messages in one place.
mail.*                                                  -/var/log/maillog


# Log cron stuff
cron.*                                                  /var/log/cron

# Everybody gets emergency messages
*.emerg                                                 :omusrmsg:*

# Save news errors of level crit and higher in a special file.
uucp,news.crit                                          /var/log/spooler

# Save boot messages also to boot.log
local7.*                                                /var/log/boot.log
```

Log rotate config is in `/etc/logrotate.conf` and there is a cron job for rotate `/etc/cron.daily/logrotate`.

If you want to log message to system log file, use `logger` command:
```bash
# you will see it in /var/log/messages
logger "hello"
# -p: priority
logger -p local4.info " This is a info message from local 4" 
```
How to search and view rsyslog, see this [article](https://www.digitalocean.com/community/tutorials/how-to-view-and-configure-linux-logs-on-ubuntu-and-centos):
Linux uses a set of configuration files, directories, programs, commands and daemons to create, store and recycle these log messages.
The default location for log files in Linux is `/var/log`.

If you check with `ls -ltr -S /var/log`, the `lastlog` file may have a big size, way bigger than the disk space, it is a sparse file.

At the heart of the logging mechanism is the rsyslog daemon. This service is responsible for listening to log messages from different parts of a Linux system and routing the message to an appropriate log file in the /var/log directory. It can also forward log messages to another Linux server.

`/var/log/messages` 可以用vim等工具正常查看. command `who`, `last`其实是使用了`/var/run/utmp` and `/var/run/wtmp`


`journalctl` have the same logs as in `rsyslogd`, from [here](https://serverfault.com/questions/959982/is-rsyslog-redundant-on-when-using-journald), persistent journal can replace rsyslogd.
```bash
# persist journald by making a dir
sudo mkdir -p /var/log/journal
sudo systemctl restart systemd-journald
# you will see journal records here
ls -l /var/log/journal
```
Or enable in `/etc/systemd/journald.conf`, set `Storage=persistent`.

You can specify date ranges:
```bash
journalctl --since "2020-12-11 15:44:32"
# time left off is 00:00:00 midight
journalctl --since "2020-10-01" --until "2020-10-03 03:00"
journalctl --since yesterday
journalctl --since 09:00 --until "1 hour ago"
```

Some useful commands:
```bash
# list boots
journalctl --list-boots
# check last boot journal
# -b -1: last boot
sudo reboot
sudo journalctl -b -1

# combine
journalctl -u nginx.service -u php-fpm.service --since today

# pid, uid, gid
journalctl _PID=8088
journalctl _UID=33 --since today

# -F: show available values
journalctl -F _GID
journalctl -F _UID

# check executable
journalctl /usr/bin/bash

# display only kernel message
journalctl -k

# by priority, can use number or name
#0: emerg
#1: alert
#2: crit
#3: err
#4: warning
#5: notice
#6: info
#7: debug
journalctl -p err -b

# the same as tail -n/-f
journalctl -n 10
journalctl -f

# disk usage
journalctl --disk-usage
# shrink
sudo journalctl --vacuum-size=1G
sudo journalctl --vacuum-time=1years
```
You can use `right arrow key` to see full entry if it is too long.

```bash
# print all on stdout, no pager with less
journalctl --no-pager

# -o output format
#cat: Displays only the message field itself.
#export: A binary format suitable for transferring or backing up.
#json: Standard JSON with one entry per line.
#json-pretty: JSON formatted for better human-readability
#json-sse: JSON formatted output wrapped to make add server-sent event compatible
#short: The default syslog style output
#short-iso: The default format augmented to show ISO 8601 wallclock timestamps.
#short-monotonic: The default format with monotonic timestamps.
#short-precise: The default format with microsecond precision
#verbose: Shows every journal field available for the entry, including those usually hidde 
#internally.
journalctl -b -u nginx -o json
journalctl -b -u nginx -o json-pretty
```

# Linux Session
Other capabilities, like log management and user sessions are handled by separated daemons and management utilities (`journald/journalctl` and `logind/loginctl` respectively).

Get info about user and the processes he is running before:
```bash
# list sessions
loginctl list-sessions

# session status 
# you can see the user action history
loginctl session-status [session id]
loginctl show-session [session id]
loginctl kill-session [session id]
```


]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>system admin</tag>
      </tags>
  </entry>
  <entry>
    <title>Tar Command Daily Work Summary</title>
    <url>/2019/02/21/linux-tar-summary/</url>
    <content><![CDATA[
This article used to walk you through some commonly `tar` usages , based on a real life scenario.

\################################################################
\#  &emsp; Date &emsp; &emsp; &emsp; &emsp; &emsp; Description
\#  &emsp; 05/29/2019 &emsp; &emsp; vim tar files
\#  &emsp; 05/29/2019 &emsp; &emsp; extract single file to another directory
\#  &emsp; 05/28/2019 &emsp; &emsp; extract file to another directory
\#  &emsp; 05/23/2019 &emsp; &emsp; extract single file from archive
\#  &emsp; 04/21/2019 &emsp; &emsp; untar keep owner and permission
\#  &emsp; 02/27/2019 &emsp; &emsp; untar to specified folder
\#  &emsp; 02/22/2019 &emsp; &emsp; list tar content
\#  &emsp; 02/21/2019 &emsp; &emsp; tar exclude
\#  &emsp; 02/20/2019 &emsp; &emsp; untar multiple files
\#  &emsp; 02/19/2019 &emsp; &emsp; tar multiple files
\#
\################################################################

Sometimes I see people use `-czf` but sometimes `czf`, dash or not to pass flags?
Historical and compatible reason, **no** dash version is probably more portable.

`tar` is one of those ancient commands from the days when option syntax hadn't been standardized. Because all useful invocations of `tar` require specifying an operation before providing any file name, most `tar` implementations interpret their first argument as an option even if it doesn't begin with a `-`. Most current implementations accept a `-`.

注意, 这里的例子大多是用的old option style for compatibility. For example `czf` this set of letters must be the first to appear on the command line, after the tar program name and some white space; old options cannot appear anywhere else.

还要注意，当前pwd是`/tmp`然后运行`tar`，则`tar`的结果就在`/tmp`, 和`-C`无关, `-C`option只是在执行中暂时去指定的位置。

### 02/19/2019
Basic operation: tar multiple files into `example.tar.gz`
```bash
## use -C to go to target directory
## target directory: the directory which contains file1/2/3
tar czf example.tar.gz -C <target directory> file1 file2 file3

## tar a directory as whole
## target directory: <folder name>'s parent folder
## untar 结果是<folder name>这个文件夹
tar czf example.tar.gz -C <target directory> <folder name>


# 如果只想打包某一文件夹内的内容, 则用-C 进入那个文件夹
## 但这样用tar tvf 查看，会有./ 前缀, 因为最后那个`.` 会展开显示所有hidden file，包括当前文件夹那个`.`
tar czf example.tar.gz -C <target directory> .

## 用`*`就没有./ 前缀，但是不会包含hidden file, 必须自己列出来
## 但这样用tar tvf 查看就没有前缀了
tar czf example.tar.gz -C <target directory> * .hidden1 .hidden2
```
> The file path matters! see my [blog](https://chengdol.github.io/2019/07/03/linux-tar-path/).


### 02/20/2019
When untar multiple files, you cannot do this, it will fail
```bash
tar zxf file1.tar.gz file2.tar.gz file3.tar.gz
```
The reason please see this [link](https://stackoverflow.com/questions/583889/how-can-you-untar-more-than-one-file-at-a-time), the solution is to use `xargs` instead:
```bash
# -I: specify replace-str
# {}: placeholder
ls *.tar.gz | xargs -I{} tar xzf {}
```
Or you can use `find` with `-exec`
```bash
find . -maxdepth 1 -name "*.tar.gz" -exec tar zxf '{}' \;
```


### 02/21/2019
For example, if you want to tar things inside a folder `foler1` but excluding some files:
```bash
## 注意最后的`.` 目标必须放最后
## target directory: 进入
cd folder1
tar czf folder1.tar.gz --exclude="folder1.tar.gz" --exclude='file1' --exclude='file2' *
## if you want to have hidden files
tar czf folder1.tar.gz --exclude="folder1.tar.gz" --exclude='file1' --exclude='file2' * .file3 .file4
```
If you don’t exclude `folder1.tar.gz`, it will tar itself again.


### 02/22/2019
List tar.gz file content, flag `z` is used to distinguish tar and tar.gz
```bash
tar tvf target.tar
tar ztvf target.tar.gz
```


### 02/27/2019
If you don't specify target folder, untar will put things in current directory, use `-C` option to specify it. For example, I want to untar `source.tar.gz` to `/etc/yum.repos.d/` folder:
```bash
tar zxf /tmp/source.tar.gz -C /etc/yum.repos.d/
```
For `-C` option, in `c` and `r` mode, this changes the directory before adding the following files.  In `x` mode, change directories after opening the archive but before extracting entries from the archive.


### 04/21/2019
When unpacking, consider using `p` option to perserve file permissions. Use this in extract mode to override your `umask` and get the exact permissions specified in the archive.. The `p` option is the default when working as the **superuser**, it will get what it has. If you are a **regular user**, add `p` to keep permissions.
```bash
tar zxpf target.tar.gz
```
It seems umask ignores execute bit? When I untar the file with `rwxrwxrwx` permission inside by regular user with umask `0002`, the final permission is `rwxrwxr-x`.

if you want to keep owner as well:
```bash
tar --same-owner -zxpf target.tar.gz
```
> Note that there is a `-` before `zxpf`.


### 05/23/2019
Extract specific files from tarball to current directory:
```bash
tar xzf target.tar.gz file1 file2
```
> Note that no leading `/` in the path (it uses relative path in tar file!), you can use `tar xtvf target.tar.gz` to check the path.


### 05/28/2019
`tar` by default extracts file to current directory, if you want to place the untar files to another directory, run:
```
tar zxf target.tar.gz -C /target/directory
```
Note that the target directory has to **exist** before running that command.


### 05/29/2019
If you want to extact files to another directory:
```bash
## file1 and file2 put at end
tar xzf target.tar.gz -C /target/directory file1 file2
```

### 11/12/2020
Latest VIM support edit on tar file:
```bash
## then select file in dashboard, edit and save normally
vim source.tar.gz
```
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>tar</tag>
      </tags>
  </entry>
  <entry>
    <title>SSH SCP SFTP Daily Work Summary</title>
    <url>/2019/02/21/linux-ssh-scp-sftp-summary/</url>
    <content><![CDATA[
This blog used to walk through some `ssh`(secure shell), `scp` and `sftp` use
cases.

**Aside:**
[difference between OpenSSH vs OpenSSL](https://security.stackexchange.com/questions/29876/what-are-the-differences-between-ssh-generated-keysssh-keygen-and-openssl-keys)
The file format is different but they both encode the same kind of keys.
Moreover, they are both generated with the same code.

Notice that restart sshd daemon will not disconnect current ssh connection, even
if you stop the sshd daemon for a short time or restart the network daemon(don't
stop it!), the current ssh session is still working, see this
[issue](https://unix.stackexchange.com/questions/27636/how-does-ssh-connection-survive-a-network-restart):
The reason is, sshd fork a child process on each connection, the child process
will not die if either sshd or the whole network is restarted. sshd listens on
port 22 for incoming connections. When someone connects it spawns a new process
for that connection and goes back to listening.
```bash
# check pid and ppid of current ssh child process
ps -ef | grep -v grep | grep ssh

# restart some daemon
systemctl restart sshd
systemctl restart network

# the old child ssh session ppid was changed to 1
ps -ef | grep -v grep | grep ssh
```

# Install SSH (SCP SFTP)
Notice that `ssh`, `scp` and `sftp` are all installed from `openssh-clients`
package. 操作的目标机器的用户以及密码就是目标机器上在`/etc/passwd`中对应的用户及其密码。

First, understand that there are openssh client and openssh server:
[Installing and Enabling OpenSSH on CentOS 7](https://phoenixnap.com/kb/how-to-enable-ssh-centos-7),
this article briefly introduces openssh configuration and firewall setting for
it.

```bash
yum –y install openssh-server openssh-clients
```
If only install `openssh-clients`, you can ssh to others but others cannot ssh
to you, since you don't have ssh server listening at port 22.


After installing openssh-server, enable and start the sshd daemon
```bash
systemctl enable sshd
systemctl start sshd
# check status
systemctl status sshd
```

The system OpenSSH server configuration file is `/etc/ssh/sshd_config`, the
custom configuration file is `~/.ssh/config`. The `/etc/ssh/ssh_config` is for
system-wide client behavior.

Restricted configuration you may need on server side:
```bash
Port 22
PermitRootLogin prohibit-password
PubkeyAuthentication yes
# after copy the public key in
PasswordAuthentication no
```
After making changes, restart sshd daemon.

Firewall setting for ssh is file `/etc/sysconfig/iptables`.

# SSHFS
This is a remote mount implemented by SSH, handy if NFS is not workable, search
my blog `<<Linux Storage System>>`.

# SSH Tunnel
Forward a local port to remote port, one on one mapping.
用在比如database or web servers 没有对外开放端口，我们可以通过SSH穿过防火墙(SSH的端口是开
放的)去远程映射它们的端口到本地，然后通过localhost访问。

```bash
# -L: port forward
# 10003: local port
# 8000: remote port of a web server, for example a python simple http server.
# -N: Do not execute a remote command. This is useful for just forwarding ports.
# If the 8000 port is blocked by firewall remotely, but after mapping,
# we can access it locally with the local port 1234.
ssh -L [127.0.0.1:]10003:remotehost:8000 user@remotehost -N
```
Then go to `localhost:10003` on browser to see the web page.

The port forwarding approach is limited on that single port mapping, for
unlimited access, you need SOCKS proxy tunneling, see next section.


# SSH SOCKS Proxy Tunnel
[Introduction to SOCKS proxy](http://www.firewall.cx/vpn/vpn-guides-articles/1191-best-socks5-proxy-guide-torrenting-free-proxy-list.html)

Although by default SOCKS proxy does not provide encryption, but we run it over
SSH, so the traffic is encrypted.

[How To Route Web Traffic Securely Without a VPN Using a SOCKS Tunnel](https://www.digitalocean.com/community/tutorials/how-to-route-web-traffic-securely-without-a-vpn-using-a-socks-tunnel):
A SOCKS proxy is basically an SSH tunnel in which specific applications forward
their traffic down the tunnel to the server, and then on the server end, the
proxy forwards the traffic out to the general Internet. Unlike a VPN, a SOCKS
proxy has to be configured on an app by app basis on the client machine, but can
be set up without any specialty client agents.

The remote host must has ssh server running.
```bash
# -D: dynamic application-level port forwarding, see curl man for more
# explanation about SOCKS support.
# [127.0.0.1:]11000: local mapping port.
# -N: Do not execute a remote command. This is useful for just forwarding ports.
# -C: Compresses the data before sending it
# -q: quiet

# -f: Forks the process in the background
# don't like tunnel, on the remote host it is dynamic forwarding
ssh -D [127.0.0.1:]11000 -f -C -N -q user@remotehost 
```
This is actaully a SOCKS5 proxy created by SSH, after it is established, you can
check by:
```bash
# Now you can access the web that original can only access by remote host.
curl -ILk -x socks5://localhost:11000 "https://web_can_only_access_by_remotehost"
```

Or configuring the web browser to use this SOCKS5 proxy: `localhost:11000`.
On Firefox `FoxyProxy` plugin, set and use it. Now we can access whatever the
remotehost can access.

Manually kill the tunnel process if you use `-f`.

# SSH X11 Forwarding
Similar to VNC, but VNC transmits whole desktop which is more expensive.
Linux has good support to X11, on Mac, need to install XQuartz(still not work on Mac).
```bash
# -X: X11 forwarding
ssh -X user@remotehost

# gedit is running on remotehost but reflect GUI locally
> gedit
```

# SSH Agent
很久之前看书的时候没明白这个概念. 一个常见的用处就是保护originating host的private key.
A handy program called `ssh-agent` simplifies working with SSH private keys.

In Mac, ssh-agent is running by default, but in Linux, start by yourself (ensure
only one instance of ssh-agent is running).
```bash
# Don't need do this if you are Mac, or your company laptop has agent running by
# default you can check by:
ssh-add -l

# First check if only one instance is running
ps aux | grep ssh-agent
# if it is there but cannot work, kill it.
```
If you run `ssh-agent`, it will output environment vars you need to set, for
example, you can also manually export these instead of using `eval`:
```bash
ssh-agent

# export these manually is OK
SSH_AUTH_SOCK=/tmp/ssh-YI7PBGlkOteo/agent.2547; export SSH_AUTH_SOCK;
SSH_AGENT_PID=2548; export SSH_AGENT_PID;
echo Agent pid 2548;

# Start it.
eval $(ssh-agent)
```

Add your private key to ssh-agent, sometimes git ssh clone failed, you may need
to add private key to agent:
```bash
# default path ~/.ssh/id-rsa
ssh-add
ssh-add <other private key path>

# list all identities 
ssh-add -l

# delete all identities
ssh-add -D
# delete specified identity
ssh-add -d <private key path>
```

How to start ssh-agent on login:
https://stackoverflow.com/questions/18880024/start-ssh-agent-on-login
Add below to your `.bash_profile`:
```bash
SSH_ENV="$HOME/.ssh/env"

function start_agent {
    echo "Initialising new SSH agent..."
    /usr/bin/ssh-agent | sed 's/^echo/#echo/' > "${SSH_ENV}"
    echo 'succeeded'
    chmod 600 "${SSH_ENV}"
    . "${SSH_ENV}" > /dev/null
    # add private key ~/.ssh/id_rsa.pub
    /usr/bin/ssh-add;
}

# Source SSH settings, if applicable
if [ -f "${SSH_ENV}" ]; then
    # need resource, but ssh-agent is there
    . "${SSH_ENV}" > /dev/null
    # ps ${SSH_AGENT_PID} doesn't work under cywgin
    ps -ef | grep ${SSH_AGENT_PID} | grep ssh-agent$ > /dev/null || {
        # statement block
        start_agent;
    }
else
    start_agent;
fi
```

When you try to make a connection to a remote host, and you have ssh-agent
running, the SSH client will automatically use the keys stored in ssh-agent to
authenticate with the host.

**Advantages:**
1. For encrypted SSH private keys，只有第一次加入ssh-agent的时候要求输入password 如果
不使用ssh-agent，每次SSH都会要求输入password
2. If you are using Ansible to manage hosts that use different SSH keys, using
an SSH agent simplifies your Ansible configuration files.
3. ssh-agent forwarding, see below

ssh-agent 还解决了一个问题，比如你有personal and work git account各一个，但各自是不同的
ssh key pairs，在git clone的时候如何指定用哪个private key呢? see this
[link](https://stackoverflow.com/questions/4565700/how-to-specify-the-private-ssh-key-to-use-when-executing-shell-command-on-git).

# SSH Agent Forwarding
If you are cloning a Git repository on remote host via SSH, you’ll need to use
an SSH private key recognized by your Git server. I like to avoid copying
private SSH keys to my remote host (for example, a EC2 instance), in order to 
limit the damage in case a host ever gets compromised.
```bash
# The example.xx.com does not have the private key to access git repo but the
# local host has.
# -A: agent forwarding
ssh -A root@example.xxx.com

# git clone via ssh mechanism on remote host with the private key provided by
# agent from local host.
git clone git@github.com:lorin/mezzanine-example.git
```
Here `-A` limit the agent forwarding in this session only, you can have ssh
config to set agent forwarding on a broad view.

# ProxyJump
现在想想，当时登录openshift or softlayer 的master时，也需要经过堡垒机，所以应该可以配置
proxyjump. 这个和ssh-agent没有必然联系，如果没用ssh-agent, 则应该可以在配置config file中
指定key的位置.

[Using OpenSSH ProxyJump](https://www.youtube.com/watch?v=QKZP9FbP3mo)
It uses vagrant VMs to demonstrate. 但是我觉得不需要再指定port, user, identifykey对
target server了，这些应该在bastion上已经配置好了。

[SSH agent and ProxyJump explained](https://smallstep.com/blog/ssh-agent-explained/)
Talking risk of SSH agent forwarding, access internal hosts through a bastion,
ProxyJump is much safer. 也谈到了SSH是怎么handshake, 传输中用的是新的对称钥匙.

**JumpBox or Bastion Host:**
Notice that you need to generate key and copy the public key to bastion host
first.
- [Using the SSH Config File](https://linuxize.com/post/using-the-ssh-config-file/)
- [SSH to remote hosts though a proxy or bastion with ProxyJump](https://www.redhat.com/sysadmin/ssh-proxy-bastion-proxyjump)

Baston hosts are usually public-facing, hardened systems that serve as an
entrypoint to systems behind a firewall or other restricted location, and they
are especially popular with the rise of cloud computing.

The ssh command has an easy way to make use of bastion hosts to connect to a
remote host with a **single** command. Instead of first SSHing to the bastion
host and then using ssh on the bastion to connect to the remote host, ssh can
create the initial and second connections itself by using ProxyJump.

```bash
# -J specify the jumphost
ssh -J <bastion-host> <remote-host> [-l <remote login user>] [-i <pem file>]
ssh -J user@<bastion:port> <user@remote:port>
# Jump through a series of hosts
ssh -J <bastion1>,<bastion2> <remote>
```

最主要的还是配置`~/.ssh/config` 文件, basic settings:
注意，之前遇到过奇怪的问题，用同样的config file，别人一切正常，但我就连不上，简化了一下config
file后就好了，当时的解决办法是把Match host模块移到匹配的Host 下面，其实Match host不要也行
很多可以合并的。
```bash
# May have more options, for example, User, Port, AgentForward, etc.
# refer `man ssh_config`

# The `Host` sections are read in order and the options matched will get
# accumulated

# The Bastion Host
Host <jump-host-nickname>
  User <user name>
  # default is no
  ProxyUseFdpass no
  # jumpbox port
  Port 22
  # jumpbox IP
  HostName <hostname or IP>

# The Remote Host
Host <remote-host-nickname>
  Hostname <remote-hostname or ip address for example: 172.12.234.12>
  User <user name>
  AddKeysToAgent yes
  IdentitiesOnly yes
  # may need pem file, the private key
  IdentityFile ~/.ssh/file.pem
  StrictHostKeyChecking no
  ServerAliveInterval 60

# The remote host match this IP will use jumpbox
# 这个可以不要，就是用来match Host的
Match host 172.??.*
  ProxyJump <jump-host-nickname>

# Or can specify jumpbox directly
Host <remote-host-nickname>
  HostName < remote-hostname or ip address>
  ProxyJump bastion-host-nickname
```
Then you can ssh directly: `ssh remote-host-nickname`

`ProxyCommand` is an alternative of ProxyJump, but it is old.
```bash
ssh -o ProxyCommand="ssh -W %h:%p bastion-host" remote-host
```

# Force SSH Password Login
Usually SSH password authentication is disabled, that is, you can only log in
over SSH using public key authentication, to enable password login:
```bash
# /etc/ssh/sshd_config
# set to yes
PasswordAuthentication yes
# you may also need to allow root login
PermitRootLogin yes
# restart sshd
systemctl restart sshd
```

Create new user to test:
```bash
useradd alice
passwd alice
```

Logout and try:
```bash
# PubkeyAuthentication may be needed
# then input the password
ssh -p 2222 \
    -o StrictHostKeyChecking=no \
    -o UserKnownHostsFile=/dev/null \
    -o PubkeyAuthentication=no \
    alice@127.0.0.1
```


# Debug
1. Use `-v`, `-vv`, `-vvv` flag in ssh command (比如之前pem file 权限和格式没设置对都
有提示)
2. Wireshark capture ssh traffic on that interface, you should see `SSHv2`
protocol and more details
3. Check system log, `journalctl | grep sshd`.
4. Launch sshd on another port in debug mode: `sudo /usr/bin/sshd -d -p 2020`,
then ssh to this port `2020` from client `ssh -p 2020 user@remote_server`.
5. Possibly restriced by firewall


# Usage Summary
```  
10/01/2018 ssh send command
11/14/2018 ssh run shell script
12/19/2018 ssh-copy-id
01/06/2019 ssh-kenscan
01/08/2019 ECDSA host key changed
01/22/2019 no prompt first time
01/23/2019 sshpass
02/21/2019 scp folder or files
03/11/2019 ssh -i option
03/12/2019 recover public key
09/05/2020 sftp
01/20/2021 ssh config
03/17/2022 ssh config permission
```

## 10/01/2018
use `ssh` send commands to execute on remote machine:
![](https://drive.google.com/uc?id=1OgNJ5NJpx6Kkr46oCxScylGhu1ShYaCV)
`-t` flag allow you to interact with remote machine:
![](https://drive.google.com/uc?id=1rgH7Eu7Qzw2icZQzroTT1_J7LH0G3EBP)

## 11/14/2018
use ssh run shell script in remote machine
![](https://drive.google.com/uc?id=1FvvQTq-ujz1ETmharmWt9JsMBC1ipuUJ)

## 12/19/2018
use `ssh-copy-id` to copy local machine public key to remote machine’s
`~/.ssh/authorized_keys` file, so next time when you `ssh`, `scp` or `sftp`
again, no prompt to require password:

![](https://drive.google.com/uc?id=1HdAmGdDB1HxPtwFSzrTQcS-g7hw5r3o4)

Sometimes I see people use `~/.ssh/id_rsa` with `ssh-copy-id`, that confused me
because that is private key, OK, `man` tells me why:
```
-i identity_file
        ...If the filename does not end in .pub this is added.  If the filename
        is omitted, the default_ID_file is used.

```

## 01/06/2019
use `ssh-keyscan` to get remote machine ecdsa identity, you can put this item
into local known_hosts file, so when first time `ssh` login, there is no prompt
to input `yes`:
![](https://drive.google.com/uc?id=1C_IHZxToXYRN4iLabvmcW21x_5wbGcQp)

Actually better to use `-o StrictHostKeyChecking=no` flag.

## 01/08/2019
I create a new cluster with the same master hostname as the deleted one, so when
I try to ssh to it, interesting thing happens:
![](https://drive.google.com/uc?id=1Fkc0M9_e0hufGmI2Es_qvFOBRaHLwPKd)

go to `~/.ssh/known_hosts` file and delete the corresponding `ECDSA` line
![](https://drive.google.com/uc?id=159d02J98eKyz2jJMUpTgJ6kCuZtqVe10)

## 01/22/2019
when you first time ssh or scp, sftp to remote machine, it will prompt to add
remote machine to `~/.ssh/known_hosts` file, this may interrupt `ansible` or
shell script running, so I want to skip it. For example:
![](https://drive.google.com/uc?id=1WfbdXCFeyg5k7tr9wS5JjMRIifJcdPaq)

use `-o StrictHostKeyChecking=no` option, it will silently add remote host name
to `~/.ssh/known_host` file.
```bash
ssh-copy-id -i .ssh/id_dsa.pub -o StrictHostKeyChecking=no root@example.com
scp -o StrictHostKeyChecking=no -r ./source root@example.com:~
```
if you don't want to add the host name, `-o UserKnownHostsFile=/dev/null` option
can save you.

## 01/23/2019
scp or ssh without prompt input password
```bash
yum install -y sshpass
# Explicitly input password
sshpass -p <password> scp/ssh ...
```
It's useful to set password-less at first time, combine all of these, no prompt
will show up:
```bash
sshpass -p <password> ssh-copy-id -i ~/.ssh/id_rsa.pub -o StrictHostKeyChecking=no ...
```

## 02/21/2019
scp `source` directory and it's content recursively to `root` user home
directory in `example.com`.
```bash
scp -o StrictHostKeyChecking=no -r ~/source root@example.com:~
```
scp all files in `source` directory to `target` directory in `example.com`.
```bash
scp -o StrictHostKeyChecking=no ./source/* root@example.com:~/target
```

## 03/11/2019
The `ssh` command has `-i` option, you associate **private** key with this
flags:
```
ssh -i ~/.ssh/id_rsa xxx
```
Note that SSH **never** send private key over the network, `-i` merely used to
answer challenge that is generated using the corresponding public key from
target machine, you don't need to explicitly use `-i` if you use default private
key in right location.

## 03/12/2019
If public key is lost, you can use existing private key to generate one:
```bash
ssh-keygen -y -f ~/.ssh/id_rsa > ~/.ssh/id_rsa.pub
```
Or just create new key pair
```bash
echo "yes" | ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa
```

## 09/05/2020
`sftp` server is up when install `openssh-server` package, and can be configured
in `/etc/ssh/sshd_config` file: 
https://www.techrepublic.com/article/how-to-set-up-an-sftp-server-on-linux/

For the interactive commands, see `man sftp`, there are logs of regular commands
can be used in sftp, for example: cd, chmod, ln, rm, etc.

The [online free ftp server](https://test.rebex.net/) for testing purpose.
Also, the [mmnt.net](https://www.mmnt.net/) can be used to find free ftp
servers.
```bash
# Use password or ssh-public key to login to sftp server
sftp -o UserKnownHostsFile=/dev/null \
-o StrictHostKeyChecking=no \
demo@test.rebex.net[:path]

# print local working directory, the default place to hold download file
sftp> lpwd
# change local working directory
sftp> lcd [path]
# escape to local shell, type `exit` to back
sftp> !
# secape to local command
sftp> ![command]

# enable/disable progress meter
sftp> progress
# download file to local working directory
sftp> get <filename>
# download file to specified directory
sftp> get <filename> <local file path>

# upload file
# default file is in local working directory and upload to sftp current folder
# if no path is specified
sftp> put [local file path] [remote file path]

# quit sftp
sftp> bye
```

For non-interactive download/upload file:
```bash
# download
sftp user@hostname[:path] <local file path>
# upload, tricky
echo "put <local file path>" | sftp user@hostname[:path]
sftp user@hostname[:path] <<< $'put <local file path>'
```

Used in shell script:
```bash
sftp user@hostname <<EOF
cd /xxx/yyy/zzz
cd /aaa/bbb/ccc
put file.tgz
bye
EOF
```

## 01/20/2021
When the network connection is poor and ruining your SSH session, you can adjust
the connection settings with larger interval probing and retry times:
```bash
Host myhostshortcut
    User XXX
    Port 22
    # or ip address
    HostName myhost.com
    User barthelemy
    # no-op probe to server interval 60s
    ServerAliveInterval 60
    # probe conunt max 10 times if noresponse
    ServerAliveCountMax 10
    # no tcp no-op probe
    TCPKeepAlive no
```

## 08/13/2021
When I ran `git pull` on Gitlab local repo from my cloudtop, I got error output:
```
Received disconnect from UNKNOWN port 65535:2: Too many authentication failures
Disconnected from UNKNOWN port 65535
fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.
```
This was resolved by adding `IdentitiesOnly yes` in ssh config file under gitlab
config block, which instructs ssh to only use the authentication identity files
specified on the command line or the configured in the ssh_config file.

Reference:
https://www.tecmint.com/fix-ssh-too-many-authentication-failures-error/

## 03/17/2022
The Ansible playbook failed to ssh target VM and reported error:
```yaml
module.backups-v2.null_resource.setup_cluster (local-exec): [WARNING]: Unhandled
error in Python interpreter discovery for host
module.backups-v2.null_resource.setup_cluster (local-exec): 10.10.16.205: Failed
to connect to the host via ssh: Bad owner or permissions
module.backups-v2.null_resource.setup_cluster (local-exec): on /root/.ssh/config
```
It turns out the permission and owner issue on /root/.ssh/config file, see this
ticket for
[detail](https://superuser.com/questions/1212402/bad-owner-or-permissions-on-ssh-config-file),
the fix is to set 600 as chmod and chown by owner.
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ssh</tag>
        <tag>scp</tag>
        <tag>sftp</tag>
      </tags>
  </entry>
  <entry>
    <title>Systemd Essential</title>
    <url>/2020/12/23/linux-systemd/</url>
    <content><![CDATA[
# Version
Different version may have different syntax and options in unit file, check it
first:
```bash
systemctl --version
```
To see systemd service unit configuration:
```bash
man 5 systemd.service
```

# Systemd
[How To Use Systemctl to Manage Systemd Services and Units](https://www.digitalocean.com/community/tutorials/how-to-use-systemctl-to-manage-systemd-services-and-units)

History:
```
SysV Init -> Upstart -> Systemd
```

The `systemd`, system and service manager, is an init system used to bootstrap
the user space and to manage system processes after boot. Use `systemctl`
command to manage the service on a systemd enabled system.

The fundamental purpose of an init system is to initialize the components that
must be started after the Linux kernel is booted (traditionally known as
"userland" components). The init system is also used to manage services and
daemons for the server at any point while the system is running.

Main commands, for example using nginx, some commands have to use `sudo` if you
are non-root user, since it will affect te state of the operating system, you
can leave off the `.service` suffix in command.
```bash
# start on boot
# This hooks it up to a certain boot “target”
# causing it to be triggered when that target is started.
sudo systemctl enable nginx.service
sudo systemctl disable nginx.service

# start and stop
sudo systemctl start nginx.service
sudo systemctl stop nginx.service

# when change the configuration of service
# To attempt to reload the service without 
# interrupting normal functionality
sudo systemctl reload nginx.service
# if reload is not available, restart instead
sudo systemctl reload-or-restart nginx.service

# status overview, you can see:
# unit file path
# drop-in
# enabled or disabled at vendor and custom
# up time
# Cgroup
systemctl status nginx.service

# find overridden config files for all units
# check unit drop in config snippets
sudo systemd-delta
```
`enable` will create a soft link into the location on disk where systemd looks
for autostart files (usually `/etc/systemd/system/some_target.target.wants`).

The exit code can be used for shell script:
```bash
# may need sudo
systemctl is-active nginx.service
systemctl is-enabled nginx.service
systemctl is-failed nginx.service
```

Check system states managed by systemd:
```bash
# list enabled units only
systemctl [list-units]

# list all of the units that systemd has loaded or attempted to load into memory
# include not currently active
systemctl list-units --all [--state=active|inactive|failed] [--type=service|target]
# list failed daemons, this is useful when reboot VM but daemons not
systemctl list-units --state failed

# show every available units installed on the system
systemctl list-unit-files
```
`list-unit-files` state column: The state will usually be `enabled`, `disabled`,
`static`, or `masked`. In this context, `static` means that the unit file does
not contain an `[Install]` section, which is used to enable a unit. As such,
these units cannot be enabled. Usually, this means that the unit performs a
one-off action or is used only as a dependency of another unit and should not be
run by itself.
```bash
# after mask, cannot enable or start service
sudo systemctl mask nginx.service
sudo systemctl unmask nginx.service
```

To see full content and path of a unit file, vanilla unit files(don't touch
them, override them if needed in other places, for example `/etc/systemd/system`)
are in `/usr/lib/systemd/system` and customized are in `/etc/systemd/system`
folder:
```bash
# show unit file content and path
# if has overriding snippet, will show them as well
systemctl cat nginx.service

# list dependencies 
systemctl list-dependencies nginx.service [--all] [--reverse] [--before] [--after]

# low-level detail of unit
# all key=values
# -p: display a single property
systemctl show nginx.service [-p ExecStart]

# edit unit file
sudo systemctl edit --full nginx.service
# append unit file snippet
sudo systemctl edit nginx.service
# then reload to pick up changes
sodu systemctl daemon-reload
```
You can also further override by creating override.conf file in
`/etc/systemd/system/xxx.service.d` folder, there are some principles for 
overriding, see [here](https://unix.stackexchange.com/questions/398540/how-to-override-systemd-unit-file-settings).

In systemd, service and other unit files can be tied to a `target`.

Targets are special unit files that describe a system state or synchronization
point. Like other units, the files that define targets can be identified by
their suffix, which in this case is `.target`. Targets do not do much by
themselves, but are instead used to group other units together.

This can be used in order to bring the system to certain states, much like other
init systems use `runlevels`. (仍然可以显示系统的runlevel的)

For instance, there is a `swap.target` that is used to indicate that swap is
ready for use. Units that are part of this process can sync with this target by
indicating in their configuration that they are WantedBy= or RequiredBy= the
swap.target. Units that require swap to be available can specify this condition
using the Wants=, Requires=, and After= specifications to indicate the nature of
their relationship.

```bash
# all available targets
systemctl list-unit-files --type=target

# current default target
systemctl get-default

# set default target
sudo systemctl set-default multi-user.target
sudo systemctl set-default runlevel3.target

# see what units are tied to a target
systemctl list-dependencies multi-user.target
```

Unlike runlevels, multiple targets can be active at one time. An active target
indicates that systemd has attempted to start all of the units tied to the
target and has not tried to tear them down again. 
```bash
# show all active targets
systemctl list-units --type=target
```

This is similar to changing the runlevel in other init systems. For instance,
if you are operating in a graphical environment with `graphical.target` active,
you can shutdown the graphical system and put the system into a multi-user
command line state by isolating the `multi-user.target`. Since graphical.target
depends on multi-user.target but not the other way around, all of the graphical
units will be stopped.
```bash
# check units that will be kept alive
systemctl list-dependencies multi-user.target
# transition
sudo systemctl isolate multi-user.target
```

Stopping and rebooting system, note `shutdown`, `reboot`, `poweroff` are
actually softlink to systemctl!
```bash
sudo systemctl poweroff
sudo systemctl reboot
# boot into rescue mode (single-user)
sudo systemctl rescue
```
These all alert logged in users that the event is occurring.

有意思的是，这些都是同一个softlink，但是调用却有不同的效果呢? 利用了`$0` 作为判断, see
[here](https://unix.stackexchange.com/questions/77029/why-are-reboot-shutdown-and-poweroff-symlinks-to-systemctl).
```bash
lrwxrwxrwx. 1 root root          16 Jun  6  2020 halt -> ../bin/systemctl
lrwxrwxrwx. 1 root root          16 Jun  6  2020 poweroff -> ../bin/systemctl
lrwxrwxrwx. 1 root root          16 Jun  6  2020 reboot -> ../bin/systemctl
lrwxrwxrwx. 1 root root          16 Jun  6  2020 runlevel -> ../bin/systemctl
lrwxrwxrwx. 1 root root          16 Jun  6  2020 shutdown -> ../bin/systemctl
lrwxrwxrwx. 1 root root          16 Jun  6  2020 telinit -> ../bin/systemctl
```

# Systemd Journal
[How To Use Journalctl to View and Manipulate Systemd Logs](https://www.digitalocean.com/community/tutorials/how-to-use-journalctl-to-view-and-manipulate-systemd-logs)

The journal is implemented with the `journald` daemon, which handles all of the
messages produced by the kernel, initrd, services, etc.

这里介绍了关于systemd service journal的使用，更详细的介绍Journal 可以参考我的blog
`<<Linux System Admin>>`.

Set time of the prompt:
```bash
# available time zone
timedatectl list-timezones
# set time zone
sudo timedatectl set-timezone America/Los_Angeles
# check
timedatectl status

# or using UTC
# --utc: time zone
journalctl --utc
```

To see log of a specific service:
```bash
# full log
journalctl -e

# -u: unit, but some log may be missing in ExecStartPre
# -b: limit to current boot
# -e: show ending logs
# -r: show in reverse order
# -f: tail log
journalctl -u nginx.service [--since] [--until] [-e]

# combine related units
# good for debug
journalctl -u nginx.service -u php-fpm.service --since today
```

# Service Unit File
[Understanding Systemd Units and Unit Files](https://www.digitalocean.com/community/tutorials/understanding-systemd-units-and-unit-files)

Here only focus on `.service` unit. Systemd manages a broad range of resources,
such as `.target`, `.socket`, `.device`, `.mount`, `.swap`, etc. They may have
different section blocks.

Section block names are case-sensitive. Non-standard section name `[X-name]` has
a `X-` prefix.
```ini
[Unit]
#This is generally used for defining metadata for the unit 
# and configuring the relationship of the unit to other units
Description=
Documentation=

Requires=
BindsTo=

Wants=
Before=
After=

Conflicts=
Conditionxxx=
Assertxxx=

[Service]
# provide configuration that is only applicable for services
Environment=
Type=simple(default)|forking|oneshot|dbus|notify|idle

PIDFile=

# deprecated syntax
# if true, User and Group only applied to ExecStart
PermissionsStartOnly=true

ExecStartPre=

ExecStart=
ExecStartPost=
ExecReload=
ExecStop=
ExecStopPost=

User=
Group=

RestartSec=
Restart=always|on-success|on-failure|on-abnormal|on-abort|on-watchdog
TimeoutSec=

[Install]
# only units that can be enabled will have this section
WantedBy=
RequiredBy=

Alias=
Also=
DefaultInstance=
```
key=value pairs 不止这些，遇到新的可以补充。

Systemd does not use a shell to execute commands and does not perform `$PATH`
lookup, so for example in ExecStartPre, must specify shell context to run
command:
```ini
ExecStartPre=/bin/sh -c 'pgrep process_name > /var/run/process_name.pid'
```
And because of no $PATH lookup, must use absolute path to binary or executable.


Setting `PermissionsStartOnly=true` means that User & Group are only applied to
ExecStart. So switching to the new syntax will be :
```ini
ExecStartPre=+/bin/bash -c '/bin/journalctl -b -u ntpdate | /bin/grep -q -e "adjust time server" -e "step time server"'
ExecStartPre=+/bin/mkdir -p /path/to/somedir
ExecStartPre=-/usr/bin/<command that may fail>
ExecStart=/path/to/myservice
ExecStop=+/bin/kill -INT ${MAINPID}
ExecReload=+/bin/kill -INT ${MAINPID} && /path/to/myservice
```
Prefix with `+` will be executed with higher privilege as root. Prefix `-` means
if any of those commands fail, the remaining exec will not be interrupted,
otherwise the unit is considered failed.

Also note that per `ExecStartPre` is running in isolation.

If needs to check the log of latest daemon start process, this will show you all
log instead of only unit, I found if use `-u` to specify unit, some log will
miss:
```bash
# use `b` and `f` to move page
# -e: tail
# -x: more details
journalctl -ex
```

[Systemd with multiple execStart](https://stackoverflow.com/questions/48195340/systemd-with-multiple-execstart):
serivce type 和 ExecStart 个数有关.
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>systemd</tag>
      </tags>
  </entry>
  <entry>
    <title>Wget Use Case Summary</title>
    <url>/2020/09/04/linux-wget-summary/</url>
    <content><![CDATA[
`Wget` is a free utility for non-interactive download of files from the Web.  It supports HTTP, HTTPS, and FTP protocols, as well as retrieval through HTTP proxies. It is fault-tolerance, has big overlap with curl, both heavily using.

`wget -h` and man page are informative.

> 值得注意的是，不同Linux版本wget支持的选项可能不完整。

## Continue download
```bash
## assume the url is disrupted and leave a incomplete file in current folder
## -c,--continue: continue
wget <url> -c
```

## Authz
If you proxy, http_proxy for http connection, https_proxy for https connection.
[Wget basic authentication challenge](https://askubuntu.com/questions/1070838/why-wget-does-not-use-username-and-password-in-url-first-time)
```bash
## http use http_proxy flag

## other options:
## --no-check-certificate: Don't check the server certificate against the available certificate authorities
## --connect-timeout: seconds, TCP connections that take longer to establish will be aborted
## -e, --execute: commands, env varaible
## --auth-no-challenge: for server never sends HTTP authentication challenges, not recommended
wget --no-check-certificate --connect-timeout 10 -e http_proxy="envoy:10000" http://www.httpbin.org/ip --auth-no-challenge --user xxx --password xxx

## https use https_proxy flag
wget --no-check-certificate --connect-timeout 10 -e https_proxy="envoy:10000" https://www.httpbin.org/ip --auth-no-challenge --user xxx --password xxx
```]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>wget</tag>
      </tags>
  </entry>
  <entry>
    <title>Tar Path Format Issue</title>
    <url>/2019/07/03/linux-tar-path/</url>
    <content><![CDATA[
I haven't noticed the path format when I created tarball, this time the format issue plays as a major blocker.

We use a module tar file to deploy `Zen` in ICP4D cluster, when I use my customized tar file, I get:
```
Loading images
/xxx/InstallPackage/modules/./xxx-iisee-zen:1.0.0//images
Loaded Images [==============================================================================] 7m29s (35/35) done
Pushed Images [==============================================================================] 15m17s (35/35) done
Deploying the chart as name xxx-iisee-zen100
Running command: /xxx/InstallPackage/components/dpctl --config /xxx/InstallPackage/components/install.yaml helm rewriteChart -i /xxx/InstallPackage/modules/./xxx-iisee-zen:1.0.0//charts/*.tgz -o /xxx/InstallPackage/modules/./xxx-iisee-zen:1.0.0//charts/updated_xxx-iisee-zen100.tgz
Running command: /xxx/InstallPackage/components/dpctl --config /xxx/InstallPackage/components/install.yaml helm installChart -f /xxx/InstallPackage/components/global.yaml   -r zen-xxx-iisee-zen100 -n zen -c /xxx/InstallPackage/modules/./xxx-iisee-zen:1.0.0//charts/updated_xxx-iisee-zen100.tgz
Starting the installation ...
There was a problem installing /xxx/InstallPackage/modules/xxx-iisee-zen:1.0.0/charts/updated_xxx-iisee-zen100.tgz chart. Reason: chart metadata (Chart.yaml) missing
```

Let's highlight the command:
```
Running command: /xxx/InstallPackage/components/dpctl --config /xxx/InstallPackage/components/install.yaml helm rewriteChart -i /xxx/InstallPackage/modules/./xxx-iisee-zen:1.0.0//charts/*.tgz -o /xxx/InstallPackage/modules/./xxx-iisee-zen:1.0.0//charts/updated_xxx-iisee-zen100.tgz
```
Look carefully there is a `./` in path `/xxx/InstallPackage/modules/./xxx-iisee-zen:1.0.0/....`, this is because I create tarball use:
```
tar cf xxx-iisee-zen-1.0.0.tar ./xxx-iisee-zen:1.0.0
```
the `./` will be put in the extract path! which is the trouble maker.

Correct way:
```bash
cd <xxx-iisee-zen:1.0.0 parent folder>
tar cf xxx-iisee-zen-1.0.0.tar xxx-iisee-zen:1.0.0
```

If we list the tarball contents, no prefix in the file structure:
```bash
tar tvf xxx-iisee-zen-1.0.0.tar

drwxr-xr-x 1001/docker       0 1969-12-31 16:00 xxx-iisee-zen:1.0.0/
drwxr-xr-x 1001/docker       0 2019-07-02 15:13 xxx-iisee-zen:1.0.0/charts/
-rw-r--r-- root/root    245868 2019-07-02 15:12 xxx-iisee-zen:1.0.0/charts/xxx-iisee-zen-1.0.1.tgz
-rw------- 1001/docker    4758 1969-12-31 16:00 xxx-iisee-zen:1.0.0/manifest.yaml
drwxr-xr-x 1001/docker       0 1969-12-31 16:00 xxx-iisee-zen:1.0.0/LICENSES/
...
```

For more information about tar commands, I have a blob about it already [`<<Tar Command Daily Work Summary>>`](https://chengdol.github.io/2019/02/21/linux-tar-summary/)

]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>tar</tag>
      </tags>
  </entry>
  <entry>
    <title>User ID Valid Range</title>
    <url>/2019/07/26/linux-uid-range/</url>
    <content><![CDATA[
From the CloudPak certification requirments, the `uid` for non-root user in container should be set in a higher range. We change it from `1000` to `100321000` but the image build got stuck and failed.

The reason is there is a range for `uid` number, it's described in `/etc/login.defs` file (you can edit this file).
```
cat /etc/login.defs | grep -i UID

UID_MIN                  1000
UID_MAX                 60000
```
So if you create a user by `useradd` without specify user id explicitly, its `uid` will start from `1000`. Also when change the `uid` by `usermod -u <new uid> <user name>`, you need to follow the limitation.

The `gid` has the same restriction:
```
cat /etc/login.defs | grep -i GID

GID_MIN                  1000
GID_MAX                 60000
```
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Set up and Use Local Yum Repository</title>
    <url>/2019/03/05/linux-yum-local-repo/</url>
    <content><![CDATA[
Just like the blogs I wrote before: `Offline Package Installation I` and `Solve Conflicts in RPM installation`, Use `rpm` or bare `yum` command to install downloaded rpm file works but I find somehow this will cause some maintenance problems, for example
```
Warning: RPMDB altered outside of yum.
** Found 51 pre-existing rpmdb problem(s), 'yum check' output follows:
bash-4.2.46-31.el7.x86_64 is a duplicate with bash-4.2.46-30.el7.x86_64
binutils-2.27-34.base.el7.x86_64 is a duplicate with binutils-2.27-28.base.el7_5.1.x86_64
coreutils-8.22-23.el7.x86_64 is a duplicate with coreutils-8.22-21.el7.x86_64
cryptsetup-libs-2.0.3-3.el7.x86_64 is a duplicate with cryptsetup-libs-1.7.4-4.el7.x86_64
```
I need to find a way that can automatically figure out the dependency chain, install the rpm required from download pool.

### Create a yum repository
Install `createrepo` package:
```bash
yum install -y createrepo
```
Next, creates the necessary metadata for your Yum repository, as well as the sqlite database for speeding up yum operations. For example, `/root/docker` directory contains all rpms that install docker needs:
```bash
createrepo --database /root/docker
```
you will find it generates a folder called `repodata` that contains:
```
304457af78cd250275222993fa0da09256f64cc627c1e31fb3ec0848b28b28d8-primary.xml.gz
3d5ab2f5b706e5750e0ebe5802a278525da9cac4b9700634c51c2bfdf04a0d0e-primary.sqlite.bz2
421810a6b2d93e49bfe417404f937e17929f0d8c55953dbe8e96cbb19f40708d-filelists.sqlite.bz2
62c33f23a9485d74076d3db77064a9bdf606ce68d6702cd84fc5c6f1bcb48f01-other.sqlite.bz2
649e08cdba02219eb660f579b89e7a86cf805e4f989222cb1be556a8e0b82b5c-other.xml.gz
6cd1c3a2d6f385b1cbb878a88f86b8ef7e32d6e5c2c32c41a81f51464c3785c7-filelists.xml.gz
repomd.xml
```

### Create yum repo file
To define a new repository, you can either add a `[repository]` section to the `/etc/yum.conf` file, or to a `.repo` file in the `/etc/yum.repos.d/` directory. All files with the `.repo` file extension in this directory are read by yum, and it is recommended to define your repositories here instead of in `/etc/yum.conf`.

For example, create a `docker-local.repo` file in `/etc/yum.repos.d/` directory, `baseurl` points to the folder that holds downloaded rpms:
```
[docker-local.repo]
name=docker-local
baseurl=file:///root/docker
enabled=1
gpgcheck=0
```
Then if you run `yum repolist all`, you will see this new added yum repository:
```
yum repolist all

Loaded plugins: product-id, search-disabled-repos
repo id                 repo name                          status
...
docker-local.repo       docker-local                       enabled:    134
...
```
You can also list enabled and disabled repository:
```
yum repolist enabled
yum repolist disabled
```

### Install using yum
Now you can install docker by running:
```bash
yum install -y docker-ce
```
yum will check local repository and launch dependencies for you.

Sometimes it's better to set `enabled=0` in `.repo` file to disable it by default, so you can run:
```bash
yum --enablerepo=docker-local.repo install -y docker-ce
```

]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>yum</tag>
      </tags>
  </entry>
  <entry>
    <title>Yum Pending Transaction</title>
    <url>/2019/02/28/linux-yum-pending/</url>
    <content><![CDATA[
I need to clean yum pending or unfinished transactions before our installer start to work, otherwise the `yum update` or `yum install` may fail. But where are these pending transactions from? Sometimes the machine is down or unexpected thing happens, the yum installation process failed.

### Problem
you may see error like this:
```
There are unfinished transactions remaining. You might consider running yum-complete-transaction first to finish them.
The program yum-complete-transaction is found in the yum-utils package.

```

### Solution
According to the prompt, we need first install `yum-utils`
```bash
yum install -y yum-utils
```
`yum-complete-transaction` is a program which finds incomplete or aborted yum transactions on a system and attempts to complete them. It looks at the transaction-all* and transaction-done* files which can normally be found in /var/lib/yum if a yum transaction aborted in the middle of execution.

If it finds more than one unfinished transaction it will attempt to complete the most recent one first. You can run it more than once to clean up all unfinished transactions.

Then just issue the following command to do a cleanup:
```bash
yum-complete-transaction --cleanup-only
```
You can also check how many pending transactions exist:
```bash
find /var/lib/yum -maxdepth 1 -type f -name 'transaction-all*' -not -name '*disabled' -printf . | wc -c
```

In Ansible playbookk add task:
```yml
# ensure existence of yum-utils first
- name: clean yum pending transactions
  command: yum-complete-transaction --cleanup-only
  become: true
  args:
    warn: no
```




]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>yum</tag>
      </tags>
  </entry>
  <entry>
    <title>Copy &amp; Paste in Terminal</title>
    <url>/2019/01/27/mac-copy-paste/</url>
    <content><![CDATA[
Mac provides the command line for copy and paste, useful for some tasks.

> reference link：http://osxdaily.com/2007/03/05/manipulating-the-clipboard-from-the-command-line/

## Copy
copy the content of the file
```bash
pbcopy < file.txt
```

Now the content is in clipboard, ready to be pasted.

> you can use pipe to combine command such as `find`,` grep`, `awk` and `cut` to
filter and augment data.

for example:
``` bash
echo "this is a demo" | pbcopy
docker images | grep "iis-" | pbcopy
```

## Paste

Simply run this command:
```bash
pbpaste
```

Redirect content to a file:
```bash
pbpaste > file.txt
```
```bash
pbpaste | grep "xxx"
```

]]></content>
      <categories>
        <category>Mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title>Yum and Rpm Management 101</title>
    <url>/2019/03/26/linux-yum-rpm-management/</url>
    <content><![CDATA[
OK, This article is a summary from IBM developer [Linux series](https://developer.ibm.com/tutorials/l-lpic1-102-5/) that contains something I haven't realized and history information about YUM and RPM.

### Introducing package management
In the past, many Linux programs were distributed as source code, which a user would build into the required program or set of programs, along with the required man pages, configuration files, and so on. Nowadays, most Linux distributors use prebuilt programs or sets of programs called *packages*, which ship ready for installation on that distribution. In this tutorial, you will learn about *package management tools* that help you install, update, and remove packages. This tutorial focuses on the **Red Hat Package Manager (RPM)**, which was developed by Red Hat, as well as the **Yellowdog Updater Modified (YUM)**, which was originally developed to manage Red Hat Linux systems at Duke University’s Physics department. Another tutorial in this series, “[Learn Linux 101: Debian package management](https://developer.ibm.com/tutorials/l-lpic1-102-4/),” covers the package management tools used on Debian systems.

### Package managers
RPM, YUM, and APT (for Debian systems) have many similarities. All can install and remove packages. Information about installed packages is kept in a database. All have basic command-line functionality, while additional tools can provide more user-friendly interfaces. All can retrieve packages from the Internet.

When you install a Linux system, you typically install a large selection of packages. The set may be customized to the intended use of the system, such as a server, desktop, or developer workstation. And at some time you will probably need to install new packages for added functionality, update the packages you have, or even remove packages that you no longer need or that have been made obsolete by newer packages. Let’s look at how you do these tasks, and at some of the related challenges such as finding which package might contain a particular command.

#### RPM
Red Hat introduced RPM in 1995. RPM is now the package management system used for packaging in the Linux Standard Base (LSB). The rpm command options are grouped into three subgroups for:

* Querying and verifying packages
* Installing, upgrading, and removing packages
* Performing miscellaneous functions

#### YUM
YUM adds automatic updates and package management, including dependency management, to RPM systems. In addition to understanding the installed packages on a system, YUM is like the Debian Advanced Packaging Tool (APT) in that it works with repositories, which are collections of packages and are typically accessible over a network connection.

### Install RPM packages
```
root@attic‑f21 ~rpm ‑i  gcc‑gfortran‑4.9.2‑6.fc21.x86_64.rpm 
error: Failed dependencies:
    libquadmath‑devel = 4.9.2‑6.fc21 is needed by gcc‑gfortran‑4.9.2‑6.fc21.x86_64
```
One good thing is that you can give the rpm command a list of packages to install and it will install them all in the right order if all dependencies are satisfied. So you at least don’t have to manually install each piece in the right order.

[What is the difference between i686 and x86_64 packages?](https://unix.stackexchange.com/questions/158244/what-is-the-difference-between-i686-and-x86-64-packages)
Technically, i686 is actually a 32-bit instruction set (part of the x86 family line), while x86_64 is a 64-bit instruction set (also referred to as amd64).

let's see the yum install output in my machine:
```
Loaded plugins: product-id, search-disabled-repos
Local-Base                                                                                | 2.0 kB  00:00:00     
Local-Extras                                                                              | 2.9 kB  00:00:00     
Local-Optional                                                                            | 2.0 kB  00:00:00     
Local-Supplementary                                                                       | 2.0 kB  00:00:00     
(1/6): Local-Base/updateinfo                                                              | 3.2 MB  00:00:00     
(2/6): Local-Supplementary/primary                                                        |  99 kB  00:00:00     
(3/6): Local-Optional/updateinfo                                                          | 2.3 MB  00:00:00     
(4/6): Local-Optional/primary                                                             | 5.0 MB  00:00:00     
(5/6): Local-Supplementary/updateinfo                                                     |  69 kB  00:00:00     
(6/6): Local-Base/primary                                                                 |  34 MB  00:00:00     
Local-Base                                                                                           23907/23907
Local-Optional                                                                                       17526/17526
Local-Supplementary                                                                                      310/310
Resolving Dependencies
--> Running transaction check
---> Package vim-enhanced.x86_64 2:7.4.160-5.el7 will be installed
--> Processing Dependency: vim-common = 2:7.4.160-5.el7 for package: 2:vim-enhanced-7.4.160-5.el7.x86_64
--> Processing Dependency: libgpm.so.2()(64bit) for package: 2:vim-enhanced-7.4.160-5.el7.x86_64
--> Running transaction check
---> Package gpm-libs.x86_64 0:1.20.7-5.el7 will be installed
---> Package vim-common.x86_64 2:7.4.160-5.el7 will be installed
--> Processing Dependency: vim-filesystem for package: 2:vim-common-7.4.160-5.el7.x86_64
--> Running transaction check
---> Package vim-filesystem.x86_64 2:7.4.160-5.el7 will be installed
--> Finished Dependency Resolution
Dependencies Resolved

=================================================================================================================
 Package                      Arch                 Version                        Repository                Size
=================================================================================================================
Installing:
 vim-enhanced                 x86_64               2:7.4.160-5.el7                Local-Base               1.0 M
Installing for dependencies:
 gpm-libs                     x86_64               1.20.7-5.el7                   Local-Base                32 k
 vim-common                   x86_64               2:7.4.160-5.el7                Local-Base               5.9 M
 vim-filesystem               x86_64               2:7.4.160-5.el7                Local-Base                10 k

Transaction Summary
=================================================================================================================
Install  1 Package (+3 Dependent packages)

Total download size: 7.0 M
Installed size: 23 M

Is this ok [y/d/N]: y
Downloading packages:
(1/4): gpm-libs-1.20.7-5.el7.x86_64.rpm                                                   |  32 kB  00:00:00     
(2/4): vim-enhanced-7.4.160-5.el7.x86_64.rpm                                              | 1.0 MB  00:00:00
(3/4): vim-filesystem-7.4.160-5.el7.x86_64.rpm                                            |  10 kB  00:00:00
(4/4): vim-common-7.4.160-5.el7.x86_64.rpm                                                | 5.9 MB  00:00:00
-----------------------------------------------------------------------------------------------------------------
Total                                                                             55 MB/s | 7.0 MB  00:00:00
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
Warning: RPMDB altered outside of yum.
** Found 1 pre-existing rpmdb problem(s), 'yum check' output follows:
mokutil-15-1.el7.x86_64 is a duplicate with mokutil-12-1.el7.x86_64
  Installing : gpm-libs-1.20.7-5.el7.x86_64                                                                  1/4
  Installing : 2:vim-filesystem-7.4.160-5.el7.x86_64                                                         2/4
  Installing : 2:vim-common-7.4.160-5.el7.x86_64                                                             3/4
  Installing : 2:vim-enhanced-7.4.160-5.el7.x86_64                                                           4/4
Local-Base/productid                                                                      | 2.1 kB  00:00:00
Local-Supplementary/productid                                                             | 2.1 kB  00:00:00
  Verifying  : 2:vim-enhanced-7.4.160-5.el7.x86_64                                                           1/4
  Verifying  : 2:vim-common-7.4.160-5.el7.x86_64                                                             2/4
  Verifying  : 2:vim-filesystem-7.4.160-5.el7.x86_64                                                         3/4
  Verifying  : gpm-libs-1.20.7-5.el7.x86_64                                                                  4/4

Installed:
  vim-enhanced.x86_64 2:7.4.160-5.el7

Dependency Installed:
  gpm-libs.x86_64 0:1.20.7-5.el7    vim-common.x86_64 2:7.4.160-5.el7    vim-filesystem.x86_64 2:7.4.160-5.el7

Complete!
```
Here we see yum find `x86_64` version of vim in `Local-Base` repository. Sometimes you will usually want the latest version of a package, but you can provide additional qualifications if you need an earlier version, or the `i686` version instead of the `x86_64` version. See the section on specifying package names in the man pages for the yum command.

### Package locations
Where do the packages come from? How does yum know where to download packages from? The starting point is the `/etc/yum.repos.d/` directory, which usually contains several repo files. This is the default location for repository information, but other locations may be specified in the YUM configuration file, normally `/etc/yum.conf`. 

In the Fyre machine, there is a `devit-rh7-x86_64.repo` file:
```
# Base OS packages
[Local-Base]
name=Fyre Local OS repository
baseurl=http://fyreyum1.fyre.ibm.com/redhat/yum/server/7/7Server/x86_64/os
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release
gpgcheck=1
enabled=1

[Local-Supplementary]
name=Fyre Local Supplementary repository
baseurl=http://fyreyum1.fyre.ibm.com/redhat/yum/server/7/7Server/x86_64/supplementary/os
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release
gpgcheck=1
enabled=1

[Local-Optional]
name=Fyre Local Optional repository
baseurl=http://fyreyum1.fyre.ibm.com/redhat/yum/server/7/7Server/x86_64/optional/os
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release
gpgcheck=1
enabled=1

[Local-Extras]
name=Fyre Local Extras repository
baseurl=http://fyreyum1.fyre.ibm.com/redhat/yum/server/7/7Server/x86_64/extras/os
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release
gpgcheck=1
enabled=1
```

YUM and RPM use a local database to determine what packages are installed. The metadata about packages that is stored in the local database is retrieved from the enabled repositories. Although you will seldom need to worry about the local database, you use the command `yum clean all` to clean out various parts of the locally stored information and `yum makecache` to create the information in your local database for the enabled repos. You might do this if you change your repo configuration, for example.

### Removing RPM packages
The RPM system does not maintain information on packages that were automatically added, so there is no trivial way to find out which dependencies might also be removed. 

If you use YUM and if the package you are trying to remove is a *dependent package* for some other installed packages, then YUM will offer to remove those as well as the dependent package. (This is different from `yum autoremove`)

### Upgrading RPM packages
You can use `yum update` to update your entire system, or you can specify a single package or a wildcard specification. Listing 8 shows how to update all the packages whose names start with “pop”. Note the use of apostrophes to prevent shell expansion of the “*”.
```
yum update 'pop*'
yum update elasticsearch-7.10.2
```
[what is the difference between yum update and yum upgrade?](https://unix.stackexchange.com/questions/55777/in-centos-what-is-the-difference-between-yum-update-and-yum-upgrade)
`yum upgrade` forces the removal of obsolete packages, while `yum update` may or may not also do this. The removal of obsolete packages can be risky, as it may remove packages that you use.
This makes `yum update` the safer option.

### Querying RPM packages
In our examples you saw that installing an rpm with the `rpm` command requires the full name of the package file (or URL), such as `gcc-gfortran-4.9.2-6.fc21.x8664.rpm`. On the other hand, installing with `yum`, or removing an rpm with either command requires only the package name, such as `gcc-gfortran`. As with APT, YUM maintains an internal database of your installed packages, allowing you to manipulate installed packages using the package name.

Note that you need to have root authority to install, upgrade, or remove packages, but non-root users can perform queries against the rpm database.

Basic query asks if package is installed, if so, show version number:
```
[root@mycentctl1 ~]# rpm -q bind-utils
bind-utils-9.9.4-73.el7_6.x86_64

[root@mycentctl1 ~]# rpm -q ansible
package ansible is not installed

[root@mycentctl1 ~]# yum list bind-utils
Loaded plugins: product-id, search-disabled-repos
Installed Packages
bind-utils.x86_64                                    32:9.9.4-73.el7_6                                    @Local-Base

[root@mycentctl1 ~]# yum list ansible
Loaded plugins: product-id, search-disabled-repos
Available Packages
ansible.noarch                                       2.4.2.0-2.el7                                       Local-Extras
```
Display info about a package:
```
[root@mycentctl1 ~]# yum info bind-utils
Loaded plugins: product-id, search-disabled-repos
Installed Packages
Name        : bind-utils
Arch        : x86_64
Epoch       : 32
Version     : 9.9.4
Release     : 73.el7_6
Size        : 431 k
Repo        : installed
From repo   : Local-Base
Summary     : Utilities for querying DNS name servers
URL         : http://www.isc.org/products/BIND/
License     : ISC
Description : Bind-utils contains a collection of utilities for querying DNS (Domain
            : Name System) name servers to find out information about Internet
            : hosts. These tools will provide you with the IP addresses for given
            : host names, as well as other information about registered domains and
            : network addresses.
            :
            : You should install bind-utils if you need to get information from DNS name
            : servers.

```
Search package names and descriptions for a term
```
[root@mycentctl1 ~]# yum search vim
Loaded plugins: product-id, search-disabled-repos
================================================= N/S matched: vim ==================================================
golang-vim.noarch : Vim plugins for Go
protobuf-vim.x86_64 : Vim syntax highlighting for Google Protocol Buffers descriptions
vim-X11.x86_64 : The VIM version of the vi editor for the X Window System
vim-common.x86_64 : The common files needed by any version of the VIM editor
vim-enhanced.x86_64 : A version of the VIM editor which includes recent enhancements
vim-filesystem.x86_64 : VIM filesystem layout
vim-minimal.x86_64 : A minimal version of the VIM editor

  Name and summary matches only, use "search all" for everything.
```

### RPM packages and files in them
List the files inside the package, you will see `host` command is here:
```
[root@mycentctl1 ~]# rpm -ql bind-utils
/etc/trusted-key.key
/usr/bin/dig
/usr/bin/host
/usr/bin/nslookup
/usr/bin/nsupdate
/usr/share/man/man1/dig.1.gz
/usr/share/man/man1/host.1.gz
/usr/share/man/man1/nslookup.1.gz
/usr/share/man/man1/nsupdate.1.gz
```
if you have a download package and want to know the files in it, `-p` means package:
```
[root@mycentctl1 ~]# rpm -qlp jq-1.5-1.el7.x86_64.rpm
warning: jq-1.5-1.el7.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID 352c64e5: NOKEY
/usr/bin/jq
/usr/lib64/libjq.so.1
/usr/lib64/libjq.so.1.0.4
/usr/share/doc/jq/AUTHORS
/usr/share/doc/jq/COPYING
/usr/share/doc/jq/README
/usr/share/doc/jq/README.md
/usr/share/man/man1/jq.1.gz
```

### Which package owns a file?
For YUM:
```
yum provides vim
```
For RPM:
```
rpm -qf `which vim`
```


]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>yum</tag>
        <tag>rpm</tag>
      </tags>
  </entry>
  <entry>
    <title>Yum and Rpm Daily Work Summary</title>
    <url>/2019/03/02/linux-yum-rpm-summary/</url>
    <content><![CDATA[
This article used to walk you through some commonly `yum` and `rpm` usages , based on a real life scenario.

\################################################################
\#  &emsp; Date &emsp; &emsp; &emsp; &emsp; &emsp; Description
\#  &emsp; 03/05/2019 &emsp; &emsp; yum autoremove
\#  &emsp; 03/02/2019 &emsp; &emsp; upgrade rpm
\#  &emsp; 03/01/2019 &emsp; &emsp; list rpm dependencies
\#  &emsp; 02/27/2019 &emsp; &emsp; yum provides
\#  &emsp; 02/25/2019 &emsp; &emsp; search rpm installed
\#  &emsp; 02/24/2019 &emsp; &emsp; install rpm
\#  &emsp; 01/19/2019 &emsp; &emsp; remove package
\#
\################################################################

Yum command [cheat sheet](https://access.redhat.com/sites/default/files/attachments/rh_yum_cheatsheet_1214_jcs_print-1.pdf)
`rpm` command is one of the package management command.

### 01/19/2019 
Remove or erase a installed package with its dependencies:
```bash
rpm -ev <package name>
yum erase <package name>
```
if the rpm is part of other dependencies, `rpm -ev` will fail, or you can use `yum erase` to delete them all:
```bash
rpm -ev containerd.io

error: Failed dependencies:
        containerd.io >= 1.2.2-3 is needed by (installed) docker-ce-3:18.09.2-3.el7.x86_64
```

Remove or erase a installed package **without** checking for dependencies
```bash
rpm -ev --nodeps <package name>
```
For example:
```bash
rpm -ev --nodpes containerd.io

Preparing packages...
containerd.io-1.2.2-3.3.el7.x86_64
```

### 02/24/2019
This command will install a single rpm file if it meets all dependencies, otherwise install will fail and the output will show you the missig rpms.
```bash
rpm -ivh <rpm name>
```
For example:
```bash
rpm -ivh 416b2856f8dbb6f07a50a46018fee8596479ebc0eaeec069c26bedfa29033315-kubeadm-1.13.2-0.x86_64.rpm

warning: 416b2856f8dbb6f07a50a46018fee8596479ebc0eaeec069c26bedfa29033315-kubeadm-1.13.2-0.x86_64.rpm: Header V4 RSA/SHA512 Signature, key ID 3e1ba8d5: NOKEY
error: Failed dependencies:
        cri-tools >= 1.11.0 is needed by kubeadm-1.13.2-0.x86_64
        kubectl >= 1.6.0 is needed by kubeadm-1.13.2-0.x86_64
        kubelet >= 1.6.0 is needed by kubeadm-1.13.2-0.x86_64
        kubernetes-cni >= 0.6.0 is needed by kubeadm-1.13.2-0.x86_64
```

### 02/25/2019
These two both work:
```bash
## query package installed
rpm -qa | grep <package name>
yum list installed | grep <package name>
```
For example:
```bash
rpm -qa | grep docker
docker-ce-18.06.1.ce-3.el7.x86_64
```
```bash
yum list installed | grep docker
docker-ce.x86_64                18.06.1.ce-3.el7           installed 
```

### 02/27/2019
Find packages that provide the queried file, for example:
```bash
yum provides host

32:bind-utils-9.9.4-14.el7.x86_64 : Utilities for querying DNS name servers
Repo        : Local-Base
Matched from:
Filename    : /usr/bin/host
...
```
Next you can install it:
```
yum install -y bind-utils
```

### 03/01/2019
If you have a local rpm file, you can list its dependencies by running:
```bash
rpm -qpR <rpm name>
```
For example:
```bash
rpm -qpR 416b2856f8dbb6f07a50a46018fee8596479ebc0eaeec069c26bedfa29033315-kubeadm-1.13.2-0.x86_64.rpm

warning: 416b2856f8dbb6f07a50a46018fee8596479ebc0eaeec069c26bedfa29033315-kubeadm-1.13.2-0.x86_64.rpm: Header V4 RSA/SHA512 Signature, key ID 3e1ba8d5: NOKEY
cri-tools >= 1.11.0
kubectl >= 1.6.0
kubelet >= 1.6.0
kubernetes-cni >= 0.6.0
rpmlib(CompressedFileNames) <= 3.0.4-1
rpmlib(FileDigests) <= 4.6.0-1
rpmlib(PayloadFilesHavePrefix) <= 4.0-1
rpmlib(PayloadIsXz) <= 5.2-1
```

### 03/02/2019
If you run `man rpm`, there are two similar statements:
```
The general form of an rpm upgrade command is

rpm {-U|--upgrade} [install-options] PACKAGE_FILE ...

       This upgrades or installs the package currently installed to a newer version.  This is the same as  install,
       except all other version(s) of the package are removed after the new package is installed.

rpm {-F|--freshen} [install-options] PACKAGE_FILE ...

       This will upgrade packages, but only ones for which an earlier version is installed.

```
Both `rpm -Fvh` and `rpm -Uvh` will perform the same task but the diff is `rpm -Uvh` is also same as `rpm -ivh`, you can use any of them I mean `rpm -ivh` or `rpm -Uvh` for installing the package.

But for upgrading installed package you can use any of `rpm -Fvh` or `rpm -Uvh`.

`rpm -Fvh` is used for upgrading the existing package (installed package).
`rpm -Uvh` is used for installing the package and upgrading the package both.

For example, upgrade `ansible` from `2.4.6.0` to `2.7.8`:
```
rpm -Fvh ansible-2.7.8-1.el7.ans.noarch.rpm

warning: ansible-2.7.8-1.el7.ans.noarch.rpm: Header V4 RSA/SHA1 Signature, key ID 442667a9: NOKEY
Preparing...                          ################################# [100%]
Updating / installing...
   1:ansible-2.7.8-1.el7.ans          ################################# [ 50%]
Cleaning up / removing...
   2:ansible-2.4.6.0-1.el7.ans        ################################# [100%]
```

### 03/05/2019
Remove dependencies which are not in use, any unneeded dependencies from your system, for example:
```bash
yum autoremove docker-ce
```
```
Dependencies Resolved

=========================================================================================================================
 Package                            Arch               Version                      Repository                      Size
=========================================================================================================================
Removing:
 docker-ce                          x86_64             18.06.1.ce-3.el7             @docker-local.repo             168 M
Removing for dependencies:
 container-selinux                  noarch             2:2.68-1.el7                 @Local-Extras                   36 k
 libcgroup                          x86_64             0.41-20.el7                  @Local-Base                    134 k
 libseccomp                         x86_64             2.3.1-3.el7                  @Local-Base                    297 k
 libtool-ltdl                       x86_64             2.4.2-22.el7_3               @Local-Base                     66 k
 policycoreutils-python             x86_64             2.5-29.el7_6.1               @Local-Base                    1.2 M

Transaction Summary
=========================================================================================================================
Remove  1 Package (+5 Dependent packages)

```
You also can add `clean_requirements_on_remove=1` in `/etc/yum.conf` file, then run
```
yum remove docker-ce
```
the same effect as using `autoremove`.
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>yum</tag>
        <tag>rpm</tag>
      </tags>
  </entry>
  <entry>
    <title>Useful Chrome Extensions</title>
    <url>/2020/09/20/mac-chrome-plugins/</url>
    <content><![CDATA[
Some useful Chrome extensions:
- `OneTab`: 把多个tabs整理成列表在一个网页中，可分享。
- `Grammarly for Chrome`: 英语语法检查。
- `SimpleUndoClose`: 重新打开关闭的网页。
- `Screen Shader`: 护眼模式，可调。
- `Dark Reader`: 夜晚模式。
- `清理大师Clean Master`: 一键清理浏览器的缓存记录，可配置。
- `Octotree`: 可以在侧边栏显示github 仓库代码的结构，点击跳转。
- `Tab Rsize`: 拆分浏览器页面，方便对比查看内容。
- `JSON Formatter`: 自动格式化JSON文件在浏览器中。
- `Awesome Autocomplete for GitHub`: 搜索github项目时可自动补全，方便查找。
- `Wappalyzer`: 识别当前网站所用的技术栈。
- `扩展管理器Extension Manager`: 插件统一管理工具。
- `ZenHub for GitHub`: zenhub 的管理插件，需要登录。
]]></content>
      <categories>
        <category>Mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
        <tag>chrome</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka Quick Start</title>
    <url>/2022/04/21/message-kafka-learn/</url>
    <content><![CDATA[
[x] how does the producer know who is the leader? Ask zookeeper, leader is partition based.
[x] each topic has its own set of partitions, each partiton is assigned subset of coming events.
[x] producer send message to which topic partition leader? depends on parition stragegies.
[x] how to ensure the message order that treats entire topic as a whole? impossible, but order is guarantee in partition level.
[x] [kafka command bootstrap-server vs zookeeper](https://stackoverflow.com/questions/46173003/bootstrap-server-vs-zookeeper-in-kafka)

The Vagrant demo file please see my git repo [Infratree](https://github.com/chengdol/InfraTree/tree/master/vagrant-zk-kafka), reading with [`Zookeeper Quick Start`](https://chengdol.github.io/2020/11/27/message-zookeeper-learn/)

I took the course from PluralSight and Udemy, both are good, Udemy course is some what out-of-date.
There are other tutorial videos:
- [Apache Kafka Crash Course](https://www.youtube.com/watch?v=R873BlNVUB4)

# Introduction
- [Kafka简明教程](https://zhuanlan.zhihu.com/p/37405836)
- [Kafka in a Nutshell](https://sookocheff.com/post/kafka/kafka-in-a-nutshell/)
- [Apach Kafka Official Doc](http://kafka.apache.org/)
- [Learn Kafka Guide](https://www.ibm.com/cloud/learn/apache-kafka)
- [kcat](https://github.com/edenhill/kcat), just like netcat to network.
- [Understanding Kafka Topics and Partitions](https://stackoverflow.com/questions/38024514/understanding-kafka-topics-and-partitions)


## Important Terms
- partition (divisions of a topic)
- topic (partitioned and distributed in broker, each partition has one leader broker + followers(replicas))
- producer (generate data, producer client only writes to the leader broker of that partition)
- consumer (read data from topic(partition) within a consumer group)
- broker (node in kafka cluster)

Producers and consumers are both kafka clients.

A topic is similar to a folder in a filesystem, and the events are the files in that folder. An example topic name could be "payments". Topics in Kafka are always multi-producer and multi-subscriber: a topic can have zero, one, or many producers that write events to it, as well as zero, one, or many consumers that subscribe to these events.

Topics are partitioned, meaning a topic is spread over a number of "buckets" located on different Kafka brokers. This distributed placement of your data is very important for scalability because it allows client applications to both read and write the data from/to many brokers at the same time. When a new event is published to a topic, it is actually appended to `one` of the topic's partitions. Events with the same event key (e.g., a customer or vehicle ID) are written to the same partition, Kafka guarantees that any consumer of a given `topic-partition` will always read that partition's events in exactly the same order as they were written.

Note that message ordering for the entire topic is not guaranteed.

Kafka clents have 3 different levels of consistency, see `Consistency as a Kafka Client` in [Kafka in a Nutshell](https://sookocheff.com/post/kafka/kafka-in-a-nutshell/).


## Partition
Producer sends event to broker and broker places the event in one of the partitions.
比如这里有3个partitions, 2个consumers 在一个consumer group中，则consumer2 会对应到p2, p3读取事件，如果后来增加一个consumer3在这个consumer group中，则kafka 会partition rebalance p2 或 p3 到新增的消费者中，于是就变成了每个consumer 对应一个partiton。这样处理就不会因为consumer 数量的变化而导致event被重复读取或者没有被处理，也增加了throughput. 

如果number of consumer > number of partition 则多余的consumer 被闲置了，因此partition 的数量几乎决定了consumer 可有的最大数量。
```js
                              broker              consumer group
                        +---------------+        +------------+
                        |  partition1---|--------|->consumer1 |
           (batch send) |/              |        |            |
producer -------------->|- partition2---|--------|->consumer2 |
                        |\              |        |/           |
                        |  partition3---|--------|            |
                        +---------------+        +------------+
```

关于event送到哪个 partition 是 Partition strategy 决定的，for example, in the logstash Kafka output plugin it has [partitioner](https://www.elastic.co/guide/en/logstash/current/plugins-outputs-kafka.html#plugins-outputs-kafka-partitioner) config, the default is round-robin, if message key is specified, then hash will be used. 所以如何选择 key 很重要，它决定了送到partition 中的event 数量是否平衡，还要注意，kafka event order matters only within a single partition. 所以如果要保证某一类event 的顺序，则需要把它们划分到同一个partition中, so custom partitioning is possible and is reserved for special cases.

I have encountered the issue that uneven districution of message among partitions with round-robin strategy, the discussion and possible cause can be seen [here](https://stackoverflow.com/questions/50893130/uneven-distribution-of-messages-in-kafka-partitions)

Multiple consumer groups can consume the same topic, each group is isolated: A topic can be consumed by many consumer groups and each consumer group will have many consumers. Also see this [question](https://stackoverflow.com/questions/45175424/how-multiple-consumer-group-consumers-work-across-partition-on-the-same-topic-in).

## Producer
For example: the logstash output kafka plugin is the producer client that sends log/message to kafka cluster.

The producer has its own set of configuration, for example the logstash kafka output [config](https://www.elastic.co/guide/en/logstash/current/plugins-outputs-kafka.html#plugins-outputs-kafka), they are aligned with the kafka producer [config](https://kafka.apache.org/documentation/#producerconfigs)

```ini
# It denotes the number of brokers that must receive the record before we consider the write as successful.
# this is important, the default is 1 in some producer config and it may introduce less explicit data loss sometimes 
acks=0,1,all(-1)
```
Could config the batch size, compress, acks for producer. This is a tradeoff between latency and throughput. In kafka host data folder, partitions are stored here. 可以通过设置batch size大小去观察固定数量的event增加的空间大小，一般batch size越大，越节约broker的disk space.

There is a article talks about `acks`:
[Kafka Acks Explained](https://betterprogramming.pub/kafka-acks-explained-c0515b3b707e)

Also `batch.size` and `linger.ms` [explained](https://www.cloudkarafka.com/blog/kafka-producer-side-settings-explained-linger-ms-and-batch-size.html) and the [behavior](https://stackoverflow.com/questions/51521737/apache-kafka-linger-ms-and-batch-size-settings) when `linger.ms` = 0 but `batch.size` > 0.

## Consumer
For example, the logstash input kafka plugin is the consumer to read the data from kafka cluster.

The consumer has its own set of configuration, for example the lostash input kafak [config](https://www.elastic.co/guide/en/logstash/current/plugins-inputs-kafka.html), they are aligned with the kafka consumer [config](https://kafka.apache.org/documentation/#consumerconfigs)

## High Availability
New Kafka node will register itself via zookeeper and learn about other broker in the cluster. In production zookeeper usually has 3 ~ 5 nodes to form a cluster. Kafka broker will discover each other through zookeeper.

Producer should set acks to receive ack from broker for message commitment.

## Performance
It is related to disk I/O, network, RAM, CPU, OS, etc.

`export KAFKA_HEAP_OPTS="-Xmx4g"`, to set the Java heap size for Kafka (max amount), then start the Kafka. You can monitor the heap size overtime, increase it if needed.

Make sure swapping is disabled for Kafka entirely.

Increase the file descriptor limits on your linux, at least 100,000 as a starting point.

Run Kafka only on your macine, anything else will slow down the machine.

## Encountered Issue
[Lose messages in Kafka](https://developer20.com/when-you-can-nose-messages-in-kafka/), on both producer and consumer side.


# Quick Start
## Single node
Use one Vanguard VM with Java pre-installed to do the expirement:
http://kafka.apache.org/quickstart
not using the self-contained zookeeper:
https://dzone.com/articles/kafka-setup

## Cluster Setup
Follow up the zk cluster setup, I use the same cluster nodes, kafka version is `2.6.0`.
3 node IPs, sudo su to use root user.
```js
// zk leader or follower not reflect real scenario

  192.168.20.20        192.168.20.21         192.168.20.22
|--------------|    |-----------------|    |----------------|
| zk server1<--|----|--> zk server2<--|----|-->zk server3   | 
|  follower    |    |     leader      |    |    follower    |
|              |    |     /  |  \     |    |                |
|              |    |    /   |   \    |    |                |
|           /--|----|---/    |    \---|----|--\             |
|  kafka   /   |    |    kafka        |    |   \kafka       |
|  broker1     |    |    broker2      |    |    broker3     |
|--------------|    |-----------------|    |----------------|
```
Kafka archive download [link](https://archive.apache.org/dist/kafka).

Download kafka binary and untar:
```bash
wget -q https://mirrors.sonic.net/apache/kafka/2.6.0/kafka_2.12-2.6.0.tgz
tar -zxf kafka_2.12-2.6.0.tgz

cd /root/kafka_2.12-2.6.0/config
```

Create custom `log.dir` folder, update in configuration file:
```bash
mkdir -p /root/kafka/log
```

Updates `zookeeper.connect`, `num.partitions` and `log.dir`. they are the same for all brokers.
`broker.id` and `advertised.listeners`, these two need to be unique for each broker, this is not a exhausted list, more info see [here](https://kafka.apache.org/documentation/#configuration):
```bash
############################# Server Basics #############################
# The id of the broker. This must be set to a unique integer for each broker.
broker.id=1

# delete topic enable, default is true
delete.topic.enable=true


############################# Socket Server Settings #############################
# Hostname and port the broker will advertise to producers and consumers. If not set,
# it uses the value for "listeners" if configured.  Otherwise, it will use the value
# returned from java.net.InetAddress.getCanonicalHostName().
advertised.listeners=PLAINTEXT://192.168.20.20:9092

############################# Zookeeper #############################

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.

# zookeeper.connect=192.168.20.20:2181,192.168.20.21:2181,192.168.20.22:2181/kafka
# /kafka的作用就是把所有kafka相关的folder都放在zk的kafka目录下, 从zk CLI中可以看到
zookeeper.connect=192.168.20.20:2181,192.168.20.21:2181,192.168.20.22:2181

############################# Log Retention Policy #############################

# The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.

# The minimum age of a log file to be eligible for deletion
log.retention.hours=168

# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining
# segments don't drop below log.retention.bytes.
#log.retention.bytes=1073741824

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes=536870912

# The interval at which log segments are checked to see if they can be deleted according 
# to the retention policies
log.retention.check.interval.ms=60000

# By default the log cleaner is disabled and the log retention policy will default to just delete segments after their retention expires.
# If log.cleaner.enable=true is set the cleaner will be enabled and individual logs can then be marked for log compaction.
log.cleaner.enable=true
############################# Log Basics #############################

# A comma separated list of directories under which to store log files
log.dirs=/root/kafka/log

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
# 指的是每个topic 分成多少份partition, can > broker number
num.partitions=5

# 指的是每个partition的备份, 小于或等于 broker的个数
default.replication.factor=3

# in-sync replica (ISR) is a broker that has the latest data for a given partition.
# A leader itself is always an in-sync replica. 
# A follower is an in-sync replica only if it has fully caught up to the partition it’s following.
# 注意，每个replica 都可以达到in-sync的状态(slow or quick)，这个min.insync.replicas only works with the acks=all config in producer side!
# it denotes that at least this number is met, acks=all can succeed
# default is 1, typicially = default.replication.factor - 1
min.insync.replicas=2
```
The log retention `log.retention.hours=168` is for deleting message in Kafka, whether or not the messages have been consumed, see this [question](https://stackoverflow.com/questions/28586008/delete-message-after-consuming-it-in-kafka)

```bash
cd /root/kafka_2.12-2.6.0
# create a log file
touch /root/kafka/run.log
# start kafka
bin/kafka-server-start.sh config/server.properties > /root/kafka/run.log 2>1 &

# or run as daemon
bin/kafka-server-start.sh -daemon config/server.properties
# stop kafka
bin/kafka-server-stop.sh
```
Kafka log is configured by `conf/log4j.properties`, by default is under `<kafka package>/logs/server.log` file.

Create and output topic description, can run in anyone of the nodes, `localhost` can be one of the broker IPs or list of IPs combination, separate by comma. In old version you use the `--zookeeper` option to list all zk <ip:port> or <ip:port>/kafka(if you use chroot).

如果想从外部connect kafka broker, `advertised.listeners` 必须使用public IP or DNS hostname, 如果用集群内部的private IP 或者 localhost 则外部不能访问了。

partition number and replic factor在 command 都可以override default的。
```bash
# create topic
# --bootstrap-server: Kafka server to connect to
bin/kafka-topics.sh --create --bootstrap-server localhost:9092 \
--topic test 

# list topics created
bin/kafka-topics.sh --bootstrap-server localhost:9092 --list 

# topic verbose
bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 \
--topic <topic name> 

# delete topic
# delete.topic.enable=true by default
# deletion is not reversible
bin/kafka-topics.sh --delete --bootstrap-server localhost:9092 \
--topic <topic name> 
```

Producer and consumer, can run on anyone of the nodes:
```bash
# produce message 
bin/kafka-console-producer.sh --topic test --bootstrap-server localhost:9092

# read last one message in topic test of consume1 group
# note that one topic can be consumed by different consumer group
# each has separate offset
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --group consume1 --max-messages 1

# view message based on consumer group
# --group: consumer group
# will not remove messge in topic, just view
# --from-beginning: read from topic beginning
# --max-messages: max message to show
# --max-messages: formater
bin/kafka-console-consumer.sh --topic test --group consume1 --from-beginning --bootstrap-server localhost:9092 --max-messages 2 --property print.timestamp=true

# read consumer groups
bin/kafka-consumer-groups.sh  --list --bootstrap-server localhost:9092

# check partition/offset/lag messages in each consumer group/topic
# Consumer lag indicates the lag between Kafka producers and consumers. If the rate of
# production of data far exceeds the rate at which it is getting consumed, consumer
# groups will exhibit lag.
bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe  --group <group name>
# or 
# they have the same output
bin/kafka-run-class.sh kafka.admin.ConsumerGroupCommand \
--group <group name> \
--bootstrap-server localhost:9092 \
--describe
```

Update topic settings:
```bash
# partition number can only be increase
bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic test --partitions 4
```

How to know all Kafka brokers are ready and running? Can check with zk ephemeral node, see [here](https://stackoverflow.com/questions/37920923/how-to-check-whether-kafka-server-is-running)
```bash
echo dump | nc localhost 2181 | grep brokers
```


## Perf Test
To see the kafka resiliency, we can have a perf test:
```bash
# generate 10KB of random data
base64 /dev/urandom | head -c 10000 | egrep -ao "\w" | tr -d '\n' > file10KB.txt

# in a new shell: start a continuous random producer
./kafka-producer-perf-test.sh --topic perf \
    --num-records 10000 \
    --throughput 10 \
    --payload-file file10KB.txt \
    --producer-props acks=1 bootstrap.servers=localhost:9092 \
    --payload-delimiter A

# in a new shell: start a consumer
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic perf

# then do:
# kill one kafka server
# kill another kafka server
# kill the last server

# start back the servers one by one
```
Then open another terminal to consume the data, meanwhile go to kill the kafka broker one by one to see the result.

## Kafka UI
There is a open source Web UI tool for managing Kafka cluster, [CMAK](https://github.com/yahoo/CMAK).
You can build a docker image of it. Another open source for viewing topic messages [Kowl](https://github.com/cloudhut/kowl)


# Run as Daemon
Similar to Zookeeper daemon set. Set Kafka as system daemon so that it will be launched every time system boots.

For example, generate service file `kafka.service` in `/etc/systemd/system` folder.
Double curly brackets is placeholder in jinja2 template.
```ini
[Unit]
Description=Apache Kafka server (broker)
Documentation=http://kafka.apache.org/documentation.html
# after zookeeper is up
After=network-online.target consul.service zookeeper-server.service dnsmasq.service
StartLimitInterval=200
StartLimitBurst=5

[Service]
# long-runing without forking
Type=simple
User={{ kafka_user }}
Group={{ kafka_group }}
# start, stop
ExecStart=/opt/kafka/bin/kafka-server-start.sh {{ kafka_conf_dir }}/server.properties
ExecStop=/opt/kafka/bin/kafka-server-stop.sh

OOMScoreAdjust=-500
Restart=on-failure
RestartSec=30

[Install]
WantedBy=multi-user.target
```
More detail about `systemd` please search and see my systemd blog.

Then you must enable kafka starts on boot:
```bash
systemctl daemon-reload
# enable start on boot
systemctl enable kafka
```

Other systemd commands:
```bash
systemctl start kafka
systemctl stop kafka
systemctl restart kafka

# reload config without restart
systemctl reload kafka
# first try relaod, if not supports then restart
systemctl reload-or-restart kafka
```
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper Quick Start</title>
    <url>/2020/11/27/message-zookeeper-learn/</url>
    <content><![CDATA[
The Vagrant demo file please see my git repo [Infratree](https://github.com/chengdol/InfraTree/tree/master/vagrant-zk-kafka) 

# Introduction
Zookeeper quorum architecture, at least 3 nodes cluster setup via Vagrant for Kafka use.
Hght level information about zk:
- distributed key value store
- has voting mechanism
- used by many big data tools

[Zookeeper role in Kafka](https://www.quora.com/What-is-the-actual-role-of-Zookeeper-in-Kafka-What-benefits-will-I-miss-out-on-if-I-don%E2%80%99t-use-Zookeeper-and-Kafka-together): 
- broker registration, heart-beating check
- maintaining a list of topics alongside
- leader election
- store kafka cluster id
- store ACLs if security is enabled
- quotas config if enabled

使用kafka自带的zookeeper 还是 独立的zookeeper呢？见这个[回答](https://segmentfault.com/q/1010000021110446/a-1020000021113974)参考. 工作项目中还是用的独立的zk. Zookeeper is going to be removed from kafka, see this [article](https://www.confluent.io/blog/removing-zookeeper-dependency-in-kafka/).

More references please see [Zookeeper main page](https://zookeeper.apache.org/).
[System Requirements](https://zookeeper.apache.org/doc/r3.6.2/zookeeperAdmin.html#sc_systemReq) for version `3.6.2`: ZooKeeper (also Kafka) runs in `Java`, release 1.8 or greater (JDK 8 LTS, JDK 11 LTS, JDK 12 - Java 9 and 10 are not supported).

Archive download [link](https://archive.apache.org/dist/zookeeper/). For example, here uses Zookeeper version `3.6.2`.

# Performance Factors
Latency is key for zookeeper:
- fast disk
- no RAM swap
- separate disk for snapshots and logs
- high performance network
- resonable number of zk servers
- isolation zk process from others


# Cluster Setup
## Architecture
[Clustered (Multi-Server) Setup](https://zookeeper.apache.org/doc/r3.6.2/zookeeperAdmin.html).
这里面说了很多注意事项, 比如Java heap size to avoid swapping or disable swapping.

The architecture diagram of this experiment, co-locate zk and kafka in the same node, this is not recommended on production.
```js
// zk leader can be anyone of them

  192.168.20.20        192.168.20.21         192.168.20.22
|--------------|    |-----------------|    |----------------|
| zk server1<--|----|--> zk server2<--|----|-->zk server3   | 
|  follower    |    |     leader      |    |    follower    |
|              |    |     /  |  \     |    |                |
|              |    |    /   |   \    |    |                |
|           /--|----|---/    |    \---|----|--\             |
|  kafka   /   |    |    kafka        |    |   \kafka       |
|  broker1     |    |    broker2      |    |    broker3     |
|--------------|    |-----------------|    |----------------|
```

## Config
Generate `zoo.cfg` file in each node:
```bash
# download release binary version 3.6.2
cd /root
wget https://archive.apache.org/dist/zookeeper/zookeeper-3.6.2/apache-zookeeper-3.6.2-bin.tar.gz
tar -zxf apache-zookeeper-3.6.2-bin.tar.gz

cd /root/apache-zookeeper-3.6.2-bin/conf
cp zoo_sample.cfg zoo.cfg
```

Create zk data, log and conf directories in each zookeeper node:
```bash
mkdir -p /root/zk/data
mkdir -p /root/zk/log
mkdir -p /root/zk/conf
```

Edit `zoo.cfg` file for each zookeeper instance, use the same configuration for them:
```bash
# the basic time unit in milliseonds used by zk
tickTime=2000
#Leader-Follower初始通信时限 tickTime * 10 = 20 seconds
initLimit=10
#Leader-Follower同步通信时限 tickTime * 5 = 10 seconds
syncLimit=5

# data dir
dataDir=/root/zk/data
# log dir
dataLogDir=/root/zk/log

# client port, 3 nodes use the same port number
clientPort=2181
#maxClientCnxns=60

# broker id and IP address, or using hostname from /etc/hosts
# for cluster, borker id must start from 1, not 0

# 2888: connect the individual follower nodes to the leader node
# 3888: used for leader election in the ensemble
# can by any port number
server.1=192.168.20.20:2888:3888
server.2=192.168.20.21:2888:3888
server.3=192.168.20.22:2888:3888
```

In zk data directory, create `myid` for each zk instance, much be unqiue:
```bash
# current in 192.168.20.20
echo 1 > /root/zk/data/myid
# run on 192.168.20.21
echo 2 > /root/zk/data/myid
# run on 192.168.20.22
echo 3 > /root/zk/data/myid
```

## Commands
Run zk service commands:
```bash
cd /root/apache-zookeeper-3.6.2-bin/bin
# start
# default uses zoo.cfg config file
# run in background by default, has flag to run in foreground
./zkServer.sh start [path/to/zoo.cfg]
# stop
./zkServer.sh stop [path/to/zoo.cfg]

# status check
# Mode: standalone, follower, leader
./zkServer.sh status [path/to/zoo.cfg]

# test zk is good
# ipv6 + port
./zkCli.sh -server :2181
# then run
# you will see some output like [zookeeper], etc
ls /
```
[x] why port 2181 is bound on ipv6? 其他环境上也是如此。

After start zookeeper, check `./zkServer.sh status` to see if current node is leader or follower.
Run `./zkCli.sh`, you can execute zk CLI, see this [reference](https://zookeeper.apache.org/doc/r3.6.2/zookeeperCLI.html).

`./zkCli.sh ` 也可用于从外部连接一个zk server, 只要指定accessable IP 和 port即可，demo中由于就在本机，所以其实是localhost.

If start failed, see `<zk package>/logs/zookeeper_audit.log`, If you don't start another 2 zk instance, you will see periodically exception errors in `<zk package>/logs/zookeeper-root-server-kafka1.out`, it will be resolved after you start all of them. The log setting is by log4j configuration from `<zk package>/conf/log4j.properties`.

After the cluster is up and running, you can check the ports:
```bash
# see 2181, 2888, 3888
# -i: network socket
# -P: show port number
# -n: shpw ip address
lsof -i -P -n

# scan 2181
# -z: scan only
# -v: verbose
nc -zv <zk instance ip> 2181
```

在旧版本中，可以使用[four letter words](https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_4lw)去检查一些状态，比如:
```bash
# are you ok
echo ruok | nc locahost 2181
```
新版本已经切换到[AdminServer](https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_adminserver). The AdminServer is enabled by default, access by http 8080 port:
```bash
# list all commands
http://192.168.20.20:8080/commands

# for example:
# state
http://192.168.20.20:8080/commands/stat
# config
http://192.168.20.20:8080/commands/configuration
```

[ZooNavigator](https://github.com/elkozmon/zoonavigator) is a web based ZooKeeper UI and editor/browser with many features. You can launched it by docker container.


# Run as Daemon
Set zookeeper as system daemon so that it will be launched every time on system boots.

`systemd` zookeeper cluster setup on ubuntu, see [here](https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-an-apache-zookeeper-cluster-on-ubuntu-18-04).

Basically speaking, first generate a zookeeper service file, for example `zookeeper.service`, the prefix `zookeeper` is the service name used in systemctl command. Place this file in `/etc/systemd/system` folder and owned by `root`.

Double curly brackets is placeholder in jinja2 template.
```ini
[Unit]
Description=Zookeeper Daemon
Documentation=http://zookeeper.apache.org
# boot after these services
# for example, here we rely consul and dnsmasq services
After=network-online.target consul.service dnsmasq.service
StartLimitInterval=200
StartLimitBurst=5

[Service]
# in zkServer.sh, it uses `&` to fork and exec
Type=forking
# systemd can identify the main process of the daemon
# in zkServer.sh it will echo pid to this file
PIDFile={{ zookeeper_data_dir }}/zookeeper-server.pid

User={{ zookeeper_user }}
Group={{ zookeeper_group }}
WorkingDirectory=/opt/zookeeper

# environment variables may be needed
# or configure in zoo.cfg file
Environment=ZOOPIDFILE={{ zookeeper_data_dir }}/zookeeper-server.pid
Environment=ZOOKEEPER_HOME=/opt/zookeeper
Environment=ZOOKEEPER_CONF={{ zookeeper_conf_dir }}
Environment=ZOOCFGDIR={{ zookeeper_conf_dir }}
Environment=CLASSPATH=$CLASSPATH:$ZOOKEEPER_CONF:$ZOOKEEPER_HOME/*:$ZOOKEEPER_HOME/lib/*
Environment=ZOO_LOG_DIR={{ zookeeper_log_dir }}
Environment=ZOO_LOG4J_PROP=INFO,ROLLINGFILE
Environment=JVMFLAGS=-Dzookeeper.log.threshold=INFO
Environment=ZOO_DATADIR_AUTOCREATE_DISABLE=true

# start, stop and reload commands
ExecStart=/opt/zookeeper/bin/zkServer.sh start {{ zookeeper_conf_dir }}/zoo.cfg
ExecStop=/opt/zookeeper/bin/zkServer.sh stop {{ zookeeper_conf_dir }}/zoo.cfg
ExecReload=/opt/zookeeper/bin/zkServer.sh restart {{ zookeeper_conf_dir }}/zoo.cfg

# OOM killer: -1000 disable ~ 1000 very likely
OOMScoreAdjust=-500
Restart=on-failure
RestartSec=30

[Install]
# usually this is your system default target
WantedBy=multi-user.target
```
More detail about `systemd` please search and see my systemd blog.

Then you must enable zookeeper starts on boot:
```bash
systemctl daemon-reload
# enable start on boot
systemctl enable zookeeper
```

Other systemd commands:
```bash
systemctl start zookeeper
systemctl stop zookeeper
systemctl restart zookeeper

# reload config without restart
systemctl reload zookeeper
# first try relaod, if not supports then restart
systemctl reload-or-restart zookeeper
```
]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>JSON</title>
    <url>/2023/12/29/markup-json/</url>
    <content><![CDATA[
In our day to day JSON usage, the JSON is usually used as an object with string
key and various value type pairs, for example:
```json
{
  "squadName": "Super hero squad",
  "homeTown": "Metro City",
  "formed": 2016,
  "secretBase": "Super tower",
  "active": true,
  "members": [
    {
      "name": "Molecule Man",
      "age": 29,
      "secretIdentity": "Dan Jukes",
      "powers": ["Radiation resistance", "Turning tiny", "Radiation blast"]
    },
    {
      "name": "Madame Uppercut",
      "age": 39,
      "secretIdentity": "Jane Wilson",
      "powers": [
        "Million tonne punch",
        "Damage resistance",
        "Superhuman reflexes"
      ]
    },
    {
      "name": "Eternal Flame",
      "age": 1000000,
      "secretIdentity": "Unknown",
      "powers": [
        "Immortality",
        "Heat Immunity",
        "Inferno",
        "Teleportation",
        "Interdimensional travel"
      ]
    }
  ]
}
```

The JSON can also made out of arrays:
```json
[
  {
    "name": "Molecule Man",
    "age": 29,
    "secretIdentity": "Dan Jukes",
    "powers": ["Radiation resistance", "Turning tiny", "Radiation blast"]
  },
  {
    "name": "Madame Uppercut",
    "age": 39,
    "secretIdentity": "Jane Wilson",
    "powers": [
      "Million tonne punch",
      "Damage resistance",
      "Superhuman reflexes"
    ]
  }
]
```

And JSON can actually take the form of any data type that is valid for
inclusion inside JSON, not just arrays or objects. So for example, a single string or
number would be valid JSON ([reference](https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/JSON#other_notes)).

Single number or string JSON:
```json
110
```
```json
"hello, json"
```

You can validate them in the [online formetter](https://jsonformatter.org/).
]]></content>
      <categories>
        <category>JSON</category>
      </categories>
      <tags>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title>Yum Package Version Lock</title>
    <url>/2021/03/29/linux-yum-versionlock/</url>
    <content><![CDATA[
[yum versionlock](https://man7.org/linux/man-pages/man1/yum-versionlock.1.html), to restrict a package to a fixed version against `yum update/upgrade`.

The plugin stores a package list in `/etc/yum/pluginconf.d/versionlock.list`, which you can edit directly. Yum will normally attempt to update all packages, but the plugin will exclude the packages listed in the versionlock.list file.
```bash
# install version lock
yum install yum-plugin-versionlock

# list lock
yum versionlock list [package name]

# delete lock
yum versionlock delete 0:elasticsearch*
# clear all version lock
yum versionlock clear

# add lock
yum versionlock add elasticsearch-7.10.2
```]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>yum</tag>
      </tags>
  </entry>
  <entry>
    <title>Miscellanea 2019</title>
    <url>/2019/03/03/mis-2019/</url>
    <content><![CDATA[
This series contains something that is too short to be a blog, so put them all here, chronologically.

### 02/25/2019
* `hostPath` in `PersistentVolume` is the mount path in host machine. `mountPath` in `containers` field is the mount path inside the container.

### 02/27/2019
* Docker uses `/var/lib/docker` to store your images, containers, and local named volumes. Deleting this can result in data loss and possibly stop the engine from running. The `overlay2` subdirectory specifically contains the various [filesystem layers](https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers) for images and containers.

* **Vim** readonly mode, can open the same file in multiple windows:
  ```bash
  vim -R file
  ```

### 02/28/2019
* reboot machine rightnow, `-r` means reboot, for example:
  ```bash
  shutdown -r now
  ```
  If you execute remotely, use `ssh example.com` to test if it bring up.

* **Jenkins**: the exit code of last command of the Jenkin's Execute Shell build step is what determines the success/failure, now it's better to wrap the code snippet as a script and execute it. Need to do more search on it.

### 03/02/2019
* For `ls` command: If no operands are given, the contents of the current directory are displayed.  If more than one operand is given, non-directory operands are displayed first; directory and non-directory operands are sorted separately and in **lexicographical order**.

   I use this feature with `tail` command to pick latest package, for example:
  ```bash
  ls | grep ansible-* | tail -1
  ```

* `ansible` has `log_path` setting in `~/.ansible.cfg` file, for example:
  ```bash
  [defaults]
  log_path = /ibm-test/DS-Kube-Installer/logs/ds_installer_20190301_1645.log
  ```

### 03/04/2019
* [Gluster file system](https://www.gluster.org/) with RedHat?

* when run `systemctl start docker`, these directories are created: `/var/lib/docker`, `/run/docker`, `etc/docker`.

### 03/05/2019
* Find Red Hat or CentOS version:
  ```
  cat /etc/os-release

  Red Hat Enterprise Linux Server release 7.6 (Maipo)
  ```

### 03/13/2019
* I see people sometimes use `/bin/cp`, `/bin/rm` in script, why they don't use `cp` or `rm` directly? The answer is `cp` or `rm` may be an alias in target machine! For example:
  ```
  alias cp='cp -i'
  alias mv='mv -i'
  alias rm='rm -i'
  ```
  So when use `cp -f source target` it will still prompt you the overwrite confirm if target and source are the same. `/bin/cp -f source target` is correct way to go.
  
### 03/16/2019
These are from `Ansible: Up and Running, 2nd Edition` book:

* [Gunicorn](https://gunicorn.org/): a Python WSGI HTTP Server for UNIX.

* [Markdown table generator](https://www.tablesgenerator.com/markdown_tables)

* [Let's Encrypt](https://letsencrypt.org/): a free, automated, and open Certificate Authority.

* [Celery](http://www.celeryproject.org/): a distributed task queue.

* [RabbitMQ](https://www.rabbitmq.com/): open source message broker

* A **staging environment** (stage) is a nearly exact replica of a production environment for software testing. Staging environments are made to test codes, builds, and updates to ensure quality under a production-like environment before application deployment.

### 03/17/2019
These are from `Ansible: Up and Running, 2nd Edition` book:

* [Mezzanine](http://mezzanine.jupo.org/): similar in spirit to WordPress. Mezzanine is built on top of Django, the free Python-based framework for writing web applications.

* [Fabric](http://www.fabfile.org/): a Python-based tool that helps automate running tasks via SSH.

* [SQLite is serveless database](https://www.sqlite.org/serverless.html): Most SQL database engines are implemented as a separate server process. Programs that want to access the database communicate with the server using some kind of interprocess communication (typically TCP/IP) to send requests to the server and to receive back results. SQLite does not work this way. With SQLite, the process that wants to access the database reads and writes directly from the database files on disk. There is no intermediary server process.

### 03/18/2019
* Today after Fyre maintenance, one of my VM cannot resolve hostname, when I run
  ```
  ping google.com
  ```
  it hangs, also `nslookup` doesn't work as well.
  Let's check `/etc/resolv.conf` file, it is good:
  ```
  ; generated by /usr/sbin/dhclient-script
  search fyre.ibm.com. svl.ibm.com.
  nameserver 172.16.200.52
  nameserver 172.16.200.50
  ```
  Then reboot VM again it works, sometimes Fyre generates weird problem.

### 03/22/2019
* [Corn job](https://kb.iu.edu/d/afiz)

### 03/23/2019
* From IBM developer website, [RPM and YUM package management](https://developer.ibm.com/tutorials/l-lpic1-102-5/)

### 03/25/2019
* [ELECTRON](https://electronjs.org/): Build cross platform desktop apps with JavaScript, HTML, and CSS

### 03/27/2019
* This is from a issue I encountered: when we setup a NFS server as storage in K8s cluster with the `/etc/exports` file, we need to  restrict the clients who is able to access the NFS mount instead of something like:
  ```
    /data *(rw,insecure,async,no_root_squash)
  ```

  Correct way is to specify which NFS client can access:
  ```
  /data example1.com(rw,insecure,async,no_root_squash)
  /data example1.com(rw,insecure,async,no_root_squash)
  /data example1.com(rw,insecure,async,no_root_squash)
  ```
  Here in ansible template, it uses `lookup`:
  ```
  for host in {{ lookup('env','nfsclienthosts') }}; do
    echo "{{ dfsDataDir }} "$host"(rw,insecure,async,no_root_squash)" >> /etc/exports
  done
  ```
  I want to say be careful with the space in exports file, from [RHEL NFS exports](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/nfs-serverconfig)
  ```
  Important

  The format of the /etc/exports file is very precise, particularly in 
  regards to use of the space character. Remember to always separate exported
  file systems from hosts and hosts from one another with a space character. 
  However, there should be no other space characters in the file except on 
  comment lines.
  
  For example, the following two lines do not mean the same thing:

  /home bob.example.com(rw)
  /home bob.example.com (rw)

  The first line allows only users from bob.example.com read and write access 
  to the /home directory. The second line allows users from bob.example.com 
  to mount the directory as read-only (the default), while the rest of the 
  world can mount it read/write.
  ```

### 03/28/2019
* `ps aux` command will not show full outputs and the lines are truncated, if you are in a lightweight Linux distributions like `BusyBox`, you can try:
  ```
  ps aux | cat
  ```
  otherwise try:
  ```
  ps auxw
  ps auxww
  ```

### 03/29/2019
* open terminal run `vimtutor`, haha.

### 03/30/2019
* see memory usage
  ```
  free -h
  ```

* clean swap space
  ```
  swapoff -a && swapon -a
  ```

* [Nagios](https://www.nagios.org/about/overview/): open source Industry Standard In IT Infrastructure Monitoring

* [HAProxy](http://www.haproxy.org/):The Reliable, High Performance TCP/HTTP Load Balancer

* `/bin/false` is a system command that is used anytime you need to pass a command to a program that should do nothing more than exit with an error. It's the companion to `/bin/true`. Both of these are very old and standard POSIX utilities and neither produce any output by definition. `true` is sometimes used for a shell script that should loop indefinitely, like:
  ```
  while true; do
      ...
      # Waste time
      if [ $wasted_time -gt 100000 ]; then
          exit 0
      fi
      ...
  done
  ```
  `/usr/sbin/nologin` is specifically designed to replace a shell and produces output complaining you can't log-in. Before it existed, it was common to use `/bin/false` for dummy users, but could be confusing since the user doesn't know why they're kicked off.

### 04/01/2019
* Sometimes when I login to a user home, the prompt is like:
  ```
  bash-4.2$
  ```
  instead of 
  ```
  [demo@myk8s1 ~]$
  ```
  the reason is `.bash_history  .bash_logout  .bash_profile  .bashrc` under `/home/demo` are missing!

### 04/02/2019
* docker [bind mounts](https://docs.docker.com/storage/bind-mounts/) and [volume](https://docs.docker.com/storage/volumes/), in our application, we use mount type in docker run command:
  ```
  docker inspect <container name>

  "Mounts": [
            {
                "Type": "bind",
                "Source": "/opt/builds",
                "Destination": "/opt/builds",
                "Mode": "",
                "RW": true,
                "Propagation": "rprivate"
            },
  ...
  ```
  I use `busybox` does a bind mount test, in the Dockerfile, I create a `/tmp/bb` folder and put `test.txt` file in it
  ```
  FROM busybox
  MAINTAINER chengdol@ibm.com

  COPY ./hello.txt /
  RUN sh -c "mkdir /tmp/bb" && \
    touch /tmp/bb/test.txt && \
    sh -c "echo '123' > /tmp/bb/test.txt"

  CMD tail -f /hello.txt
  ```
  After build image and run as:
  ```
  docker run -d --name mybb -v /tmp/bb:/tmp/bb mybb:v1
  ```
  Then I get into the container, the `test.txt` is missing. But if we first put something in the host `/tmp/bb` folder, it will show up inside container.

* occasionally find a online drawing tool, [draw.io](https://www.draw.io/), but this cloud service may not approved by company use.

### 04/03/2019
* `echo -n` will disable the newline, `echo -e` will enable the escape, so if you run
  ```
  echo -e "\n"
  ```
  it will output a newline

### 04/04/2019
* Linux capability, how to know what capabilities a process required to work properly?

* Linux user and group with permission problem

* `docker commit`, commit what

* `docker save`, `docker export` difference

* suid `rws` bit field for file and `t` sometimes, [link](https://www.linode.com/docs/tools-reference/linux-users-and-groups/#additional-file-permissions)

* `usermod`, specify or change home directory

* `useradd`/`userdel`, `groupadd`/`groupdel` usage, `passwd` add password for user

* sudo run process will be root USER

* `cp -p`, don't change permission, owner and timestamps of the file

### 04/09/2019
* find files owned by particular user
  ```
  find . -user "xxx"
  ```

###04/10/2019
* check the parent of the process, show `PPID` (parent id) in ps command:
  ```
  ps -efj
  ```

* setuid **only** set when owner is **root**, and other user with permission to run the file will be the owner of that process:
  ```
  -rwsr-xr-x 1 root  root  62 Apr 10 15:40 hang.sh
  ```
  if I'm `demo` user to run it, the process is owned by demo
  ```
  demo     32403  0.0  0.0 113176  1388 pts/1    S+   10:13   0:00 /bin/bash ./hang.sh
  ```
  There is also `setgid` concept.
  
### 04/11/2019
* copy with hidden files and directories

### 04/21/2019
* global user start file: `/etc/bashrc`, the `umask` is inside it. 
  > Note that `umask` uses subtraction.

* tools which preserve permissions apply the appropriate mode and ignore umask: `cp -p`, `tar -p`.

### 04/24/2019
* I find sometimes I use `grep -r XXX .` cannot find the pattern in files in current and subdirectories. The reason is `-r` flag will not process symbolic link except it's on the command link. you can use:
  ```
  grep -Rn XXX .
  ```
  `-R` will follow symbolic links
  `-n` will show line number for each matched result
  `-i` make it case-insensitive
  `-F` used looking for fixed string to save time
  ```
  grep -Rn --include "*.txt" XXX .
  ```
  if you know the pattern of the file, you can specify that using `--include`, you can also mention using `--exclude` option.

### 05/10/2019
* scp from linux to windows machine
  ```
  scp ~/Downloads/PXSmokeTest_outputs.dsx Administrator@indraniwindows1.fyre.ibm.com:
  ```
  the file will be put in `C:\Users\Administrator>` folder in windows.

* How to scp files from remote host to container in k8s?
  inside the container, install `openssh-clients`
  ```
  sudo yum install openssh-clients
  ```
  then just like normal:
  ```
  sudo scp root@mycentctl1:/GitRepos/cognitive-designer-api/DSNexus_Build/Docker_Scripts/Kubernetes_11.7.DS/buildengine/opt/IBM/InformationServer/initScripts/* .
  ```

### 05/20/2019
* `Coordinated Universal Time (UTC)` is 7 hours ahead of `Pacific Time`.

### 06/12/2019
* grep exclude pattern use `-v`:
  ```bash
  docker images | grep -E "xmeta|services|engine|compute" | grep -v "mycluster" | awk '{print $1}'
  ```
  this will exclude results have `mycluster`.

### 06/19/2019
* workaround when you cannot find rpm or package to install in linux, download the binary and put it in working PATH, for example, to use `jq`, download binaries from [here](https://stedolan.github.io/jq/download/). Add executable bit and move to `/usr/bin`.

### 07/01/2019
* check directory current used size:
  ```bash
  # -s: total
  # -h: human
  du -sh <path to directory>
  ```
  if you want to know the partition size associated with this directory, for example `/var/lib/docker`, use
  ```
  df -h /var
  ```

### 07/02/2019
* change file or directory time stamp to `1969-12-31 16:00`
  ```
  touch -a -m -t 196912311600 xx.txt
  ```
  `-a`: change the access time of a file. By default it will take the current system time and update the atime field.
  `-m`: change the modification time of a file.
  `-t`: explicitly specify the time
  
  Check status by `stat` command:
  ```
  stat xx.txt

  File: `xx.txt'
  Size: 3         	Blocks: 8          IO Block: 4096   regular file
  Device: 801h/2049d	Inode: 394283      Links: 1
  Access: (0644/-rw-r--r--)  Uid: ( 1000/lakshmanan)   Gid: ( 1000/lakshmanan)
  Access: 2038-01-18 12:05:09.000000000 +0530
  Modify: 2038-01-18 12:05:09.000000000 +0530
  Change: 2012-10-19 00:40:58.763514502 +0530 
  ```
  
  reference and update the time stamp of file a.txt from the time stamp of b.txt file
  ```
  touch a.txt -r b.txt
  ```

  Change time stamp recursively
  ```
  find . -type f -exec touch -a -m -t 196912311600 {} +
  ```

* Convert format from `DOS` to `UNIX`:
  If you open a file via `vim` and see there are many `^M`:
  ```
  NOTICE^M
  ^M
  This document includes License Information documents below for multiple Programs. Each License Information document identifies the Program(s) to which it applies. Only those License Information documents for the Program(s) for which Licensee has acquired entitlements apply.^M
  ^M
  ^M
  ```
  This is because it's `DOS` format:
  ```
  file LA_en.ORIG

  LA_en.ORIG: ASCII text, with very long lines, with CRLF, LF line terminators
  ```
  how to convert to `UNIX`?
  ```
  yum install -y dos2unix
  dos2unix <file name>
  ```

### 07/12/2019
* Previously I use
```
ls -ltr --block-size=M
```
to see the human readable size for each file, actually use
```
ls -ltrh
```
is enough!

### 07/15/2019
* new tech word `linting`: the process of running a program that will analyse code for potential errors. For example, `PHPLint`, `JSLint`.

### 07/24/2019
* if run script using `sudo`, for example:
  ```
  sudo script.sh
  ```
  then every command in scipt is executed with sudo.

### 08/05/2019
* find file owned by a particular user or group
  ```
  find <path> -user <dsadm> -group <dstage>
  ```
  find file by case-insensitive name and use `-ls` format
  ```
  find <path> -iname <name> -type f -ls
  ```
* find particular files and change the chown or chmod
  ```
  find <path> -user <dsadm> -group <dstage> -exec chmod 755 {} /;
  ```
  explain:
  `chmod 755 {} \;` specifies the command that will be executed by find for each file. `{}` is replaced by the file path, and the `semicolon(;)` denotes the end of the command (escaped, otherwise it would be interpreted by the shell instead of find).

### 08/09/2019
* if var is not set, use default value `123456`:
  ```
  var=${var:-"123456"} 
  ```

### 08/28/2019
* docker commit will not apply `chmod 777 /` in new image, the permission mode of `/` directory is still original. Not sure why.

### 09/05/2019
* `curl` can be used to verbose request in detail, to check the RESTful API content.

### 11/13/2019
* `uniq` [command](https://www.geeksforgeeks.org/uniq-command-in-linux-with-examples/), used to deduplicates 

### 11/30/2019
* `https://distrowatch.com/` linux forum, contains lots of different linux distribtions and release informations.
]]></content>
      <categories>
        <category>Miscellanea</category>
      </categories>
      <tags>
        <tag>miscellanea</tag>
      </tags>
  </entry>
  <entry>
    <title>Recording on MacOS</title>
    <url>/2019/12/28/mac-record-sounds/</url>
    <content><![CDATA[
Make recording on MacOS with both internal and external sounds for screen and
audio.

This configuration summary is from this Youtube
[episode](https://youtu.be/K7UE8fZjox4) and credit to it :)

This configuration only works for Mac microphone and speaker and wire connected
headphone, the bluetooth connected AirPod Max does not work =(

Please don't use prior `Soundflower`(deprecated), ignore it and go to download
`BlackHole`.

1. Download and install `BlackHole 2ch` audio driver from its Github website
[here](https://github.com/ExistentialAudio/BlackHole) for MacOS.

Go to the download page, first you need to input your email and it will send you
another downland link, choose the `BlachHold 2ch` and download/install it.

2. On your Mac, open `Audio MIDI Setup` app, you can see the `BlackHold 2ch` is
in the left bar. Then use `+` button to create a `Aggregate Device`, name it as
`Quicktime Player Input`, check the `BlackHold 2ch` and `External Microphone`
(Usually if I need to say something in recording, I will use external headphone,
if not, please check the built-in `MacBook Pro Microphone` instead, don't check
both becuase that will generate big recording file!). Then select the Clock
Source as `BlackHold 2ch`.
![](https://drive.google.com/uc?id=1guPw4QHv_G9zb4PU7gVnSvVVThAtIMV6)

3. Next, use `+` button to Create `Multi-Output Device`, name it as
`Screen Record w/Audio`, check the `BlackHold 2ch` and `External Headphones`
(If no headphone is used, check the `MacBook Pro Speakers` instead). Then select
the Clock Source as `BlackHold 2ch`.
![](https://drive.google.com/uc?id=1uu_2V3igDIbD_XFmyLHQFv1kOj1AmAV_)

4. In the `Audio MIDI Setup` left bar, set the MacBook Pro and External
Microphone both to the highest value, otherwise the sounds will be small in the
recording.
![](https://drive.google.com/uc?id=1w_vcWv1OtWkE4bmhkwgGRb9RrdC2jDS0)

5. Open system perference `Sound`,  in `Output` select `Screen Record w/Audio`,
in `Input` section, select `External Microphone` if you are using headphone etc.
![](https://drive.google.com/uc?id=1XsNtXT6hxRQNLVDPU6IQvNyheFVI9BoS)
![](https://drive.google.com/uc?id=1IM1ctFzDnoA7lLnDAwfWOGT6kRm1KXRN)

6. Now we are all set, using `command` + `shift` + `5` as shortcut to launch
the screen recording, in the `Options` select `Quicktime Player Input`, that's
it. If you only want to record internal sounds, select `BlackHold 2ch` is enough.

7. After recording, please revert the `Output` in `Sound` app.
![](https://drive.google.com/uc?id=138bAFH7kAzzDnQ35IDAFnvtxJ5Dt3hs7)

> NOTE: Audio recording is the same, just replace step 6 with audio launch.
]]></content>
      <categories>
        <category>Mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title>iTerm2 and Oh-My-Zsh Setup</title>
    <url>/2020/07/26/mac-iterm2-zsh/</url>
    <content><![CDATA[
This post is about set up efficient, beautiful theme emulator with oh-my-zsh.

Reference from:
https://gist.github.com/kevin-smets/8568070


# Install iTerm2
Install iTerm2 on Mac  
https://www.iterm2.com/

## Configure iTerm2
Solarized Dark is bad for some syntax highlighting, to import other themes,
download from there, zip file, you can uncompress and put the downloaded folder
to the user home directory, for example `/Users/chengdol`:
https://iterm2colorschemes.com/

The theme I use is `Chalk`, import `Chalk.itermcolors` file(s) in the
`scheme(s)` folder:
![](https://drive.google.com/uc?id=1YBwdxAjAG_V8IztiBAz0ePf01m0N6eS2)

![](https://drive.google.com/uc?id=1Q6qoOp8xFw6wa-XxCLEUp0jG9NebV_7W)

Additionally，go and set iTerm2:
```js
Preference ->
Advanced ->
mouse ->
Scroll wheel sends arrow keys when in alternate screen mode ->
yes
```
This fix the mess-up issue when you scroll cursor inside Vim.

## iTerm2 Tips
1. Hotkeys, the floating terminal window setup:

Go to `Preferences -> Keys -> Hotkey` and create a Dedicated Hotkey Window.
My customized hotkey is `ctrl + shift + t`. Set the hotkey window profiles text
font the same as the default iTerm2 window, here I use `MesloLGS NF`.

2. Locating the cursor in iTerm2 terminal: `command + /`.

3. Send commands to mutliple panes in the same tab `shift + command + i`,
disable use the same command.

4. Go to split pane by direction: `command + shift + arrow key`.

5. [zsh cat command appends '%' in line end](https://unix.stackexchange.com/questions/649739/why-am-i-seeing-as-the-line-end-when-running-cat-file-cmd) if no newline at end.


# Configure Oh My Zsh
Install `ohmyz` on Mac.
Mac zsh is pre-installed, you can check by:
```
zsh --version
```
If not, [install zsh](https://github.com/ohmyzsh/ohmyzsh/wiki/Installing-ZSH)
first. Then [install ohmyzsh](https://ohmyz.sh/#install):
```bash
sh -c "$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)"
```

Then install `Powerlevel10k` theme, this theme is what exactly I need:
```bash
git clone https://github.com/romkatv/powerlevel10k.git $ZSH_CUSTOM/themes/powerlevel10k
```

By default it uses `robbyrussell` theme, you can see it from `~/.zshrc` file,
the theme files are located in `~/.oh-my-zsh/themes` folder.
```bash
# Set name of the theme to load --- if set to "random", it will
# load a random theme each time oh-my-zsh is loaded, in which case,
# to know which specific one was loaded, run: echo $RANDOM_THEME
# See https://github.com/ohmyzsh/ohmyzsh/wiki/Themes
ZSH_THEME="powerlevel10k/powerlevel10k"

# Which plugins would you like to load?
# Standard plugins can be found in $ZSH/plugins/
# Custom plugins may be added to $ZSH_CUSTOM/plugins/
# Example format: plugins=(rails git textmate ruby lighthouse)
# Add wisely, as too many plugins slow down shell startup.
plugins=(git kubectl docker docker-compose gcloud)
```

Next when you start a new terminal session, the `Powerlevel10` configure wizard
will be launched to set your prompt pattern, it will automatically check and
install the font for you. When select encoding, choose `Unicode`, otherwise no
icon will show.

If you want to reset the configuration, simply run:
```bash
p10k configure
```
`Powerlevel10` project web page:
https://github.com/romkatv/powerlevel10k#extremely-customizable

Then set `zsh` as the default shell` on Mac:
https://askubuntu.com/questions/131823/how-to-make-zsh-the-default-shell
```bash
## verify default shell is zsh or not
echo $SHELL

## If not zsh, set it
chsh -s $(which zsh)
## show current running shell
echo $0
```
After changing the theme, relaunch iTerm2.

`~./zshrc` works the same as `~/.bashrc` for bash, append other alias here,
append following snippet in `~/.zshrc`:
```bash
# Example aliases
# alias zshconfig="mate ~/.zshrc"
# alias ohmyzsh="mate ~/.oh-my-zsh"
# ### some alias for convenience
alias cdd='cd ~/Desktop'
alias cdmb='cd ~/Desktop/chengdol.blog/'

## docker
alias di='docker images'
alias dp='docker ps'
alias drp='docker rm -f'
alias dri='docker rmi -f'
alias dl='docker logs -f'

function dexec()
{
 docker exec -it $1 sh
}

## python virtual env
export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3
source /usr/local/bin/virtualenvwrapper.sh
export WORKON_HOME="/Users/chengdol/virtualenvs"
export PROJECT_HOME="/Users/chengdol/virtualprojects"
alias svenv="source /Users/chengdol/virtualenvs/py3.10/bin/activate"

export OC_EDITOR=vim
export KUBE_EDITOR=vim

bindkey "^[^[[C" forward-word
bindkey "^[^[[D" backward-word
bindkey "^U" backward-kill-line
```

To list all shortcut in Zsh, run
```bash
bindkey
```

Reference:
- [Ctrl-u in Bash](https://stackoverflow.com/questions/3483604/which-shortcut-in-zsh-does-the-same-as-ctrl-u-in-bash)
- [Zsh key codes](https://stackoverflow.com/questions/12382499/looking-for-altleftarrowkey-solution-in-zsh)

Use `cat` command to see what exactly the key will present is a good idea.

## Visual Studio Code Terminal
To make the VSC build-in terminal good with `ohmyz` theme, needs to add below
setting to `User` VSC `setting.json` file(open by `shift + command + P`):
```json
{
    "terminal.integrated.fontFamily": "MesloLGS NF",
}
```
Then restart the VSC, the theme should be good.

# Bash with Starship
Still using iTerm2, if you want to stick to `Bash` shell, try this:

Starship: cross shell prompt.
https://starship.rs/

Install by running:
```bash
curl -fsSL https://starship.rs/install.sh | bash
```

After install, append this line in `~/.bashrc` 
```bash
eval "$(starship init bash)"
```

My config file:
```bash
mkdir -p ~/.config && touch ~/.config/starship.toml
```

My current `~/.config/starship.toml` file:
```bash
# Disable the newline at the start of the prompt
add_newline = true

[line_break]
disabled = false

[character]
symbol = "➜"
error_symbol = "✗"
use_symbol_for_status = true

[battery]
full_symbol = "🔋"
charging_symbol = "⚡️"
discharging_symbol = "💀"

[time]
disabled = false
format = "🕙[ %T ]"
utc_time_offset = "-5"
time_range = "10:00:00-14:00:00"

[directory]
truncation_length = 8

## show virtual environments
[python]
disabled = false

[docker_context]
disabled = false
symbol = "🐋 "
only_with_files = true

[kubernetes]
symbol = "⛵ "
style = "dimmed green"
disabled = false

[terraform]
symbol = "🏎💨 "
disabled = false

[package]
disabled = true

[ruby]
disabled = true

[rust]
disabled = true

[nodejs]
disabled = true

[java]
disabled = true

[golang]
disabled = false

[aws]
disabled = true

## add more
```]]></content>
      <categories>
        <category>Mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
        <tag>iterm2</tag>
      </tags>
  </entry>
  <entry>
    <title>Miscellanea 2020</title>
    <url>/2020/01/22/mis-2020/</url>
    <content><![CDATA[
### 01/22/2020
* NFS network file system, list nfs port:
  https://serverfault.com/questions/377170/which-ports-do-i-need-to-open-in-the-firewall-to-use-nfs
  ```js
  rpcinfo -p | grep nfs
  ```
  It depends on the version of the protocol you intent to use. NFS 4 only require `2049` while older versions require more.

* setup nfs cluster to prevent single point of failure
  https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/high_availability_add-on_administration/ch-nfsserver-haaa


### 02/26/2020
* 如果不想让一个executable 执行多次，可以在每次run的时候在当前或固定文件夹create一个hidden file, for example: `.commad.lock`，然后写入当前正在run的command 参数等。通过检查这个file是否存在去决定是不是正在执行。

* redhat 一个很不错的网站: https://www.redhat.com/sysadmin/

* command `uuidgen` 可以用来generate random unique number.

### 04/07/2020
* `find` softlink file, use type `l`:
```
find . -type l -name dsenv
```
or use `-L`, then the `-type` predicate will always match against the type of the file that  a  symbolic  link  points  to  rather  than  the link itself (unless the symbolic link is broken).
```
find -L / type f -name dsenv
```
类似于`readlink`, 会去softlink directory里面寻找original file.

### 04/09/2020
* `> /dev/null 2>&1` can be written as `&> /dev/null`

* `su` vs `su -`. 都是login to another user, `-` 表示normal login，login后会完全变成当前user的初始环境，比如在当前user的home dir且$PATH也是当前user的。没有`-`, 则会继承上个user的环境，比如dir还是上次的。2个login都会执行~/.bashrc.

### 04/18/2020
* 在pod container中，如果script process不是init process (pid 1)，那么script中的trap 内容不会被执行。 还不太清楚为什么（这是docker container的一个特性）

### 04/26/2020
* `declare -F` show function name in current shell
* `declare -f` show function definition in current shell, `declare -f <f name>` get just that definition

### 06/16/2020
* 很多build可以用Makiefile + make command去实现，应该是一个通用的工具。

### 06/21/2020
* [What is the difference between “#!/usr/bin/env bash” and “#!/usr/bin/bash”?](https://stackoverflow.com/questions/16365130/what-is-the-difference-between-usr-bin-env-bash-and-usr-bin-bash#:~:text=Using%20%23!%2Fusr%2Fbin%2Fenv,want%20to%20search%20for%20it.&text=If%20the%20shell%20scripts%20start,run%20with%20bash%20from%20%2Fbin%20.)
Using `#!/usr/bin/env NAME` makes the shell search for the first match of NAME in the `$PATH` environment variable. It can be useful if you aren't aware of the absolute path or don't want to search for it.

### 06/24/2020
* `bash -x ./script`, no need `set -x`

### 06/25/2020
* script performance comparison `strace -c ./script`. 也就是说，多用bash internal command from `help`. `-c` will output show system time on each system call.

### 06/26/2020
* Create custom systemd service: https://medium.com/@benmorel/creating-a-linux-service-with-systemd-611b5c8b91d6. Interesting. 印象里面和当时设置K8s systemd 有点类似.

### 07/05/2020
- `shopt -s nocasematch`, set bash case-insensitive match in `case` or `[[ ]]` condition. 这个是从bash tutorial 中文版中学到的, `shopt` is bash built-in setting, unlike `set` is from POSIX.

### 07/09/2020
- vim can directly operate on file in tarball: `vim xx.tgz` then save the changes

### 07/12/2020
- [Database SQL course](https://www.youtube.com/watch?v=D-k-h0GuFmE&list=PLroEs25KGvwzmvIxYHRhoGTz9w8LeXek0) online from Stanford or Edx.com

### 07/29/2020
- `!!` re-execute last command.
- `!<beginning token>` re-execute last command start with this token.
- `$_` last token in last command line

### 08/22/2020
- Interesting: top 10 most used CLI: `history | awk {'print $2'} | sort | uniq -c | sort -nr | head -n 10`
- same idea to check my blog theme: `ls -ltr | awk 'NR>1 {print $NF}' | cut -d'-' -f1 | sort | uniq -c | sort`

### 09/09/2020
- Mac netstat list listening port, the `netstat` on Mac is not that powerful as on Linux:
  ```bash
  ## -p: protocol
  ## -n: numerical address
  ## -a: show all connections
  netstat -an -ptcp | grep -i listen
  ## or
  ## -i: lists IP sockets
  ## -P: do not resolve port names
  ## -n: do not resolve hostnames
  lsof -i -P -n | grep -i "listen"
  ```

### 09/18/2020
- Interesting project https://github.com/hubotio/hubot
- `seq` commands, i.e. `seq 1 9` generate sequence 1 to 9, used in shell for loop as counter.

### 10/25/2020
- Interesting, `ed` editor: https://sanctum.geek.nz/arabesque/actually-using-ed/
- `Dash` is a Mac app browser API
- `Typora` markdown editor

### 11/03/2020
Known from IBM channel, docker security talk
- [docker security scan](https://github.com/docker/docker-bench-security)
- [k8s cilium](https://cilium.io/): securing the network connectivity between application

### 11/20/2020
- shell nop command: `:`, synopsis is `true`, do nothing, similar to `pass` in python
  ```bash
  ## : as true
  while :; do
    sleep 1
    echo 1
  done
  ```

- shell mutliline comment, can use heredoc
  ```bash
  : <<'COMMENT'
  ...
  COMMENT
  ```

### 11/24/2020
- python shebang:
```py
#!/usr/bin/env python
# -*- coding: utf-8 -*-
```

### 11/27/2020
- `ls -1` 每行只输出一个文件名.
- `mv -v` verbose

### 12/06/2020
- bash read file line by line
```bash
cat $file | while read line
do
  echo $line
done

# or
input="/path/to/txt/file"
while IFS= read -r line
do
  echo "$line"
done < "$input"
```

### 12/09/2020
- sudo to edit restricted permission file:
```bash
# executing as non-root user
# wrong, because redirection is done by the shell which doesn't has write permission.
sudo echo "sth" >> /root/.bashrc

# correct
echo "sth" | sudo tee -a /root/.bashrc
# or
sudo sh -c 'echo sth >> /root/.bashrc'
# or mutli-line
sudo tee -a /root/.bashrc << EOF
dexec()
{
  docker exec -it \${1} sh
}
EOF
```

### 12/10/2020
- [semantic versioning](https://app.pluralsight.com/guides/introduction-to-semantic-versioning)
major.minor.patch

### 12/25/2020
- `cat jsonfile | python -m json.tool` 和`jq` 类似，一个json的format工具，但只能pretty print.
- [sudo: unable to resolve host:](https://askubuntu.com/questions/59458/error-message-sudo-unable-to-resolve-host-none) 我不太理解为什么sudo 会涉及到hostname resolution?]]></content>
      <categories>
        <category>Miscellanea</category>
      </categories>
      <tags>
        <tag>miscellanea</tag>
      </tags>
  </entry>
  <entry>
    <title>Miscellanea 2022</title>
    <url>/2022/01/27/mis-2022/</url>
    <content><![CDATA[
### 01/25/2022
git branch command behaves like `less` pager, but I want to dump the ouput:
```
git config --global pager.branch false
```

### 02/04/2022
[Makefile tutorial](https://makefiletutorial.com/)

### 03/02/2022
`you have new mail in /var/spool/mail/root`, what is it, see this [post](https://superuser.com/questions/306163/what-is-the-you-have-new-mail-message-in-linux-unix), to read it from end:
```bash
less +G $MAIL
```

### 03/17/2022
The selection of VM disk type should be aware of write/read performance as well, is the usage write or read heavy? Not only disk size, for example, this is gcloud disk properties [form](https://cloud.google.com/compute/docs/disks#introduction).

## 03/18/2022
Multiple successive forward slashes in Linux path have no effect, they are treated the same as single slash, see this [ticket](https://stackoverflow.com/questions/41856552/what-does-double-slash-mean-in-the-file-path). But successive forward slashes in GCS bucket path are not merged!

## 04/26/2022
[Backups vs Snapshots](https://www.geeksforgeeks.org/difference-between-backup-and-snapshot/), what is the [difference](https://blog.qnap.com/snapshot-different-backup/). A snapshot only needs to save enough information in local to undo a change and that makes it take less space than a backup. Backups is stored in different places.

]]></content>
      <categories>
        <category>Miscellanea</category>
      </categories>
      <tags>
        <tag>miscellanea</tag>
      </tags>
  </entry>
  <entry>
    <title>Grafana Quick Start</title>
    <url>/2021/09/19/monitor-grafana/</url>
    <content><![CDATA[
# Docker Compose Demo
Github repo:
https://github.com/chengdol/InfraTree/tree/master/docker-monitoring

The steps are in README.

# Grafana
Grafana document:
https://grafana.com/docs/grafana/latest/

Grafana docker image:
https://grafana.com/docs/grafana/latest/installation/docker/

- Running in container
- Connecting to Prometheus
- Visualizing query results
- Packaging the dashboard

Grafana support querying time-series database like **prometheus** and influxdb, also support **Elasticsearch** logging & analytics database.

```bash
## pull image alpine based
docker pull grafana/grafana:7.0.0
docker run --detach --name=grafana --publish-all grafana/grafana:7.0.0
```
The default login is `admin/admin`. After login, go to set `Data Sources`, select prometheus and specify the url, then import data in dashboard.

# Readings
- [Grafana vs. Kibana: The Key Differences to Know](https://logz.io/blog/grafana-vs-kibana/)
The key difference between the two visualization tools stems from their purpose. Grafana is designed for analyzing and visualizing metrics such as system CPU, memory, disk and I/O utilization. Grafana does not allow full-text data querying. Kibana, on the other hand, runs on top of Elasticsearch and is used primarily for analyzing log messages.]]></content>
      <categories>
        <category>Monitor</category>
      </categories>
      <tags>
        <tag>grafana</tag>
      </tags>
  </entry>
  <entry>
    <title>Miscellanea 2021</title>
    <url>/2020/01/04/mis-2021/</url>
    <content><![CDATA[
### 2021/01/04
- [GoCD](https://www.gocd.org/), the CI/CD tool like Jenkins

### 2021/01/17
- `!<command prefix letter>` will rerun the last command that starts with this prefix.

### 2021/01/23
- runbook/playbook is for commonly problems solution and instruction.

### 2021/01/25
- pipeline `echo 123 |& cat`, `|&` means piping message to both stdout and stderr.

### 2021/01/27
- `envsubst` command: substitutes environment variables in shell format strings.

### 2021/04/03
-  `yq`, similar with `jq` command but for yaml parsing.

### 2021/05/11
- `ps -o etime -o time -p <pid>`, show process the elapsed time since it started, and the cpu time.

### 2021/09/05
- `docker container run` is the same as `docker run`
- `docker container run` = `docker container create` + `docker container start` + [`docker container attach`]

### 2021/10/06
- [Docker has OS or not? ](https://stackoverflow.com/questions/53542498/in-docker-from-scratch-which-os-will-run), obviously no, remember docker vs virtual machine. Docker is lightweight virtualization, For Linux docker, the container always runs/shares on linux/host kernel, docker image supplies the necessary files, libs, utilities, etc.

- We have ubuntu, centos, alpine docker image, they can run in the same host OS, the key differences are filesystem/libraries, they share the host OS kernel. see this [post](https://stackoverflow.com/questions/33112137/run-different-linux-os-in-docker-container), also see the second answer: "Since the core kernel is common technology, the system calls are expected to be compatible even when the call is made from an Ubuntu user space code to a Redhat kernel code. This compatibility make it possible to share the kernel across containers which may all have different base OS images."

- [Docker run on MacOS](https://stackoverflow.com/questions/43383276/how-does-docker-run-a-linux-kernel-under-macos-host), Docker on Mac actually creates a Linux VM by `LinuxKit` and containers run on it.

### 2021/10/31
- DNS record type `TXT` and `A`: `TXT` is for owner custom info, `A` is IP hostname mapping.

### 2021/12/25
- Why `ls -l` does not show group name: because of alias `alias ls="ls -G"`, `-G` will suppress the group name in long format, using `/bin/ls -l` instead.]]></content>
      <categories>
        <category>Miscellanea</category>
      </categories>
      <tags>
        <tag>miscellanea</tag>
      </tags>
  </entry>
  <entry>
    <title>Prometheus Quick Start</title>
    <url>/2021/09/19/monitor-prometheus/</url>
    <content><![CDATA[
//TODO:
[ ] see reading section
[ ] series https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-b190cc97f0f6
[ ] prom config with service discovry, for example consul

# Docker Compose Demo
Github repo:
https://github.com/chengdol/InfraTree/tree/master/docker-monitoring

The steps are in README.

# Prometheus
Open-source monitoring and alerting system: 
https://prometheus.io/
Prometheus collects and stores its metrics as `time-series` data, i.e. metrics
information is stored with the timestamp at which it was recorded, alongside
optional `key-value` pairs called labels.

Architecture 
![](https://drive.google.com/uc?id=1WrobLras0BsfiVB_RphmprnA6-6Dm83U)

Learning targets:
- Know how to set up prometheus cluster for testing purpose
- Know how to [configure](https://prometheus.io/docs/prometheus/latest/configuration/configuration/) prometheus/alertmanager/grafana
- Know how to export different kind of metrics
- Know how to `PromQL`
- Know how to integrate with Grafana

## Metric type
Explanation (counter, gauge, histogram, summary):
https://www.youtube.com/watch?v=nJMRmhbY5hY

**Conuter**: request count, task completed, error count, etc.
Query how fast the value is increasing, `rate()` only applies for `counter` as
it is monotonic increasing.

**Guage**: memory usage, queue size, kafka lag, etc.
For example, `avg_over_time()` on gauge type.

**Histogram**: duration of http request, response size, etc.
To late calculate average and percentile, happy with approximation.
You can use default bucket or customizing your own.
The vaule in bucket is accumulated, add to all buckets that greater than current value.

**Summary**: duration of http request, response size, etc.
complex than Histogram, no idea the value range so cannot histogram.

## PromQL
First, understand [metric type](https://prometheus.io/docs/concepts/metric_types/) in prometheus
https://www.youtube.com/watch?v=nJMRmhbY5hY

Helpful promQL visualizing tool, cheat sheet:
https://promlabs.com/promql-cheat-sheet

How to know [**labels**](https://prometheus.io/docs/concepts/data_model/#metric-names-and-labels)
of a specific metric? Using prometheus query browser run metric name and see the
console output, it will contains all labels of that metric.

## Alert Expr
### [Excluding Time Slot from Alert Expr](https://stackoverflow.com/questions/59157537/how-to-snooze-prometheus-alert-for-specific-time)
This is helpful as we know it is no-ops. Now Prometheus supports
[time-based muting](https://github.com/prometheus/alertmanager/pull/2393):

```bash
# excluding specific time slot
some_metrics_vector and ON() absent(day_of_week() == 0 AND hour() >=3 < 4 AND minute() >= 10 < 50)
```
The explanation please see
[here](https://www.robustperception.io/combining-alert-conditions), so for 
logical operators they are case-insensitive, `and` or `AND`, either is fine.

You can verify in prometheus expression browser first then writing to alert expr.

**Tips:**:
In alert debug, to see label instance or job value in description, for example:
```yaml
    annotations:
      description: '{{ $labels.instance }} is not responding for 10 minutes.'
``` 
Just run that alert expression manually in expression browser, modify the alert
expression to see the output labels.

### [Capture the Counter First Event](https://stackoverflow.com/questions/66532785/prometheus-alerting-rule-not-detecting-first-time-metric-increase)
There is case the we want to capture the first 0(non-existence) -> 1 counter
event and fire alert, this can be captured by `unless` + `offset`, and after 1
we can use increase to catch:

```bash
# ((0 -> 1 case capture) or (1 -> 1+ case capture))
((_metric_counter_ unless _metric_counter_ offset 15m) or (increase(_metric_counter_[15m]))) > 0
```


# Query Example
Here I list some examples to explain and practice common PromQL. Part of them
are from Grafana dashboard as they have embedded variables, but the syntax and
usage is the same in prometheus expression browser and Grafana.

Understand `instant` and `range` vector and how `rate` and `irate` works:
https://www.metricfire.com/blog/understanding-the-prometheus-rate-function/

`rate`(average rate!) or `irate`(instant rate, last 2 data points only)
calculates the per-second average rate of how fast a value is increasing over a
period of time, they automatically adjusts for counter resets. If you want to
use any other aggregation(such as `sum`) together with `rate` then you must
apply `rate` first, otherwise the counter resets will not be caught and you will
get weird results.

`irate`(spike) should only be used when graphing volatile, fast-moving counters.

Use `rate`(trend) for alerts and slow-moving counters, as brief changes in the
`rate` can reset the FOR clause and graphs consisting entirely of rare spikes
are hard to read.

Also remember `rate` first then aggregation rather than inversely
https://www.robustperception.io/rate-then-sum-never-sum-then-rate

For `group_left`(many to one!) and `group_right`(one to many!), here is the
[example](https://www.robustperception.io/using-group_left-to-calculate-label-proportions).

One query example for system load average dashboard:
```bash
# ${interval}, ${load}, ${service}, $env: 
# these variables are defined from by dashboard config variables

# explain on label_replace
# 在avg_over_time()得到的向量中，对于instance这个label，看是否match $env-(.+) 这个正则表达式
# 如果有match，则$1 就是对应正则中的第一个(.+)的真实值，然后在label_replace返回的新向量中，增加一个
# label name=$1，如果没有match，则返回原来的向量
avg(
label_replace(avg_over_time(node_load${load}{instance=~"^.+-${service:regex}-[0-9]+$"}[${interval}]),
"instance_group",
"$1",
"instance",
"$env-(.+)")
) by (instance_group) > 6
# then average the new vector, group it by instance_group label and check if the average group level LA > 6
```
Above varaibles are Query type:
https://grafana.com/docs/grafana/latest/variables/variable-types/add-query-variable/


# Grafana
As the key visual component of monitoring system, please see the separate post
`<<Grafana Quick Start>>`

# AlertManager
How to connect alertmaneger to prometheus:
https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alertmanager_config

How to config alertmanager itself:
https://prometheus.io/docs/alerting/latest/configuration/

Config example to start:
https://github.com/prometheus/alertmanager/blob/main/doc/examples/simple.yml
Tool to generate routing tree:
https://prometheus.io/docs/alerting/latest/configuration/

Alertmanager repo and docker:
https://github.com/prometheus/alertmanager

The example alertmanager start command:
```bash
/bin/alertmanager --config.file=/etc/config/alertmanager.yml --storage.path=/data --web.route-prefix=/ --web.external-url=https://xxx.xxx/alertmanager
``` 

Run identical Prometheus servers on two or more separate machines. Identical
alerts will be deduplicated by the Alertmanager.

For high availability of the Alertmanager, you can run multiple instances in a
Mesh cluster and configure the Prometheus servers to send notifications to each
of them.

To silence one alert, using **New Silence** and in matcher use `alertname` as
key and alertname vaule as value(can add more key-value to filter more). If
silence multiple alerts, using regex. Preview silence can show you how many
current active alerts are affected, or you can just silence it so no new alert
will come.

# Integrated with K8s
https://www.youtube.com/watch?v=bErGEHf6GCc&list=PLpbcUe4chE7-HuslXKj1MB10ncorfzEGa
https://www.youtube.com/watch?v=CmPdyvgmw-A
https://www.youtube.com/watch?v=h4Sl21AKiDg

https://www.youtube.com/watch?v=5o37CGlNLr8
https://www.youtube.com/watch?v=LQpmeb7idt8


# Readings
- [Prometheus vs ELK](https://www.metricfire.com/blog/prometheus-vs-elk)
- [Metrics vs Logs](https://prometheus.io/docs/introduction/faq/#how-to-feed-logs-into-prometheus)















]]></content>
      <categories>
        <category>Monitor</category>
      </categories>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title>Music in My Life</title>
    <url>/2019/05/27/music/</url>
    <content><![CDATA[
把这十几年间喜欢的音乐收集列出来...(有一段时间没更新了)

## 我弹奏的钢琴曲😃
2019年6月17号我拿到了我买的YAMAHA P-125，从此开始了我钢琴自学之旅，记录一下自己的进步与快乐。
- Youtube:
  🎹[Because of You](https://youtu.be/xReoYTG_0-4)
  🎹[The Sound of Silence](https://youtu.be/zatnaCmdDm4)
  🎹[stay tuned]

- Blibli:
  🎹[Because of You](https://www.bilibili.com/video/av65980280)
  🎹[The Sound of Silence](https://www.bilibili.com/video/av73571372)
  🎹[stay tuned]

## 最喜欢的😍
* `With an Orchid (与兰同馨)`(夜深人静戴耳机😝)
🎹🎻[现场版](https://www.youtube.com/watch?v=rsNrpw2vPrA)
🎹[原版](https://www.youtube.com/watch?v=xCUjBPS9WTw) 
🎤[演唱版](https://www.youtube.com/watch?v=FRWgAV8i65o)

* `Because of You`(夜深人静戴耳机😝)
🎻[小提琴版](https://www.youtube.com/watch?v=3oYee5CEHRI) 
🎹[钢琴版](https://www.youtube.com/watch?v=PZIH_btEqFc)
🎤[原版MV](https://www.youtube.com/watch?v=atz_aZA3rf0)


## 喜欢的🌹
🎤[奔跑](https://www.youtube.com/watch?v=QN-N8jrinOM)
🎤[嘻唰唰](https://www.youtube.com/watch?v=R1i2kMxELJM)
🎤[浪漫满屋](https://www.youtube.com/watch?v=hzzqzfPGrOQ)
🎤[快乐崇拜](https://www.youtube.com/watch?v=qWV4gJIRRPY)
🎤[数码宝贝Brave Heart](https://www.youtube.com/watch?v=kqgYyfuGAag)
🎤[数码宝贝Butterfly](https://www.youtube.com/watch?v=RMFVuCURZIs)
🎤[你的微笑](https://www.youtube.com/watch?v=5-2ednIJsa8)
🎹[梦中的婚礼](https://www.youtube.com/watch?v=KWdhcOtGtcE)
🎤[不死之身](https://www.youtube.com/watch?v=h03_hs_QbEE)
🎤[江南](https://www.youtube.com/watch?v=G97_rOdHcnY)
🎹[星空](https://www.youtube.com/watch?v=WkM2Tqy-_OY)
🎤[Lydia](https://www.youtube.com/watch?v=ZOHsd6Zk7DM)
🎤[Soledad](https://www.youtube.com/watch?v=FQaid7WMr4g)
🎤[Evergreen](https://www.youtube.com/watch?v=8B3q6upO-Vw)
🎤[Black Black Heart](https://www.youtube.com/watch?v=hqpDebYpP6w)
🎤[7 Days](https://www.youtube.com/watch?v=VNSgKDRndpQ)
🎹[Summer](https://www.youtube.com/watch?v=J7or0noYfMA)
🎤[紫藤花](https://www.youtube.com/watch?v=E9a1W9hNkVo)
🎤[I swear](https://www.youtube.com/watch?v=25rL-ooWICU)
🎹[River Flows in You](https://www.youtube.com/watch?v=7maJOI3QMu0) 
🎤[Brave](https://www.youtube.com/watch?v=oXvYw-kgFz8)
🎹[忧伤还是快乐](https://www.youtube.com/watch?v=MyziqLYNoNM)
🎤[留在我身边](https://www.youtube.com/watch?v=Xp_EAKEWA-g)
🎤[情非得已](https://www.youtube.com/watch?v=CS-oP-XAzmU)
🎹[The Sound of Silence](https://www.youtube.com/watch?v=cisd1_nUGkE)
🎤[残酷な天使のテーゼ](https://www.youtube.com/watch?v=o6wtDPVkKqI)
🎤[分开旅行](https://www.youtube.com/watch?v=-7aY4tomcEE)
🎤[老男孩](https://www.youtube.com/watch?v=HwXIImlUFmY)
🎹[幽灵公主](https://www.youtube.com/watch?v=ZrFQ88l2lAQ)
🎤[心跳](https://www.youtube.com/watch?v=w41gqTFYh08)
🎹🎻[Santorini](https://www.youtube.com/watch?v=E9kcNFpOavk)
🎤[The Fox](https://www.youtube.com/watch?v=jofNR_WkoCE)
🎤[Counting Stars](https://www.youtube.com/watch?v=hT_nvWreIhg)
🎤[Stronger](https://www.youtube.com/watch?v=Xn676-fLq7I)
🎤[转动命运之轮](https://www.youtube.com/watch?v=EX_YWo1h-kQ&list=PLRndcoWkF2_4Gv2QWYmI0pdToTYROsfQq&index=16)
🎤[下个，路口，见](https://www.youtube.com/watch?v=wJ61ENvN9e4)
🎹[Run Away With Me](https://www.youtube.com/watch?v=LsrEEAFH0O8)
🎻[Love Me Like You Do](https://www.youtube.com/watch?v=VZRVou4cyic)
🎤[生来倔强](https://www.youtube.com/watch?v=W52V9a4iSEQ)
🎤[飞-致我们的星辰大海](https://www.youtube.com/watch?v=1Op8xDcgpUY)
🎤[追梦赤子心](https://www.youtube.com/watch?v=6GHJhhmVgdE)
🎤[小苹果](https://www.youtube.com/watch?v=Hpnub-uM6eo)
🎤[荷塘月色](https://www.youtube.com/watch?v=TRgum7sGAXw)
🎤[温柔](https://www.youtube.com/watch?v=nWb_X3ZJQjw)
🎤[远走高飞](https://www.youtube.com/watch?v=AQ-cLkZ7Pqw)
🎤[美人鱼](https://www.youtube.com/watch?v=jdf3gxFP0F8)
🎤[杀破狼](https://www.youtube.com/watch?v=TmIk61_Msgg)
🎤[三国恋](https://www.youtube.com/watch?v=GggTls8KznY)
🎤[时间飞了](https://www.youtube.com/watch?v=WWSxxV0KBaI)
🎤[曾经的你](https://www.youtube.com/watch?v=OU3kSSMLNvE)
🎤[稻香](https://www.youtube.com/watch?v=sHD_z90ZKV0)
🎤[偏爱](https://www.youtube.com/watch?v=k8IiCim0iW4)
🎤[Adventure Of A Lifetime](https://www.youtube.com/watch?v=QtXby3twMmI)


## 随便记记😆
🎤[没那么简单](https://www.youtube.com/watch?v=rmPHuvQoh0g)
🎤[王妃](https://www.youtube.com/watch?v=qnDlH_S4Fak)
🎤[最炫民族风](https://www.youtube.com/watch?v=Ynypvs5s75Y)
🎤[猪之歌](https://www.youtube.com/watch?v=a_Dmq21YoIc)
🎤[我相信](https://www.youtube.com/watch?v=4GWH8q8BTtU)
🎤[燃烧你的卡路里](https://www.youtube.com/watch?v=C_91v3amKDw)
🎤[Rolling in the Deep](https://www.youtube.com/watch?v=rYEDA3JcQqw)
🎤[幸福糊涂虫](https://www.youtube.com/watch?v=Nn9HqibQYCc)
🎤[No Promises](https://www.youtube.com/watch?v=jn40gqhxoSY)
🎤[Whataya Want from Me](https://www.youtube.com/watch?v=X1Fqn9du7xo)
🎤[默](https://www.youtube.com/watch?v=ABvAbpusRbc)
🎤[羅曼蒂克的愛情](https://www.youtube.com/watch?v=pUTqs4c1mSA)
🎤[江湖笑](https://www.youtube.com/watch?v=TMBe2ulmNIA)
🎤[你不是真正的快乐](https://www.youtube.com/watch?v=MP241noyop8)
🎤[新贵妃醉酒](https://www.youtube.com/watch?v=-5YFaTrZiAg)
🎤[Attention](https://www.youtube.com/watch?v=nfs8NYg7yQM)
🎤[最初的梦想](https://www.youtube.com/watch?v=wGAmgmZg-48)
🎤[给我你的爱](https://www.youtube.com/watch?v=meruGj7Gxdw)
🎤[咱们结婚吧](https://www.youtube.com/watch?v=F901ls8TVnY)
🎤[最好的舞台](https://www.youtube.com/watch?v=Q0qD-EGNncs)
🎤[最美情侣](https://www.youtube.com/watch?v=PvWAApWoYmc)
🎹[Horizon](https://www.youtube.com/watch?v=bMPM3o0NYAU)
🎤[空空如也](https://www.youtube.com/watch?v=b_goBPeiD7w)
🎤[渴望光荣](https://youtu.be/ayaGw9un1wE)
🎤[那么骄傲](https://www.youtube.com/watch?v=E8NC_MH7dok)
🎤[CAN'T STOP THE FEELING](https://www.youtube.com/watch?v=ru0K8uYEZWw)
🎤[明天会更好](https://www.youtube.com/watch?v=lEDZyIUbSd0)
🎤[Love Me Again](https://www.youtube.com/watch?v=CfihYWRWRTQ)
🎤[平凡之路](https://www.youtube.com/watch?v=NjTT5_RSkw4)
🎤[怒放的生命](https://www.youtube.com/watch?v=ft-D_5OmsRY)
🎤[新的心跳](https://www.youtube.com/watch?v=k-9Q--LbPXU)
🎤[Be what You Wanna Be](https://www.youtube.com/watch?v=0sXxhp9Zcow)
🎤[回到过去](https://www.youtube.com/watch?v=qySMxzayRcM)
🎤[青花瓷](https://www.youtube.com/watch?v=CZ78y__MIzM)
🎤[夜的第七章](https://www.youtube.com/watch?v=AdkkF6MT0R0)
🎤[不仅仅是喜欢](https://www.youtube.com/watch?v=6j0riQjd7Sc)
🎤[夜曲](https://www.youtube.com/watch?v=6Q0Pd53mojY)
🎤[只要有你](https://www.youtube.com/watch?v=QpmygPPuUQ4)
🎤[有點甜](https://www.youtube.com/watch?v=OtEJ6LGCW-U)
🎤[Nevada](https://www.youtube.com/watch?v=AnMhdn0wJ4I)
🎤[光年之外](https://www.youtube.com/watch?v=T4SimnaiktU)
🎤[戰火榮耀](https://www.youtube.com/watch?v=OgjFJKUh34I)

]]></content>
      <categories>
        <category>Music</category>
      </categories>
  </entry>
  <entry>
    <title>On-Call Helm3</title>
    <url>/2021/10/18/oncall-helm/</url>
    <content><![CDATA[
Cheatsheet to help install/upgrade/rollback helm charts.

## Repo Chart
> Note chart release name can be different from chart name.

Add and sync helm repo:
```bash
# add helm repo if needs
helm repo add <repo name> <repo URL> \
--username xx \
--password xx

# sync helm repo
# you need to run this if any chart version updated
helm repo update

# list helm repos
helm repo list

# search
# -l: lish all verions
# --version: regexp to filter version
helm search repo <chart name> [-l] [--version ^1.0.0]
# CHART VERSION is for chart upgrade or install
# show latest VERSION here
NAME           CHART VERSION	APP VERSION	DESCRIPTION
xxx            2.1.0       	    3.3.2     	xxxxxx
```

View helm installed chart:
```bash
# list installed charts
helm list [-n <namespace>]

# check chart history
# xxx-1.0.1: xxx is chart name, 1.0.1 is chart version
helm history <release name> [-n <namespace>]
REVISION	UPDATED                 	STATUS    	CHART                	APP VERSION	DESCRIPTION
1       	Mon Oct 11 19:41:41 2021	superseded	xxx-1.0.1   	1.8.3      	Install complete
2       	Mon Oct 18 17:39:37 2021	superseded	xxx-1.0.2   	1.8.3      	Upgrade complete
3       	Mon Oct 18 17:41:29 2021	superseded	xxx-1.0.1   	1.8.3      	Rollback to 1
4       	Mon Oct 18 17:44:21 2021	superseded	xxx-1.0.1   	1.8.3      	Upgrade complete
5       	Mon Oct 18 17:55:28 2021	deployed  	xxx-1.0.2-66	1.8.3      	Upgrade complete

# check chart installed status/output
helm status <release name> [-n <namespace>]
```

Install or Upgrade chart:
```bash
# get current chart values
# edit if necessary
helm get values <release name> [-n <namespace>] > values.yaml

# upgrade with specified version and values.yaml
# -f: specify vaules if necessary, if not, will reuse the existing as helm get values output
# where to get version: helm search
helm install/upgrade <release name> [repo/chart] [-n <namespace>] --version <version> [-f values.yaml]
# helm upgrade example gcloud-helm/example --version 1.0.1 -f values.yaml
# if run helm history, will displayed as xxx-1.0.3 in CHART column
# 1.0.3 is the --version value
# note that if the version format is 1.0.3.19, have to convert to 1.0.3-19 in command

# see upgrade result
helm history <release name> [-n <namespace>]
```

Rollback chart:
```bash
# REVISION is from helm history, see above
# if REVISION is ignored, rollback to previous release
helm rollback <release name> [-n <namespace>] [REVISION]
# note that rollback will also rollback the values

# see rollback result
helm history <release name> [-n <namespace>]
```

Uninstall and install chart:
```bash
# --keep-history if necessary
# in case need to rollback
helm uninstall <release name> [--keep-history] [-n <namespace>]
```

Download chart package to local:
```bash
# helm-repo is repo name from "helm repo list"
# example is chart name from that repo
# --version specify version of the chart
helm pull helm-repo/example --version 1.0.1
```


## Local Chart
For developing purpose, we can install directly from local chart folder.

You need to create [Chart.yaml](https://helm.sh/docs/topics/charts/#the-chartyaml-file) which contains chart version, appVersion, etc.

```bash
# without k8s cluster
# rendering template and check
# --debug: verbose output
helm template [--values <path to yaml file>] \
[--debug] \
<path to chart folder> \
| less

# in k8s cluster
# real helm install but without commit
# can generate a release name as [release]
helm install [release name] <path to chart folder> \
[-n <namespace>] \
--dry-run \
--debug 2>&1 \
| less

# real install
helm install [release name] <path to chart folder>

# uninstall
helm uninstall <release name> [-n <namespace>]
```


]]></content>
      <categories>
        <category>Oncall</category>
      </categories>
      <tags>
        <tag>oncall</tag>
      </tags>
  </entry>
  <entry>
    <title>On-Call Elasticsearch Quick Check</title>
    <url>/2021/09/06/oncall-elasticsearch/</url>
    <content><![CDATA[
List some common Elasticsearch APIs to check different objects:

# Elastic Stack Compatibility
The [table](https://www.elastic.co/support/matrix#matrix_compatibility) shows the stack components' version compatibility.

# Some Strategies
1. If the cluster is unhealthy, check health API the shard status, as well as check node API for node status, are all nodes in good roles? Also check path.data in date node. Check this [post](https://www.datadoghq.com/blog/elasticsearch-performance-scaling-problems/)

2. If remove and rejoin node intentionally in a short time, e.g: upgrade, OS maintenance, etc, can [delay the unassigned replica shards re-allocation](https://www.elastic.co/guide/en/elasticsearch/reference/current/delayed-allocation.html). This setting goes into every index so it may take some time, you can revert setting back after node rejoins.


# Cluster Check
```bash
# cluster health, examine:
# total shard number, primary shard number, etc
# 1. yellow or red (relocating or unassigned shards?)
# 2. node number: master and data (any lost?)
# 3. huge number of pending tasks (may get stuck the whole cluster, then
# need to check what kind of pending tasks)
GET _cluster/health
curl -s "http://localhost:9200/_cluster/health?pretty"

# total index number in cluster
GET _cluster/stats?filter_path=indices.count
# total shard number in cluster
GET _cluster/stats?filter_path=indices.shards.total

# explain shard’s current allocation.
# can see on going allocation explanation
GET _cluster/allocation/explain

# current settings
# defaults, transient and persistent(usually made by dynamic APIs)
GET _cluster/settings?flat_settings&include_defaults

# set persistent/transient setting
PUT _cluster/settings
{
    "<transient or persistent>" : {
        "cluster.routing.allocation.disk.watermark.flood_stage": "90%",
        "cluster.routing.allocation.disk.watermark.high": "75%",
        "cluster.routing.allocation.disk.watermark.low": "70%",
        "cluster.routing.allocation.cluster_concurrent_rebalance" : "6"
    }
}

# set cluster_concurrent_rebalance and node_concurrent_recoveries bigger can
# speed up rebalancing
PUT _cluster/settings
{
    "transient" : {
        "cluster.routing.allocation.cluster_concurrent_rebalance" : "10",
        "indices.recovery.max_bytes_per_sec" : "250mb",
        "cluster.routing.allocation.node_concurrent_recoveries": "10"
    }
}

# remove persistent/transient setting
PUT _cluster/settings
{
    "<transient or persistent>" : {
        "cluster.routing.allocation.disk.watermark.low": null
    }
}

# super helpful!
# show allocation details on each node
# shards, disk.percent columns can help observe the rebalancing
GET _cat/allocation?v&s=shards:desc
GET _cat/allocation?v&s=disk.percent:desc
```

# Pending Task
```bash
# pending tasks list
# usually when cluster is yellow or heavy load
GET _cluster/pending_tasks?pretty
curl -s "http://localhost:9200/_cluster/pending_tasks?pretty" > pending_tasks

# then analyze the "source" to examine what kind of pending tasks is occupied
```

# Node Check
```bash
curl -s "http://localhost:9200/_cat/nodes?pretty"
# list all header parameters
GET _cat/nodes?v

# check master and data role are properly set
GET _cat/nodes?help

# check node ES version, useful in upgrade
GET _cat/nodes?v&h=ip,v

# check node heap used, ram.percent
# ram.percent: used + cached!!
GET _cat/nodes?v&h=ip,heap.current,heap.percent,ram.percent,ram.current,node,role,master

# node metrics
# desc sort used disk space percent
# as well as show index number in each node
GET _cat/nodes?h=ip,disk.total,disk.used_percent,indexing.index_total&s=disk.used_percent:desc

# list custom node attributes
GET _cat/nodeattrs?v
```

# Index Check
Delete indices can be performed on Kibana Index Management browser.
```bash
# list all header parameters
# h=xx,xx,xx
GET _cat/indices?help
curl -s "http://localhost:9200/_cat/indices?help"

# check index mapping and setting
GET <index name>?pretty

# view 2 doc in this index
# so you can have a glapse of doc content
GET <index name>/_search?pretty
{
  "size": 2
}
# when you know doc id
GET <index name>/_doc/<unique id>

# sort by creation date
# creation.date.string: human-readable
# creation.date: Epoch & Unix Timestamp
# sort to see the creation time window for a specific index pattern
GET _cat/indices/[*-index-pattern-2021.11]h=i,creation.date.string&s=creation.date
# you can use converter here just in case
# https://www.epochconverter.com/

# pri.store.size: combined size of all primary shard for an index
GET _cat/indices/h=i,pri.store.size

# delete index
curl -XDELETE 'localhost:9200/<index name>/'
```

## Index Template
```bash
# cat
GET _cat/templates/<template name>?v
curl -s "http://localhost:9200/_cat/templates/<template name>?v"

# display template definition, for example 
# index-patterns field
# alias field
GET _template/<template name>
```

## Index Alias
```bash
# get index alias
GET <index name>/_alias

# get available alias list
GET _alias/*

# get list of alias for all indexes, empty is showed
GET */_alias
```

# Shard Check
```bash
# list primary/replic shards of specific index
# show doc number in each and host node
GET _cat/shards/<index name>?v
curl -s "http://localhost:9200/_cat/shards/<index name>?v"

# check shard allocation per node, disk usage
# useful for distribution/balance assess
GET _cat/allocation?v

# unassigned reason
# relocating direction
GET _cat/shards?h=index,state,prirep,unassigned.reason

# list reallocating shard
# show reallocating shard source -> target node
GET _cat/shards?v&h=index,shard,state,node&s=st:desc

# sort shards by size
# s=sto:desc, descending order
GET _cat/shards?h=i,shard,p,ip,st,sto&s=sto:desc,ip:desc

# list all shareds in specified node and sort by shard size desc
curl "localhost:9200/_cat/shards?h=i,shard,ip,prirep,st,store&s=store:desc" \
| grep "<node ip or node name>" > shards.txt

# query hot shards distribution on data nodes
# sort node ip with order: desc or asc
curl -s "localhost:9200/_cat/shards/<hot index pattern>?s=node:asc" > shards \
&& cat shards | awk {'print $8'} | uniq -c | sort -rn
# get hot shard total based on shards per node
cat shards | awk {'print $8'} | uniq -c | sort -rn | \
awk 'BEGIN { sum = 0 } { sum += $1} END { print sum }'
# get hot shard number average
cat shards | awk {'print $8'} | uniq -c | sort -rn | \
awk 'BEGIN { sum = 0; count = 0 } { sum += $1; count += 1 } END { print sum / count }'

# reroute shard pri/rep
# try dryrun first, the output can be big
# the dryrun output contains the reasons from success or failure
curl -XPOST "localhost:9200/_cluster/reroute?dry_run" \
-H 'Content-Type: application/json' \
-d \
'{
  "commands": [
    {
      "move": {
        "index": "<index name>",
        "shard": "<shard number>",
        "from_node": "<ip or node name>",
        "to_node": "<ip or node name>"
      }
    }
  ]
}'

# it is sometimes possible there is unassigned shard due to max reties failed
# https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-reroute.html#cluster-reroute-api-request-body
curl -XPOST "localhost:9200/_cluster/reroute?retry_failed=true" \
-H 'Content-Type: application/json' \
-d \
'{
  "commands" : [
  {
    "allocate_replica" : {
       "index" : "<index name>",
       "shard" : <shard number>,
       "node" : "<target node>"
     }
  }]
}'

# attempt a single retry 
# if there are many good(if shards are stale/corrupted, this will not work) 
# unassigned shards blocked, retry all without request body
# you may need to run multiple times to clean the backlog
curl -XPOST "localhost:9200/_cluster/reroute?retry_failed=true"


# if shard gets stucked in initializing status from recovery
# the reason could be
# 1. the shard is big
# 2. lot of replicas that lingers initial process
```

# Data Stream
It is easy to examine DS in Kibana index management console.

```bash
# get specificed ds backing indices, template, ILM
GET _data_stream/<data stream name>

# rollover a data stream
POST <data stream name>/_rollover

# delete ds and all its backing indices
DELETE _data_stream/<data stream name>
```

What I care about is the current write index of DS:
```bash
# list of all non-system, non-hidden data stream
curl -s "http://localhost:9200/_data_stream/*?format=json&pretty" \
| jq -r '.data_streams[].name' | sort -r

# find current write index(last one), health status, template and policy
curl -s "http://localhost:9200/_data_stream/<data-stream-name>?format=json&pretty" \
| jq -r '.data_streams[].indices[-1].index_name'

# data stream stats
# total shards, total backing indices, total storage size
curl -s "http://localhost:9200/_data_stream/<data-stream-name>/_stats?pretty&format=json"
```

Another important statistics is the distribution of data stream based hot shard, it is not straightforward and needs some calculation, I have wrote a [script](https://gist.github.com/chengdol/be9e90efa69fe075d1da6d5c90307178) to display it.


# ILM
The index [lifecycle management APIs](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management-api.html).
I have observed the huge number of pending tasks from ILM operations that slow down the cluster holistically (cs, traffic, usage, etc)

```bash
# examine shard age and phase state
# and any ILM error
GET <index name>/_ilm/explain

# remove ILM from ds or alias
POST <ds or alias>/_ilm/remove
# need to check if any index is closed by forcemerge, if yes, open it
GET <ds or alias>

# retry after the ILM gets fixed(updated)
POST <index name>/_ilm/retry
```

Stop and start the ILM system, used when performing schedule maintenance on cluster nodes and cloud impact ILM actions.
```bash
# check status
GET _ilm/status

# stop ILM system
POST _ilm/stop

# start ILM system
POST _ilm/start
```
]]></content>
      <categories>
        <category>Oncall</category>
      </categories>
      <tags>
        <tag>oncall</tag>
      </tags>
  </entry>
  <entry>
    <title>On-Call System Performance</title>
    <url>/2021/10/08/oncall-sys-performance/</url>
    <content><![CDATA[
## Benchmark
Need to first install `stress`, `stress-ng` packages, benchmark with multi-process.

```bash
# stress cpu with 1 process
stress --cpu 1 --timeout 600
# stress cpu with 8 processes
stress -c 8 --timeout 600

# stress io
# stress -i 1 --timeout 600 does not work well
# because VM sync buffer is small
stress-ng --io 1 --hdd 1 --timeout 600
```

Need to install `sysbench`, benchmark with multi-thread.
```bash
# 以10个线程运行5分钟的基准测试，模拟多线程切换的问题
sysbench --threads=10 --max-time=300 threads run
```

Send TCP/IP packets, for network, firewall check.
```bash
yum install hping3 -y

# -S: TCP SYN
# -p: target port
# -i: interval, u100: 100 microsecond
hping3 -S -p 80 -i u100 192.168.0.30
```

Another useful benchmark tool is `iperf3` to measure various network performance in a server-client mode. Search `iperf3 Command` for more details.

## Analysis
Need to install `perf`.

```bash
# similar to top, real time cpu usage display
# Object: [.] userspace, [k] kernal
perf top
# -g: enables call-graph (stack chain/backtrace) recording.
perf top -g -p <pid>

# record profiling and inspect later
# -g: enables call-graph (stack chain/backtrace) recording.
perf record -g
perf report
# record can also help find short live process
```


## CPU
```bash
# boot time, load average(runnable/running + uninterruptable IO), user
uptime
w

# -c: show command line
# -b: batch mode
# -n: iteration
top -c -b -n 1 | head -n 1

# -d: highlight the successive difference
watch -d "uptime"

# overall system metrics
# focus on in, cs, r, b, check man for description
# 注意r 的个数是否远超CPU 个数
# in 太多也是个问题
# us sy 看cpu 主要是被用户 还是 内核 占据
vmstat -w -S m 2

# cpu core number
lscpu
## press 1 to see cpus list
top

# check all cpus metrics
# 判断cpu usage 升高是由于iowait 还是 computing
mpstat -P ALL 1

# check which process cause cpu utilization high
# -u: cpu status: usr, sys, guest, total
pidstat -u 1

# short live process check
perf top
execsnoop
```

Versatile tool for generating system resource statistics
```bash
# combination of cpu, disk, net, system
# when CPU iowait high, can use it to compare
# iowait vs disk read/wirte vs network rec/send
dstat
```

Context switch check:
```bash
# process context switch metrics
# -w: context switch
# cswch: voluntary, 系统资源不足时，就会发生自愿上下文切换
# nvcswch: non-voluntary, 大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换
# -t: thread
pidstat -t -w 1 -p <pid number>
```

Interrupts check:
```bash
# hard interrupts
# RES: 重调度中断
watch -d cat /proc/interrupts

# acculumated soft interrupt 
# big change *rate* is usually from:
# RCU: kernel ready-copy update lock
# NET_TX
# NET_RX
# TIMER
# SCHED
watch -d cat /proc/softirqs
# soft interrupt kernel thread
# [ksoftirqd/<CPU id>]
ps aux | grep ksoftirq

# if softiq NET_RX/NET_TX is too high
# -n DEV: statistics of network device
# PPS: rxpck/s txpck/s    
# BPS: rxkB/s  txkB/s
sar -n DEV 1
```

## Memory
To check process memory usage, using `top`(VIRT, RES, SHR) and `ps`(VSZ, RSS)
```bash
# adjust oom score [-17,15], the higher the kill-prone
echo -16 > /proc/$(pidof <process name>)/oom_adj
```

Check OOM killed process:
```bash
dmesg |grep -iE "kill|oom|out of memory"
```

```bash
# check memory
# -h: readable
# -w: wide display
free -hw

# buffer is from /proc/meminfo Buffers
cat /proc/meminfo | grep -E "Buffers"
# cache is from /proc/meminfo Cached + SReclaimable
cat /proc/meminfo | grep -E "SReclaimable|Cached"
# understand what is buffer and cache, man proc
# --Buffers: 
# Relatively temporary storage for raw disk blocks that shouldn't get tremendously large (20MB or so)
# --Cached:
# In-memory cache for files read from the disk (the page cache).  Doesn't include SwapCached
# --Slab:
# In-kernel data structures cache.
# --SReclaimable:
# Part of Slab, that might be reclaimed, such as caches.

# -w: wide display
# -S: unit m(mb)
# 2: profile interval
vmstat -w -Sm 2
```

Check cache hits (need to install from BCC):
```bash
# system overall cache hit
cachestat
# process level cache hit
cachetop
```

Check memory leak:
```bash
# or valgrind
# in bcc-tools with cachestat and cachetop
memleak -a -p $(pidof app_name)
```

If swap is enabled, we can adjust the swappiness:
```bash
# [0, 100], the higher the swappiness-prone
# reclaim anonymous page from heap
echo 90 > /proc/sys/vm/swappiness
```
As oppose to swappiness, another reclamation is for file-backed page from cache/buffer.

Release caches, used carefully in production:
```bash
# sync: flush dirty pages to disk
# use carefully, drop both inode and dentry cache
sync; echo <1 or 2 or 3> > /proc/sys/vm/drop_caches
```

Check kernel slab details:
```bash
# man slabinfo
# pay attendtion to dentry and inode_cache
cat /proc/slabinfo | grep -E '^#|dentry|inode'

# real time kernel slab usage
# -s c: sort by cache size
slabtop -s c
```


## I/O
1. Check `top` for overall iowait performance
2. Check `iostat/sar` for device performance
3. Check `pidstat` for outstanding I/O process/thread
4. Check `strace` for system read/write calls
5. Check `lsof` for process/thead operating files

System `proc` files correlates to I/O:
- /proc/slabinfo
- /proc/meminfo
- /proc/diskstats
- /proc/pid/io

Check disk space usage and inode usage
```bash
# -T: file system type
df -hT
df -hT /dev/sda2

# -i: inode
df -ih

# directory storage size
# -s: summary
# -c: total
du -sc * | sort -nr
```

Check overall device statistics:
```bash
# -d: device report
# -x: extended fields
iostat -dx 1
# pay attention to
# %util: 磁盘使用率
# r/s,w/s: IOPS
# rKB/s, wKB/s: throughput
# r_await,w_await: delay

# -d: disk report
# -p: pretty print
# tps != IOPS
sar -dp 1
```

Check process I/O status
```bash
# -d: io status
# -p: pid
# -t: thread
pidstat -d [-t] -p <pid number> 1

# simple top-like I/O monitor
# -b: batch mode
# -n: iteration number
# -o: only show actually doing I/O processes/threads
# -P: only show process
iotop -b -n 1 -o [-P]
```

Check system calls on I/O to locate files:
```bash
# -f: threads
# -T: execution time
# -tt: system timestamp
# any read/write operations?
strace [-f] [-T] [-tt] -p <pid>

# check files opened by process
lsof -p <pid>
```

Also search and check `<<Linux Check Disk Space>>` for `lsof` usage.
And `<<Linux Storage System>>` to manage disk storage.
And `<<Linux Make Big Files>>` to make big file for testing.

Other BCC tools useful:
```bash
# trace file read/write
filetop
# trace kernel open system call
opensnoop
```


## Network

System kind errors:
```bash
# not only used for network but for general purpose
# -e: show local timestamp
dmesg -e | tail
```

`sar` network related commands:
```bash
# -n: statistics of network device
# DEV: network devices statistic
sar -n DEV 1

# see man sar for details
sar -n UDP 1
# ETCP: statistics about TCPv4 network errors
sar -n ETCP 1
# EDEV: statistics on failures (errors) from the network devices
sar -n EDEV 1
```

Network stack statistics:
```bash
# see tcp, udp numeric listening 
# -p: PID and name of the program to which each socket belongs.
netstat -tunlp

# check tcp connection status
# LISTEN/ESTAB/TIME-WAIT, etc
# -a: display all sockets
# -n: no reslove service name
# -t: tcp sockets
ss -ant | awk 'NR>1 {++s[$1]} END {for(k in s) print k,s[k]}'

# check interface statistics
ip -s -s link
```

Network sniffing, see another blog `<<Logstash UDP Input Data Lost>>` for more tcpdump usage. Last resort and expensive, check if mirror traffic is available in production.
```bash
# -i: interface
# -nn: no resolution
# tcp port 80 and src 192.168.1.4: filter to reduce kernel overhead
tcpdump -i eth0 -nn tcp port 80 and src 192.168.1.4 -w log.pcap
```
For example, tcpdump to pcap file and analyzed by wireshark later, using rotated or timed file to control the file size. Don't force kill the tcpdump process because that will corrupt the pcap file.

UDP statistics
```bash
# -s: summary statistics for each protocol
# -u: UDP statistics
# for example 'receive buffer errors' usually indicates UDP packet dropping
watch -n1 -d netstat -su
```
For example, the `receive buffer errors` increases frequently usually means UDP packets dropping and needs to increase socket receiving buffer size or app level buffer/queue size.

Simulate packet loss for inbound(iptables) and outbound traffic(tc-netem), check this [post](https://sandilands.info/sgordon/dropping-packets-in-ubuntu-linux-using-tc-and-iptables) for detail.

]]></content>
      <categories>
        <category>Oncall</category>
      </categories>
      <tags>
        <tag>oncall</tag>
      </tags>
  </entry>
  <entry>
    <title>On-Call Cloud VM Instance Reboot</title>
    <url>/2021/05/03/oncall-vm-reboot/</url>
    <content><![CDATA[
Occasionally VM instance reboot in an unplanned way and triggers alert, usually this is due to hardware or software issue on physical machine hosting the VM that causes the VM to crash.

From `last reboot` will see reboot records, or `who -b` displaying last time reboot info.

For CentOS/RHEL systems, you’ll find the logs at `/var/log/messages` while for Ubuntu/Debian systems, its logged at `/var/log/syslog`. 

```bash
# exclude irrelevant info
# then looking around the possible key word log
sudo grep -iv ': starting\|kernel: .*: Power Button\|watching system buttons\|Stopped Cleaning Up\|Started Crash recovery kernel' /var/log/messages | grep -iw 'recover[a-z]*\|power[a-z]*\|shut[a-z ]*down\|rsyslogd\|ups'
```

From the command output above, skim `/var/log/messages` file around the timestamp, output example:
```bash
May  3 23:31:52 xxxxx systemd: Started Update UTMP about System Boot/Shutdown.
May  3 23:31:55 xxxxx rsyslogd: [origin software="rsyslogd" swVersion="8.24.0-52.el7" x-pid="949" x-info="http://www.rsyslog.com"] start
May  3 23:31:55 xxxxx systemd: Started Google Compute Engine Shutdown Scripts.
May  3 23:37:43 xxxxx audispd: node=xxxxx type=EXECVE msg=audit(1620085063.649:1984): argc=3 a0="last" a1="-x" a2="shutdown"
```

I see the reboot was caused by [`Shutdown Scripts`](https://cloud.google.com/compute/docs/shutdownscript), further check the VM instance log or StackDriver log (if on public cloud platform, check platform log system is more convenient than ssh to checking VM log), get error:
```
compute.instances.hostError
```
Then it was clear, [What happened with host error?](https://cloud.google.com/compute/docs/faq#hosterror)
]]></content>
      <categories>
        <category>Oncall</category>
      </categories>
      <tags>
        <tag>oncall</tag>
      </tags>
  </entry>
  <entry>
    <title>GNS3</title>
    <url>/2023/04/01/network-simulator-gns3/</url>
    <content><![CDATA[
//TODO:
[ ] read through GNS3 official tutorial
[ ] start GNS3 labs:
  * NAT, how does it work in larget network

The [GNS3](https://www.gns3.com/software) can be ran through different ways,
check from "Help -> Setup Wizard":

* GNS3-VM, run on VM such as Vmware Fusion.
* Local computer, limited number of appliances available.
* Remote server and can be shared with multiple users, for example run on GCP or
another computer.

# Run via GNS3-VM
> NOTE: 04/01/2023 Unfortunately the GNS3-VM cannot run on Mac M2 architecutre and I
got error when booting the GNS3 VM on Vmware Fusion: "This virtual machine
cannot be powered on because it requires the X86 machine architecture, KB-84273"

> NOTE: 04/01/2023 The workaround is run GNS3 on local computer, please see set
it from "Help -> Setup Wizard".

## Install Vmware Fusion
> VMware Fusion is the marketing name for VMware Workstation on Mac OSX.

The GNS3 setup wizard failed at first time because I don't have VMware installed
on my Mac to start GNS3 as the VM.

So first let's download Vmware Fusion Player for Mac:
https://www.vmware.com/products/fusion/fusion-evaluation.html

> Fusion Player offers a Personal Use License, available for free with a valid
CustomerConnect account. Home users, Open Source contributors, students, and
anyone else can use Fusion Player Free for Non-Commercial activity.

Need to create Vmware account and register the `Personal Use License`, then the
download can be started, the Fusion version I downloaded:

```
VMware Fusion 13.0.1 (for Intel-based and Apple silicon Macs)
2023-02-02 13.0.1 672.09 MB dmg
```

## Download and Run GNS3-VM
As I will start GNS3 insdie a VM running by VMware Fusion, download the GNS3-VM
from here:

https://gns3.com/software/download-vm

At the time of installation, I use:
```
VMware Workstation and Fusion
Version 2.2.38
```

The image you can also download from Github 
[release](https://github.com/GNS3/gns3-gui/releases), for example:
```
GNS3.VM.VMware.Workstation.2.2.38.zip
```

# Run via Local Computer
> This is also called a local `Dynamips` implementation of GNS3.

In setup wizard, select run applicance on local computer, the next step is to
install IOS routers for Dynamips.

## Install IOS Routers

How to install Dynamips IOS, please see
[video](https://youtu.be/ErAmkZVKA7I)(00:00:00 to 05:10:00).

Use c7200 `c7200-adventerprisek9-mz.152-4.M7.bin` is enough, downloaded it from:

https://yaser-rahmati.gitbook.io/gns3/cisco-ios-images-for-dynamips

And I place it in local `Users/chengdol/GNS3/images/IOS` folder.

# Run via Remote Server
This [tutorial](https://github.com/infotechca/gns3-on-gcp) and
[video](https://youtu.be/N5XNhct-0tQ) talks about how to run GNS3 on GCP VM
instance and use its web UI.

For complex network simulation, local resource is not enough and we can utilize
Cloud.

# GNS3 Tutorial
https://docs.gns3.com/docs/
https://yaser-rahmati.gitbook.io/gns3/lab-7-basic-switch-setup

# GNS3 Labs
https://www.gns3.com/marketplace/labs






]]></content>
      <categories>
        <category>Network Simulator</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title>On-Call Kafka Quick Check</title>
    <url>/2021/09/13/oncall-kafka/</url>
    <content><![CDATA[
On any of Kafka cluster nodes runs following commands.

## Topic
```bash
# list topics created
./bin/kafka-topics.sh \
--bootstrap-server localhost:9092 \
--list

# display:
# number of partitions of this topic
# relica factor
# overridden configs
# in-sync replicas
./bin/kafka-topics.sh \
--bootstrap-server localhost:9092 \
--topic <topic name> \
--describe
# the output for example:
# "Isr" is a status, it shows which replica is in-sync, the below means all replicas are good
# Configs field shows the override settings of default
Topic: apple	PartitionCount: 3	ReplicationFactor: 3	Configs: cleanup.policy=delete,segment.bytes=536870912,retention.ms=172800000,retention.bytes=2000000000
	Topic: apple	Partition: 0	Leader: 28	Replicas: 28,29,27	Isr: 28,29,27
	Topic: apple	Partition: 1	Leader: 29	Replicas: 29,27,28	Isr: 29,27,28
	Topic: apple	Partition: 2	Leader: 27	Replicas: 27,28,29	Isr: 27,28,29

# only show overridden config
./bin/kafka-topics.sh \
--bootstrap-server localhost:9092 \
--topics-with-overrides \
--topic <topic name> \
--describe
# or using kafka-config
./bin/kafka-configs.sh \
--zookeeper <zookeeper>:2181 \
--entity-type topics \
--entity-name <topic name> \
--describe

# add or update topic config
./bin/kafka-configs.sh \
--zookeeper <zookeeper>:2181 \
--entity-type topics \
--entity-name <topic name> \
--alter \
--add-config 'max.message.bytes=16777216'

# remove topic overrides
./bin/kafka-configs.sh \
--zookeeper <zookeeper>:2181 \
--entity-type topics \
--entity-name <topic name> \
--alter \
--delete-config 'max.message.bytes'

# list all consumer groups of a topic
# no direct command
```

## Consumer Group
```bash
# list consumer groups
./bin/kafka-consumer-groups.sh  \
--bootstrap-server localhost:9092 \
--list 

# check partition/offset/lag messages in each consumer group/topic
# also see topics consumed by the group

# Consumer lag indicates the lag between Kafka producers and consumers. If the rate of
# production of data far exceeds the rate at which it is getting consumed, consumer
# groups will exhibit lag.
# From column name:
# LAG = LOG-END-OFFSET - CURRENT-OFFSET
# CLIENT-ID: xxx-0-0: means consumer 0 and its worker thread 0
./bin/kafka-consumer-groups.sh \
--bootstrap-server localhost:9092 \
--group <consumer group name> \
--describe

# read last one message in topic of consumer group
# note that one topic can be consumed by different consumer group
# each has separate consumer offset
./bin/kafka-console-consumer.sh \
--bootstrap-server localhost:9092 \
--topic <topic name> \
--group <consumer group name> \
--max-messages 1
```

## Partition
```bash
# increase partition number
# partition number can only grow up
./bin/kafka-topics.sh \
--bootstrap-server localhost:9092 \
--topic <topic name> \
--partitions <new partition number> \
--alter
```

### Delete Messages
If there is bad data in message that stucks the consumer, we can delete them
from the specified partition:
```bash
./bin/kafka-delete-records.sh \
--bootstrap-server localhost:9092 \
--offset-json-file deletion.json
```

```json
{
  "partitions": [
    {
      "topic": "<topic name>",
			// partition number, such as 0
      "partition": 0,
			// offset, delete all message from the beginning of partition till this
			// offset(excluded).
			// The offset specified is one higher than the problematic offset reported
			// in the log
      "offset": 149615102
    }
  ],
	// check ./bin/kafka-delete-records.sh --help to see the version
  "version": 1
}
```

>  Note that if all messages need deleting from the topic, then specify in the
JSON an offset of `-1`.]]></content>
      <categories>
        <category>Oncall</category>
      </categories>
      <tags>
        <tag>oncall</tag>
      </tags>
  </entry>
  <entry>
    <title>Open Sources</title>
    <url>/2019/03/09/open-source/</url>
    <content><![CDATA[
# Open source projects
- [awesome-list](https://github.com/sindresorhus/awesome)
- [awesome-shell](https://github.com/alebcay/awesome-shell)
- [awwsome-docker](https://github.com/veggiemonk/awesome-docker)
- [awesome-kubernetes](https://github.com/ramitsurana/awesome-kubernetes)
- [awesome-java](https://github.com/akullpp/awesome-java)


# Books
- [pure bash bible](https://github.com/dylanaraps/pure-bash-bible)
这个是中文版的简明教程，非常不错了
- [Bash tutorial](https://wangdoc.com/bash/)
- [Linux Productivity Tools](https://www.usenix.org/sites/default/files/conference/protected-files/lisa19_maheshwari.pdf)
- [git manual](https://github.com/xjh22222228/git-manual)


# Blogs
- [廖雪峰的官方网站](https://www.liaoxuefeng.com/)]]></content>
      <categories>
        <category>Open Source</category>
      </categories>
  </entry>
  <entry>
    <title>Cluster Image Registry in OpenShift</title>
    <url>/2020/02/26/openshift-image-registry/</url>
    <content><![CDATA[
OpenShift version: `4.3`

## Create Internal Image Registry Route
From the Infra Node, run the following commands. 
This will create a accessable path for you to push image to internal image registry.
```bash
oc project openshift-image-registry
oc patch configs.imageregistry.operator.openshift.io/cluster --type merge -p '{"spec":{"defaultRoute":true}}'
## we need this output when tag and push image
CLUSTER_IMAGE_REGISTRY_ROUTE=$(oc get route)
```

## Pull, Tag and Push Image
Here we use podman:
```bash
## pull original from other registry
## or use podman to load image archive
podman login -u <user> -p <password> docker.io
podman load -i <image>.tar.gz

podman pull <path>/<image>:<tag>

export PRIVATE_REGISTRY=${CLUSTER_IMAGE_REGISTRY_ROUTE}/<project>
## kubeadmin is the default cluster admin
podman login -u kubeadmin -p $(oc whoami -t)  $PRIVATE_REGISTRY --tls-verify=false

podman tag  <path>/<image>:<tag> $PRIVATE_REGISTRY/<image>:<tag>
podman push $PRIVATE_REGISTRY/<image>:<tag>
```

## Create Role and Binding
You need to get authenicated when pull image from cluster image registry, here we create a dedicated service account under the target project, then grant privileges to this service account and specify it to yaml file.

```yaml
oc apply -f - << EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: <service account name>
  namespace: <projetc>
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: <cluster role name>
rules:
  - apiGroups: ["*"]
    resources: ["*"]
    verbs: ["*"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: <couster role binding name>
subjects:
  - kind: ServiceAccount
    name: <service account name>
    namespace: <project>
roleRef:
  kind: ClusterRole
  name: <cluster role name>
  apiGroup: rbac.authorization.k8s.io
```
Example pod yaml file:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec: 
  ## specify the service accout
  serviceAccountName: <service account name>
  containers:
  - name: test-cotainer
    image: image-registry.openshift-image-registry.svc:5000/<project>/<image>:<tag>
    command: ['sh', '-c', 'tail -f /dev/null']
```

> Note that the default cluster registry path is `image-registry.openshift-image-registry.svc:5000`, consist of `<svc name>.<project>.svc:<port>`. don't use that route path.]]></content>
      <categories>
        <category>OpenShift</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift RBAC</title>
    <url>/2020/02/20/openshift-rbac/</url>
    <content><![CDATA[OpenShift version: `4.3`

OpenShift current is evolved to version `4.3` (last time when I was working on it, it was version `3.11`), I am assigned to try non-root install for DS assembly (a plugin) in CP4D cluster. what non-root means
```
No cluster admin privileges
No root processes in containers
No host access or ssh requirements
No elevated SCCs (other than the cpd defaults) 
```
We have met last 3 requirements, so focus on first one.

After doing research, the steps are clear but not that straightforward (相比3.11目前版本的配置变化还挺大的，支持的内容更丰富了)
1. Create regular user
2. Specify identity provider for OAuth
3. Bind necessary cluster role or local role
4. Run installation

4.3版本的一大变化是kubeadmin是默认的cluster-admin user，如同之前的systemadmin, kubeadmin is treated as the root user for the cluster. The password is dynamically generated and unique to your OpenShift Container Platform environment. 
```
oc login -u kubeadmin -p IVfPS-FvJZI-Vagzw-nIpVA --server=https://api.dsocp43.os.fyre.ibm.com:6443
```
password is provided in output when install is done，记下来就行，之前3.11 systemadmin是不需要password login的。

## Create user and specify identity provider
[Understanding identity provider configuration](https://docs.openshift.com/container-platform/4.3/authentication/understanding-identity-provider.html)。
By default, only a kubeadmin user exists on your cluster. To specify an identity provider, you must create a Custom Resource (CR) that describes that identity provider and add it to the cluster.

这里我选择htpasswd当作identity provider, [Configuring an HTPasswd identity provider](https://docs.openshift.com/container-platform/4.3/authentication/identity_providers/configuring-htpasswd-identity-provider.html)
```bash
## htpasswd -c -B -b </path/to/users.htpasswd> <user_name> <password>
htpasswd -c -B -b ~/.htpasswd demo demo
```
Then create htpasswd secret:
```bash
oc create secret generic htpass-secret --from-file=htpasswd=~/.htpasswd -n openshift-config
```
Create Custom Resource and add it to cluster
```yaml
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: my_htpasswd_provider 
    mappingMethod: claim 
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpass-secret 
```
```bash
oc apply -f <yaml file>
```
Verify:
```bash
oc login -u demo -p demo
oc whoami
```

## Bind cluster role or local role
[Using RBAC to define and apply permissions](https://docs.openshift.com/container-platform/4.3/authentication/using-rbac.html). Cluster administrators can use the cluster roles and bindings to control who has various access levels to the OpenShift Container Platform platform itself and all projects.

Use `kubeadmin` to create custom cluster role or local role and bind
目前为止`demo` user只能对自己创建的project都基本的project admin权限，如果要想操作其他project的内容，可以local bind一个project admin:
```bash
oc adm policy add-role-to-user admin demo -n <target project>
```
如果需要access其他resource，比如：
```
[ERROR] [2020-02-19 15:19:21-0617] Error verifying current oauth token - Error from server (Forbidden): 
oauthaccesstokens.oauth.openshift.io "XKRLOq8286U3YnRbf6lsv99Uk2rD1A6wanNVxgp5NNs" is forbidden:
User "demo" cannot get resource "oauthaccesstokens" in API group "oauth.openshift.io" at the cluster scope
```
这里提示user `demo` cannot get resource `oauthaccesstokens` at the cluster scope, 我们可以先根据这个resource创建一个cluster role，然后bind it to user demo. 创建cluster role时可以指定操作，verb有get, list, create, delete, patch, watch, deletecollection。然后把创建好的cluster role 用cluster role binding 绑定到demo上:
```bash
## new custom custerrole oauthaccesstokens
oc create clusterrole oauthaccesstoken_custom \
  --verb=get,list,create,delete,patch,watch \
  --resource=oauthaccesstokens
oc adm policy add-cluster-role-to-user oauthaccesstoken_custom demo
```
Check OpenShift Web can see what exactly bindings are there for user `demo`，这样就很方便了。]]></content>
      <categories>
        <category>OpenShift</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Cisco Packet Tracer</title>
    <url>/2023/03/25/network-simulator-packet-tracer/</url>
    <content><![CDATA[
> For more general and professional alternative: [GNS3](https://www.gns3.com/software)

Cisco packet tracer is a powerful network simulation tool for free. It helps:

* Practice building simple and complex networks.
* Visualize how a network works.
* Practice rack, stack and cabling skills in the virtual labs.
* Integrate IoT devices, Python code or network automation.

# Courses
Go to course website and register to kick off:

https://www.netacad.com/courses/packet-tracer

I have completed the `Getting Started` course.

# What is PTTA
This is a form of the interactive toturial with hints provided along the way.

# File Type

* `.pka`: activity for initial and answer network.
* `.pkt`: for simulated network built and saving.
* `.pksz`: for PTTA tutorial.
* `.pkz`: deprecated file type.

# Cable Type

There are 2 copper cable types provided in packet tracer:

* `Straight-Through` cable: connect two devices of different types.
* `Cross-Over` cable: connect two devices of the same type.

# Issues

## DHCP Failed. APIPA is being used
It is odd that sometimes PC and Home router connection with cable results in
failure: `DHCP Failed. APIPA is being used`, if I switch to wireless connection,
it works good.

Solution:
* Wait until Home router is **fully** up, delete PC cable connection, power
cycle the PC, reconnect and set DHCP again.
* Use wireless connection.
]]></content>
      <categories>
        <category>Network Simulator</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift Security Context Constraint</title>
    <url>/2019/06/21/openshift-scc/</url>
    <content><![CDATA[
OpenShift version: `3.10`

I have some doubts about `Security Context Constraint (SCC)` in OpenShift, for example, I give `privileged` SCC to service account, but some containers are still running as non-root user.

First what is `SCC` used for: control the actions that a pod can perform and what it has the ability to access, also very useful for managing access to persistent storage.

## Prerequisite
Spin up a fresh OpenShift Enterprise cluster with version:
```
openshift v3.9.31
kubernetes v1.9.1+a0ce1bc657
```

Create regular user `demo1`
```
htpasswd -b /etc/origin/master/htpasswd demo1 demo1
```
After login as `demo1`, you have its records in cluster, if run as `system:admin` user:
```
oc get user
oc get identity
```
You will get demo1 information.

Fetch integrated docker registry address and port from `system:admin` user:
```
oc get svc -n default | grep -E "^docker-registry"

docker-registry                     ClusterIP   172.30.159.11    <none>        5000/TCP                  1h
```

## Experiment
This experiment will show you:
1. How to enable pulling image from other project.
2. How to run container as root user.

Start with `demo1`, login by `oc login -u demo1` and create 2 projects:
```
oc new-project demo1-proj ## this one is for deploying app
oc new-project demo1-ds   ## this one is for storing imagestream
```

Pull `busybox` and update `entrypoint` to tail `/dev/null`:
```
docker pull busybox
```
```
docker run -d \
  --name mybb \
  --entrypoint=/bin/sh \
  busybox \
  -c 'tail -f /dev/null'
```
Then commit:
```
docker commit <container id> busybox
```
Then docker tag to add docker registry address prefix:
```
docker tag docker.io/busybox 172.30.159.11:5000/demo1-ds/busybox:v1
```

Docker login to integrated docker registry and push:
```
docker login -u openshift -p `oc whoami -t` 172.30.159.11:5000
docker push 172.30.159.11:5000/demo1-ds/busybox:v1
```

Go back to `demo1-proj` project by `oc project demo1-proj`, write a simple deployment yaml `bb-deploy.yml`:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bb-deployment
  labels:
    app: bb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bb
  template:
    metadata:
      labels:
        app: bb
    spec:
      containers:
      - name: bb
        image: 172.30.159.11:5000/demo1-ds/busybox:v1
```

### Enable Pull from Other Projects
If now run:
```
oc apply -f bb-deploy.yml
```
It will fail to pull the image from `demo1-ds` (describe pod can see) because we deploy objects on project `demo1-proj`, it doesn't have the permission to pull image from other project, let's enable it:
```
oc policy add-role-to-user \
    system:image-puller system:serviceaccount:demo1-proj:default \
    --namespace=demo1-ds
```

> Note that if you run this several times, it will create severl duplicate `system:image-puller`.

Then if you check `rolebindings` in `demo1-ds`, you see there is a new binding `system:image-puller` with service account `demo1-proj/default`:
```
oc get rolebindings -n demo1-ds

NAME                    ROLE                    USERS     GROUPS                            SERVICE ACCOUNTS     SUBJECTS
admin                   /admin                  demo1
system:deployers        /system:deployer                                                    deployer
system:image-builders   /system:image-builder                                               builder
system:image-puller     /system:image-puller                                                demo1-proj/default
system:image-pullers    /system:image-puller              system:serviceaccounts:demo1-ds
```

Ok, then we can deploy the busubox in `demo1-proj` project:
```
NAME                            READY     STATUS    RESTARTS   AGE
bb-deployment-78bdb8c4f-lzqfj   1/1       Running   0          6s
```
Let's next see the container UID:
```
kbc exec -it bb-deployment-78bdb8c4f-lzqfj sh

/ $ id
uid=1000130000 gid=0(root) groups=1000130000
```

### Set Security Context Constraint
Correct, OpenShift by default doesn't spin up container run as root user due to security issue. Acutally this is about `SCC`, let's dig deeper:

These 2 articles cover lots of things for SCC and service account:
[Managing Security Context Constraints](https://docs.openshift.com/container-platform/3.9/admin_guide/manage_scc.html#enable-images-to-run-with-user-in-the-dockerfile)
[Security Context Constraints official](https://docs.openshift.com/container-platform/3.9/architecture/additional_concepts/authorization.html#security-context-constraints)
[Configuring Service Accounts](https://docs.openshift.com/container-platform/3.9/admin_guide/service_accounts.html)

you must have `cluster-admin` privilege to manage SCCs (you can grant `cluster-admin` privilege to regular user), there are 7 SCCs:
```
oc get scc

NAME               PRIV      CAPS      SELINUX     RUNASUSER          FSGROUP     SUPGROUP    PRIORITY   READONLYROOTFS   VOLUMES
anyuid             false     []        MustRunAs   RunAsAny           RunAsAny    RunAsAny    10         false            [configMap downwardAPI emptyDir persistentVolumeClaim projected secret]
hostaccess         false     []        MustRunAs   MustRunAsRange     MustRunAs   RunAsAny    <none>     false            [configMap downwardAPI emptyDir hostPath persistentVolumeClaim projected secret]
hostmount-anyuid   false     []        MustRunAs   RunAsAny           RunAsAny    RunAsAny    <none>     false            [configMap downwardAPI emptyDir hostPath nfs persistentVolumeClaim projected secret]
hostnetwork        false     []        MustRunAs   MustRunAsRange     MustRunAs   MustRunAs   <none>     false            [configMap downwardAPI emptyDir persistentVolumeClaim projected secret]
nonroot            false     []        MustRunAs   MustRunAsNonRoot   RunAsAny    RunAsAny    <none>     false            [configMap downwardAPI emptyDir persistentVolumeClaim projected secret]
privileged         true      [*]       RunAsAny    RunAsAny           RunAsAny    RunAsAny    <none>     false            [*]
restricted         false     []        MustRunAs   MustRunAsRange     MustRunAs   RunAsAny    <none>     false            [configMap downwardAPI emptyDir persistentVolumeClaim projected secret]
```

By default, when a container or pod does not request a user ID under which it should be run, the effective UID depends on the SCC that emits this pod. Because `restricted` SCC is granted to all authenticated users by default, it will be available to all users and service accounts and used in most cases. 
```yaml
oc describe scc restricted

Name:                                           restricted
Priority:                                       <none>
Access:
  Users:                                        <none>
  Groups:                                       system:authenticated
Settings:
  Allow Privileged:                             false
  Default Add Capabilities:                     <none>
  Required Drop Capabilities:                   KILL,MKNOD,SETUID,SETGID
  Allowed Capabilities:                         <none>
  Allowed Seccomp Profiles:                     <none>
  Allowed Volume Types:                         configMap,downwardAPI,emptyDir,persistentVolumeClaim,projected,secret
  Allowed Flexvolumes:                          <all>
  Allow Host Network:                           false
  Allow Host Ports:                             false
  Allow Host PID:                               false
  Allow Host IPC:                               false
  Read Only Root Filesystem:                    false
  Run As User Strategy: MustRunAsRange
    UID:                                        <none>
    UID Range Min:                              <none>
    UID Range Max:                              <none>
  SELinux Context Strategy: MustRunAs
    User:                                       <none>
    Role:                                       <none>
    Type:                                       <none>
    Level:                                      <none>
  FSGroup Strategy: MustRunAs
    Ranges:                                     <none>
  Supplemental Groups Strategy: RunAsAny
    Ranges:                                     <none>
```
The `restricted` SCC uses `MustRunAsRange` strategy for constraining and defaulting the possible values of the securityContext.runAsUser field. The admission plug-in will look for the `openshift.io/sa.scc.uid-range` annotation on the current project to populate range fields, as it does not provide this range. In the end, a container will have runAsUser equal to the first value of the range that is hard to predict because every project has different ranges. 

```yaml
oc describe project demo1-proj

Name:                   demo1-proj
Created:                3 hours ago
Labels:                 <none>
Annotations:            openshift.io/description=
                        openshift.io/display-name=
                        openshift.io/requester=demo1
                        openshift.io/sa.scc.mcs=s0:c11,c10
                        openshift.io/sa.scc.supplemental-groups=1000130000/10000
                        openshift.io/sa.scc.uid-range=1000130000/10000
Display Name:           <none>
Description:            <none>
Status:                 Active
Node Selector:          <none>
Quota:                  <none>
Resource limits:        <none>
```
You see, here `openshift.io/sa.scc.uid-range` start from `1000130000`, is the UID of our `busybox` container.

SCCs are not granted directly to a project. Instead, you add a service account to an SCC and either specify the service account name on your pod or, when unspecified, run as the `default` service account.

#### Add and Remove SCC
Add service account `default` in project `demo1-proj` to SCC `privileged`
```
oc adm policy add-scc-to-user privileged system:serviceaccount:demo1-proj:default

scc "privileged" added to: ["system:serviceaccount:demo1-proj:default"]
```

Where to examine the result:
```yaml
oc describe scc privileged

Name:                                           privileged
Priority:                                       <none>
Access:
  Users:                                        system:admin,system:serviceaccount:openshift-infra:build-controller,system:serviceaccount:management-infra:management-admin,system:serviceaccount:management-infra:inspector-admin,system:serviceaccount:glusterfs:default,system:serviceaccount:glusterfs:router,system:serviceaccount:glusterfs:heketi-storage-service-account,system:serviceaccount:demo1-proj:default
...
```
In the `Users` field, we now have `system:serviceaccount:demo1-proj:default`.

How to remove the SCC from a service account?
```
oc adm policy remove-scc-from-user privileged system:serviceaccount:demo1-proj:default

scc "privileged" removed from: ["system:serviceaccount:demo1-proj:default"]
```

### Deploy Again
Then login as `demo1`, go to `demo1-proj`, deploy again:
```
oc apply -f bb-deploy.yml
```
```
oc exec -it bb-deployment-78bdb8c4f-rgj88 sh

/ $ id
uid=1000130000 gid=0(root) groups=1000130000
```

Why the UID is still `1000130000`? We have applied `privileged` right? 
Because `privileged` is just a constraint, you need to **Ensure** that at least one of the pod’s containers is requesting a privileged mode in the security context.

So update the yaml file, add `securityContext`, and deploy again:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bb-deployment
  labels:
    app: bb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bb
  template:
    metadata:
      labels:
        app: bb
    spec:
      containers:
      - name: bb
        image: 172.30.159.11:5000/demo1-ds/busybox:v1
      securityContext:
        runAsUser: 0
```
Now if you check the UID, it's `0`:
```
oc rsh bb-deployment-58cb44b56b-zmdcw

/ # id
uid=0(root) gid=0(root) groups=10(wheel)
```

> Note that `oc rsh` is the same as `kubectl exec -it ... sh`

## Conclusion
Add `privileged` SCC to service account is not enough, need to specify `runAsUser: 0` in yaml file.]]></content>
      <categories>
        <category>OpenShift</category>
      </categories>
      <tags>
        <tag>openshift</tag>
        <tag>scc</tag>
      </tags>
  </entry>
  <entry>
    <title>Jenkins CI/CD Concept</title>
    <url>/2019/07/22/pipeline-jenkins-concepts/</url>
    <content><![CDATA[
`Jenkins` is a great tool for continuous integration and continuous delivery.The `CI/CD` and their responsibilities:  
```
            +-----------------------+           +----------------------+           +------------------------+
            |                       |           |                      |           |                        |
            |   continuous          |           |   continuous         |           |   continuous           |
            |         integration   +----------->         delivery     +-----------+       deployment       |
            |                       |           |                      |           |                        |
            +---------+-------------+           +--------------+-------+           +---------------------+--+
                      ^                                        ^                                         ^
                      |                                        |                                         |
                      |                                        |                                         |
+---------------------+----------+              +--------------+----------------------+      +-----------+--------------------+
|  kick off depends on commits   |              |  platform specific testings:        |      | ready to be used by customers  |
|  or schedule                   |              |  security, performance, API...      |      |                                |
+-----------+--------------------+------+       |                                     |      +--------------------------------+
            |   code version control    |       +-------------------------------------+
            |   branching strategy      |
+-----------+-------------+-------------+
|  regression testings    |
|                         |
+-------------------------+

```
`Regression testing`: re-running functional and non-functional tests to ensure that previously developed and tested software still performs after a change. If not, that would be called a `regression`. Changes that may require regression testing include bug fixes, software enhancements, configuration changes, etc.

Git branching (tag):
[Branching strategy: feature branch and primary development branch](https://nvie.com/posts/a-successful-git-branching-model/)

Devops practice:
[Blue-green deploy](https://docs.cloudfoundry.org/devguide/deploy-apps/blue-green.html), blue live and green idle.
[Smoke/Sanity test](https://en.wikipedia.org/wiki/Smoke_testing_(software))
[TDD test-driven development](https://en.wikipedia.org/wiki/Test-driven_development)
[A/B testing](https://en.wikipedia.org/wiki/A/B_testing), user experience research methodology.

## Resource
[**What is CI/CD? Continuous integration and continuous delivery explained**](https://www.infoworld.com/article/3271126/what-is-cicd-continuous-integration-and-continuous-delivery-explained.html)
`Continuous integration` is a coding philosophy and set of practices that drive development teams to implement small changes and check in code to version control repositories frequently. 

`Continuous delivery` automates the delivery of applications to selected infrastructure environments.

A mature `CI/CD` practice has the option of implementing `continuous deployment` where application changes run through the `CI/CD` pipeline and passing builds are deployed directly to production environments. 

A best practice is to enable and require developers to run all or a subset of `regressions tests` in their local environments (or use `Travis`). This step ensures that developers only commit code to version control after regression tests pass on the code changes.

Test that require a full delivery environment such as performance and security testing are often integrated into CD and performed after builds are delivered to target environments.

To recap, `CI` packages and tests software builds and alerts developers if their changes failed any unit tests. `CD` is the automation that delivers changes to infrastructure and executes additional tests. 

`CI/CD` is a devops best practice because it addresses the misalignment between developers who want to push changes frequently, with operations that want stable applications.



]]></content>
      <categories>
        <category>Pipeline</category>
      </categories>
      <tags>
        <tag>infra</tag>
        <tag>jenkins</tag>
        <tag>pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title>Jenkins Quick Start II</title>
    <url>/2020/08/02/pipeline-jenkins-learn2/</url>
    <content><![CDATA[
This course is from PluralSight `Automating Jenkins with Groovy`.
这里不是讲如何编写declarative pipeline, 而是对Jenkins进行本地化配置, by Groovy script.

使用 Groovy 操控Jenkins API 去替代一些重复的UI 操作，就是从API 层面去使用jenkins，比如schedule job等等，甚至改变一些系统设置，毕竟Jenkins 也是Java 应用。

除了对Jenkinsfile 进行 version control, 对Jenkins本身的配置，也可以，这时候就需要Groovy script帮忙了. 在哪里去Run groovy script呢?
https://foxutech.com/how-to-run-groovy-script-in-jenkins/
- Groovy Plugin
- Jenkins Script Console
- Scriptler Plugins

第1, 2个接下来👇都提到了。

# Jenkins Configure Groovy
Install Groovy plug-in from Jenkins Plugin Manager, search `Groovy` in Available section. 

You can also run Groovy script in `Manage Jenkins` -> `Script Console`, you can write `system script` here, for example:
```groovy
// default packages are imported
def svr = Jenkins.instance;
// get existing TEST job
def job = svr.getJob("TEST");
def sched= job.scheduleBuild2(0);
sched.get();
```
还可以用groovy 调用API 去读取设置的Parameters, then use them in script. The jenkins API documention is in:
https://javadoc.jenkins.io/
Hudson的名字由来是历史原因.

You can put this inline script in `freestyle project`, go to configure -> `Build`, there are 2 options about Groovy:
- Execute Groovy Script: `non-system` script, has less security freight, do jobs that out of jenkins internals.
- Execute System Groovy Script: To run `system script`, you need to be admin (unchecked use groovy sandbox) or be approved by admin.

# Groovy Startup Script
Configuration as code, the startup script will be executed after Jenkins starts immediately to set userful properties.

Create `init.groovy.d` folder under `/var/lib/jenkins` and put the script `1_config.groovy` in it:
```groovy
import jenkins.model.*;
import java.util.logging.Logger
// add change into logs
Logger logger = Logger.getLogger("")
logger.info "Executing init script"

// disable remember me checkbox
Jenkins.instance.setDisableRememberMe(true)
Jenkins.instance.setSystemMessage('Jenkins Server - Automating Jenkins with Groovy')
Jenkins.instance.save()

logger.info "Init script complete"
```
on the browser, type `http://localhost:8080/restart`, after restart Jenkins, you will see the difference, the checkbox is gone.

**Grape** is the package manager for Groovy:
http://docs.groovy-lang.org/latest/html/documentation/grape.html

# Creating Builds
这里主要说了declarative pipeline 的Jenkinsfile, 如同工作中用到的，大量的Groovy functions.

# Credentials and Users
You can do the same job on UI, here show you how to do it via script.
- `Credentials`: used by Jenkins to access something outside
- `Users`: for users to login Jenkins

This Groovy script will create, delete and list Jenkins user:
```groovy
import hudson.model.User;
import hudson.security.*

class UserManager
{
  Set allUsers()
  {
    return User.getAll();
  }
  
  void createUser(String userName, String password){
    if (!this.userExists(userName))
    {
      Jenkins instance = Jenkins.getInstance();
      def realm = new HudsonPrivateSecurityRealm(false);
        
      realm.createAccount(userName, password);
      instance.setSecurityRealm(realm);
      instance.save();
    }
  }
  
  Boolean userExists(userName)
  {
	  return User.get(userName) != null;
  }
  
  void deleteUser(String userId)
  {
  	if (this.userExists(userId))
    {
	    User u = User.get(userId);
	    u.delete();
	  }
  }
}

def mgr = new UserManager();

mgr.createUser("test", "user");
mgr.deleteUser("test");

println(mgr.userExists("cbehrens"));

for (user in mgr.allUsers()){
	println(user.id); 
}
```

Create credentials in script:
```groovy
import com.cloudbees.plugins.credentials.*;
import com.cloudbees.plugins.credentials.common.*;
import com.cloudbees.plugins.credentials.impl.UsernamePasswordCredentialsImpl;

class CredentialManager
{
  Set getAll()
  {
	return CredentialsProvider.lookupCredentials(StandardUsernameCredentials.class, Jenkins.instance, null, null);
  }  
  
  void changePassword(String credentialId, String password){
    println 'change password';
    def creds = CredentialsProvider.lookupCredentials(StandardUsernameCredentials.class, Jenkins.instance, null, null);
    
    def credential = creds.findResult { it.id == credentialId ? it : null };
      
    if (credential != null)
    {
      def credentials_store = jenkins.model.Jenkins.instance.getExtensionList(
            'com.cloudbees.plugins.credentials.SystemCredentialsProvider'
            )[0].getStore()

        def success = credentials_store.updateCredentials(
            com.cloudbees.plugins.credentials.domains.Domain.global(), 
            credential, 
            new UsernamePasswordCredentialsImpl(credential.scope, credential.id, credential.description, credential.username, password)
            )

        if (!success) 
        {
            throw new RuntimeException("Changing password failed.");
        }
      
      	println("password change complete");
    }
  }
}

def mgr = new CredentialManager()
mgr.changePassword("githubcreds", "password");
```]]></content>
      <categories>
        <category>Pipeline</category>
      </categories>
      <tags>
        <tag>infra</tag>
        <tag>jenkins</tag>
        <tag>pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title>Jenkins Quick Start</title>
    <url>/2020/05/12/pipeline-jenkins-learn/</url>
    <content><![CDATA[
Vagrant Jenkins server git repo for testing purpose:
https://github.com/chengdol/vagrant-jenkins

# Certified Jenkins Enginee
Certified Jenkins Engineer (CJE):
https://github.com/bmuschko/cje-crash-course

# Jenkins
Jenkins is not a build system, it is a structured model.
Other interesting project: `Tekton`, `Jenkins X` (k8s involved)

Installing Jenkins, must have compatible `openjdk` installed.
```bash
## can use war file:
java -jar jenkins.war
```
or using rpm install for centos/redhat, then using systemctl to start/enable Jenkins
https://www.jenkins.io/doc/book/installing/#red-hat-centos
```bash
## you will see the default jenkins port is 8080
ps aux | grep jenkinx
```
then you can open the web interface by `<node ip>:8080`. 如果发现是中文版，可能是浏览器的语言设置出了问题，改一下chrome的语言设置为English即可.

If wizard install failed with some plugins, you can fix this later in `Manage Plugins`.

Jenkins use file system to store everything, if using systemd, the configuration is in `/var/lib/jenkins`. You can backup this folder, or if you want to wipe it out, then run `systemctl restart jenkins`, then jenkins goes back to init state.

Even Jenkins UI to create project is not needed, you can mkdir and files in the Jenkins working directory, then go to `Manage Jenkins` -> `Reload Configuration from Disk`.

> 遇到一件很神奇的事情，有次Jenkins的Credentials配置消失了。。。重启也不见恢复，后来我直接stop daemon, 清空workspace下所有文件，再次重启初始化，就恢复了。后来想想我应该是在配置Credentials时把配置改错了，可以通过`Manage Jenkins` -> `Configure Credentials` 改回去。

## Creating app build
freestyle project -> pipeline (series of freestyle project), freestyle project is not recommended.

Jenkins **Workspace**, you can see it in console output or click `Workspace` icon in your project dashboard. Everything running is in project workspace. Every build will override the previous one. You can use tar command to backup and restore this workspace, or clean workspace.

> 注意, 如果在Jenkins configuration中直接使用pipeline script 而不是 SCM, 是不会创建workspace的。

To store the build artifact, use `Post-build Action` in configure. for example, you want to archive some jar or zip files. then after build is done, these archives will show in the build page.

### Build trend
Right to `Build History` list, there is a `trend` button, click it will see the build time history statistics and distribution.

## Testing and Continuous integration
Now start the `pipeline` job type. After creating a pipeline job, you will see `pipeline syntax` button in the page bottom, it contains necessary resources to start. You can also use `Copy from` to copy a pipeline configure from another, for quick start.

### Add slave nodes
`Manage Jenkins` -> `Manage Nodes and Clouds`
To add slaves, usually use SSH to launch agent nodes. (如果node没有被发现，会显示错误，根据错误指示排查问题即可)

Before adding a slave node to the Jenkins master we need to prepare the node. We need to install `Java` on the slave node. Jenkins will install a client program on the slave node. 

To run the client program we need to install the same Java version we used to install on Jenkins master. You need to install and configure the necessary tool in the slave node.
```bash
yum install -y java-1.8.0-openjdk
```

When configure add node agent,	Host Key Verification Strategy:
- [Host Key Verification for SSH Agents](https://support.cloudbees.com/hc/en-us/articles/115000073552-Host-Key-Verification-for-SSH-Agents)


### Pipeline steps
> This is scripted pipeline syntax, not recommended! Please use declarative pipeline directives! what are the differences between them: https://www.jenkins.io/doc/book/pipeline/#pipeline-syntax-overview

如果直接在Jenkins configure UI中设置Jenkins file，则常用的Steps (in snippet generator):
* node: Allocate node
  Jenkins use `master <-> agent model`, you can configure tasks to be executed in agent node.

* stage: stage
* stash: stash some files to be used later in build
* unstash: restore files previous stashed
* parallel: execute in parallel (如果注册了多个slave nodes，则parallel会在上面执行并行的任务， 一般用在测试的时候，比如测试不同的环境和配置, see declarative pipeline demo code below)
* git: Git
* dir: Change Current Directory
* sh: Shell Script
* step: General Build Step
* emailext: Extended Email

### Triggering auto build
在pipeline configure中有`Builder Triggers`可以选择:
* Build after other projects are built
* Build periodically
* Poll SCM
* Disable this project
* Quiet period
* Trigger builds remotely

### Email Notification
Using `emailext: Extended Email`，可以用groovy函数包装，传入email的subject以及内容再调用。

## Managing plugins
`Manage Jenkins` -> `Manage Plugins`, then you can select and install plugin in `Available` section. For example:
* Pipeline (如果这个没安装，则不会在UI中显示pipeline的动态流程图)
* Html publisher (常用于发布unit test后的html report，这些html文件其实是被相关test生成好的, publisher then renders it)
  ```groovy
  publishHTML([allowMissing: true,
              alwaysLinkToLastBuild: true,
                  keepAll: true,
                  reportDir: "$WORKSPACE/cognitive-designer-api/DSJsonApiServletJUnitTests/build/reports/tests/payloadtests",
                  reportFiles: 'index.html',
                  reportName: 'Payload Test',
                  reportTitles: ''])
  ```
* Green Balls (show green color for success)
* Blue Ocean (embedded site with new Jenkins UI)
* Job DSL: Allowing jobs to be defined in a programmatic form in a human readable file.

> Pipeline compatible plugins: 
  https://github.com/jenkinsci/pipeline-plugin/blob/master/COMPATIBILITY.md

在初始化设置Jenkins的时候，有可能有plugins安装失败，可以自己在`Manage plugin`中安装，然后restart Jenkins (关于restart Jenkins请在没有job运行的情况下进行，不同的安装方式restart的方法不同，或者在安装plugin的时候选择restart jenkins after install), for example: `systemctl restart jenkins`, 这可以消除控制面板上的plugin failure警告。。

## Continuous delivery
In `Blue Ocean`, you can run multiple builds in parallel. if more than one builds run in the same agent, the workplace path is distinguished by suffix (@count number). 但是能不能run multiple builds in one agent depends on how you design your pipeline and tasks.

`Blue Ocean`中的UI对parallel的显示也很直观，方便查看。

### Trigger builds remotely
Set pipeline can be triggered remotely by URL, also optionally set pipeline trigger token in pipeline configure UI (can be empty).

You also need to know the user token, set from current user profile menu, you must keep the your user token to somewhere, for example, store it in credential secret text. So you can refer the token for example:
```groovy
TRIGGER_USER = "chengdol.example.com"
TRIGGER_USER_TOEKN = credentials('<token id>')
```

Then in the upstream pipeline script, trigger other pipeline by running curl command:
```bash
## can be http or https connection
## --user is the jenkin user and its token or password
## token=${PIPELINE_TRIGGER_TOKEN} can be ignored if it's empty
curl --user ${TRIGGER_USER}:${TRIGGER_USER_TOEKN} --request POST http/https://<url>/job/${PIPELINE_NAME}/buildWithParameters?token=${PIPELINE_TRIGGER_TOKEN}\\&para1=val1\\&&para2=val2
```
You don't need to specify all parameters in URL, the parameters default values will be used if they are not specified in the URL.

Notice that `para1` and `para2` must exist in parameters section of the triggered pipeline, otherwise you cannot use them. So far, based on testing, I can pass string, bool and file parameter types.

### Check Status of Another pipeline
reference: https://gist.github.com/paul-butcher/dc68adc1c05ca970158c18206756dab1
```bash
curl --user ${LOGIN_USER}:${LOGIN_USER_TOEKN} --request GET http/https://<url>/job/${PIPELINE_NAME}/<build number>/api/json
```
Then you can parse the json returned:
artifacts -> result: SUCCESS, FAILURE, ABORTED

### Flyweight executor
Flyweight executor reside in Jenkins master, used to execute code outside of node allocation. Others are heavyweight executors. Flyweight executor will not be counted into executor capacity.

For example, we use flyweight executor for pause, in Jenkins script:
https://stackoverflow.com/questions/44737036/jenkins-pipeline-with-code-pause-for-input
```bash
## see below declarative pipeline demo code
input "waiting for approval, move to staging stage..."
```
The job will be paused, you can go to `Paused for Input` to decide what to do next: proceed or abort. (In `Blue Ocean`, the pause interface is more clear)

# Declarative pipeline
Please use `Groovy` style to wirte declarative pipeline!

github demo:
https://github.com/sixeyed/jenkins-pipeline-demos

Declarative Pipelines: 
https://www.jenkins.io/doc/book/pipeline/syntax/

这里有关于declarative pipeline的介绍视频, Jenkins file lives in source control!
https://www.jenkins.io/solutions/pipeline/, 

Using `Blue Ocean` to setup pipeline from github need personal access token (must check out repo and user options):
https://www.youtube.com/watch?v=FhDomw6BaHU

In Jenkins UI, go to pipeline syntax then `declarative directive generator`, it will help you generate pipeline code for declarative pipeline:
https://www.jenkins.io/doc/book/pipeline/getting-started/#directive-generator

Jenkins parameters variables can be accessed by both `params` and `env` prefix, see this [issue](https://stackoverflow.com/questions/28572080/how-to-access-parameters-in-a-parameterized-build).

This is just a basic structure demo:
```groovy
pipeline {
  // default agent specify
  agent any
  // pipeline level env var, global scope
  environment {
    // you can put release number here
    // referred by env.RELEASE
    RELEASE = '1.1.3'
  }
  // can have multiple stages
  stages {
    // list tool version
    stage('Audit tools') {
      steps {
          sh '''
            git version
            docker version
          '''
      }
    }
    stage('Build') {
      // agent specify
      agent any
      // stage level env var, stage scope
      environment {
        USER = 'root'
      }
      steps {
        echo "this is Build stage"
        // executable in your repo
        sh 'chmod +x ./build.sh'
        // 把jenkins中一个名为api-key的密匙的值 放入 API_KEY这个环境变量中
        // 且这个API_KEY仅在block中可见
        withCredentials([string(credentialsId: 'api-key', variable: 'API_KEY')]) {
          sh '''
              ./build.sh
          '''
          }
      }
    } 
    // can have different type
    stage('Test') {
      environment {
        LOG_LEVEL = "INFO"
      }
      // parallel tasks
      parallel {
        // they can be running on different agent
        // depends on you agent setting
        stage('test1')
        {
          steps {
            // show current stage name test1
            echo "parallel ${STAGE_NAME}"
            // switch to ./src directory
            dir('./gradle') {
              sh '''
              ./gradlew -p xxx test1
              '''
            }
          }
        }
        stage('test2')
        {
          steps {
            echo "parallel ${STAGE_NAME}"
          }
        }
        stage('test3')
        {
          steps {
            echo "parallel ${STAGE_NAME}"
          }
        }
      }
    }
    stage('Deploy') {
      // waiting for user input before deploying
      input {
        message "Continue Deploy?"
        ok "Do it!"
        parameters {
          string(name: 'TARGET', defaultValue: 'PROD', description: 'target environment')
        }
      }
      steps {
        echo "this is Deploy with ${env.RELEASE}"
        // groovy code block
        // potential security hole, jenkins will not make it easy for you
        script {
          // you need to approve use of these class/method
          if (Math.random() > 0.5) {
              throw new Exception()
          }
          // you can use try/catch block for security reason
        }
        // if fail, this wouldn't get executed
        // write 'passed' into file test-results.txt
        writeFile file: 'test-results.txt', text: 'passed'
      }
    } 
  }
  
  post {
    // will always be executed
    always {
      echo "prints whether deploy happened or not, success or failure."
    }
    // others like: success, failure, cleanup, etc
    success {
      // archive files
      archiveArtifacts 'test-results.txt'
      // slack notifation
      slackSend channel: '#chengdol-private',
                   message: "Release ${env.RELEASE}, success: ${currentBuild.fullDisplayName}."
    }
    failure {
         slackSend channel: '#chengdol-private',
                   color: 'danger',
                   message: "Release ${env.RELEASE}, FAILED: ${currentBuild.fullDisplayName}."
      }
  }
}
```

if you don't want to checkout SCM in stages that run in same agent, you can use this option:
```bash
options {
  skipDefaultCheckout true
}
```

## Configure slack
This is a slightly out-of-date video, 其实在各自pipeline中可以自己单独配置token以及选择channel。
https://www.youtube.com/watch?v=TWwvxn2-J7E

First install `slack notifaction` plugin. Then go to `Manage Jenkins` -> `Configure System`, scroll down to bottom you will see slack section, see question mark for explanation.

Then go to your target slack channel, select `Add an app`, search `Jenkins CI`, then add it to slack, follow the instructions to get the secret token, add this token to Jenkins credentials and use it in above slack configuration.

After all set, try `Test connection`, you will see message in your slack channel.

## Reusable
Reusable functions and libraries are written in `Groovy`.
https://www.eficode.com/blog/jenkins-groovy-tutorial

Let's see some demos:
```groovy
pipeline {
    agent any
    // options
    parameters {
        booleanParam(name: 'RC', defaultValue: false, description: 'Is this a Release Candidate?')
    }
    environment {
        VERSION = "0.1.0"        
        VERSION_RC = "rc.2"
    }
    stages {
        stage('Audit tools') {                        
            steps {
                // call function
                auditTools()
            }
        }
        stage('Build') {
            environment {
                // call function
                VERSION_SUFFIX = getVersionSuffix()
            }
            steps {
              echo "Building version: ${VERSION} with suffix: ${VERSION_SUFFIX}"
              sh 'dotnet build -p:VersionPrefix="${VERSION}" --version-suffix "${VERSION_SUFFIX}" ./m3/src/Pi.Web/Pi.Web.csproj'
            }
        }
        stage('Unit Test') {
            steps {
              // switch directory
              dir('./m3/src') {
                sh '''
                    dotnet test --logger "trx;LogFileName=Pi.Math.trx" Pi.Math.Tests/Pi.Math.Tests.csproj
                    dotnet test --logger "trx;LogFileName=Pi.Runtime.trx" Pi.Runtime.Tests/Pi.Runtime.Tests.csproj
                '''
                mstest testResultsFile:"**/*.trx", keepLongStdio: true
              }
            }
        }
        stage('Smoke Test') {
            steps {
              sh 'dotnet ./m3/src/Pi.Web/bin/Debug/netcoreapp3.1/Pi.Web.dll'
            }
        }
        stage('Publish') {
            // condition
            when {
                expression { return params.RC }
            } 
            steps {
                sh 'dotnet publish -p:VersionPrefix="${VERSION}" --version-suffix "${VERSION_RC}" ./m3/src/Pi.Web/Pi.Web.csproj -o ./out'
                archiveArtifacts('out/')
            }
        }
    }
}

// groovy methods, can run straight groovy code
def String getVersionSuffix() {
    if (params.RC) {
        return env.VERSION_RC
    } else {
        return env.VERSION_RC + '+ci.' + env.BUILD_NUMBER
    }
}

def void auditTools() {
    sh '''
        git version
        docker version
        dotnet --list-sdks
        dotnet --list-runtimes
    '''
}
```

## Shared library
Demo structure and code:
https://github.com/sixeyed/jenkins-pipeline-demo-library
Invoking shared library at head of jenkins file:
```groovy
// this is dynamic reference, explicitly specify the library in jenkins file
library identifier: 'jenkins-pipeline-demo-library@master', retriever: modernSCM(
        [$class: 'GitSCMSource',
        remote: 'https://github.com/sixeyed/jenkins-pipeline-demo-library.git',
        // if the repo is private, you can have credential here
        credentialsId: '<credential id>'])

pipeline {
    agent any
    stages {
        stage('Audit tools') { 
            environment {
                // pass parameters as map
                VERSION_SUFFIX = getVersionSuffix rcNumber: env.VERSION_RC, isReleaseCandidate: params.RC
            }
            steps {
                auditTools()
            }
        }
    }
}
```

You can add `Global Pipeline Libraries` in `Configure Jenkins` for any pipeline use.
这种情况下，可以设置默认的shared library，然后在jenkins file中直接调用相关函数。

### Shared pipelines
You can put the shared pipeline into a shared library, for example:
```groovy
library identifier: 'jenkins-pipeline-demo-library@master', 
        retriever: modernSCM([$class: 'GitSCMSource', remote: 'https://github.com/sixeyed/jenkins-pipeline-demo-library.git'])

crossPlatformBuild repoName: 'sixeyed/pi-psod-pipelines',
                   linuxContext: 'm4', 
                   windowsContext: 'm4'
```
Shared library is under `vars` folder. In Groovy, we can add a method named call to a class and then invoke the method without using the name `call`, crossPlatformBuild is actually the file name, inside file there is a call method.

## Multi-branch pipeline
https://www.jenkins.io/doc/book/pipeline/multibranch/
Jenkins automatically discovers, manages and executes Pipelines for branches which contain a Jenkinsfile in source control.

Orphaned item strategy, for deleted branch, you can discard or reserve it.

# Pipeline development tools
## Validating pipeline syntax
First enable `anonymous read access` in `Configure Global Security`:
https://www.jenkins.io/doc/book/pipeline/development/#linter

Issue curl command:
```bash
## if not success, it will show you the overall problems with your jenkins file
curl -X POST -F "jenkinsfile=<[jenkins file path]" http://<IP>:8080/pipeline-model-converter/validate
```
Visual Studio code has Jenkins linter plugin, you need to configure it with linter url.

## Restart or replay
In every build interface, `restart from stage`, you can select which stage to restart (sometimes stage may fail due to external reason), `replay`, you can edit your jenkins file and library then rerun, the changes only live in current build (after succeed, check in your updates to source control).

## Unit test
> https://github.com/jenkinsci/JenkinsPipelineUnit

- Supports running pipelines and library methods
- Can mock steps and validate calls

# Jenkins with Docker
学习步骤:
首先是agent 使用docker，然后master + agent 都使用docker, 最后交由K8s去管理。

这块很有意思，加入容器后就太灵活了，只要有build agent支持docker且已安装，则jenkins就可以把container运行在之上。
https://www.jenkins.io/doc/book/pipeline/docker/

https://hub.docker.com/r/jenkins/jenkins
you can parallelly run several jenkins version in one machine, for purposes like testing new features, testing upgrade, so on and so forth. But you may need to customize the jenkins docker image and expose to different port.

- containers as build agents
- customizing the build container
- using the docker pipeline plugin

Jenkins master and slave with docker:
https://medium.com/@prashant.vats/jenkins-master-and-slave-with-docker-b993dd031cbd

From agent syntax:
https://www.jenkins.io/doc/book/pipeline/syntax/#agent
```groovy
agent {
    docker {
        image 'myregistry.com/node'
        // can pass argument to docker run
        args  '-v /tmp:/tmp'
        // the node must pre-configured to have docker
        label 'my-defined-label'
        // optional set the registry to pull image
        registryUrl 'https://myregistry.com/'
        registryCredentialsId 'myPredefinedCredentialsInJenkins'
    }
}
```
If there is no `label` option, Jenkins will dynamically provisioned on a node and it will fail if no docker installed, you can set docker label to filter:
https://www.jenkins.io/doc/book/pipeline/docker/#specifying-a-docker-label


I got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock:
https://stackoverflow.com/questions/47854463/docker-got-permission-denied-while-trying-to-connect-to-the-docker-daemon-socke

还解决了一个权限的问题，在实验中container默认用户是`jenkins`，没有root权限无线运行container内部的程序，解决办法是 `args  '-u root'`在上面的配置中。

此外jenkins会把workspace注入到container中，通过环境变量查找:
```bash
sh 'printenv'
sh 'ls -l "$WORKSPACE"
```
Additionally, you can use agent with `Dockerfile` and install Docker Pipeline plugin.




]]></content>
      <categories>
        <category>Pipeline</category>
      </categories>
      <tags>
        <tag>infra</tag>
        <tag>jenkins</tag>
        <tag>pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenShift SCC Customized</title>
    <url>/2019/07/16/openshift-scc-customized/</url>
    <content><![CDATA[
OpenShift version: `3.10`

There are by default 7 SCCs in OpenShift, but that may not satisfy the demands and it's better to create a new dedicated one to use for non-root deployment.

> To get basic understand about `SCC`, see my blog [`<<OpenShift Security Context Constraint>>`](https://chengdol.github.io/2019/06/21/openshift-scc/).

7 default existing SCCs are:
```bash
oc get scc

NAME               PRIV      CAPS      SELINUX     RUNASUSER          FSGROUP     SUPGROUP    PRIORITY   READONLYROOTFS   VOLUMES
anyuid             false     []        MustRunAs   RunAsAny           RunAsAny    RunAsAny    10         false            [configMap downwardAPI emptyDir persistentVolumeClaim projected secret]
hostaccess         false     []        MustRunAs   MustRunAsRange     MustRunAs   RunAsAny    <none>     false            [configMap downwardAPI emptyDir hostPath persistentVolumeClaim projected secret]
hostmount-anyuid   false     []        MustRunAs   RunAsAny           RunAsAny    RunAsAny    <none>     false            [configMap downwardAPI emptyDir hostPath nfs persistentVolumeClaim projected secret]
hostnetwork        false     []        MustRunAs   MustRunAsRange     MustRunAs   MustRunAs   <none>     false            [configMap downwardAPI emptyDir persistentVolumeClaim projected secret]
nonroot            false     []        MustRunAs   MustRunAsNonRoot   RunAsAny    RunAsAny    <none>     false            [configMap downwardAPI emptyDir persistentVolumeClaim projected secret]
privileged         true      [*]       RunAsAny    RunAsAny           RunAsAny    RunAsAny    <none>     false            [*]
restricted         false     []        MustRunAs   MustRunAsRange     MustRunAs   RunAsAny    <none>     false            [configMap downwardAPI emptyDir persistentVolumeClaim projected secret]
```
Don't forget to examine SCC, such as `oc describe scc privileged`.


## SCC Yaml Demo
> How to write SCC yaml and what does each field mean? [OpenShift SCC official](https://docs.openshift.com/container-platform/3.9/architecture/additional_concepts/authorization.html#security-context-constraints)

Create a file named as `scc-customized.yaml`, carefully fill the value to satisfy the demands
```yaml
kind: SecurityContextConstraints
apiVersion: v1
metadata:
  name: scc-customized
## permission
allowPrivilegedContainer: false
allowHostIPC: true
allowHostNetwork: true
allowHostPID: true
allowHostPorts: true
#allowedFlexVolumes: null
## linux capabilities, some pods require these
allowedCapabilities:
- SYS_NICE
- IPC_OWNER
- SYS_RESOURCE
requiredDropCapabilities: []
defaultAddCapabilities: []
## strategies
runAsUser:
  type: MustRunAsNonRoot
seLinuxContext:
  type: RunAsAny
fsGroup:
  type: RunAsAny
supplementalGroups:
  type: RunAsAny
## who can access this SCC
users: []
groups:
- system:authenticated
## may narrow down insteaf of `*`
volumes:
- '*'
```
```
oc create -f scc-customized.yaml
```
Then, for example, you can bind `default` service account to this `SCC`:
```
oc adm policy add-scc-to-user scc-customized system:serviceaccount:<project>:default
```

> A `default` service account is used by all other pods unless they specify a different service account.

]]></content>
      <categories>
        <category>OpenShift</category>
      </categories>
      <tags>
        <tag>openshift</tag>
        <tag>scc</tag>
      </tags>
  </entry>
  <entry>
    <title>Jenkins Git Check in</title>
    <url>/2020/06/02/pipeline-jenkins-git-checkin/</url>
    <content><![CDATA[
有时有这种需求: pipeline 结束后，有新生成或被改动的文件，需要把这些变化check in 到 remote github repository中，其实就是git add/commit/push 操作。这在Jenkins中如何实现呢?

注意这里的Github repository is secured, 比如Github Enterprise。一般我们设置SSH credentials access (SSH Username with private key), 这个credential 会提前写到 Jenkins Credential Management中，在配置pipeline的时候，最后一步设置SCM -> Git, 除了输入Reporsity URL, 还要add SSH credential. 这样Jenkins才能正常地check out code. 当然，在pipeline steps 中 check out code也行，比如使用 `git`, `checkout` snippets.

对于check in code, 也可以使用snippet 比如:
- `withCredentials`
  Bind credential to variables, 这个snippet 可以提供通过环境变量访问credential. 但在这里对于git SSH credential access, 需要设置让git去使用这个变量，this is unknown to me.

- `sshagent`: 
  需要install plugin: https://plugins.jenkins.io/ssh-agent/, pass credential to it. 然后把git 操作放在这个snippet中即可. 比如:
  ```groovy
  steps {
    sshagent(['<credential id>']) {
      // fetch a branch, edit and check in the code
      sh '''
        ## or git pull other repository
        git fetch
        git checkout $TARGET_BRANCH
        git reset --hard origin/$TARGET_BRANCH
        git pull

        CHECKOUT_BRANCH="feature/${TARGET_BRANCH}-${COMPONENT_NAME}-${COMPONENT_VERSION}"
        echo "Creating feature branch: $CHECKOUT_BRANCH"
        git checkout -b $CHECKOUT_BRANCH

        sed -i "/.*version.*/c\  version: $COMPONENT_VERSION" files/$COMPONENT_NAME.yaml
        git add files/$COMPONENT_NAME.yaml
        ## list file changes
        git status
        git -c user.name="unibot" -c user.email="unibot@il.example.com" commit -m "Update ${COMPONENT_NAME} to ${COMPONENT_VERSION}"
        git push --set-upstream origin $CHECKOUT_BRANCH
        '''
    }
  }
  ```
  参考这里的代码:
  https://github.com/jenkinsci/pipeline-examples/blob/master/pipeline-examples/push-git-repo/pushGitRepo.groovy


在这里，如果我没有权限去安装`sshagent` plugin, 还有一个比较好的办法是，设置一个 dedicated node with pre-set SSH credential. 然后需要执行git check in任务的时候指定在这个node上进行即可。



]]></content>
      <categories>
        <category>Pipeline</category>
      </categories>
      <tags>
        <tag>infra</tag>
        <tag>jenkins</tag>
        <tag>pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title>Openshift Vs Kubernetes</title>
    <url>/2019/07/09/openshift-intro/</url>
    <content><![CDATA[
OpenShift version: `3.10`

I have worked on Kubernetes and OpenShift for several months, both are good container management and schedule tools, they have many overlaps and the kubectl commands are compitable in OpenShift, so what the exactly differences between them are?

我大概是从2018年下半年开始接触OpenShift的，当时要求将部署移植到OpenShift平台上。推荐一本书:`<<开源容器云OpenShift 构建基于Kubernetes的企业应用云平台>>`，这本书基本上介绍清楚了OpenShift的一些基本操作和概念，也提到了和K8s不同的地方。

> A brief Digression: Today IBM officially close the acquisistion of Red Hat!

### Resource
[OpenShift-wikipedia](https://en.wikipedia.org/wiki/OpenShift)
1. OpenShift is a family of containerization software developed by Red Hat. Its flagship product is the `OpenShift Container Platform`, an on-premises platform as a service built around `Docker containers` orchestrated and managed by `Kubernetes` on a foundation of `Red Hat Enterprise Linux`. 

[The Differences Between Kubernetes and Openshift](https://www.redhat.com/en/blog/openshift-and-kubernetes-whats-difference)
1. Kubernetes is the kernel
2. Openshift is the distribution

[10 most important differences between OpenShift and Kubernetes](https://cloudowski.com/articles/10-differences-between-openshift-and-kubernetes/)
1. OKD can install on rhel and centos
2. Kubernetes is an integral part of OpenShift with more features built around it.
3. OpenShift has more strict security policies than default Kubernetes, most of container images available on Docker Hub or we have built before won’t run on OpenShift, because it forbids to run a container as root. we need to relax this strict to give the workspace higher privileges to run container as root, or update our image to

[The Differences Between Kubernetes and Openshift](https://medium.com/levvel-consulting/the-differences-between-kubernetes-and-openshift-ae778059a90e)














]]></content>
      <categories>
        <category>OpenShift</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>Jenkins Pass Variables between Stages</title>
    <url>/2020/06/04/pipeline-jenkins-pass-vars/</url>
    <content><![CDATA[
今天遇到一个问题，如何将在一个stage中产生的变量，传递到另一个stage中。一种解决办法是使用global variable, [for example](https://serverfault.com/questions/884764/jenkins-pipeline-file-passing-jenkinsfile-variables-into-further-commands/884798), in declarative pipeline:
```groovy
// variable to be used
def jobBaseName

stage ('Construct Img name') {
  // this is from scripted pipeline syntax
  jobBaseName = sh(
    script: "echo ${BUILD_TAG} | awk '{print tolower($0)}' | sed 's/jenkins-//'",
    returnStdout: true
  )
}

stage ('Build Target Container') {
  sh "ssh -i ~/ssh_keys/key.key user@somehost 'cd /dockerdata/build/${BUILD_TAG} && docker build -t localrepo/${jobBaseName}:${BUILD_NUMBER} .'"
}
```
还有人通过将变量写入文件中，再从另一个stage读取加载的方式，但这需要保证Stages are running on the same node agent.

此外，关于Jenkins中的`environment variable` 和 `build parameters`，有如下需要注意的地方:
<https://stackoverflow.com/questions/50398334/what-is-the-relationship-between-environment-and-parameters-in-jenkinsfile-param>

Basically it works as follow

- `env` contains all environment variables, for example: `env.BUILD_NUMBER`
- Jenkins pipeline automatically creates a global variable for each environment variable
- params contains all build parameters, for example: `params.WKC_BUILD_NUMBER`
- Jenkins also automatically creates an environment variable for each build parameter (and as a consequence of second point a global variable).

Environment variables can be overridden or unset (via Groovy script block) but params is an **immutable** Map and cannot be changed. Best practice is to always use params when you need to get a build parameter.

这些信息哪里来的呢？在配置pipeline时，查看pipeline syntax -> Global Variables Reference.
]]></content>
      <categories>
        <category>Pipeline</category>
      </categories>
      <tags>
        <tag>infra</tag>
        <tag>jenkins</tag>
        <tag>pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title>Jenkins X Quick Start</title>
    <url>/2020/06/25/pipeline-jenkinsX-learn/</url>
    <content><![CDATA[
An opinionated CI/CD platform built on top of Kubernetes.

Introduction from Cloudbees:
- https://www.youtube.com/watch?v=PqSfYuKEkVU
- https://www.youtube.com/watch?v=dO3ZHr0Rsvs

Terraform is recommended going forward.
K8s cluster must be compatiable with `jx`.

Prerequisites:
- Know how classic Jenkins pipeline works
- Know how to operate on Kubernetes
- Know how to run Docker container
- Know how to use Helm

All JX projects are deployed to Kubernetes using Docker and Helm.

# Aerial View
Jenkin X: https://jenkins-x.io/

Traditional CI/CD pipeline like classic Jenkins they require heavy customization, and not cloud-native.

Jenkins X is opinionated and cloud-native CI/CD pipeline built on top of Kubernetes. Jenkins X uses `Tekton` (a Kubernetes-native pipeline engine) to do the job.
![](https://jenkins-x.io/images/jx-arch.png)

# Setup Jenkins X
Create a cluser with Terraform on GCP.

Steps please see my github repository:
https://github.com/chengdol/jenkins-X-deployment

First install gcloud SDK and Terraform at local.
```bash
## init the environment, configuration and link to the target gcloud project
gcloud init
## we need kubectl to interact with K8s cluster
gcloud components install kubectl
```

Then create `main.tf` file to provision jenkins X cluster on GCP, go to `https://registry.terraform.io/` and search `jx`.
```

```

# Create First App on Jenkins X Pipeline
Key components:
- Applciation code source
- Docker file: All services store in docker image
- Helm chart: All docker images wrapped in helm packages
- Jenkins X file: Defines the build pipeline

```bash
jx create quickstart
```

JX commands recap:
```bash
## out of box workflow for many language projects
jx create qucikstart
## import an existing project to jx
jx import

## watch pipeline for a project
jx get activity -f <project name> -w

## view logs for a build pipeline
jx get build logs <project name>
```


# Environment with GitOps
Introduce `GitOps`:
- [What is GitOps](https://www.cloudbees.com/gitops/what-is-gitops)
- [Guide to GitOps](https://www.weave.works/technologies/gitops/#key-benefits-of-gitops)

Jenkins X adopts GitOps, where Git is the single-source-of-truth for our environment.


```bash
## list all available environment
jx get env
## create new env
jx create env

jx promote
```

# Pull Requests and ChatOps
Jenkins streamlines the pull request workflow.

`Prow`: https://jenkins-x.io/docs/reference/components/prow/
- Kubernetes CI/CD system
- Orchestrates Jenkins X pipelines via GitHub events
- Automates interactions with pull requests Enables 
- ChatOps driven development 
- GitHub only, to be superseded by Lighthouse

Github webhook will call `Prow`, the webhook is actuall a HTTP POST request that contains the event payload, then `Prow` will execute pipeline. Conversely, `Prow` can call Github API.

Introduce `ChatOps`:
- [What is ChatOps](https://victorops.com/blog/what-is-chatops)

```bash
jx create pullrequest
jx get previews
jx delete previews
```

# Creating Custom QucikStart and Build Packs
`Build Packs`: https://buildpacks.io/, the Jenkins X project template, powered by [Draft](https://github.com/Azure/draft), contains:
- Dockerfile
- Production and Preview Helm charts
- Jenkins X pipeline

Quick start workflow:
jx create quickstart -> choose
quick start projects -> generate
Vanilla project      -> detects
Build packs          -> modifies
Jenkins X project

So bascially, we use quickstart to create a vanilla project skeleton, then Jenkins X-ify this project by build packs (generate languange specific Jenkins X template files). So `jx import` existing project also use build packs to do the job.

```bash
## list repositories containing quickstarts
jx get quickstartlocation

## create new quickstart repository
jx create quickstartlocation

## delete new quickstart repository
jx delete quickstartlocation

## edit current buildpack location
jx edit buildpack
```

# Customize Jenkins X Pipeline
Jenkins X file definition has YAML file structure, make use of inheritance to reduce repetition.

```bash
## validate pipeline syntax
jx step syntax validate pipeline

## show full pipeline
jx step syntax effective
```

# Versioning and Releasing App
Jenkins X adopts `Semantic versioning`, split into 3 components.
- Major: breaking changes
- Minor: non-breaking changes
- Patch: bug fixes

Patch number is auto-incremented, Major/Minor versions are set manually.

# Custom Domain and TLS
`nip.io`, the IP with it is not human readable and prevents TLS, insecure. 比如外界访问部署后的应用。 Jenkins X allows custom domains and HTTPS

External DNS:
- Makes Kubernetes resources discoverable on public DNS servers
- Automates creation of DNS records

Certificate Manager:
- Issues SSL certificates for our applications
- Leverages `Lets Encrypt` behind the scenes

First buy domain from Google Domains web site then use it in Google Cloud settings.





]]></content>
      <categories>
        <category>Pipeline</category>
      </categories>
      <tags>
        <tag>infra</tag>
        <tag>jenkins</tag>
        <tag>pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title>Python Click</title>
    <url>/2021/10/30/python-click/</url>
    <content><![CDATA[
Command line application ingredients:
1. arguments, options(flags)
2. stdin, stdout, stderr
3. meaningful exit code and logging
4. signal handle
5. output colors: rich, colorama modules

## Non-click
If no click, how to split subcommand into dedicated file, [here](https://docs.python.org/3/howto/logging-cookbook.html#a-cli-application-starter-template) is a good example with `argparse` and `importlib` module.

## Click
Introduction video: 
https://youtu.be/kNke39OZ2k0

My bootstrap click subcommand lazy loading framework [github repo](https://github.com/chengdol/python_click), then I can focus on the functional part.

- [Arguments](https://click.palletsprojects.com/en/latest/arguments/), can also be used to check file existence, read/write file or stdin/stdout.
- [Options](https://click.palletsprojects.com/en/latest/options/), commonly use features:
is_flag, help message, default value, type with choice.
- [Commands and Groups](https://click.palletsprojects.com/en/latest/commands/)
- [Context](https://click.palletsprojects.com/en/8.0.x/commands/#nested-handling-and-contexts)

Colorful output, but `logging` module(for general purpose) is prefered, 如果不是complex应用，其实使用click 自带的颜色输出就足够了.
- [click.echo](https://click.palletsprojects.com/en/latest/api/#click.echo)
- [click.style](https://click.palletsprojects.com/en/latest/api/#click.style)
- [click.secho](https://click.palletsprojects.com/en/latest/api/#click.secho)
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>click</tag>
      </tags>
  </entry>
  <entry>
    <title>Proxy Squid</title>
    <url>/2020/08/30/proxy-squid/</url>
    <content><![CDATA[
//TODO
[ ] https tunnel setup
[ ] docker image build and test

# Introduction
http://www.squid-cache.org/
Squid is a caching proxy for the Web supporting HTTP, HTTPS, FTP, and more. It reduces bandwidth and improves response times by caching and reusing frequently-requested web pages. Squid has extensive access controls and makes a great server accelerator.

Squid docker image github, includes Dockerfile and entrypoint.sh:
https://github.com/sameersbn/docker-squid
dockerhub:
https://hub.docker.com/r/sameersbn/squid

Features useful:
[1] store cache on disk(persistent data) vs. in memory.
[2] fine tune cache size and age.
[3] user authz and authn
[4] filter traffic


# Github
This repository contains squid demo as both HTTP(s) forward proxy and reverse proxy:
https://github.com/chengdol/proxy/tree/main/squid


]]></content>
      <categories>
        <category>Proxy</category>
      </categories>
      <tags>
        <tag>squid</tag>
        <tag>proxy</tag>
      </tags>
  </entry>
  <entry>
    <title>Proxy Envoy</title>
    <url>/2022/12/10/proxy-envoy/</url>
    <content><![CDATA[
> The first edition was written on 2020-08-30.

# Demo
This Github [repo](https://github.com/chengdol/proxy/tree/main/envoy) has demos
for some important types of envoy proxies.

Some issues I had at the time of using Envoy:
- [CONNECT with domain match](https://github.com/envoyproxy/envoy/pull/13630)
- [Block or allow downstream source IPs](https://github.com/envoyproxy/envoy/issues/8388) 

# About Source Code
The protobuf plays a central role on Envoy configuration and every component in
Envoy is defined by protobuf. Here I will show some of them I explored.

For example, in external authz
[gRPC server](https://github.com/solo-io/hoot/blob/master/15-envoy-external-services/server/server.go)
demo code:
```go
import (
	auth_pb "github.com/envoyproxy/go-control-plane/envoy/service/auth/v3"
)

func (s *service) Check(ctx context.Context,
                        r *auth_pb.CheckRequest)
                        (*auth_pb.CheckResponse, error) {
	fmt.Println("received check request")
	// return nil, fmt.Errorf("error")
	return &auth_pb.CheckResponse{}, nil
}

func main() {
	auth_pb.RegisterAuthorizationServer(grpcServer, s)
}
```
The `Check` handler is specified in module
[external_auth.pb.go#L704](https://github.com/envoyproxy/go-control-plane/blob/main/envoy/service/auth/v3/external_auth.pb.go#L704) and defined in proto file service
[external_auth.proto#L33](https://github.com/envoyproxy/envoy/blob/ad7817904f/api/envoy/service/auth/v3/external_auth.proto#L33).

# Testing Facilities
There are some CLI and online facilities can help proxy testings:
- HTTP client: curl
- HTTP(S) server: www.httpbin.org, www.example.org
- TCP client: nc, telnet
- TCP server: www.tcpbin.com, nc

For complex testing that multiple components are involved, utilizing docker
compose to make them work together.

> NOTE: nc and telnet can also work with HTTP server, but you need to input HTTP
directives in connection, for example: `GET /<path> HTTP/1.1`

# Envoy Training
So far the best Envoy learning [series](https://github.com/solo-io/hoot#previous-episodes).
The key takeaways are summarized in subsequent sections.

## Episode 1: Intro to Envoy
The codelab Github [repo](https://github.com/solo-io/hoot/tree/master/01-intro).
- Cloud Native L4/L7 proxy.
- Extenability.
- Control via API(usually gRPC): control plane/ data plane.
- Observability: traces and metrics.

Core concepts and data flow, the same order in Envoy config yaml file:
```js
Requests
-> Listeners
-> Filters(routing decision): chained and order matters.
  -> TCP Filters
  -> HCM(http_connection_manager) Filters: turns envoy to http L7 proxy.
    -> HTTP Filters: operates on http header, body, etc.
      -> Router Filters: sends traffic to upstream.
-> Clusters: upstream destinations
  -> Endpoints/Cluster member/Cluster Load Assignment
```

## Episode 05: Envoy filters 
Envoy HTTP Filters:
- Code that can interact with request/response.
- Async IO.
- Transparently work with HTTP 1.1 or 2/3.
- Chained together.

## Episode 15: Envoy + External Services
The external authz gRPC server is referenced from this episode, super helpful,
see [codelab](https://github.com/solo-io/hoot/tree/master/15-envoy-external-services)


# Other Learning Resources
- [Envoy proxy blogs](https://blog.envoyproxy.io/)
- [Intro: Envoy - Matt Klein & Constance Caramanolis, Lyft](https://youtu.be/P719qI2h2yY)
([slides](https://speakerdeck.com/mattklein123/velocity-2017-lyfts-envoy-experiences-operating-a-large-service-mesh))
- [Envoy Internals Deep Dive](https://youtu.be/gQF23Vw0keg)
([slides](https://speakerdeck.com/mattklein123/kubecon-eu-2018))
- [Ambassador Envoy blogs](https://www.getambassador.io/learn/envoy-proxy)
- [Some Envoy basics](https://jvns.ca/blog/2018/10/27/envoy-basics/)

Istio (as far as I understand it) is basically an Envoy discovery service that uses information from the Kubernetes API (eg the services in your cluster) to configure Envoy clusters/routes. It has its own configuration language.

- [Envoy supports traffic shadowing](https://blog.getambassador.io/envoy-proxy-in-2019-security-caching-wasm-http-3-and-more-e5ba82da0197)
- [Load balancing and HTTP Routing with Envoy Proxy](https://youtu.be/D0cuv1AEftE)
- [Envoy Proxy Crash Course, Architecture, L7 & L4 Proxying, HTTP/2, Enabling TLS 1.2/1.3 and more](https://youtu.be/40gKzHQWgP0) 
]]></content>
      <categories>
        <category>Proxy</category>
      </categories>
      <tags>
        <tag>proxy</tag>
        <tag>envoy</tag>
      </tags>
  </entry>
  <entry>
    <title>Python Concurrency</title>
    <url>/2020/09/27/python-concurrency/</url>
    <content><![CDATA[

这里把Python concurrency的内容单独拿出来整理一下, 主要是multithreading, multiprocessing and asyncio.

根据需要选择适合场景的concurrency pattern, typs of concurrency (here we talk running app on a single machine):
**Parallel programming**
Working with multi-process cores, suited for `CPUs intensive tasks (CPU-bound tasks)`: solving the problem rather than reading to or writing from a device. Better CPU gets better performance:
- String operations
- Search algorithns
- Graphics processing

**Asynchronous programming**
Suited for `IO intensive tasks (IO-bound tasks)`: most of time reading to or writing from a device, Either to disk or to a network. 经常和callback function一起实现，或者使用future, promise or task, 主线程可以检查完成情况, use cases:
- Databse reads, writes
- Web service calls
- Copying, downloading, uploading data


# Python Concurrency
Python has concurrency support as the diagram shows:
```bash
+---------------------------------------------------+
|                                                   |
|            concurrent.futures (3.2+)              |          
|                                                   |
|  +-------------------+ +------------------------+ |
|  | threading (1.5+)  | | multiprocessing (2.6+) | |
|  +-------------------+ +------------------------+ |
+---------------------------------------------------+

            +------------+
            |  asyncio   |
            |  (3.4+)    |
            +------------+
```

这里要提一下`subprocess` 和 `multiprocessing` modules的区别:
- [What is the difference between multiprocessing and subprocess?](https://stackoverflow.com/questions/13606867/what-is-the-difference-between-multiprocessing-and-subprocess)
- [Deciding among subprocess, multiprocessing, and thread in Python?](https://stackoverflow.com/questions/2629680/deciding-among-subprocess-multiprocessing-and-thread-in-python)

## Git Repo
Demo code without threads and mulitprocessing:
https://github.com/tim-ojo/python-concurrency-getting-started

In newer version, Logging is disabled by pytest, need to explicitly enable it:
```bash
pytest -p no:logging
```

# Threading
这里的介绍已经说得很清楚了:
https://docs.python.org/3/library/threading.html

如果有多个cores, threads running in parallel, if only single core, threads share time on that core.

A process starts with a main thread (注意main thread并不是这个process, process就像一个container, 提供资源和环境，thread才是真正用来执行任务), the main thread spawns other worker threads. 不过仅仅使用thread的基本并发功能有很多缺陷，比如thread interference, race condition.

这里说一下Python threading 的局限, `GIL(Global Interpreter Lock)`, only `one` Python thread can run at a time, it is not true concurrency, it is a cooperative multithreading, so using Python threads in `IO-bound` tasks rather than `CPU-bound` tasks.

GIL workarounds:
- Jython (write python wrapped by Java)
- IronPython
- Python Multiprocessing
- concurrent.futures.ProcessPoolExecutor

如何构造threads呢? You can pass callable object (function) to constructor, the Thread class is defined as 
```py
class threading.Thread(group=None, target=None, name=None, args=(), kwargs={}, *, daemon=None):
    pass
```
For example:
```py
import threading

def do_some_work(val):
    print ("doing some work in thread")
    print ("echo: {}".format(val))
    return

val = "text"
## pass callable to constructor
## args is tuple
t=threading.Thread(target=do_some_work,args=(val,))
## start thread t
t.start()
## main thread waits until called thread terminates
t.join()
```

Or by overriding the `run()` method in a subclass. No other methods (except for the constructor) should be overridden in a subclass. In other words, only override the `__init__()` and `run()` methods of this class.
```py
import threading

class FibonacciThread(threading.Thread):
    def __init__(self, num):
        Thread.__init__(self)
        self.num = num

    def run(self): 
        fib=[0]*(self.num + 1) 
        fib[0] = 0
        fib[1] = 1
        for i in range(2, self.num + 1): 
            fib[i] = fib[i - 1] + fib[i - 2] 
            print fib[self.num]

myFibTask1 = FibonacciThread(9)
myFibTask2 = FibonacciThread(12)
myFibTask1.start()
myFibTask2.start()

myFibTask1.join()
myFibTask2.join()
```

`Thread interference`, a typical example is bank account deposit and withdraw, a `race condition` may occur. To **synchronze** threads, can use `lock` (**primitive** or **reentrant**)
- primitive lock, any thread can release it.
- reentrant lock, only holder can release, can be acquired multiple times, by the same thread.

Lock benefit: faster then other thread sync mechanisms.
```py
import threading

## not owned by a particular thread
## create in main thread
lock = threading.Lock()

## call in work threads
## automically acquire and release lock
with lock:
    pass ## do something

## use try-finally block
lock.acquire()
try:
    pass ## do something
finally:
    lock.release()

## check is the lock is acquired
lock.locked()
```

`Semaphore`: maintains a set of permits. Semaphores are often used to guard resources with limited capacity, for example, a database server.
```py
## create in main thread, default permit is 1
## BoundedSemaphore can prevent release operation number exceeds acquire's
maxconnections = 5
pool_sema = threading.BoundedSemaphore(maxconnections)

## call in worker threads
## automically acquire and release semaphore
with pool_sema:
    ## connect to the database server
    pass
```

`Events`: This is one of the simplest mechanisms for communication between threads: one thread signals an event and other threads wait for it.
```py
import threading
## initially false
event = threading.Event()

## work thread
## block until event is set to true
event.wait()

## main thread
## set to true
event.set()
## set to false
event.clear()
```

`Conditions`: combine lock and event, used for producer-consumer pattern.
```py
import threading

cond = threading.Condition()
## consumer
cond.acquire()
while not an_item_is_available():
    ## wait will release lock
    ## Once awakened, it re-acquires the lock and returns.
    cond.wait()
get_an_available_item()
cond.release()

## producer
cond.acquire()
make_an_item_available()
## Since notify() does not release the lock, its caller should.
cond.notify()
cond.release()
```

感觉这个还比较有用:
Inter-thread communication using `queues`. Python的queue module实际上是一个synchronized queue class, 用来threaded programming. 4 common methids: `put()`, `get()`, `task_done()`, `join()`. The put and get calls are blocking call.
```py
from queue import Queue
from threading import Thread

def img_down(producer):
    while not producer:
        try:
            url = producer.get(block=False)
            ## do sth
            producer.task_done()
        except producer.Empty:
            ## write some logs

## image download queue
producer = Queue()
urls = ["https://image1", "https://image2", "https://image3"]
for url in urls:
    producer.put(url)

## specify only 2 threads to download
num_thread = 2
for i in range(num_thread):
    t = Thread(target=img_down, args=(producer,))
    t.start()

producer.join()
```


# Multiprocessing
Process benefits:
- sidesteps GIL, one GIL for every python process.
- less need for synchronization.
- can be paused and terminated.
- more resilient, one crash will not bring down other prcesses.

`multiprocessing` is a package that supports spawning processes using an API similar to the threading module.
```py
import multiporcessing
```
`picklable` arguments: serializing and deserializing.

`Pool`, `apply_async`, `apply`
inter-process communication: `pipe` and `queue`


# AsycnIO
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>concurrency</tag>
      </tags>
  </entry>
  <entry>
    <title>Proxy Concepts</title>
    <url>/2022/12/11/proxy-concepts/</url>
    <content><![CDATA[
I separate the content from original Envoy proxy blog to make it shorter. The
original [Envoy proxy](https://chengdol.github.io/2022/12/10/proxy-envoy/) was
redacted to focus on Envoy concepts and demos.

# General Proxy Related Concepts
Youtube 
[Channel](https://www.youtube.com/playlist?list=PLQnljOFTspQVMeBmWI2AhxULWEeo7AaMC)
about proxy basics.

**1. What is Proxy (Server).**

A [server application](https://en.wikipedia.org/wiki/Proxy_server) that acts as
an intermediary between a client requesting a resource and the server providing
that resource.

**2. What is Forward Proxy (Proxy) and Reverse Proxy.**

- `Forward proxy`: anonymity, caching, block unwanted sites, geofencing.
- `Reverse proxy`: load balancing, ingress, caching, isolating internal
  traffic, logging, canary deployment.

A `Forward` proxy is a proxy connecting from private to public IP space (which
was the original idea for a proxy) while a `Reverse` proxy connects from public
to private IP space, e.g. mapping different web servers behind the proxy to a
single, public IP.

How does forward proxy know the final destination? via the `HOST` header, start
from HTTP/1.1. The `ping` will not pass HTTP proxy, it is a lower protocol L3.
也就是说，不是所有traffic都走的proxy. 你也可以设置哪些访问用proxy, 哪些不用。

Proxy can add additional header to tell server where is the originating IP:
`X-Forwarded-For` header. Proxy is dedicated: HTTP proxy(for HTTP but can
upgarde to support tunnel), SOCKS proxy(only for L4).

> NOTE: Reverse proxy is not necessarily a load balancer. Load balancer is one
form of reverse proxy types.

**3. What is HTTP Tunnel**

Well [explained](https://en.wikipedia.org/wiki/HTTP_tunnel) from WIKI.

The most common form of HTTP tunneling is the standardized HTTP `CONNECT` method.

In this mechanism, the client asks an HTTP proxy to **forward** the
TCP connection to the desired destination. The proxy server then proceeds to
make the connection on behalf of the client. Once the connection has been
established by the server, the proxy server continues to proxy the TCP stream
to and from the client. Only the initial connection request is HTTP - after
that, the server simply proxies the established TCP connection.

This mechanism is how a client behind an HTTP proxy can access websites using
SSL or TLS (i.e. **HTTPS**). Proxy servers may also limit connections by only
allowing connections to the default HTTPS port 443, whitelisting hosts, or
blocking traffic which doesn't appear to be SSL.

A proxy server that passes unmodified requests and responses is usually called a
gateway or sometimes a `tunneling proxy`(From WIKI proxy server).

> NOTE: More details please see my Envoy proxy demo in Github.

**4. What is HTTP Proxy**

HTTP proxy is the proxy server that speaks with HTTP protocal. It's especially
made for HTTP connections but can be abused for other protocols as well (which
is kinda standard already).

The [examples](https://everything.curl.dev/usingcurl/proxies/http) about using
`curl` with HTTP proxy to do HTTP or HTTPS(through CONNECT method if the proxy
support it!).

The `-p`(`--proxytunnel`) flag is not necessary for HTTPS, curl will ask
tunnel for you, but if you want to explicitly tunneling for other protocols such
as `ftp`, you need to specify this flag(Of course the proxy needs to support
CONNECT method).

Also please be aware that, curl new version support
[HTTPS proxy](https://everything.curl.dev/usingcurl/proxies/https) that connect
to proxy over ssl/tls(not tunnel, see curl man): `-x https://<proxy-url>:<port>`,
otherwise `-x <proxy-url>:<port>` is default with `http://`.s

> NOTE: More details please see my Envoy proxy demo in Github, especially how to
use curl to do tunnel for other protocols.

**5. Can Proxy & Reverse Proxy be Used in the Same Place?**

Yes, for example, service mesh.

**6. VPN vs Forward Proxy.**

[Proxy vs VPN, what's the difference](https://nordvpn.com/zh/blog/vpn-vs-proxy/#:~:text=A%20VPN%20and%20a%20proxy,for%20extra%20security%20and%20privacy.)

Main differences:
- VPN encrypt the traffic all the way, the proxy(socks, etc) not.
- VPN for all traffic, proxy works on app level (specific app or browser).

**7. L4 and L7 Reverse proxy.**

L7 proxy works on layer 7, it will redirect the request after it completely
received. Proxy check client request and reassemble new request to target server.

L4 proxy works on layer 4 (packet level), it will redirect the request packet
immediately to target server (don't wait all packets).

**8. TLS termination proxy and TLS forward proxy.**

TLS termination proxy:
```js
           (proxy cert)
client <=================> proxy <------------------> servers
            https                        http
```
TLS forward proxy, it is not `tunneling` (对于tunnel的类型或许叫做`Tunneling Proxy`
更合适):
```js
         (proxy cert)                  (server cert)
client <=================> proxy <==================> servers
            https                        https
```

**9. SNI.**
SNI (Server Name Indication) is an `extension to TLS` that allows a client to
specify which hostname it is attempting to connect to at the start of the TLS
handshaking process. (Because one single virtual server may host several secure
web sites, the HOST header is hidden in TLS.)

SNI sends host name in clear text, since it is in first **hello message** in
handshake. ESNI is new proposal to encrypt SNI hello message.  

[Demo](https://www.youtube.com/watch?v=t0zlO5-NWFU): launch 3 web sites in
laptop: 127.0.0.1:8080, 127.0.0.1:8081, 127.0.0.1:8082 and a haproxy 0.0.0.0:80
(reverse proxy), configuring the router routes internet inbound traffic to
haproxy to mimic situation in public cloud. 

Then use `noip` create 3 different domain names, then assign route's public IP
to each domain name.

如果使用HTTP, 则虽然访问的domain 不一样，但背后的IP是一样的，根据haproxy内部的设置通过parse
HOST header把流量转发到对应的web site上。

如果要使用HTTPS, 用certbot 生成3个certs, private keys 对应于3个web sites, 然后配置
haproxy使用SSL/TLS 和这些certs. 这时因为haproxy无法看到HOST head了，SNI才开始起作用
从而client (browser)能获取正确的cert。这里haproxy 应该是做了TLS termination.

这里Demo解释了当时envoy demo没看懂的地方，实际上就是更改了router的配置，所以才能用noip
domain去访问private网站!

There is also Envoy sandbox for
[TLS SNI demo](https://www.envoyproxy.io/docs/envoy/latest/start/sandboxes/tls-sni.html).
]]></content>
      <categories>
        <category>Proxy</category>
      </categories>
      <tags>
        <tag>proxy</tag>
      </tags>
  </entry>
  <entry>
    <title>Python Linter</title>
    <url>/2020/12/29/python-linter/</url>
    <content><![CDATA[
最近项目codebase 迁移到了GoB/Gerrit 的体系中，提交代码后CI 会做code linting 操作并且提示错误信息，最好是在提交前在本地先自检，但目前似乎没有集成本地linting 的功能, 经过观察，可以自己搭建linting 的环境去对任意2次commits 之间改动过的py, yaml or other 文件进行语法，格式的检查。

## Lint Steps
为了方便，这里使用python virtualenv, 也可以使用docker 环境，mount整个repo 然后处理.  work on a python virtual env, for example `virtualenv -p python3 venv`:
```bash
# activate the venv first
# need to be the same yamllint pylint version as the team use
# different version has different output maybe 
pip install yamllint==1.17.0
pip install pylint==2.12.2

# filter and get updated yaml and python file name list
# diff between HEAD~1 HEAD, order matters!
# the between is for endpoint not for range!
YAML_FILES=$(git diff --name-only --diff-filter=ACMR HEAD~1 HEAD | grep -E "(.+\.yaml$|.+\.yml$)" || echo "")
PY_FILES=$(git diff --name-only --diff-filter=ACMR HEAD~1 HEAD | grep -E ".+\.py" || echo "")

# .pylintrc and .yamllint
# should be in the code repo
# -r: no run if input is empty
# xargs default input will be placed at end
echo $PY_FILES | xargs -r pylint -sn --rcfile=.pylintrc >> linter_results_tmp
echo $YAML_FILES | xargs -r yamllint -c .yamllint >> linter_results_tmp

cat linter_results_tmp && rm -f linter_results_tmp
```

## Disable PyLint
You may need to update .pylintrc setting to skip warnings/errors, edit disable line to add error code or statement, for example:
```ini
disable=C0103,missing-docstring,too-many-branches
```

Or disabling the pylint check inline:
```py
if __name__ == '__main__':
    run() # click command, pylint: disable=E1120
```
Or more readable, use the symbolic name:
```py
if __name__ == '__main__':
    run() # click command, pylint: disable=no-value-for-parameter
```
And disable in a function level, for example:
```py
def wont_raise_pylint():
  # pylint: disable=W0212
  some_module._protected_member()
  some_module._protected_member()
```
And disable in a file or bigger scoup, the following part will be disabled by this rule, you can enable it again:
```py
# pylint: disable=use-implicit-booleaness-not-comparison
...
# pylint: enable=use-implicit-booleaness-not-comparison
```
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>linter</tag>
      </tags>
  </entry>
  <entry>
    <title>Python Co-routine</title>
    <url>/2020/09/13/python-coroutine/</url>
    <content><![CDATA[
Seeing the demo in [`websocket`](https://github.com/chengdol/websocket-demo), Python websockets module is taking advantage of `asyncio` which provides an elegant coroutine-based API.

So what is co-routine (协程)? 这个建议多看几遍如果忘了, 把多进程，多线程和协程优缺点都讲到了, 然后从generator + yield 出发，升级到asyncio:
[Python perspective of view on coroutine](https://www.youtube.com/watch?v=GSiZkP7cI80)
Demo code github:
https://github.com/jreese/pycon

A variant of functions that enables concurrency via `cooperative multitasking` (task yields control when they are waiting for external resources 也就是在做完重要工作需要其他资源的时候，再转换到其他任务，所以协程很适合IO-bound tasks). We can run all tasks in one thread in one process (就是在一个线程中模拟多线程，本质还是单线程，注意Python threading module也是如此), better than multi-threading and multi-processing in some situations(因为切换内核态和用户态要消耗系统时间和资源, 协程由用户自己控制切换，不用陷入系统内核态).

> to analyze function bytecode, the Python `dis` module can help, for example:
  ```py
  from dis import dis
  def func():
    pass

  dis(func)
  ```

视频中举了个naive python code实现的例子, 用到了`yield` keyword, 它不仅可以输出数据，也可以通过generator的send() method接收外部的参数. 当`yield` 整个过程结束的时候，会有一个StopIteration exception 抛出。这实现了AsyncIO的基本思想。

For example, asyncio with aiohttp:
```py
import asyncio
import time
## 支持async io 的http 库
from aiohttp import request

URLS = [
    "https://2019.northbaypython.org",
    "https://duckduckgo.com",
    "https://jreese.sh",
    "https://news.ycombinator.com",
    "https://python.org",
]

# Coroutines with aiohttp
async def fetch(url: str) -> str:
    async with request("GET", url) as r:
        return await r.text("utf-8")


async def main():
    coros = [fetch(url) for url in URLS]
    ## gather will run tasks concurrently
    ## *coros: unpacking waitable objects
    results = await asyncio.gather(*coros)
    for result in results:
        print(f"{result[:20]!r}")

if __name__ == "__main__":
    asyncio.run(main())
```]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python Hash</title>
    <url>/2021/07/16/python-hash/</url>
    <content><![CDATA[
Just a kindly reminder that Python built-in `hash()` uses random seed in some specific version, that leads to different hash value from the same input in different sessions.

See questions and comments [here](https://stackoverflow.com/questions/27522626/hash-function-in-python-3-3-returns-different-results-between-sessions).

You can use `export PYTHONHASHSEED=1` or fixed integer seed to avoid the randomness, or using `import hashlib` module, for example:
```py
import hashlib
hash_output = hashlib.md5("input".encode()).hexdigest()
# or usign differet algorithm
hash_output = hashlib.sha256("input".encode()).hexdigest()
```


]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python3 Quick Start</title>
    <url>/2019/08/05/python-learn/</url>
    <content><![CDATA[
Python build-ins modules:
https://docs.python.org/3/library/functions.html#open

# Template
With Docstring, you can use `help()` command to get module information, for example:
```py
## "words" is the script name: words.py
import words
help(words)
## fetch_words is a function inside words.py script
help(words.fetch_words)
```
Acutally `help()` works on every object.

This is a python script named `words.py` with demonstration for Docstring:
```py
"""Retrieve and print words from a URL.

Usage:

   python3 words.py <URL>
"""

import sys
from urllib.request import urlopen


def fetch_words(url):
    """Fetch a list of words from a URL.

    Args:
        url: The URL of a UTF-8 text document.

    Returns:
        A list of strings containing the words from
        the document.
    """
    ## can use with-block here
    story = urlopen(url)
    story_words = []
    for line in story:
        line_words = line.decode('utf8').split()
        for word in line_words:
            story_words.append(word)
    ## file like object, still need to close
    story.close()
    return story_words


def print_items(items):
    """Print items one per line.

    Args:
        An iterable series of printable items.
    """

    for item in items:
        print(item)


def main(url):
    """Print each word from a text document from at a URL.

    Args:
        url: The URL of a UTF-8 text document.
    """

    words = fetch_words(url)
    print_items(words)


if __name__ == '__main__':
    ## pass parameter from command line
    ## argv[0] is the module name
    main(sys.argv[1])
```

On Linux, if you add shebang `#!/usr/bin/env python3` at the top of the script, then you can run it by:
```bash
chmod +x words.py
./words.py http://sixty-north.com/c/t.txt
```
后面会专门学习一下python script方面的知识, see my blog `<<Python3 Scripting>>`.


## Exception
`try` statements do not create a new scope! the variables in try block can be seen from outside try block:
```py
import sys

def convert(s):
    try:
        number = ''
        for token in s:
            number += DIGIT_MAP[token]
        return int(number)
    except (KeyError, TypeError) as e:
        ## !r is a shortcut to call the repr of the value supplied.
        print(f"Conversion error: {e!r}", file=sys.stderr)
        ## re-raising exception
        raise
    else:
        print("I will be called only if exception didn't occur!")
    finally:
        ## will execute no matter normal or not
        pass


def sqrt(x):
    """Compute square roots using the method 
    of Heron of Alexandria.

    Args:
        x: The number for which the square root 
            is to be computed.

    Returns:
        The square root of x.

      Raises:
          ValueError: If x is negative.
    """
    if x < 0:
        ## upper level can catch this exception
        raise ValueError(
            "Cannot compute square root of "
            f"negative number {x}")

    guess = x
    i = 0
    while guess * guess != x and i < 20:
        guess = (guess + x / guess) / 2.0
        i += 1
    return guess
```

Common exception types:
- `indexError`: index out of boundary
- `keyError`: mapping
- `TypeError`: usually avoiding check this, increase function usability.
- `valueError`: int("hello")
- `OSError`: os module open file


# Modularity
Import module and attribute, 掌握import的语法, module normally is a single python source file, e.g. `hello.py`. When import, it is represented by `module` objects.
```py
## import custom module
import hello
## Hello is a class in hello.py
from hello import Hello

## system modules
import math
math.factorial(10)

from math import factorial
factorial(10)

from math import factorial as fac
fac(10)

# other import forms
from math import (factorial, exp)
from math import *
```

In the interactive python console, use `help` to explore modules:
```py
# you can search for modules, keywords, symbols, topics
help()
>>> math
>>> urllib.request

# or check math module by first import it
import math
help(math)
## request is a nested module after urllib
import urllib.request
## or
from urllib import request as req
help(urllib.request)
```

You can check the attributes of an object:
```py
## show all methods
dir(math)
dir(time.ctime)
## show type
type(math)
type(time.ctime)
```

Commonly used modules:
- `requests`: simple http library
- `urllib`: urllib is a package that collects several modules for working with URLs
- `sys`: access argv, stdin, stdout, etc.
- `time`: principally for working with unix time stamps
- `datetime`: UTC datetime, Unix timestamp, timedetla
- `pprint`: pretty print
- `os`: interface to system services
- `itertools`: iteration processing
- `contextlib`: use with `with` statement
- `typing`: type hints, built-in after python 3.5
- `functools`: functools.wraps() 用来 copy original function metadata

最近在做project的时候遇到几个新的modules:
- `threading`: threading operation
- `subprocess`: spawn new processes

`time` vs `datetime` modules:
https://stackoverflow.com/questions/7479777/difference-between-python-datetime-vs-time-modules
the `time` module is principally for working with unix time stamps; expressed as a floating point number taken to be seconds since the unix epoch. the `datetime` module can support many of the same operations, but provides a more object oriented set of types, and also has some limited support for time zones.


## Function
Function name in python uses lowercase and `-` as delimiter. `def` keywork bind a function to a name, function in Python is treated as object.

`Extended arguments`, for example: `*args`(act as tuple), `**kwargs`(act as dict). This is called parameters packing, these applies to all types of callables, for example lambda.

The parameter type order must follow: regular positional -> *args -> keyword -> **kwargs, for example:
```py
def print_args(arg1, arg2, *args, kwarg1, kwarg2, **kwargs):
    print(arg1)
    print(arg2)
    print(args)
    print(kwarg1)
    print(kwarg2)
    print(kwargs)

print_args(1, 2, 3, 4, 5, kwarg1 = 6, kwarg2 = 7, kwarg3 = 8, kwarg4 = 9)
```

The parameters after `*` must be passed by key word:
```py
def function_name(para1, *, para2 = "hello", para3):
    pass

function_name(1, para2 = "world", para3 = 567)
```

Correspondingly, we have `extended call syntax`, unpacking when pass the parameters to function call, `*` is for tuple or list, `**` is for dict. 

Unpacking parameters:
```py
def fun(a, b, c, d): 
    print(a, b, c, d) 

my_list = [1, 2, 3, 4] 
  
# Unpacking list into four arguments 
# 在变量前加单星号表示将元组（列表、集合）拆分为单个元素
# 双星号同上，区别是目标为字典，字典前加单星号的话可以得到“键”
fun(*my_list)
```

Positional-only arguments:
```py
## no kwarg can be used here
def function_name(x, /):
    print(x)
```

If no `return`, then will implicitly return `None`.
```py
def function_name(para1 = "hello", para2 = 34):
    ## rebind the global variable to local
    global count
    ## return None
    return
```


Notice that always use immutable value for default value!! Default value的赋值会在最初执行函数的时候运行一次，之后调用不会再重新赋值，看样子是一直存在内存里了。
```py
def append_word(org=[]):
    org.append("!")
    return org
## if you call multiple times with default value, the "!" get accumulated
print(append_word()) # ["!"]
print(append_word()) # ["!", "!"]
print(append_word()) # ["!", "!", "!"]
```

Another example:
```py
import time
def show_time(t = time.ctime()):
    print(t)

## the print will not get updated
show_time()
show_time()
```

Function is also an object, can be used as parameters:
```py
def print_card(words):
    banner = "+" + "-" * (len(words) + 2) + "+"
    output = "| " + words + " |"
    lines = [banner, output, banner]
    print("\n".join(lines))
    print()

def print_words(printer, words):
    printer(words)

## print_card passed as parameter
print_words(print_card, "hello, world!")
```

*args and **kwargs 可以用来`argument forwarding`:
```py
def trace(f, *args, **kwargs):
    res = f(*args, **kwargs)
    return res
```

## Special Functions
Detect whether a module is run as a script or imported as a module.
```py
# only execute function when it is run as a script
if __name__ == "__main__":
    main()
```

## Functional Programming
The special function `__call__`, 使用后class object可以当做function来调用，`__call__`就相当于定义了一个调用接口，并且加上其他数据结构，可以实现caching的效果 stateful.

You can use `timeit` module `timeit` method to measure exection time.
```py
## resolver.py file
import socket

class Resolver:
    def __init__(self):
        self._cache = {}

    def __call__(self, host):
        if host not in self._cache:
            self._cache[host] = socket.gethostbyname(host)
        return self._cache[host]

    def clear(self):
        self._cache.clear()

    def has_host(self, host):
        return host in self._cache
```

Run in REPL:
```py
from resolver import Resolver
res = Resolver()
## just like call function
res("www.google.com")
## call second time, the execution time reduces a lot
res("www.google.com")
```
How to know object is callable, use `callable()` function to test.

### Lambda
Create anonymous callable objects, syntax: `lambda [args]: expr`, the args are separated by commas or empty, the body is a **single** expression.
```py
## the key is assigned a callable function
## similar to Java
sorted(name_list, key=lambda name: name.split()[-1])
```

### Functional-style Tools
`map()`: maps function to a sequence, lazy implementation, return iterator.
`filter()`: remove elements from sequence which don't meet some criteria, lazily.
`functools.reduce()`: 2-arguments function with a sequence, reduce the sequence to one result.


## Local Function
functions defined inside function.
```py
## 实现了和前面lambda类似的功能
def sort_by_last_letter(strings):
    def last_letter(s):
        return s[-1]
    return sorted(strings, key=last_letter)
sort_by_last_letter(["hesd", "sddn", "pplea"])
```

Name resulation in the scope is checked by `LEGB` rule: Local -> Enclosing (the containing function) -> Global -> Build-in:
```py
g = "global"
def outer(p = "param"):
    l = "local"
    def inner():
        print(g, p, l)
    inner()

## global param local
outer()
## this call is wrong!
outer.inner()
```

Local function usage cases:
1. define one-off functions close to their use.
2. code organization and readability.
3. similar to lambda but more general, can have mutliple expressions.

Local function can be returned, working with `Closure`(在返回local function时，对其需要的资源进行保留，防止被垃圾回收, keep enclosing-scope objects alive):
```py
def enclosing():
    x = "closed over"
    def local_func():
        print(x)
    return local_func
lf = enclosing()
## call it
lf()

## check closure env
## (<cell at 0x106739e88: str object at 0x1067b5ab0>,)
lf.__closure__
```

`Function factories`, return other functions, returned function use both their own arguments as well as arguments to the factory.
```py
def raise_to(exp):
    def raise_to_exp(x):
        return pow(x, exp)
    return raise_to_exp

square = raise_to(2)
square(9)
```

`nonlocal` is like `global` keyword, to name binding in enclosing scope. 有点类似于local function使用的全局变量，但只针对同一个local function.

## Function Decorators
Allow you to modify existing functions or methods without changing their definition (在原函数中加入上下文). Decorators can be:
1. Local function 以及 closure 结合使用.
2. class, the class must implement `__call__()`, all class define variables are gave to decorated function.
3. instance of a class, can control decorator behavior via instance variable.
 
You can think decorator as a function accepting a function and returning a function (callable).

这里举一个local function作为decorator的例子，其他类型decorator暂时没用到:
```py
## f: the target decorated function
def escape_unicode(f):
    ## local function wrap
    def wrap(*args, **kwargs):
        ## in below, f is `city_name`
        res = f(*args, **kwargs)
        return ascii(res)
    ## wrap uses a closure to access f after escape_unicode returns
    return wrap

## original function
def city_name():
    return "Tomの"

## the city_name pass to decorator
@escape_unicode
def city_name():
    return "Tomの"

## now unicode is translated to ascii
print(city_name())
```

You can have multiple decorators, act in order 3->2->1:
```py
@decorator1
@decorator2
@decorator3
def function():
    pass
```

Keep original function metadata, for example `__name__` and `__doc__`, using functooks.wraps():
```py
import functools

def noop(f):
    ## 
    @functools.wraps(f)
    def noop_wrapper():
        return f()
    return noop_wrapper

@noop
def hello():
    """Print a dummy message
    """
    print("hello. world!")

## check metadata is there
help(hello)
hello.__name__
hello.__doc__
```

Parameterized decorator的一个用途是检查传入原函数的参数，比如这里检查第二个参数不能为负数:
```py
def check_non_negative(index):
     ## real decorating part                                      
     def validator(f):                                                       
         def wrap(*args):                                                    
             if args[index] < 0:                                             
                 raise ValueError(                                           
                     'Argument {} must be non-negative.'.format(index))      
             return f(*args)                                                 
         return wrap                                                         
     return validator                                                        

## 这里实际上是调用了check_non_negative，返回的结果作为decorator 
## 和上面的用法不一样了                                                     
@check_non_negative(1)                                                      
def create_list(value, size):                                               
    return [value] * size

## good
create_list('a', 3) 
## bad
create_list('a', -4)
```


# Basic
Unlike other programming languages, Python has **no** command for declaring a variable, python is dynamic type.
```py
# explicit conversion
int("234")
# toward 0 cutting
int("-342.134")
# 2 is the base
int("100", 2)
float("34.56")

# special
float("nan")
float("inf")
float("-inf")

None 
True, False
# True
bool(3.4)
bool("False")
# False
bool(0)
bool([])
bool(())
bool("")
```


## Operators
Python will not perform implicit type conversion, for example `"123" + 56` is wrong. Exception is in `if` and `while` condition.

Notice that `==` vs `is` when compare strings, `==` compare the value but `is` compares the identity equality, you can check the unique number by `id()`. And comparsion by value can be controlled programatically.

The logic operators are `and`, `or` and `not`. 这里注意它们会返回最后eval的值，可以利用这个特点:
```py
## 999
"hello" and 999
### "world"
0 or 'world'
```
Check if an object is None using `is` operator.
Function parameters and `return` are transferred using **pass-by-object-reference**.

Notice that sequence of the same type also support comparison, just like string comparison, item by item from left to right
```py
(3, 99, 5, 2) < (5, 7, 3)
[5, 3, 1] > [1, 9, 2]
```


## Control Flows
Python does not have switch statement, there are several ways to mimic it:
[Python switch replacements](https://stackoverflow.com/questions/60208/replacements-for-switch-statement-in-python)

```py
## condition
if x > 100 and x <= 999:
    pass
elif x >= 34 or x < -23:
    pass
elif not x:
    pass
else:
    pass

## ternary
res = "big" if x >= 100 else "small"

## loop
while True:
    pass

for item in iterable:
    pass

break/continue
```

## String
Unicode characters.

Python does not have a character data type, a single character is simply a string with a length of `1`. Square brackets can be used to access elements of the string or slice string.

Escape will work on both `"` and `'`.

The same as Java, string in Python is immutable.
```py
## raw string, no escape
x = r"\n\r\x 234\n\r"
type(x[2])

## list str methods
help(str) ## have definition
dir(str)

## string length
len(str)
## more efficient than "+"
items = "+".join(["hello", "world"])
items.split("+")
"   abc   ".strip()

## _ is the dummy var that will not be used
up, _, down = "good:bad".partition(":")

## operator
"123" + "456"
"123" * 5
```

Python `f-Strings` is better and concise then `format()`:
```py
a = 23
b = "apple"
print(f"I have {a} {b}s")
## others
print("{1} and {0}".format(a, b))
print("%d and %s" % (a, b))

# Example of accessing the attributes of an object
import os
file = "/etc"
info = os.stat(file)
print("file {0} uid {1.st_uid}, size {1.st_size}".format(file, info))

# Example of specifying field width and precision
# Print table of powers of two and their square roots
import math
x = 1
for i in range(10):
    x = x * 2
    y = math.sqrt(x)
    print("{0:4}{1:10}{2:10.3f}".format(i, x, y))
    # print("%4d%10d%10.3f" % (i, x, y))
```

## Bytes
In python3, Strings are represented by sequences of unicodes, but textual data in Linux is a sequence of bytes, we need to use `encode()` and `decode()` to convert python string to/from bytes.

You get byte object from HTTP request, need to convert to str to use.
```py
x = "hello world"
type(x)
##convert to byte stream
data = x.encode("utf-8")
## convert back
x = data.decode("utf-8")

## byte string
b = b"hello world"
type(b)
```

## List
```py
## can have comma at end
a = [1, 2, "abc", 34, 76.887, "jark", -84.124]
a = []
## slice
a[1: -1: 3]
a[: 5: 3]
a[2:]
## reverse the sequence
a[::-1]
## remove element from list
def a[3]

a.append(998)
## join list, not append, they are different!
a += [1, 2, 3]
a.append([1, 2, 3])

a.pop()
a.index("abc")
a.count(34)
a.remove("abc")
a.insert(3, "apple")
a.reverse()
## reverse sort
a.sort(reverse = True)
## sort by length of each object
a.sort(key=len)

## copy the list, but they are all shallow copy!!
a[:]
a.copy()
list(a)
```
除了list自带的sort and reverse, out-of-place functions: sorted(), reversed() can also be used, they create a new list, reversed() will return a reversed iterator.


## Dict
```py
d = dict()
d = {}
## add or update
d["one"] = 34
del d["one"]

## key can be string, number and bool
d = {"k1": "v1", "k2", 123}
d = dict(a="343", b="uuys", c=123)
## can even be generated by tuple
a = [("sure", 643), (98, "341"), ("name", "monkey")]
d = dict(a)
```

The copy of dict is shallow.
```py
d.copy()
dict(d)
```

merge dict:
```py
## if keys are overlapped, the value will be updated by the merged one
d.update({"away": 998})
```

iterate dict via foreach loop:
```py
for k in d.keys()
for v in d.values()
for k, v in d.items()
```
Use `in` and `not in` to check the existence.


## Set
Immutable collection with unique immutable objects.
```py
## s = () is tuple!
s = set()
s = {3, 676, 1, 34, 89}
type(s)

s.add(99)
s.update({5, 232, 89, -45})
s.remove(3)
```
Use `in` and `not in` to check the existence.

The copy of dict is shallow.
```py
s.copy()
set(d)
```

Set 有很多代数运算法则可以使用:
```py
s.union(t)
s.intersection(t)
s.difference(t)
s.symmetric_difference(t)
s.issubset(t)
s.issuperset(t)
s.isdisjoint(t)
```

## Tuple
Tuples are unchangeable, or **immutable** as it also is called.
```py
## useless, because immutable
t1 = ()

t1 = ("apple", "banana", "cherry")
## create one item tuple, must append comma!
t2= ("apple",)
type(t2)

## this is also tuple, used in function return
t2 = 1, 2, 4, "ok"
## for example
def min_max(items):
    return min(items), max(items)

## tuple unpacking
lower, upper = min_max([2, 3, 7, 4, 34])
## can be nested unpacking
(a, (b, c)) = (1, (2, 3))
## swap value
a, b = b, a
```

Other operations:
```py
t3 = ("hello", 10.23, 99)
len(t3)
t3 += ("world!")
t3 * 2
```

## Range
```
range(stop)
range(start, stop)
range(start, stop, step)
```

Used usually for loop counter:
```py
for i in range(10):
    pass
```
Other usages, for example, generate a list:
```py
list(range(0, 10, 2))
```

## Enumerate
If you want to have index pair with the item, use `enumerate()`:
```py
t = [3, 35, 546, 76, 123]
for i, v in enumerate(t):
    print(f"i = {i}, v = {v}")
```

# Iteration and Iterables
Comprehensions with filtering
```py
## list comprehension
[expr(item) for item in iterable if filtering(item)]
## set comprehension, no meaningful order
{expr(item) for item in iterable if filtering(item)}
## dict
{key_expr(item): val_expr(item) for item in iterable if filtering(item)}

## can also be mixed and nested
## this will create a list with 3 identical dict that value is None
## _ here is just for count
[{letter: None for letter in "ABCDEFG"} for _ in "123" if int(_) % 2 != 0]

## y is nested
## x is outer loop
[(x, y) for x in range(3) for y in range(5)]
```

`Iterable` can be passed to `iter()` to produce a iterator.
`Iterator` can be passed to `next()` to get the next value in the sequence.
```py
iterable = [1, 2, 3, 4, 5, 6, 7]
iterator = iter(iterable)
next(iterator)
next(iterator)
```

Generator function, stateful, laziness with `yield`:
```py
def gen():
    yield 0
    a = 1
    b = 2
    while True:
        yield a
        a, b = b, a + b

for i in gen():
    print(i)
    if i > 1000:
        break
```

Generator expression, can save big memory than list comprehension:
```py
(expr(item) for item in iterable if filtering(item))
## if used as function parameter, no parenthesis is needed
## 多余的()可以省掉
sum(x * x for x in range(10000000) if x % 2 == 0)
```
Generator is only single use object, 用完就没了，需要重新造一个。

There are several aggregation functions: bool: `any()`, `all()`; sync unlimited number of iterables `zip()`:
```py
for i, j in zip([1,2,3],[4,5,6]):
    print(f"output: {i}, {j}")
```

# Class
Python does not have public, private, protected key word, everything is public.

Polymorphism is implemented by late binding in Python, 并不是和Java 通过继承实现多态，Python中你可以对一个Object 调用任意method，只要它有这个method就行，用的时候才会检查。 并且继承在Python中主要用来分享共用的方法，当然也可以来多态, 但是继承在python中用得不多。

```py
class Parent:
    """ A Parent class """

    ## no number() in this class, but child class has
    def get_treble_number(self):
        """ description """
        ## editor会报错，但构造parent()不会，除非你调用这个方法才会出错
        return self.get_number() * 3

## inheritance
class Child(Parent):
    """ A Child class """

    ## class variable, shared among all instances
    class_variable = 0
    ## initializer not quite a constructor
    ## self is similar to this in Java
    def __init__(self, number):
        """ description """

        ## _ prefix variable is class property, treated as private, although you can access it
        self._number = number
        ## still needs self to call method
        self._lst = self._gen_list()
        Child.class_variable += 1

    def _gen_list(self):
         """ description """
        return [None] + [item for item in range(1, 10)]

    def get_number(self):
        """ description """

        return self._number

    def get_double_number(self):
         """ description """

        ## method local variable
        double = self._number * 2
        return double
```

`Parent.__doc__` can be used to access doc string.


注意，class method的第一个parameter `self` 可以是任意其他的名字，就是个标记而已，比如:
```py
class Test:
    def __init__(other, number):
        other._number = number
```


# File I/O and Resource Management
Check default encoding, if not specified in open(), will use this default encoding format.
```py
import sys
sys.getdefaultencoding()
```

Open file with options, for example:
```py
## mode can be any combination of `crwa`|`tb`
## r+ read/write
f = open('words.txt', mode = 'rwt', encoding = 'utf-8')
```

Some useful methods after open the file:
```py
## read all return string
f.read()
## read one line
f.readline()
## read all lines return a list
f.readlines()

## use print, f is the file descriptor
print("wirte this line", file = f)
## write lines into file
f.writelines(["first\n", "second\n"])
## rewind to beginning
f.seek(0)
## close file after use, to flush updates to disk
f.close(0)
```

For reading file, you can also use loop:
```py
for line in f:
    ## will not print \n, or you can use print(line.strip())
    sys.stdout.wirte(line)
```

Use `with-block` to auto close the resource (or you can use finally block)，不仅仅是用在file上，比如网络上的读写也可以，它们背后的实现都遵循了同样的规则，所以可以使用with-block:
```py
with open(...) as f:
    pass
```
注意这个`as` 是可以省略的，在threading module的lock使用中，就没有`as`.


# Data Structure
这里主要是和Java 对比，一些常用的数据结构，比如Stack, Queue, Deque, priorityQueue, Map, etc.

## Queue
https://www.geeksforgeeks.org/queue-in-python/
There are 3 ways to implement queue in python:

1. `list`, quite slow by `append()` and `pop(0)`, need shift all elements.
2. `collections` module import `deque`, can be used as queue.
3. `queue` module import `Queue`, A maxsize of zero `0` means a infinite queue. (this can be a synchronzied queue for thread programming)

## Deque
see Queue section:
1. `collections` module import `deque`, can be used as queue.

## Stack
https://www.geeksforgeeks.org/stack-in-python/

There are 3 ways to implement stack in python:
1. `list`, but when growing it has speed issues.
2. `collections` module import `deque`, can be used as stack (use this one).
3. `queue` module import `LifoQueue`, A maxsize of zero `0` means a infinite stack.


## Priority Queue
https://www.geeksforgeeks.org/priority-queue-in-python/

By using heap data structure to implement Priority Queues, Time complexity:
Insert Operation: O(log(n))
Delete Operation: O(log(n))

Min heap can be implemented by `heapq` module: 
https://www.geeksforgeeks.org/heap-queue-or-heapq-in-python/

Max heap is not implemented in `heapq`, the alternative way is invert the priority:
https://stackoverflow.com/questions/2501457/what-do-i-use-for-a-max-heap-implementation-in-python

from `queue` import `PriorityQueue`, min heap too:
https://www.geeksforgeeks.org/priority-queue-using-queue-and-heapdict-module-in-python

## Map
use primitive type `dict`.





]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python3</tag>
      </tags>
  </entry>
  <entry>
    <title>Python3 Package</title>
    <url>/2020/10/31/python-package/</url>
    <content><![CDATA[

最近在做Project的时候发现一个package无法在jump box之外的机器上通过pip安装，后来发现这是一个内部开发的python package， 并且为jump box的pip做了设置，加入了内部的package repo address/credential。关于package 的创建和发布还不是很了解，这里专门总结一下。
[Python Packaging User Guide](https://packaging.python.org/)


# Package and Module
Packages contains modules (module is normally a single python source file) or other packages.
Modules also are objects with special attributes.

```py
## urllib is package because it contains other modules or packages
## request is a nested module 
import urllib.request
from urllib import request

## although both are marked as module type
type(urllib)
type(urllib.request)

## show you the package location
urllib.__path__
## error, because only package has this attribute
urllib.request.__path__
```

How does python know where to import?
```py
import sys
## for system built-in modules
sys.path
## you can manipulate on it 
sys.path.append("<path>")
```
Or specify in environment variable (see `python --help`):
```bash
## will append to sys.path
export PYTHONPATH=path1:path2:path3
```


# Package Structure
```bash
package/:
  |     ## init usually is empty, > 3.3 version, it is optional
  |     ## but explicitly have it is good 
  |---  __init__.py
  |---  module1.py
  |---  module2.py
  |
  |--- subpackage1/
  |      |
  |      |--- __init__.py
  |      |--- module3.py
  |
  |--- subpackage2/
          |
          |--- __init__.py
          |--- module4.py
```
When import package, `__init__.py` will be executed if it has contents, so you can have init code here. 
module1.py and modul2.py are normal python source files, subpackage1 and subpackage2 are nested packages that has its own module. module1.py can import subpackage1 resources, and so on.

```py
## absolute imports
import package
import package.module1
from package import module2

import package.subpackage1
from package.subpackage1 import module3

## relative imports
## for example, in module3 it wants to use something in module4
## .. the same meaning in bash `cd` command
from ..subpackage2 import module4

## other forms
from . import sth
from .. import sth
```
> Note that relative import can only be used to import modules within the current top-level package and can only in the form if `from ... import`.

Sometimes you will see `__all__` in `__init__.py`, it control the public objects you can use when `from .. import *`. If you want to import other modules or packages manually, it is fine.


# Namespace Package
For splitting a single python package across multiple directories on disk.
Namespace package may not have `__init__.py`.

For example, split package1 to different path: path1 and path2, 注意这里package1 top-level 没有`__init__.py`.
```bash
path1/
  |
  |--- package1/
         |
         |--- module1.py
         |--- ## other packages
       
path2/
  |
  |--- package1/
         |
         |--- module2.py
         |--- ## other packages
```
When import:
```py
import sys
## must include both paths
sys.path.extend()['path1', 'path2']

import package
## you will see 2 paths
package.__path__
import package.module1
import package.module2
```

# Executable Directory
You can execute a directory if it contains `__main__.py`, then you can zip the directory and run the zip file.
```bash
directory/
  |
  |--- __main__.py
  |--- ## other modules or packages
```
注意directory 没有`__init__.py`，它不是一个package.

```bash
## it will run __main__.py
python directory

## zip it
cd directory
python -m zipfile -c ../directory.zip *
## run it
python directory.zip
```
这就相当于打包了一个executable，别人使用时就不需要安装其他依赖了。


# Executable Package
if you want to execute a package, also need to adds `__main__.py`, you cannot use `__init__.py` since it is only executed when import.
```bash
package/
  |    ## you can wrap the function here
  |--- __main__.py
  |--- __init__.py
  |--- ## other modules or packages
```
```bash
## run it, arguments will be read by __main__.py
python -m package <arguments>
```

# Package Layout
This is the recommended structure:
```bash
project_name/
  |
  |--- REAMDME.rst
  |--- doc/
  |--- src/
  |     |    ## package is here
  |     |--- package/
  |    ## unit test code
  |--- tests/
  |      |
  |      |--- test_code.py
  |    ## use setuptolls package
  |--- setup.py
  |    ## see later discussion
  |--- tox.ini
```
The `setup.py` for example:
```py
import setuptools

setuptools.setup(
    name="<package name>",
    version="<version number>",
    author="chengdol",
    author_email="chengdol@xxx.com",
    description="...",
    url="<package access url>",
    packages=setuptools.find_packages('src'),
    package_dir={'': 'src'},
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
    install_requires=['Flask==3.0.0', 'pysocks', 'pyyaml'],
)
```

关于`tox`, 是一个方便自动化测试的工具:
[tox](https://tox.readthedocs.io/en/latest/): Automate and standardize testing in Python.

后面讲了plugins的实现 via setuptools or namespace packages. 目前没用到。


# Package Distribution
When you create a virtualenv, there are pip, wheel and setuptools installed already.

There are `source` and `built` distrubutions, `built` package can place directly into installation directory and can be platform-specific, it is a `.whl` file. `source` package is tar.gz file, need to build before installing it. If you run `pip download`, you will see these distribution files.

For `source` package:
```bash
cd package
python setup.py sdist

## you will see a xxx.tar.gz file
cd dist
pip install xxx.tar.gz
```

For `built` package:
```bash
cd package
python setup.py bdist_wheel

## you will see a xx-py3-none-any.whl file
cd dist
## py3: python 3
## none: ABI requiremens, work with other language
## any: platform specifc
pip install xx-none-any.whl
```
Reading about what is `wheel`:
https://realpython.com/python-wheels/
A Python `.whl` file is essentially a ZIP (.zip) archive with a specially crafted filename that tells installers what Python versions and platforms the wheel will support.

A wheel is a type of `built` distribution. In this case, built means that the wheel comes in a ready-to-install format and allows you to skip the build stage required with source distributions.


Then you register account on PyPI and upload the package:
```bash
## install twine
python -m pip install --user --upgrade twine

cd package
python setup.py sdist bdist_wheel && \
          twine upload dist/* -u ${USER_NAME} -p ${PASSWORD}

## or upload to your personal repo
twine upload  --repository-url ${PACKAGES_REPO} dist/* -u ${USER_NAME} -p ${PASSWORD}
```
Tools used:
[twine](https://twine.readthedocs.io/en/latest/): Twine is a utility for publishing Python packages on PyPI.

After uploading you can use pip install the paclage in your new virtual environment.


]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python3</tag>
      </tags>
  </entry>
  <entry>
    <title>Python3 Scripting</title>
    <url>/2020/08/14/python-scripting/</url>
    <content><![CDATA[
Why Python scripting?
Easy to learn and write, interactively, powerful built-in data types and object oriented. Included packages to support huge range of tasks.

The demos of this blog:
https://github.com/chengdol/python-scripting

# Common Modules for Scripting
- `math`: Trig functions, sqrt, logarithms, exponentials
- `pickle`: Serialise / de-serialize objects for persistent storage
- `random`: Generate random numbers with various distributions
- `re`: Regular expression pattern matching and substitution
- `string`: Comprehensive string formatting
- `configparser`: Configuration file parser
- `bz2`: gzip, read and write compressed files
- `tarfile`: Read and write tar archives
- `datetime`: Represent and manipulate dates and times
- `logging`: Log message generator with various backends
- `argparse`: Parser for command-line options
- `optparse`: Parse command-line arguments
- `click`: command-line argument toolkit, decorator
- `os`: Interface to operating system services
- `sys`: Access argv, stdin, stdout, etc.
- `socket`: Python binding for the traditional BSD socket API
- `http`: Modules for client and server side http, and cookies
- `shutil`: Copy / remove files and directory trees
- `glob`: Shell-style wildcard expansion
- `xml`: Processing of XML data
- `hashlib`: common interface to many hash functions
- `signal`: single handling
- `subprocess`: execute command by spawn new processes, connect to their input/output/error pipes, and obtain their return codes
- `shlex`: parsing unix shell commands, for example, split long arguments
- `smtplib`: email handling
- `threading`: threading operations
- `timeit`: measure executing time 
- `pyyaml`: parse yaml file
- `requests`: simple http library
- `retrying`: [retrying](https://pypi.org/project/retrying/) flaky function
- `python-terraform`: [terraform wrapper](https://github.com/beelit94/python-terraform)

# Work Environment
## REPL
`REPL`: the interactive console. 这是最基本的一个python interactive shell, can be used for testing purpose.

## IPython
`ipython`: the enhanced python interpreter, can run shell commands + REPL, make alias for arbitrary shell commands and with TAB completion.

How to install: `yum install -y ipython3`
Then in terminal, run `ipython3`

可以直接在ipython中运行比如`ls -ltr`, `cd` 之类的命令，这些都属于magic function:
```py
## off magic
automagic
## then you need % prefix to run shell commands
%ls -ltr

## open magic again
%automagic
## no need % prefix
ls -ltr

## helper
ls?

## list env variables
whos
```

Create alias:
```py
## create alias 'findsymlinks'
alias findsymlinks ls -l /etc/ | grep '^l'
%findsymlinks
```

Run and edit files in IPython:
```py
run script.py
## default using vi or vim, $EDITOR
edit script.py
```

对于不能直接运行的shell commads, use shell escape with prefix `!`, similar to vim feature:
```py
## can store the output to a variable
out = !df
for line in out.grep("tmpfs"):
    print(line)
```


## IDLE
How to install: `yum install -y idle3`
To run idle3, you need desktop environment.


# Managing File System
找能实现Bash中功能的函数就行。
```py
## walk around file system, wrap around Linux system call
import os

os.getcwd()
## cd /tmp
os.chdir("/tmp")
## current level dir list
os.listdir("/")
## recursively to subdir, will return a tuple
## (current dir name, child dirs, child files)
os.walk("/tmp")
os.stat("/etc/hosts")

os.mkdir(path [, mode])
os.rename(src, dst)
os.remove(file)
## remove empty dir
os.rmdir(dir)

## 0o644, prefixed with '0o'
os.chown(file, mode)
os.chmod(file, uid, gid)
## hardlink
os.link(src, dst)
## softlink
os.symlink(src, dst)

## high-level file operations
import shutil as st
st.copy(src, dst)
## also copy attr
st.copy2(src, dst)
## mv
st.move(src, dst)
## rm -f
st.rmtree(path)
st.copytree(src, dst, ignore=None)
## which
st.which("java")

st.make_archive(basename, format)
st.unpack_archive(filename, extrac_dir, format)
```


# Interacting with Linux System
```py
import sys

## print command line parameters
for arg in sys.argv[:]:
    print(f"{arg}", end = ' ')
print()
```
To parse parameters, use `optparse` module, see example in git repo `1_optparse-demo.py`.
Besides optparse and argparse from the standard library, `click` module is a good alternative.

To get env varaible:
```py
import os

## set default value when empty response
os.getenv("EDITOR", "/usr/bin/vim")
os.getenv("HOME")
os.environ.get("HOME")
```

这节的git repo例子很有意思`5_signal-primes-v5.py`, `6_timeout.py`, 用signal handler 去改变条件变量的值，从而改变运行逻辑。之前一直在BASH中用trap去做终止前的处理。Linux has 2 signals set aside for user: `SIGUSR1`, `SIGUSR2`.


# Executing Commands
Run external commands, for example, call other shell commands or executables by `subprocess`, similar to linux `()`.

```py
import subprocess
import os

## 这是最简单的调用
## this will not disturb current env
env = os.environ.copy()
## the run method will call Popen under the nood
cmd = ["ls", "-ltr"]
## if does not set stdout/stderr, then command print result to console
process = subprocess.run(cmd, 
               env=env, 
               stdout=subprocess.PIPE, 
               stderr=subprocess.PIPE)


process.returncode
process.stdout.decode("utf-8")
process.stderr.decode("utf-8")


## 如果要运行和shell有关的命名且需要用到shell的特性 such as shell pipes, filename wildcards, environment variable expansion, and expansion of ~ to a user’s home directory.
## If shell is True, it is recommended to pass args as a string rather than as a sequence.
env = os.environ.copy()
cmd = "ls -ltr ~ | grep -i download"
process = subprocess.run(cmd, 
               shell=True,
               env=env, 
               stdout=subprocess.PIPE, 
               stderr=subprocess.PIPE)

## the same as
cmd = ["/bin/sh", "-c", "--", "ls -ltr ~ | grep -i download"]
process = subprocess.run(cmd,
               env=env, 
               stdout=subprocess.PIPE, 
               stderr=subprocess.PIPE)

process.returncode
process.stdout.decode("utf-8")
process.stderr.decode("utf-8")



## 高级操作 Popen, run 在背后调用的它
env = os.environ.copy()
## the same as export KUBECONFIG=clusters.yaml
env['KUBECONFIG'] = "clusters.yaml"
## this kubectl will refer KUBECONFIG env variable above
cmd = ["kubectl", "get", "sts"]
process = subprocess.Popen(cmd, 
                 env=env, 
                 stdout=subprocess.PIPE, 
                 stderr=subprocess.PIPE)

## out, err is byte type
out, err = process.communicate()
## conver to string
out = out.decode("utf-8")
err = err.decode("utf-8")

if process.returncode != 0:
  raise subprocess.CalledProcessError(process.returncode, cmd)
```

关于python concurrency专门的总结: `Python Concurrency`


# String Manipulation
Besides basic operation, it talks about datetime:
```py
import datetime

## depends on your local time zone
rightnow = datetime.datetime.now()
utc_rightnow = datetime.datetime.utcnow()

rightnow.month
rightnow.hour

## lots of % format
rightnow.strftime("Today is %A")
## datetime.timedelta, to add and decrease time slot
```
然后讲了`re` module. 可以参考这节的git code.


# Processing Text, Logging
For long running background service, logging is a must, we can log events into: file, syslog or systemd journal.

Logger has different handlers, for example: StreamHandler(stdout, stderr), FileHanlder, watchFileHandler, SysLogHandler, SockerHandler, JournalHandler, etc.

Logging levels: NOTSET, DEBUG, INFO, WARNING, ERROR, CRITICAL.

]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python3</tag>
      </tags>
  </entry>
  <entry>
    <title>Python Logging</title>
    <url>/2021/11/13/python-logging/</url>
    <content><![CDATA[
//TODO:
[ ] warnings.warn, for example, from elasticsearch package
https://docs.python.org/3/howto/logging.html#when-to-use-logging
[Default handler for child logger](https://stackoverflow.com/questions/58560505/what-is-the-default-handler-for-the-child-logger): When no handler set to child
logger and you call logger.info() and so on, there is a last resort:
`logging.lastResort`:
```py
import logging

# effective level inherits from root logger: warning
logger = logging.getLogger(__name__)
# where do they go? see below
logger.warning("hello")
logger.warning("world")

# <_StderrHandler <stderr> (WARNING)>
print(logging.lastResort)
# set it to Nono will get error when you call
# above logger.warning()
logging.lastResort = None
```

From experience, for complex application logging:
- inherit logging.Logger class to creat customized logger class
- create log output folder if does not exist
- update default logging dict config and applied
- set custom logger class
- get logger and return to module

[My logging framework](https://github.com/chengdol/python_logging) for complex
application, for simple application, just use code configuration to set up
module's logger layout.

# Introduction
[Official document](https://docs.python.org/3.8/howto/logging.html), read
through carefully.

`Default output`: by default, no destination is set for any logging messages.
They will check to see if no destination is set; and if one is not set, they
will set a destination of the console (`sys.stderr`).

First, know
[when to use logging](https://docs.python.org/3/howto/logging.html#when-to-use-logging).

[Logging flow](https://docs.python.org/3/howto/logging.html#logging-flow)

Multiple calls to `getLogger()` with the same name will return a reference to
the same logger object (singleton).

Loggers have a concept of `effective level`. If a level is not explicitly set on
a logger, the level of its parent is used instead as its effective level. If the
parent has no explicit level set, its parent is examined, and so on. When
deciding whether to process an event, the effective level of the logger is used
to `determine` whether the event is passed to the logger’s handlers.

Child loggers propagate messages up to the `handlers` associated with their
ancestor loggers. Because of this, it is unnecessary to define and configure
handlers for all the loggers an application uses. It is sufficient to configure
handlers for a top-level logger and create child loggers as needed. (You can,
however, turn off propagation by setting the `propagate` attribute of a logger
to `False`.) 注意，propagate 不会受到 parent logger level 的影响, 都会收到下级的信息.


[Thread safety](https://docs.python.org/3/library/logging.html#thread-safety):
It achieves this though using threading locks; there is one lock to serialize
access to the module’s shared data, and each handler also creates a lock to
serialize access to its underlying I/O.

[Cookbook](https://docs.python.org/3/howto/logging-cookbook.html) highlights:

1. logger name can be chained, logging.getLogger: apple, apple.pear,
apple.pear.peach, child log will pass to parent.
2. support threads.
3. one logger can have multiple handler and formatter: Sometimes it will be
beneficial for an application to log all messages of all severities to a text
file while simultaneously logging errors or above to the console. The root
logger level should <= handler level, otherwise handler will not receive
messages.
4. console message may no need to contain timestamp, but log file needs.
5. To dynamic change log configuration, you can use signal handler or using log
[config server](https://docs.python.org/3/howto/logging-cookbook.html#configuration-server-example),
it is listening on port to receive new configuration.
6. pass log level to
[cli application](https://docs.python.org/3/howto/logging-cookbook.html#a-cli-application-starter-template)


Logging level: `DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITIAL`, in increasing
order of severity. 设置这个的好处是可以根据level输出对应的logging信息，在设定level之下的
logging不会被记录, 比如当前level 设定为INFO, 则logging.debug()不会被输出。

The default level is `WARNING`, which means that only events of this level and
above will be tracked, unless the logging package is configured to do otherwise.

# Basic Usage
See [Basic Logging Tutorial](https://docs.python.org/3.8/howto/logging.html).
This is not good for practical use as set basicConfig will impact other imported
modules, but for simple usage it is fine.

> logging.basicConfig only initializes root logger.

```py
import logging
import sys
import pprint

# see logging record attributes
# https://docs.python.org/3/library/logging.html#logrecord-attributes
FORMAT = "[%(threadName)s, %(asctime)s, %(levelname)s] %(message)s"

# filename: write log to a file
logging.basicConfig(filename='/opt/logfile.log',
                    level=logging.DEBUG,
                    format=FORMAT)
# default the log will append to file
# if you want to overwrite, set filemode='w'
logging.basicConfig(filename='/opt/logfile.log',
                    filemode='w',
                    level=logging.DEBUG,
                    format=FORMAT,
                    # custom timestamp
                    datefmt='%m/%d/%Y %H:%M:%S')

# stream: write to system stdout via stream
logging.basicConfig(stream=sys.stdout,
                    level=logging.DEBUG,
                    format=FORMAT)

# use lazy %s formatting in logging functions
logging.debug("%s", "something")
logging.info("%s", "something")
logging.warning("%s", "something")
logging.error("%s", "something")
logging.critical("%s", "something")

# print pretty
logging.info(pprint.pformat([json or yaml]))
```
For json format log, there is a open source module:
https://github.com/madzak/python-json-logger

> NOTE: The desired `logging.basicConfig()` should come at very first and it
only take effect **once**.

# Advanced Usage
More advanced usage please see: 
https://docs.python.org/3.8/howto/logging.html#advanced-logging-tutorial

[Module-Level Functions](https://docs.python.org/3/library/logging.html#module-level-functions)

Configuring separated logger for different modules.
```py
import logging
import elasticsearch
import traceback

# for current module
logger = logging.getLogger(__name__)
# have to set handler for logger otherwise only logging.WARNING level will work
logger.setLevel(logging.DEBUG)

# set a stream_handler, you can choose other handlers
stream_handler = logging.StreamHandler(sys.stdout)
formatter = logging.Formatter(
    '[%(threadName)s, %(asctime)s, %(levelname)s] %(message)s')
stream_handler.setFormatter(formatter)
# if handler logging level > logger logging level, the level in gap will not show
# if handler logging level <= logger logging level, show everything logger allowed
stream_handler.setLevel(logging.DEBUG)
logger.addHandler(stream_handler)

# use logger instead of logging
# use lazy % formatting in logger function
logger.debug("%s", "something")

# reset different level for imported elasticsearch module 
# elasticsearch.logger is found by dir(elasticsearch)
es_logger = elasticsearch.logger
# you can enrich or suppress log from elasticsearch
es_logger.setLevel(elasticsearch.logging.INFO)
es_logger.addHandler(stream_handler)

# logging exception detail
try:
  a = [1,2]
  b = a[10]
except IndexError as e:
  logger.error(e, exc_info=True)
# if you don't know the exception type
try:
  pass
except:
  logger.error("Exception is %s", traceback.format_exc())
```

## Log Rotating 
```py
# size based
from logging.handlers import RotatingFileHandler
# time based
from logging.handlers import TimedRotatingFileHandler

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
# rotating log file when size > 1000 bytes
# keep 2 backup logs: app.log.1, app.log.2
file_r = RotatingFileHandler('file_app.log', maxBytes=1000, backupCount=2)
# rotating every 2 days with 5 backups
time_r = TimedRotatingFileHandler('time_app.log', when='d', interval=2,
                                  backupCount=5)

logger.addHandler(file_r)
logger.addHandler(time_r)

for _ in range(1000):
    logger.info("this is %d", _)
```

## Log Config File
For complex logging configuration, we can write a config file, the format can be
[ini-style](https://docs.python.org/3/library/logging.config.html#logging.config.fileConfig)
or
[dict-style](https://docs.python.org/3/library/logging.config.html#logging.config.dictConfig).
This way we don't need to hard code or change the configuration in the code.
```py
import logging
import logging.config

logging.config.fileConfig('logging.ini')
logging.config.dictConfig('logging.dict')
# or input a dict object with content
LOGGING_CONFIG = {}
logging.config.dictConfig(LOGGING_CONFIG)

# 'simplelogger' is defined in config file
logger = logging.getLogger('simplelogger')
```

## Signal Trap
In production we can use signals to change logging level dynamically, for
example:
```py
import signal

# Note to include signal_num and frame marked them as unused and del if no use.
def switch_logging_level(unused_signal_num, unused_frame): 
  '''Set logging level to INFO when receives SIGUSR1

    For example:  kill -10 <PID>
  ''' 
    del unused_signal_num
    del unused_frame

    if logger.isEnabledFor(logging.DEBUG):
      logger.setLevel(logging.INFO)
      logger.info('Disable logging.DEBUG level')
    else:
      logger.setLevel(logging.DEBUG)
      logger.info('Enable logging.DEBUG level')

signal.signal(signal.SIGUSR1, switch_logging_level)
```

It depends on how you initializes the logger, you can also update the handler's
logging level:
```py
import logging

logger = logging.getLogger()
logger.setLevel(level)
...
for handler in logger.handlers:
    handler.setLevel(logging.DEBUG)
```

For docker container please run signal script as init process or use `tini` to
forward signal.

]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>logging</tag>
      </tags>
  </entry>
  <entry>
    <title>Apache Server</title>
    <url>/2020/09/07/server-apache/</url>
    <content><![CDATA[
因为需要验证Envoy CONNECT feature的缘故，我打算自己设置一个server with SSL/TLS测试:
```
client <--------------> envoy proxy <--------------> Apache server
                     (docker container)                (certbot)
```
目前遇到的问题:
1. localhost conigure domain name: https://www.youtube.com/watch?v=gBfZdJFxjew
2. localhost set https: certbot
3. apache set user/password authentication: https://www.youtube.com/watch?v=HVt2E_Ny4po

Tool involved:
1. httpd apache
2. certbot
3. vagrant
4. docker container
5. envoy

other links:
1. set virtualhost domain:
https://httpd.apache.org/docs/2.4/vhosts/examples.html

2. apache config file in centos:
https://www.liquidweb.com/kb/apache-configuration-centos/]]></content>
      <categories>
        <category>Server</category>
      </categories>
      <tags>
        <tag>apache</tag>
        <tag>server</tag>
      </tags>
  </entry>
  <entry>
    <title>Blog Review List</title>
    <url>/2024/01/20/review-list/</url>
    <content><![CDATA[
**AUTO GENERATION**
There are **16** blogs written or updated in last **60** days: 

- [Better Small Talk](https://chengdol.github.io/2023/12/23/book-better-small-talk)
- [Engineer Survival Guide](https://chengdol.github.io/2023/12/02/book-engineer-survival-guide)
- [Career Growth](https://chengdol.github.io/2019/04/30/career-growth)
- [Spanner Google SQL](https://chengdol.github.io/2024/01/15/database-spanner-googlesql)
- [SQL Quick Revisit](https://chengdol.github.io/2023/07/01/database-sql-quick-revisit)
- [Chinese Life in English](https://chengdol.github.io/2023/11/27/english-chinese-life)
- [CNBC News](https://chengdol.github.io/2023/07/23/english-cnbc)
- [Conversation Resume for Small Talk](https://chengdol.github.io/2023/01/01/english-conversation-resume)
- [English Vocab Improvements](https://chengdol.github.io/2023/09/24/english-vocab)
- [Issue Layout](https://chengdol.github.io/2020/02/26/git-issue-layout)
- [Golang JSON Marshal and Unmarshal](https://chengdol.github.io/2023/04/08/golang-json-unmarshal)
- [Linux Paste Command](https://chengdol.github.io/2023/12/17/linux-paste)
- [Copy & Paste in Terminal](https://chengdol.github.io/2019/01/27/mac-copy-paste)
- [JSON](https://chengdol.github.io/2023/12/29/markup-json)
- [BASH Pipeline Demo](https://chengdol.github.io/2019/02/24/shell-pipeline)
- [Daily Talk](https://chengdol.github.io/2019/04/11/softskill-daily-talk)
]]></content>
  </entry>
  <entry>
    <title>Python Packages and Virtual Environment</title>
    <url>/2020/08/22/python-virtual-environment/</url>
    <content><![CDATA[
> There is a better alternative, please see `<<VSC Developing inside a Container>>`.

To use and manage third-party libraries without messing up python environment, organizing different project that has its own unique dependencies:
- `pip`: package management
- `virtualenv`: project and dependencies
- `virtualenvwrapper`: making venv more convenient

Does not talk the package that is the form of `__init__.py` under a folder, we are talking python distribution package.


# Pip
**Best practice:**
1. always work inside a virtual environment, keep things nice and clean.
2. be careful using pip with `sudo` when install packages, otherwise the installation is system-wide.

Mac's pre-installed Python is not meant for development, you can use `Homebrew` to install or download Python from python.org, that will go along with `pip`. For Linux, adhere to system manager to install pip or python. In Mac, try check if `pip` is there and it's version.

To install pip(2/3) on Linux:
```bash
# search pip2 or pip3 package
sudo yum search python* | grep pip
# this will also install libtirpc
# python3, python3-libs, and python3-setuptools
sudo yum install python3-pip

pip3 -V
pip 9.0.3 from /usr/lib/python3.6/site-packages (python 3.6)

pip2 -V
pip 9.0.3 from /usr/lib/python2.7/site-packages (python 2.7)
```

`pip` commonly use commands:
```bash
# local or global config info
# you will see the package repo
pip3 config [debug, edit, get, list, set, unset]

# search
pip3 search <package name>
# download package in current dir
pip3 download <package name>
# will auto install other dependencies
pip3 install <package name>

# list packages installed
pip3 list
# show outdate packages
pip3 list -o

# uninstall
# will not uninstall its dependencies
pip3 uninstall <package name>

# show package info
# you will see the location where the package is installed
# and its source code url
pip3 show <package name>

# seek help
pip3 help
```

`pip` is actually fetching packages from Python package index (or your own package repo)
https://pypi.org/

How to work:
1. search key work directly.
2. go to Browse projects -> Operating system -> Linux, then select other classifier (but this is still hard to search what is exactly needed).
3. check `development status`, select package in production/stable version.

Pip install from specified repo:
```bash
# use additional repo
pip install --extra-index-url '<repo url>' vault-client==0.0.4

# or set by
# pip config set <ket> <value>
global.extra-index-url='<repo url>'
global.timeout='10'
global.trusted-host='registry.corp.xxx.com'
# then run
pip install vault-client==0.0.4

# 或者创建一个~/.pip/pip.conf 文件
[global]
timeout=10
trusted-host = egistry.corp.xxx.com
extra-index-url = <repo url>
# 然后
pip install --no-cache-dir vault-client==0.0.4
```

# Virtual Environment
Combining with `virtualenvwrapper` is good, recommended.

Install `virtualenv`:
```bash
# install system-widely
# preferred way
# -m: run module
sudo python3 -m pip install virtualenv
# or
sudo pip3 install virtualenv
```

Create virtualenv:
```bash
mkdir ~/virtualenvs && cd ~/virtualenvs

# create a virtual env called rates in python3
virtualenv -p python3 rates_py3
virtualenv -p python3.8.4 rates_py3
# python2 based
virtualenv -p python2 rates_py2

# activate
cd rates_py3
# after this you will see a prefix 
# 一旦激活，不管在其他任何地方，都是这个环境！
. ./bin/activate
# check
python -V
pip -V
# you will only see less packages installed
pip list

# then start your work in your project folder...

# deactivate
deactivate
```

Other similar tool, this venv may pre-installed or need to pip install globally:
```bash
# python >= 3.3, may popular in furture
python3 -m venv <virtual env name>
```

Syncing packages with colleagues, put this requirement file in version control to share and update:
```bash
# fist activate the virtual environment
# package list
python -m pip freeze > requirements.txt
# the condition can be ==, !=, >=, <=

# create another virtual environment with the same python verion like yours
# activeate this new environment
# then run
python -m pip install -r requirements.txt
```

You can specify version in pip install:
```py
pip install flask==1.0.0
pip install 'Django<2.0'
# upgrade to latest version
pip install -U flask

# upgrade pip
# take care not to overwrite system pip using sudo
pip install -U pip
```

**How to manage the project and virtual environment?**
Separating project with virtual environment! 放在不同的文件夹中，使用时激活就行了，一般一个venv对应一个project, 但如果要测试多个不同的环境，也可以多个venvs map to one project.
```
--dev
|  |-----my_game
|  |-----my_website
|
--virtual environment
   |-----my_game
   |-----my_website
```

Real-world example, when develop flask framework, use `setup.py` with `editable` pip to install packages in virtual environment, so you can edit the flask source code and it will reflect in real-time:
[When would the -e, --editable option be useful with pip install?](https://stackoverflow.com/questions/35064426/when-would-the-e-editable-option-be-useful-with-pip-install)
```bash
git clone https://github.com/pallets/flask

# activate virtual environment
# go to root level of flask directory
python -m pip install -e .
```
Now have developing env for flask. 

You can also see `tox.ini` file in flask git repo, it is used for testing against different python versions in different virtual environments.



# Virtualenvwrapper
A user-friently wrapper around virtualenv， easy creation and activation, bind projects to virtualenvs.

Setup:
```bash
# install system-widely
sudo python3 -m pip install virtualenvwrapper
sudo pip3 install virtualenvwrapper


# get path
which virtualenvwrapper.sh
/usr/local/bin/virtualenvwrapper.sh

# add below lines to ~/.bashrc
# point virtualenvwrapper to pyhton3 explicitly
# the path could be /usr/local/bin/python3, check the config
export python3=/usr/local/bin/python3
export VIRTUALENVWRAPPER_PYTHON=/usr/local/bin/python3
# the example path:
source /Library/Frameworks/Python.framework/Versions/3.10/bin/virtualenvwrapper.sh
# if you don't want to use default virtual env home
# use absolute path
export WORKON_HOME="/home/<user>/virtualenvs"
# set the project homes, when use mkproject will create project folder here
# use absolute path
export PROJECT_HOME="/home/<user>/dev"
```

Operations:
```py
# show virtual environments list
workon

# enter or switch virtual environment
workon <venv name>

# will create both venv and project
# if the project is bound with venv, use workon will auto switch to project folder
mkproject <pro name>
mkproject -p python3 <pro name>
mkproject -p python2 <pro name>

# create a venv only
mkvirtualenv <venv name>

# for old project does not bind with venv
# activate venv and go to old project folder, run below to bind them
setvirtualenvproject 

# remove a venv
# you need to manually remove project folder is you want
rmvirtualenv <venv name>

# deactivate venv
deactivate
```


# Other Future Tools
New projects:
- pipenv: https://pipenv-fork.readthedocs.io/en/latest/
- poetry: https://python-poetry.org/





]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell Arguments Format</title>
    <url>/2019/09/17/shell-argument-format/</url>
    <content><![CDATA[
When I check the `cmd` and `entrypoint` of one docker image, I see something like this:
```
cmd: zkServer.sh start-foreground
entrypoint: /docker-entrypoint.sh
```
It actually by default works as:
```bash
/docker-entrypoint.sh zkServer.sh start-foreground
```

Let's see what is inside `/docker-entrypoint.sh`:
```bash
# Generate Zookeeper configuration
# zkGenConfig.sh is in PATH, it will be executed if condition matches
[ "$1" = 'zkServer.sh' ] && (zkGenConfig.sh || exit 1)

# execute zkServer.sh start-foreground in current process
exec "$@"
```
It just like a wrapper and executes the passed parameters as a new command. This pattern gives me some inspirations. Actually lots of containers use this pattern, like Envoy, please search my blog `<<Docker Entrypoint Script>>`

About `$@`, reference from https://stackoverflow.com/questions/9994295/what-does-mean-in-a-shell-script:
`$@` is nearly the same as `$*`, both meaning all command line arguments. They are often used to simply pass all arguments to another program (thus forming a wrapper around that other program).

The difference between the two syntaxes shows up when you have an argument with spaces in it:
```bash
wrappedProgram "$@"
# ^^^ this is correct and will hand over all arguments in the way
#     we received them, i. e. as several arguments, each of them
#     containing all the spaces and other uglinesses they have.
wrappedProgram "$*"
# ^^^ this will hand over exactly one argument, containing all
#     original arguments, separated by single spaces.
wrappedProgram $*
# ^^^ this will join all arguments by single spaces (IFS)as well and
#     will then split the string as the shell does on the command
#     line, thus it will split an argument containing spaces into
#     several arguments.
```

For example:
```bash
# in terminal, type:
wrapper "one two    three" four five "six seven"
```
What you will get:
```bash
# the same format as we passed
"$@":  "one two    three" "four" "five" "six seven"
# joined each argument by IFS shell variable and produce one final argument
"$*":  "one two    three four five six seven"
# all separated
$*:  one two three four five six seven
$@:  one two three four five six seven
```
`"$@"` is by far the most useful for most situations because it preserves the integrity of each positional parameter.

]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Array in Script</title>
    <url>/2019/08/13/shell-array/</url>
    <content><![CDATA[
Reference:
- https://wangdoc.com/bash/array.html

The first thing to do is to distinguish between bash `indexed` array and bash `associative` array. The former are arrays in which the keys are ordered integers, while the latter are arrays in which the keys are represented by strings.

Although indexed arrays can be initialized in many ways, associative ones can **only** be created by using the `declare` command.


# Create Indexed Array
```bash
# create array with out declare
# 注意这里index 只能是number，否则会出问题，因为associated array 必须用declare -A先声明
array=([0]=Sun [1]=Mon [100]=Tue [3]=Wed [4]=Thu [5]=Fri [6]=Sat)
# or
array=(
  x
  y
  z
)
# create array via declare
declare -a array=(x y z) 
# create separately
declare -a array
# the same as array=xx
array[0]=xx
array[1]=yy
array[8]=zz

# print
declare -p array
# delete item
unset array[2]
# empty and delete the array
unset array
# this will not have any impact on array content
array=
```

Add new element into array:
```bash
array+=(${var})
# or multi-item
array+=('foo' 'cat')
```
> Note that in the context where an assignment statement is assigning a value to a `shell variable` or `array` index (see Arrays), the `+=` operator can be used to append to or add to the variable's previous value.

Using `"${array[@]}"`(have double quotes) in for loop to fetch array item. `"${array[*]}"` is not recommended, the same reason as shell positional parameters with @ and *.
```bash
for item in "${array[@]}"
do
  echo ${item}
done
```

Or fetch item by index(key):
```bash
# this will only return the value existed index
# the same as associated array iteration, but here it returns index number instead
for i in "${!array[@]}"
do
  # ${array[i]} also OK
  echo ${array[$i]}
done

# ${#array[@]}: length of array
# but it bash array may have gaps! some items are empty
for((i=1;i<${#array[@]};i++))
do
  echo ${array[i]}
  echo ${array[i-1]}
done
```

Sort array, 这里对array的输出使用了pipeline，一个很好的启发:
```bash
a=(f e d c b a)

echo "Original array: ${a[@]}"
# out most () is for forming a array
a_sorted=($(for i in "${a[@]}"; do echo $i; done | sort))
echo "Sorted array:   ${a_sorted[@]}"
```


# Create Associated Array
Statement `declare` is the only way to go, see reference for more details. Actually this is `Map` in bash. 这里的key 默认是string，虽然可以写成数字.
```bash
# create all at once
declare -A array=([a]=xx [b]=yy [c]=zz)
# create separately
declare -A array
array[a]=xx
array[b]=yy
array[c]=zz
# or write in one line
# 注意这里先declare -A 了
array=([a]=xx [b]=yy [c]=zz)

# print
declare -p array
# delete item
unset array[2]
# empty and delete array
unset array
# this will not have any impact on array content
array=
```

Iterate over the associated array:
```bash
declare -a array=([a]=a [b]=b [c]=c)
# The keys are accessed using an exclamation point
# can do this to index array as well!
for key in "${!array[@]}"
do
  echo "key  : $key"
  echo "value: ${array[$key]}"
done

# assign all keys to other array
array2=("${!array[@]}")
# copy whole array
array3=("${array[@]}")
```


# Array Counterpart
Loop through comma separated string, or other delimiter.
```bash
var="1,2,3,4,5"
# use sed
for i in $(echo $var | sed "s/,/ /g")
# or use pattern match
for i in ${var//,/" "}
do
  echo $i
done
```
还可以使用`IFS`设置不同的separator (`IFS` also is used with `read`), 也可以用`tr`去替换分隔符。


# Caveat
If you use `declare -a` or `declare -A` to create array in shell function, it by default is **local scope** in that function. You can move the `declare` out to make the array globally access, or use `declare -ga` and `declare -gA (**only bash 4.2 and later support this**).

Notice that variable defined without `local` in function is global access.

Assign array to other variables:
```bash
a=('a' 'b' 'c')
# must have double qoutes to protect key that has whitespace
b=("${a[@]}")
```

Notice that you **cannot** `export` array directly. `Array variables may not (yet) be exported.` in some bash verions, but there are some workarounds.


# References
[Bash associative array examples](https://www.artificialworlds.net/blog/2012/10/17/bash-associative-array-examples/)
[Bash indexed arrays](https://www.artificialworlds.net/blog/2013/09/18/bash-arrays/)
[Indexed and associative array](https://linuxconfig.org/how-to-use-arrays-in-bash-script)]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Python Unittest</title>
    <url>/2022/11/24/python-unittest/</url>
    <content><![CDATA[
# Write Better Tests
A single unittest case should have:
1. Cover only one path through code.
2. Have better asserts and documentation.
3. Providing informative failure messages.

To avoid repeition, using test fixtures:
```mermaid
graph TD;
  setUpModule --> setUpClass;
  setUpClass --> setUp;
  setUp --> test_*;
  test_* --> tearDown;
  tearDown --> setUp;
  tearDown --> tearDownClass;
  tearDownClass --> setUpClass;
  tearDownClass --> tearDownModule;
```

Assert methods allow custom messages, for example:
```py
self.assertEqual(
    10, call_count,
    'The call count should be 10, not {}.'
    .format(call_count))
```

# Write Testable Code
Some important techniques to make code easier to test:

- Documentation.
- Dependencies.
- Decomposition.
- Graceful and informative failure.

Google Blog
[Writing Testable Code](https://testing.googleblog.com/2008/08/by-miko-hevery-so-you-decided-to.html).

# Dependency Replacement
> Test Double: A simplified replacement for any dependency of a system under
test.

You should use test doubles if  the real thing:
- Isn't available
- Won't return the results needed
- Would have undesirable side effects
- Would be too slow

Types of test doubles:
|             | What it does                                  | When to use                                             |
|-------------|-----------------------------------------------|---------------------------------------------------------|
| Placeholder | Does nothing.<br>Passed around but never used | You need a placeholder.                                 |
| Stub        | Provides canned answers.                      | You want the same result every time.                    |
| Spy         | A stub that remembers <br>how it was called.  | You want to verify functions were called the right way. |
| Mock        | Configurable mimic of<br>a particular object. | Can behave like a dummy, stub, or spy.                  |
| Fake        | A simplified version of<br>the real thing     | Interacting with a complicated system.                  |

# Other Fakes
You can find on Internet such as MySQL fake, etc.

# Mock
//TODO:
[ ] mock usage, cookbook
[ ] MagicMock
[ ] @mock.patch.object(), multi on func and the parameters order matters!

With mock object, we can make assertions about how it has been used.

A mock object can pretend to be anything. It can:
- Return expected values when called.
- Keep track of how it was called.
- Have attributes.
- Call other functions.
- Raise exceptions.

Some demos about basic usage:
```py
m = mock.Mock()
# Set up return value.
m.return_value = 'foo'
# Tracking calls and arguments
self.assertTrue(m.called) # bool
self.assertEqual(10, m.call_count) # int

# With what args called?
# All calls
m = mock.Mock()(return_value=None)
m(1, 2, 3)
m(4, 5, 6)
m()
expected = [mock.call(1, 2, 3), mock.call(4, 5, 6), mock.call()]
self.assertEqual(expected, m.call_args_list)
# Most recent call.
args, kwargs = m.call_args

# Check most recent calls.
m.assert_called_with('foo', a=1)
m.assert_called_once_with('foo', a=1)

# Check other calls.
# Use mock.ANY as a placeholder.
self.assertTrue(m.assert_any_call('foo', a=1))
self.assertTrue((m.assert_has_calls([
  mock.call('foo', a=1),
  mock.call(mock.ANY, a=1),
  ])))

# Set attributes.
m = mock.Mock()
# Then you can use m.foo it will return 'bar'.
# Uninitialized attributes are mock objects.
m.foo = 'bar'

# Use another function to determine the return value
# m(3) will return 6
m.side_effect = lambda x: x * 2
# Raises an exception when m() is called
m.side_effect = Exception('Boom!')
# If using iter, each call will return the next element.
m.side_effect = iter([1, 2, 3])
```

You can use patcher object to replace existing code with mock objects, for
example this warpper function can be used in `setUp` method.
```py
def _mock_method(self, module, function_name, **kwargs):
  """Simplifies mocking via patchers."""
  kwargs.setdefault('autospec', True)
  patcher = mock.patch.object(module, function_name, **kwargs)
  # patcher.stop must be called to restore mock object.
  # If setUp() fails, meaning that tearDown() is not called, then any cleanup
  # functions added will still be called with addCleanup.
  self.addCleanup(patcher.stop)
  return patcher.start()

# Used in setUp method.
def setUp(self):
  super().setUp()
  self._mock_send_rpc = _mock_method(self, grpc_cli, '_send_rpc')
```

# Demo with Fake gRPC Service
//TODO: Combined with gRPC python
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>unittest</tag>
      </tags>
  </entry>
  <entry>
    <title>Asynchronous Processes</title>
    <url>/2019/05/29/shell-async-process/</url>
    <content><![CDATA[
在学习orphan 和 zombie process的时候引出了一个`wait` command的问题，因为我发现shell script中尽管没有用wait 但background children processes 仍然被reap了没有造成zombie, for explanation please see my [question](https://unix.stackexchange.com/questions/644529/zombie-process-reap-without-wait).

To recap, bash `wait` 和 linux API `wait()` 是不一样的，Bash takes care of reaping processes for you, The `wait` bash command has no effect on reaping processes. And the bash stores the child process exit status in memory and it becomes available to your upon calling `wait`.

Sometimes when I run some time-consuming tasks I want to make them execute parallelly to improve the CPU utilization and reduce execution time (if the machine is multi-core or multi-processing unit)

Let's talk about different patterns to do that in shell script, for example, I have scripts:
`back.sh`
```bash
#!/bin/bash
tail -f /dev/null
```

`hello.sh`
```bash
#!/bin/bash
echo "====== hello"
exit 0
```

# Wait for all background tasks
In `main.sh`, if:
```bash
# $! capture the immediate background process id
declare -a nums=(1 2 3)
for i in "${nums[@]}"
do
  ./hello.sh &
  echo "###### PID is $!"
done
wait
echo "done!"
```
you will get the result like this, only get `done!` after all background processes finished:
```
###### PID is 11649
###### PID is 11650
###### PID is 11651
====== hello
====== hello
====== hello
done!
```

But if the `main.sh`:
```bash
./back.sh &
declare -a nums=(1 2 3)
for i in "${nums[@]}"
do
  ./hello.sh &
  echo "###### PID is $!"
done
wait
echo "done!"
```
The `wait` will hold on until all background tasks complete, you will never see `done!` because back.sh will never exit. Have to use `kill` command to kill it.

The improved way is to only pass related PIDs to wait, so scheduler will not care unrelated background task `back.sh`:
```bash
declare -a nums=(1 2 3)
declare -a pids
./back.sh &
for i in "${nums[@]}"
do
  ./hello.sh &
  echo "###### PID is $!"
  pids+=($!)
done
wait ${pids[@]}
echo "done!"
```

# Wait background task in arbitrary order
This way is very similar to above example, but we wait individually.
Also notice that `wait PID` will return the subprocess exit code! If PID is not given, all currently  active child processes are waited for, and the return status is zero. Check `man wait` for detail.

In `main.sh`, write:
```bash
#!/bin/bash
declare -a nums=(1 2 3)
declare -a pids
for i in "${nums[@]}"
do
 ./hello.sh &
 echo "###### PID is $!"
 # pids[n]=$! also works, [ ] treat n as number already
 pids[${n}]=$! 
 # n is not declared, treat as 0 as default number
 let n+=1
done

for pid in "${pids[@]}"
do
  # check exit code
  if wait ${pid}; then
    echo "success"
  else
    echo "abnormal"
  fi
done
echo "done!"
```

# SIGCHLD signal
When child process is done or terminated, it will send `SIGCHLD` signal to parent, can trap it and do something may be recycle resources. You need to enable job control first, see this [issue](https://superuser.com/questions/1206421/does-bash-have-a-hook-to-determine-when-child-shell-exits)

using `SIGCHLD` to catch the point of child process termination.
```bash
#!/bin/bash
# enable job control, see man set
# set -m is the same
set -o monitor
# trap sigchld
trap "reaping child process" SIGCHLD

(sleep 2) &

# do other things
tail -f /dev/null
```

# Others
Acutally `jobs` command can monitor the background processes:
```bash
# ./back.sh  &
[1] 15405
# jobs -l
[1]+ 15405 Running  ./back.sh &
# jobs -p
15405
```]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>async</tag>
      </tags>
  </entry>
  <entry>
    <title>Common Code Snippets</title>
    <url>/2019/02/24/shell-common-code/</url>
    <content><![CDATA[
More information please see `man bash`, it has comprehensive information.

This blog collects the commonly used code snippets based on my daily work, also do summary from related *stackoverflow* topics.


# set builtin
Usually I use `set -x` for debugging purpose, today I see a new statement `set -ex`. What is this and what is set in Bash? 后来又知道了很多，见awesome list中的bash tutoral.

[The Set Builtin](https://www.gnu.org/software/bash//manual/html_node/The-Set-Builtin.html), in short, `set` allows you to change the values of shell options and set the positional parameters, or to display the names and values of shell variables.

`set -e`, causes the shell to exit if any subcommand or pipeline returns a non-zero status. This tells bash that it should exit the script if any statement returns a non-true return value. The benefit of using `-e` is that it prevents errors snowballing into serious issues when they could have been caught earlier. 

But sometimes `set -e` may not be good, see these two posts:
[What does 'set -e' do, and why might it be considered dangerous?](https://serverfault.com/questions/143445/what-does-set-e-do-and-why-might-it-be-considered-dangerous)
这个回答很有启发，用哪种方法还得看具体场景。一定要考虑清楚。

["set -e" usage](https://www.cnblogs.com/YatHo/p/7682344.html)


# get path of running script
```
curpath=$(dirname $(readlink -f $0))
```
`readlink -f $0` will follow every symlink in every component of the given name recursively and get the `canonical` path. A single file existing on a system can have **many** different paths that refer to it, but only one `canonical` path, `canonical` gives a unique **absolute** path for a given file. That means even though you call a script in it's current directory, `readlink -f $0` will give you the absolute path!

`dirname $0` cut the script name to get the calling path, the path is relative not absolute.

# run script in it's driectory
Sometimes we want to run script in it's folder by `./xxx.sh`. we can check that:
```bash
SCRIPT_PATH=$(dirname $0)
if [[ "X""${SCRIPT_PATH}" != "X." ]]; then
  LogMsg "###### ERROR: Please run this script in it's directory!"
  exit 1
fi
```


# create tmp file to store log
Create a temporary file or directory, this temp file is owned and grouped by the current user. Aside from the obvious step of setting proper permissions for files exposed to all users of the system, it is important to give temporary files nonpredictable filenames, for example:
```bash
# $$: current PID
OUT_FILE=/tmp/$(basename $0).$$.$RANDOM$RANDOM
# or
OUT_FILE=$(mktemp /tmp/log.$$.XXXXXXXXX)
```
For regular use, it may be more wise to avoid /tmp and create a /tmp under its home.

it will randomly generate 6 characters to replace `XXXXXX`. You may need to delete the tmp file when script exits, for example, use `trap`:
```bash
function exitHook {
  rm -f $OUT_FILE
  rm -f ${OUT_FILE}.yml
  rm -f ${OUT_FILE}.out
  rm -f ${OUT_FILE}.err
}
## must put at beginning of script
trap exitHook EXIT
```

Actually, you can get random number from
```bash
echo $RANDOM
```
you can also seed it to generate reproducible sequence:
https://stackoverflow.com/questions/42004870/seed-for-random-environment-variable-in-bash


# if condition
List of [test command condition](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html) Or check manual `man test`. 

The test command, it can be written as`[]` or `test expression`, `[[ ]]` is modern format, it supports regular expression `=~` for string. which one if preferred: `test` is traditional (and part of the POSIX specification for standard shells, which are often used for system startup scripts), whereas [[ ]] is specific to bash (and a few other modern shells). It’s important to know how to use test since it is widely used, but [[ ]] is clearly more useful and is easier to code, so it is preferred for modern scripts.
```bash
## don't double quote regexp
if [[ "$name" =~ colou?r ]]; then
  echo "..."
fi
```
其他test 的变量operands 一般用double quote括起来，防止值为空的时候出错.

对于file system, 主要检测`-e`, `-f`, `-d`, `-L`, `-r -w -x`, etc. 还有更多的检测选择，参考man. 

对于string 则主要就是检测`-n`, `-z`, `=`, `==`, `!=`, `=~`, `>`, `<`.

For comparing integers, `-eq`, `-ne`, `-ge`, `-gt`, `-le`, `-lt`. Or use `(( xxx ))`, this is a compound command designed for integers:
```bash
INT=-3
if [ -z "$INT" ]; then
  echo "INT is empty." >&2
  exit 1 
fi
if [ "$INT" -lt 0 ]; then
  echo "INT is negative."
else
  echo "INT is positive."
fi
if [ $((INT % 2)) -eq 0 ]; then
  echo "INT is even."
else
  echo "INT is odd."
fi

# or using (())
if ((1)); then echo "It is true."; fi
if ((0)); then echo "It is true."; fi

# 需要注意(()) 中的变量不再需要expansion symbol $了，直接用变量名
declare -i day=30
if (( day > 0 || day < 31 )); then
  echo "day is good"
fi

# 这里结合read command，判断输入是否是有一个item
read -p "input one item -> "
(( "$(echo \"$REPLY\" | wc -w)" > 1 )) && echo "invalid input"
```

`==` or `=`, `!=` and `=~` are used for string comparision:
```bash
# sth does not exist? or using -z
if [[ "${sth}""X" == "X" ]]; then
  LogMsg "###### INFO: ..."
fi
```
or
```bash
# True if the length of "STRING" is zero.
if [[ -z "${sth}" ]]; then
  LogMsg "###### INFO: ..." >&2
  exit 1
fi
```
```bash
# directory does not exist?
if [[ ! -d "${folder_path}" ]]; then
   LogMsg "###### ERROR: ${folder_path} directory doesn't exist!"
   exit 1
fi
```

对于logial operators, 有2种模式，一种是在command内部使用，比如: `test`(`-a`, `-o`, `!`), `[[ ]]`, `(())`(`&&` `||` `!`):
```bash
if [[ "$INT" -ge "$MIN_VAL" && "$INT" -le "$MAX_VAL" ]]
# same as test
if [ "$INT" -ge "$MIN_VAL" -a "$INT" -le "$MAX_VAL" ]
# note in test need escape
if [[ ! ("$INT" -ge "$MIN_VAL" && "$INT" -le "$MAX_VAL") ]]
if [ ! \( "$INT" -ge "$MIN_VAL" -a "$INT" -le "$MAX_VAL" \) ]
```
Since all expressions and operators used by test are treated as command arguments by the shell (unlike `[[ ]]` and `(( ))` ), characters that have special meaning to bash, such as <, >, (, and ), must be quoted or escaped.

一种是外部使用的, provided by bash, for example: `[[ ]] && [[ ]] || [[ ]]`, `[[ ! xxx ]]`. They obey short circuit rule.

Tips: 对于简单的if-condition, 可以替换为形如:
```bash
# chaining commands
[ -r ~/.profile ] && . ~/.profile
cat ~/.profile && echo "this is profile" || echo "failed to read profile"
test -f "$FILE" && source "$_" || echo "$_ does not exist" >& 2
[ ! -r "$FILE" ] && { echo "$FILE is not readable" ; exit 1 }
# parameters expansion 甚至都不需要if-condition
${var:="hello"}
```

# select loop
The select loop provides an easy way to create a numbered menu from which users can select options. It is useful when you need to ask the user to choose one or more items from a list of choices.

> Note that this loop was introduced in ksh and has been adapted into bash. It is not available in sh.

```bash
# PS3 is designed for select command
PS3="Enter your choice (must be a number): "
select DRINK in tea cofee water juice appe all none
do
   # After a match is found, no further matches are attempted.
   # don't need the double quote
   # the pattern match is the same as pathname expansion
   # for example: ???) [[:alpha:]]) *.txt)
   case $DRINK in
      tea | cofee | water | all) 
        echo "Go to canteen"
        break
        ;;
      juice|appe)
        echo "Available at home"
        break
        ;;
      none) 
        break 
        ;;
      # match anything at last
      *) 
        echo "ERROR: Invalid selection" 
        ;;
   esac
done
```
When select you can use index number or literal, if no `break`, it will loop forever.
If want case to match more than one terms, use `;;&` instead of `;;` at end of each case. The addition of the `;;&` syntax allows case to continue to the next test rather than simply terminating.


# input password and confirm
Must not show password user input:
```bash
echo "****************************************************************"
echo "Please input the password:"
echo "****************************************************************"
while true; do
  read -s -p "PASSWORD: " PASSWORD
  echo
  read -s -p "CONFIRM:  " PASSWORD_CONFIRM
  echo
  [ ${#PASSWORD} -lt 6 ] && echo "The length of password at least 6, please try again" && continue
  [ "${PASSWORD}" = "${PASSWORD_CONFIRM}" ] && break
  echo "Passwords do not match please try again..."
done
```

# script input parameters
```bash
if [ $# -eq 0 ]; then
  echo "No command-line arguments were specified..."
  # call Usage function here
  exit 1
fi

## case和C语言中有一样的性质，如果没有break，会继续对比接下来的选项
## 这里并不需要，因为shift 且没有相同的flags
while [ $# -gt 0 ]
do
  case "$1" in
    -p1)
       shift
       P1=${1}
       shift;;

    -p2)
       shift
       P2=${1}
       shift;;

    -h|--help)
       # Usage
       exit 0;;
       
    *) # Usage
       exit 1;;
  esac
done

[[ "X$P1" = "X" ]] &&  exit 1
[[ "X$P2" = "X" ]] &&  exit 1
```

> Note there are 2 shift in one case, after each `shift`, `$#` minus 1.


# function
The function refers to passed arguments by their position (not by name), that is `$1`, `$2`, and so forth. `$0` is the name of the script itself.
```bash
function example()
{
  ## local var prevent var leaking to shell
  local first=$1
  local second=$2
  ## return code is similar to exit code but this is return
  ## will break the rest execution
  return <return code>
}
```
Need to call your function after it is declared.
```bash
example "p1" "p2"

args #0 is <absolute path to script itself>
args #1 is p1
args #2 is p2
```

Show functions:
```bash
## list all function names
declare -F
## show definition
declare -f [function name]
## clear a function
unset -f <function name>
```

Export functions, to make it available to subshells, similarly to export variables:
```bash
## -xf: export a function
declare -xf <function name>
```

# log message
```bash
LogMsg()
{
  # parse input and reformat
  logMsg="$@"
  echo "["`date +"%Y/%m/%d %r"`"] " ${logMsg}
}
```
```bash
LogMsg "[INFO] ..."
LogMsg "[WARNING] ..."
LogMsg "[ERROR]..."
```

Actually, this style `[INFO] [2019-10-11 15:59:26-0081] ...` it better.

# check last command result
```bash
echo_success_failure() {
  if [ $? -eq 0 ]; then 
    LogMsg "###### INFO: Success..."
  else 
    LogMsg "###### INFO: Failure..."
  fi
}
```

# run as root
```bash
effective_uid=`id -u` 2>/dev/null
if [ $effective_uid -ne 0 ]; then
 LogMsg "###### ERROR: Please run this script as root or sudo"  
 exit 1
fi
```

# IFS and read array
The default value of IFS contains a space, a tab, and a newline character.
Convert string to array with specific delimiter, for example:
```bash
string="item1:item2:item3"
# <<<: is here string, the same as here doc but shorter single string
OLD_IFS=$IFS
IFS=':' read -a array <<< "${string}"
# or using process substitution
IFS=':' read -a array < <(echo "${string}")
IFS=$OLD_IFS
```
This version has no globbing problem, the delimiter is set in `$IFS` (here is space), variables quoted. Don't forget to do sanity check after converting.
```bash
${array[0]}  ===> item1
${array[1]}  ===> item2
${array[2]}  ===> item3
```

Why we use here string rather than pipeline, for example:
```bash
echo "${string}" | read
```
这是不行的，因为pipeline 的本质是subshell, 但是read 需要更改当前parent shell的内容的。这里read 实际上在更改了subshell中$REPLY的内容，一旦command 结束，subshell就没了, parent shell 并没有变化. 

此外，验证输入的正确性也很重要，一般用`[[ =~ ]]` regular expression 去检测了.

Actually if the string use spaces as delimiter, we can loop items directly:
```bash
string="item1 item2 item3"
for i in ${string}
do
  echo ${i}
done
```

# loop array
`break` and `continue` can be used on loop. 还要注意当`do` 写在下一行的时候，`do`前面不需要`;`.

```bash
declare -a array=("element1" "element2" "element3")
for i in "${array[@]}"
do
   echo "${i}"
done
```
`declare` or `typeset` are an explicit way of declaring variable in shell scripts.

In BASH it is safer to quote the variable using `""` for the cases when `$i` may contain white spaces or shell expandable characters.

If you want to use index of array element
```bash
# get length of an array
arraylength=${#array[@]}

# use for loop to read all values and indexes
for (( i=0; i<${arraylength}; i++ ))
do
  ## ${array[$i]} 这里注意，先解析的$i
  echo $i " / " ${arraylength} " : " ${array[$i]}
done
```

If we use `declare` to define a integer variable:
```bash
declare -i x=10
while (( x > 0 ))
do
  echo $x
  ## no need to use 'let x=x-1'
  ## because x is type integer
  x=x-1
done

# true loop 3 种写法
while true | while ((1)) | while :
do
 ## pass
done
```

Until loop continues until it receives a zero exit status.
```bash
count = 1

until [[ "$count" -gt 5 ]]; do
  echo "$count"
  count=$((count + 1))
done
```

In ZSH shell, you can use foreach loop:
```zsh
## () is a must
foreach item (`ls /tmp`)
  echo $item
end
```

Another index loop using `seq`:
```bash
for i in $(seq 1 10)
do
  echo $i
done
```

# read file
```bash
# read 3 fields a line, line by line from distros.txt file
# note that < is placed after done, it is the input for loop
while read distro version release; do
  printf "Distro: %s\tVersion: %s\tReleased: %s\n" \
    "$distro" \
    "$version" \
    "$release"
# no need cat here
done < distros.txt
# or
done < <(cat distros.txt)

# can also pipeline input to a loop
# while and read is running on subshell
sort -k 1,1 -k 2n distros.txt | while read distro version release; do 
  printf "Distro: %s\tVersion: %s\tReleased: %s\n" \
    "$distro" \
    "$version" \
    "$release"
done

# using process substitution
# list last 3 lines of dir
while read attr links owner group size date time filename; do
  cat << EOF
    Filename: $filename
    Size:     $size
EOF
done < <(ls -ltrh | tail -n +2)
```

# chmod
`chmod` recursively for directory and it's content
```bash
chmod -R 0755 <target directory>
```
Or only add executable for file
```bash
find . -name '<file name>' -type f | xargs chmod +x
```
```bash
-rwxr-xr-x ...
```

# pass parameters to script for read
Read can read from keyboard input or file or pipeline: `read [-options] [variables...]`. If no variable name is supplied, the shell variable `$REPLY` contains the line of data. If read receives fewer than the expected number, the extra variables are empty, while an excessive amount of input results in the final variable containing all of the extra input.
```bash
# pass parameters to read command
# must stick to this format
echo "admin
123456" | ./script.sh

# receive code snippet in script.sh
# ${username}  ===> admin
# ${password}  ===> 123456
echo -n "Please enter username -> "
read username
echo -n "Please enter an password -> "
# -s: silent
read -s password
```
Other options:
```bash
# -p: prompt
read -p "Enter one or more values > "
echo "REPLY = '$REPLY'"

# -t: timeout
# -s: silent
if read -t 10 -sp "Enter secret passphrase > " secret_pass; then
  echo -e "\nSecret passphrase = '$secret_pass'"
else
  echo -e "\nInput timed out" >&2
  exit 1
fi

# -e: pair with -i
# -i: default valut passed to read
read -e -p "What is your user name? " -i $USER
echo "REPLY = '$REPLY'"
```

# setup ssh password-less
Idempotence：
```bash
ssh-keyscan -H ${remote} >> ~/.ssh/known_hosts
sshpass -p "<password>" ssh-copy-id -i ~/.ssh/id_rsa.pub root@${remote}
if [[ $? -ne 0 ]]; then
  LogMsg "######ERROR: Something went wrong with ssh-copy-id. Check for incorrect credentials ... "
  exit 1
fi
```

# recursive call
```bash
example()
{
  <execute sth>
  if [[ $? -ne 0 ]]; then
       LogMsg "######ERROR: Something went wrong… "
       example
  fi
}
```

# tee command
`tee` command reads the standard input and writes it to `both` the standard output and one or more files, `-a` flag used to append output to existing file, if no `-a`, tee will create the file if not exist.
  ```bash
  LogMsg()
  {
    logMsg="$@"
    echo "["`date +"%Y/%m/%d %r"`"]" ${logMsg} | tee -a logs/ds_${stage}_${timeStamp}.log
  }
  ```
  ```bash
  # 注意这里tee 有2个方向的输出，可以用来检查pipeline的中间输出是什么
  +-------------+     +-------+    +--------------+
  |  command    |     | tee   |    |   stdout     |
  |   output    +---->+       +--->+              |
  +-------------+     +---+---+    +--------------+
                          |
                      +---v---+
                      |  file |
                      |       |
                      +-------+
  ```

# statement block
  这个很有意思，之前都没见过: `{}`
  [Statement block in shell script](https://unix.stackexchange.com/questions/390329/statement-blocks-mechanism-in-shell-scripting)

# do something after reboot
```bash
#!/usr/bin/env bash
# this script will do sth after reboot 
# in /root/completeme.sh
# then restore /etc/profile
#################################################

echo "Warning! This script is going to reboot now to complate the procedure"
echo "After reboot, login as root to perform the final steps"
echo "Press Ctrl-C now to stop this script in case you don\'t want to reboot"

## heredoc
cat << REBOOT >> /root/completeme.sh
## do sth after reboot

touch /tmp/after-reboot                            
rm -f /etc/profile
mv /etc/profile.bak /etc/profile
echo DONE
REBOOT

chmod +x /root/completeme.sh
cp /etc/profile /etc/profile.bak
## after reboot /etc/profile will be executed so /root/completeme.sh
echo /root/completeme.sh >> /etc/profile
reboot
```

# monitor CPU load
```bash
#!/usr/bin/env bash

## to increase CPU load
## dd if=/dev/zero of=/dev/null
## or use stress command!

while sleep 60
do
  ## to remove header of ps output, append `=` or user --no-headers flag
  ## CPU$ 0.0 will be in part if CPU$ > 0.0
  REC=`ps -eo pcpu= -o pid= -o comm= | sort -k1 -n -r | head -1`
  USAGE=`echo $REC | awk '{print $1}'`
  ## truncate decimal part
  USAGE=${USAGE%.*}
  PID=`echo $REC | awk '{print $2}'`
  PNAME=`echo $REC | awk '{print $3}'`

  # Only if we have a high CPU load on one process, run a check within 7 seconds
  # In this check, we should monitor if the process is still that active
  # If that's the case, root gets a message

  ## man test
  if [ $USAGE -gt 80 ] 
  then
    USAGE1=$USAGE
    PID1=$PID
    PNAME1=$PNAME
    sleep 7
    REC=`ps --no-headers -eo pcpu,pid -o comm= | sort -k1 -n -r | head -1`
    USAGE2=`echo $REC | awk '{print $1}'`
    USAGE2=${USAGE2%.*}
    PID2=`echo $REC | awk '{print $2}'`
    PNAME2=`echo $REC | awk '{print $3}'`
    
    # Now we have variables with the old process information and with the
    # new information

    [ $USAGE2 -gt 80 ] && [ $PID1 = $PID2 ] && mail -s "CPU load of $PNAME is above 80%" root@blah.com < .
  fi
done
```

]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Python Simple Http(s) Server</title>
    <url>/2020/09/02/python-simpleHttpServer/</url>
    <content><![CDATA[
最近为了在局域网中传输文件，用Python搭建了一个很简单的
[HTTP web server to serve files](https://docs.python.org/3/library/http.server.html),
其实有其他软件也可以完成类似功能，甚至scp command也行。不过这个简单的HTTP web server里面的
文件结构一目了然，下载非常方便。

后续还可以添加basic auth service, 甚至SSL/TLS support, 作为其他服务的测试工具。

How to:
1. 用`virtualenvwrapper` 先搭建一个python3 项目和对应的环境。
2. 激活环境后, 写一个shell script，输出运行时的网址和端口

比如在Mac中，使用如下脚本:
```bash
#!/bin/bash

# Use the output in browser in another machine
echo "$(ifconfig | grep -A 10 ^en0 | grep inet | grep -v inet6 | cut -d" " -f2)":8000

# Launch the server
# port number: 8000
# --directory: old version python may not support this option
python3 -m http.server 8000 --bind 0.0.0.0 --directory <absolute path to share folder>
```

# Basic Auth
https://gist.github.com/fxsjy/5465353
You can package it in container or run on virtualenv.
```bash
# Works with python2
# port: 8080
# auth: user:password
python2 -m main.py 8080 chengdol:123456
```

# HTTPS Basic Auth
https://github.com/tianhuil/SimpleHTTPAuthServer
You can run the pip in python2 virtualenv or build a docker container.

Modify to build a image with https auth server, for example:
```dockerfile
# works on python2
FROM python:2

WORKDIR /usr/src/app

USER root
# pip install
RUN pip install SimpleHTTPAuthServer
# to setup self-signed certificate, need to pre-generated key and cert
RUN mkdir -p /root/.ssh
# From source code it use .ssh folder to store the pem.
RUN openssl req \
    -newkey rsa:2048 -new -nodes -x509 -days 3650 \
    -keyout /root/.ssh/key.pem -out /root/.ssh/cert.pem \
    -subj "/C=US/ST=CA/L=San Jose/O=GOOG/OU=Org/CN=localhost"

# copy other files to WORKDIR
COPY file.txt .

EXPOSE 8080
# start with https
CMD ["python", "-m", "SimpleHTTPAuthServer", "8080", "chengdol:123456", "--https"]
```
Then go to firefox, type and hit `https://localhost:8080`.
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>server</tag>
      </tags>
  </entry>
  <entry>
    <title>Login and Non-login Shell</title>
    <url>/2019/05/27/shell-login-nonlogin/</url>
    <content><![CDATA[
In the non-root development for DataStage, in order to launch applications as non-root user, I use `su - dsadm` with following commands. Later I occassionally notice that badal use `su dsadm`... So what are the differences?

There are two main shell instance types: `interactive` and `noninteractive`, but of those, only interactive shells are of interest because noninteractive shells (such as those that run shell scripts) usually don’t read any startup files. 

`Interactive shells` are the ones that you use to **run commands from a terminal**, they can be classified as `login` or `non-login`. I know there are lots of startup files under each user's home directory, how do they be called and in what order?

# Login Shell
Logging in remotely with `SSH` will give you a login shell (Because we actually need credentials to login).

You can tell if a shell is a login shell by running `echo $0`; if the first character is a `-`, the shell’s a login shell.
```bash
# if not sure it's login or non-login, check it
echo $0
-bash
```

When Bash is invoked as a `Login` shell:
1. Login process calls `/etc/profile` (this is for all users)
2. `/etc/profile` calls the scripts in `/etc/profile.d/`
3. Login process calls `$HOME/.bash_profile`, `$HOME/.bash_login` and `$HOME/.profile` in order, the first found file is run and rest are ignored, most Linux distributions use only one or two of these four startup files. Notice that `$HOME/.bashrc` are not in the list, it typically run from one of these files.

Login Shells created by explicitly telling to login, there is a `-` or `-l` flag:
```bash
su -
su -l
su --login
su USERNAME - 
su -l USERNAME
su --login USERNAME
sudo -i
```

# Non-login Shell
When bash is invoked as a `Non-login` shell (for example, you just run `bash` or `sh` without login):
如果先ssh进入系统，再运行`bash`，则就是non-login shell了。
1. Non-login process(shell) calls `/etc/bashrc`
2. then calls `$HOME/.bashrc` (remember!)

`Non-Login` shells created using the below commands:
```bash
su
su USERNAME
```

> Of course you can source `$HOME/.bashrc` in `$HOME/.bash_profile` files to satisfy both login and non-login shell, for example: add `. $HOME/.bashrc` in `$HOME/.bash_profile` (it's usually there by default setting).

The reasoning behind the two different startup filesystems is that in the old days, users logged in through a traditional terminal with a `login shell`, then started `non-login` subshells with windowing systems or the screen program. For the `non-login` subshells, it was deemed a waste to repeatedly set the user environment and run a bunch of programs that had already been run. With login shells, you could run fancy startup commands in a file such as `.bash_profile`, leaving only aliases and other “lightweight” things to your `.bashrc`.

This can explain that if you use non-login like `su dsadm`, the parent exported environment variables are still there in `env` scope. But if you run `su - dsadm`, the parent exported environment variables are gone.

# Bash Parameters
```bash
# Make bash act as if it had been invoked as a login shell
# 但实际上并不是真正的login shell, echo $0 可知
bash --login
bash -l

# -c: string If the -c option is present, then commands are read from string.
# If there are arguments after the string
# they are assigned to the positional parameters, starting with $0.
# 这就不是interactive shell了
bash -c /tmp/test.sh hello world!

# do not run ~/.bashrc, by default same as `sh`
bash --norc

# 这个只对login shell才有用，不进行任何初始化
bash --noprofile

# specify other script to replace .bashrc
bash --rcfile <path to file>

# do syntax check only
bash -n <script>
```
Interesting question:
[How to start a shell in clean](https://stackoverflow.com/questions/9357464/how-to-start-a-shell-without-any-user-configuration)

`~/.bash_logout` will be executed when exit login shell.

# Question
Docker or K8s pod init process 会初始化 `~/.bashrc`吗？ 这个init process 是user login shell呢 还是non-login shell? 应该这么理解: k8s pod或者 docker container运行的时候，是可以设置环境变量的，这些环境变量可以被script 使用. 环境变量是独立于login/non-login这个概念的，只要是bash 环境，则环境变量就可以存在而被使用。这个init process 既不是login，也不是non-login，它就是一个普通的内部的程序而已。]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>BASH Pipeline Demo</title>
    <url>/2019/02/24/shell-pipeline/</url>
    <content><![CDATA[
This blog reformats and builds on top of this *stackoverflow* 
[topic](https://unix.stackexchange.com/questions/30759/whats-a-good-example-of-piping-commands-together).
Big thanks to **rahmu** and people contributed.


### Problem
Let's say the command `conky` stopped responding on my desktop, and I want to
kill it manually. I know a little bit of Unix, so I know that what I need to do
is execute the command `kill <PID>`. In order to retrieve the PID, I can use
`ps` or `top` or whatever tool my Unix distribution has given me. But how can I
do this in one command?

### Answer

```bash
ps aux | grep conky | grep -v grep | awk '{print $2}' | xargs kill
```
*DISCLAIMER*: This command only works in certain cases. Don't copy/paste it in
your terminal and start using it, it could kill processes unsuspectingly.
Rather learn **how to build it**.

### How it works
- **`ps aux`**

This command will output the list of running processes and some info about them.
The interesting info is that it'll output the PID of each process in its 2nd
column. Here's an extract from the output of the command on my box:

```bash
$ ps aux
 rahmu     1925  0.0  0.1 129328  6112 ?        S    11:55   0:06 tint2
 rahmu     1931  0.0  0.3 154992 12108 ?        S    11:55   0:00 volumeicon
 rahmu     1933  0.1  0.2 134716  9460 ?        S    11:55   0:24 parcellite
 rahmu     1940  0.0  0.0  30416  3008 ?        S    11:55   0:10 xcompmgr -cC -t-5 -l-5 -r4.2 -o.55 -D6
 rahmu     1941  0.0  0.2 160336  8928 ?        Ss   11:55   0:00 xfce4-power-manager
 rahmu     1943  0.0  0.0  32792  1964 ?        S    11:55   0:00 /usr/lib/xfconf/xfconfd
 rahmu     1945  0.0  0.0  17584  1292 ?        S    11:55   0:00 /usr/lib/gamin/gam_server
 rahmu     1946  0.0  0.5 203016 19552 ?        S    11:55   0:00 python /usr/bin/system-config-printer-applet
 rahmu     1947  0.0  0.3 171840 12872 ?        S    11:55   0:00 nm-applet --sm-disable
 rahmu     1948  0.2  0.0 276000  3564 ?        Sl   11:55   0:38 conky -q

```

- **`grep conky`**

I'm only interested in one process, so I use `grep` to find the entry
corresponding to my program `conky`.

```bash
$ ps aux | grep conky
 rahmu     1948  0.2  0.0 276000  3564 ?        Sl   11:55   0:39 conky -q
 rahmu     3233  0.0  0.0   7592   840 pts/1    S+   16:55   0:00 grep conky
```

- **`grep -v grep`**

As you can see in step 2, the command ps outputs the `grep conky` process in its
list (it's a running process after all). In order to filter it, I can run
`grep -v grep`. The option` -v` tells grep to match all the lines excluding the
ones containing the pattern.
```bash
$ ps aux | grep conky | grep -v grep
 rahmu     1948  0.2  0.0 276000  3564 ?        Sl   11:55   0:39 conky -q
```

- **`awk '{print $2}'`**

Now that I have isolated my target process. I want to retrieve its PID. In other
words I want to retrieve the 2nd word of the output. Lucky for me, most (all?)
modern unices will provide some version of `awk`, a scripting language that does
wonders with tabular data. Our task becomes as easy as `print $2`.
```bash
$ ps aux | grep conky | grep -v grep | awk '{print $2}'
 1948
```

- **`xargs kill`**

I have the PID. All I need is to pass it to `kill`. To do this, I will use
`xargs`.

`xargs kill` will read from the input (in our case from the pipe), form a command
consisting of `kill <items>` (`<items>` are whatever it read from the input),
and then execute the command created. In our case it will execute `kill 1948`.
Mission accomplished.

### Final words

Note that depending on what version of unix you're using, certain programs may
behave a little differently (for example, `ps` might output the PID in column
$3). If something seems wrong or different, read your vendor's documentation
(or better, the man pages). Also be careful as **long pipes can be dangerous**.

Don't make any assumptions especially when using commands like `kill` or `rm`.
For example, if there was another user named 'conky' (or 'Aconkyous') my command
may kill all his running processes too!

### Complement

Actually you can simplify the pipeline further to
```bash
pkill conky
```
or
```bash
kill $(pgrep conky)
```]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Background Sign Caution</title>
    <url>/2019/09/09/shell-background-sign-caution/</url>
    <content><![CDATA[
I want to wrap several commands into one line format to put in `command`/`args` field in yaml file for Kubernetes. But always get syntax error with bad `;`, for example: `-bash: syntax error near unexpected token ;'`, finally understand the error is from `&`.

Notice that here `&` should not have `;` after it:
```bash
(echo 123)&; sleep 5; echo 456
```

Instead the correct way is:
```bash
(echo 123)& sleep 5; echo 456
```
]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Source vs Execute Script</title>
    <url>/2019/05/07/shell-source-vs-execute/</url>
    <content><![CDATA[
Understand the difference between `source` and execute a script is important, otherwise you will be confused why something doesn't run as you expect.

# Source Script
The file may **not** necessary to be a executable (`chmod -x`) but should be valid shell script. We usually use `source` to load shell functions and export environment variables into current shell process.

For example, both syntax are good:
```bash
source ./xx.env
. ./xx.env
```
注意`source` 在当前上下文中执行脚本，不会生成新的进程！！执行完毕后回到当前进程。

> Note that `.` is not an alias for `source`, but rather the other way around. `source` is a bash extension, while `.` works in any POSIX compatible shell.

You can also put the file path in `$PATH` so that you don't need to specify the path in command.

# Execute Script
The file is **executable** (`chmod +x`) and you are in right permission to run it. And you need to specify the path even in current directory, or put path in `$PATH`:
```bash
./xx.sh
```
The current shell spawns a new shell to run the script. The script is running in the new shell and all changes to the environment only in the new shell. After the script is done all changes to the environment in the new shell are destroyed.

# Summary
Use execution method will run the script as another process, so variables and functions in child script will not be accessible in parent shell. 

The `source` method executes the child script in the parent script's process, the parent process can access the variabels or functions in child script. If you are using `exit` in script, it will exit the parent script as well. Which will not happen in execution method.

# exec Command
`exec` command, 这个命令在docker container的entry script中很常见:
- https://www.youtube.com/watch?v=nwm7rJG90i8
- https://askubuntu.com/questions/819910/what-are-possible-use-of-exec-command

此外还有`su-exec` 命令，但这个不是built-in的:
https://github.com/ncopa/su-exec
]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell Pattern Matching</title>
    <url>/2020/11/28/shell-pattern-match/</url>
    <content><![CDATA[

# Parameter expansion
这是在script 中处理string, number 数据的常用方法. 可以用来代替`sed`, `cut`这些external programs, speed up significantly. As our experience with scripting grows, the ability to effectively manipulate strings and numbers will prove extremely valuable.

对于变量值的检查(比如参数是否为dash 开头，否则当作argument使用)，提取(比如提取一个文件名去掉后缀)很有帮助，这个手册概括了所有情况，但可能不好理解，可以动手试一下就知道了, [Shell parameter expansion](https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html).  其实用pipeline 也可以达到相同的效果，但是会麻烦一些。

这是中文总结，还不错[Shell扩展(Shell Expansions)-参数扩展(Shell Parameter Expansion)](https://juejin.cn/post/6844903842966929422). 有个地方写错了 `$$` 才是当前shell 的PID。

注意`null` 和 `unset` variable的区别, `set -u` 可以检测报错使用没有定义的variable (也就是unset), `null` 在这里就是empty的意思，比如`var=`，这个变量是存在的，只是没有值:
```bash
var=
# 空
echo $var
# true
[[ -z $var ]]
# false
[[ -n $var ]]

set -u
unset var
# 报错
echo $var
```

Bash's various forms of parameter expansion can also distinguish between unset and null values: 
```bash
# `w` can be literal or another variable

# if a is unset, value of w is used
a=${a-w}
# if a is unset or null(empty), value of w is used
# for example, used for positional parameter passed from outside
a=${a:-w}

# if a is unset or null(empty), value of w is assigned to a
# 不能用于positional parameters的赋值, 比如 ${3:=hello}
${a:=w}

# if a is unset or null(empty), w is written to stderr
# and script exits with err
a=${a:?w}
```
Expansion that return variable names:
```bash
# return variable name starts with prefix
# these 2 are identical
${!prefix*} 
${!prefix@}
# return all BASH prefixed variables
echo ${!BASH*}
```

Indirect parameter expansion:
```bash
parameter="var"
var="hello"

# echo is hello
echo ${!parameter}
```

Substring expansion，来自上面的链接中 `Shell parameter expansion` 的例子:
```bash
$ string=01234567890abcdefgh
# 7 is start index
$ echo ${string:7}
7890abcdefgh
# 0 is number of char to cut
$ echo ${string:7:0}

$ echo ${string:7:2}
78
$ echo ${string:7:-2}
7890abcdef
# 要空格, 防止和:- 混淆
$ echo ${string: -7}
bcdefgh
$ echo ${string: -7:0}

$ echo ${string: -7:2}
bc
$ echo ${string: -7:-2}
bcdef

# set $1 positional parameter
$ set -- 01234567890abcdefgh
$ echo ${1:7}
7890abcdefgh
$ echo ${1:7:0}

$ echo ${1:7:2}
78
$ echo ${1:7:-2}
7890abcdef
$ echo ${1: -7}
bcdefgh
$ echo ${1: -7:0}

$ echo ${1: -7:2}
bc
# -2:  start from end
$ echo ${1: -7:-2}
bcdef

# array
$ array[0]=01234567890abcdefgh
$ echo ${array[0]:7}
7890abcdefgh
$ echo ${array[0]:7:0}

$ echo ${array[0]:7:2}
78
$ echo ${array[0]:7:-2}
7890abcdef
$ echo ${array[0]: -7}
bcdefgh
$ echo ${array[0]: -7:0}

$ echo ${array[0]: -7:2}
bc
$ echo ${array[0]: -7:-2}
bcdef
```

其他常见的用法，主要是针对string 操作，特别是pathname, much faster than `cut` extraction!!
```bash
# get string length
${#string}
# note, the number of positional parameters
${#@}

# 检查第一个位置参数是不是以-开头，扩展结果是删除最短的match部分
# 对 ${1} 的操作, for example ${1} is --verbose
# get: -verbose
${1#-} 
# 同上，但删除最长的match部分
# get: verbose
${1##-}

# get filename name from a download url
# \ is used to escape / in path
${1##*\/}
# get path of a url
${1%\/*}
```

This can be also used to do substring contains checking:
```bash
# empty the whole string if substring target is inside
[ ! -z "${1##*target*}" ]
```

其实`#` or `##` 后面可以使用[pattern matching](https://www.gnu.org/software/bash/manual/html_node/Pattern-Matching.html#Pattern-Matching), 这样功能更强, 比如:
```bash
${1#+(-)}
${1##+(-)}

# remove leading space or blank
# note that double [[]] wrapper!
shopt -s extglob
${1##*([[:blank:]]|[[:space:]])}
# remove trailing space or blank
${1%%*([[:blank:]]|[[:space:]])}

# remove .tar.gz or .tgz suffix
${1%%(.tgz|.tar.gz)}
```

[参数替换, search and replace](https://juejin.cn/post/6844903842966929422#heading-19) ${parameter/pattern/string}, replace pattern in parameter with string.
```bash
foo=JPG.JPG 
# replace first match
# jpg.JPG
echo ${foo/JPG/jpg} 
# replace all matches
# jpg.jpg
echo ${foo//JPG/jpg} 
# replace only start
# jpg.JPG
echo ${foo/#JPG/jpg} 
# replace only end
# JPG.jpg
echo ${foo/%JPG/jpg} 
```
[大小写变换](https://juejin.cn/post/6844903842966929422#heading-20)


# Shell Globs
A glob is a wildcard that is processed by the shell and expands into a list of arguments.
 
Glob is like regular expression but less expressive and eaiser to use. Glob match file names, for example `ls [0-9]?file*.txt`, whereas regular expression match text, for example `ls | grep '[0-9].file.*\.txt'`. Sometimes the funtionality can look blurred depending on how you use it. 都可以用在if condition [[ =~ ]], case condition中.

In `ls [0-9]?file*.txt`, `ls` does not support regular expression, shell expands the glob and used by `ls`.
`grep '^A.*\.txt' *.txt`, grep is using regular expression on the files context that file name is expanded by shell from glob.

Shell expansion types and execution order (precedence high to low from up to bottom):
1. brace expansion `touch file{1..2}`
2. tilde expansion `ls ~`
3. parameter and variable expansion `${1:1:1}, ${PATH}`
4. command substitution `$()` or ``
5. word splitting
6. arithmetic expansion `echo $((11 + 22))`
7. filename expansion `echo file{1..2}.*`
8. quote removal `echo "$USER"`

## Wildcards
```bash
ls *.txt
# ? is any one char
ls file?.txt
ls file??.txt
```

## Character Set
注意，在Linux中，根据`locale`的设置，这个regular expression 其实是不包含`a`的:
```bash
ls /usr/sbin/[A-Z]*
# 默认字典顺序 is actually in order
# aAbBcCdDeEfFgGhHiIjJkKlLmMnNoOpPqQrRsStTuUvVwWxXyYzZ
# 所以[A-Z] 不含a
```
解决办法是用POSIX Character class(见下一节), this standards introduced a concept called a `locale`, which could be adjusted to select the character set needed for a particular location. We can see the language setting of our system using the following command:
```bash
echo $LANG
# usually is
en_US.UTF-8
# 所以对于上面[A-Z]*的正确写法是
ls /usr/sbin/[[:upper:]]*
```

注意和brace expansion `{}` 区别，brace expansion是展开，character set是一种match:
```bash
# character set
# match one of them
ls [123abc]file.txt
ls file[0-9].txt
ls file[a-z9].txt

# 这个显示的结果和bash设置有关, 和上面提到的问题一样，不过这里更改了LC_COLLATE的值
# locale, In bash terminal, set LC_COLLATE=C (collation)
ls file[A-G].txt
# ! is inversion, not include a-z
ls file[!a-z].txt
# put at end to match !, it is a special char
ls file[a-z!].txt
ls file[az-].txt
```

## Character classes
```bash
# [:upper:] is the character class
# put char class in char set []
ls file[[:upper:]?].txt
ls file[[:lower:]?].txt
ls file[![:lower:][:space:]].txt
```
Others class useful:
```bash
# numbers
[:digit:]
# upper and lower case
[:alpha:]
# upper and lower and numbers
[:alnum:]
# upper
[:upper:]
# lower
[:lower:]
# space, tab, carriage return, newline, vertical tab, and form feed.
# is superset of [:blank:]
[:space:]
# space and tab characters
[:blank:]
```

## Shell globbing Options
使用`shopt` command的设置 glob的一些特性，比如设置nullglob, extglob, etc.

`shopt -s extglob`, when using extended pattern matching operators. see [here](https://www.gnu.org/software/bash/manual/html_node/Pattern-Matching.html#Pattern-Matching)
`shopt -s nocasematch`, set bash case-insensitive match in `case` or `[[ ]]` condition. 
这个是从bash tutorial 中文版中学到的, `shopt` is bash built-in setting, unlike `set` is from POSIX.


# Extended Globs
You need to open it:
```bash
shopt | grep extglob
shopt -s extglob
```

For example, create test cases:
```bash
touch file1.png photo.jpg photo photo.png file.png photo.png.jpg
rm -f file1.png photo.jpg photo photo.png file.png photo.png.jpg
```
```bash
# @(match): match one or others
# match photo.jpg
ls photo@(.jpg)
ls @(file)
# photo.jpg or photo.png
ls photo@(.jpg|.png)

# ?(match): match 0 or 1
ls photo?(.jpg|.png)

# +(match): match 1 or more
ls photo+(.jpg|.png)

# *(match): match 0 or more
ls photo*(.jpg|.png)

# !(match): invert match
# all files that do not have photo or file name and do not end with jpg or png
!(+(file|photo)*+(.jpg|.png))
```
主要用在command line, `if condition [[ =~ ]]`, case condition上，比regular expression matching更快。


# Brace Expansion
这个用在比如for loop的counter, create file pre/suffix.
```bash
# create file1.txt file2.txt file4.txt
touch file{1,2,4}.txt
touch file{1..1000}.txt

# expand from left to right, 两两组合
echo {a..c}{10..15}

# specify increase step
echo {1..100..2}
# can pad heading 0
echo {0001..10..2}
echo {10..0}

echo {a..z..2}

# can be nested
echo file-201{1..9}-{0{0..9},1{1..2}}-{1..30}.{tar,bak}.{tgz,bz2}
# create folder structure
mkdir -p 20{10..20}/{01-12}
```

For easy copy and rename file:
```bash
# match before and after ,
# file file.bkp
cp -f a/long/path/file{,.bkp}
```


# Regular Expression
注意regular expression 和 globs 的区别，regular expression 是match text的，globs是 shell来扩展的.
会使用到的地方:
- grep
- sed
- awk
- if [[ =~ ]]
- vim for search
- less for search
- find -regex
- locate -regex

[Regular Expression Info](https://www.regular-expressions.info/)
POSIX regular expression has basic regular expression(`BRE`) and extended regular expression(`ERE`).
ERE Syntax, note that ERE has `() {} ? + |` expression that BRE does not:
- `.` matches one char
- `[ ]` character set
- `\` escape single char
- `|` alternation: match to occur from among a set of expressions
- `( )` pattern grouping, for example, separate `|` with others: `^(AA|BB|CC)+`
- `? * + { }` repetition operators
- `^abc` leading anchor
- `abc$` trailing anchor
- `[^abc]` netates pattern, `^` must appear at beginning

Use ERE whenever possible!!!! The support in GNU tools are:
1. `grep -E` ERE, `grep [-G]` default is BRE
2. `sed -E` ERE, `sed` default BRE
3. `awk` only supports ERE
5. `[[ =~ ]]` ERE

```bash
# match one char with zero to 3 occurances
.{,3}
# match one char with 3 to 7 occurances
.{3,7}
# match one char with 3 to more occurances
.{3,}
```

## Backreferences
A pattern stored in a buffer to be recalled later, limit of nine for example: `\1` to `\9`:
```bash
# \1 is (ss) pattern
(ss).*\1
(ss).*\1.*\1

# radar
# opapo
^(.)(.).\2\1$
```
POSIX ERE does not support `backreferences`, GNU version supports it.


# Bash Extended Regexp
Used in `[[ =~ ]]` in if condition, it is simple to write then extended globs but less efficiency.

BASH_REMATCH: regular expression match, the matched text is placed into array `BASH_REMATCH`:
```bash
[[ abcdef =~ b.d ]]
# the matched is bcd in BASH_REMATCH[0]
echo ${BASH_REMATCH[0]}
# if no match BASH_REMATCH[0] is null
```
`${BASH_REMATCH[0]}`很有用，因为存了match的内容，如果是多个group `( )` pattern的组合，则每个group一次存放在`${BASH_REMATCH[n]}`, n is 0/1/2/3...。


# Grep EREs
`Grep` is global regular expression print. Stick to `grep -E 'xxxx'`.
`grep -E -w` only match a whole word.
`grep -E -x` only match whole line, same as using anchors.
`grep -E -o` only return the text that match the expression.
`grep -E -q` quiet mode, used to verfiy existence of search item, 用于以前没有`[[ =~ ]]`的时候.


# Sed EREs
See my Sed blog.


# Awk EREs
Only support ERE by default, see my awk dedicated blog.]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>regular expression</tag>
        <tag>parameter expansion</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell Script Template</title>
    <url>/2019/09/14/shell-script-template/</url>
    <content><![CDATA[
This is a good script template which returns JSON response, it has exit hook, logs and arguments checking. I will continuously update it if I find good patterns. 

Use `Python` script is more developer friendly and easy to maintain.

对于一般用途的script，有几点需要注意:
1. shebang 必须有 `#!/usr/bin/env bash` or `#!/usr/bin/env sh`
2. debug `set` 放开头， 如同这里一样
3. usage 函数格式要清晰，可以参考这个例子，且必须有`-h/--help` flag
4. 传入的argument 要检查个数，以及是否为空
5. 参数输入多多测试不同情况
6. 可能需要log，可用temp 以及 exit hook解决，log放 `/tmp` directory
7. 输出信息用[date time] prefix, 可以用LogMsg function wrap一下

Using `#` for comment is preferred, but you can comment multi-line by heredoc:
```bash
: <<'END_COMMENT'
...
END_COMMENT
```

```bash
#!/usr/bin/env bash
#########################################################################
# Please insert the update at top of the logs
# Date           Author           Description
#
#########################################################################
## syntax check only, dry run
#set -n
## add line number for -x
export PS4='$LINENO + '
## pipeline subcommand failure check
set -o pipefail
## e: command line failure check
## u: undefined variable check
## x: print command executed
set -eux

## find canonical file path
## or you can restrict run script in it's current directory
CUR_PATH=$(dirname $(readlink -f $0))
source $CUR_PATH/<env file>
JQ="$CUR_PATH/jq"

## create log file for this script
OUT_FILE=$(mktemp /tmp/log.XXXX)
#OUT_FILE=/tmp/log

########################### function #########################
function exitHook {
  rm -f $OUT_FILE
  rm -f ${OUT_FILE}.out
  rm -f ${OUT_FILE}.err
}
## clean job for log file, you can add other signals
trap exitHook EXIT

function usage() {
 echo "<Description>"
 echo
 echo "Usage:"
 echo "  $0 [...]"
 echo
 echo "Flags:"
 echo "  -h, --help     help info"
}

## this is for return json format messages
function returnError() {
  msg="$1"
  echo "{\"result\": \"Failure\", \"message\": \"$msg\"}" |${JQ} '.'
  exit 1
}

function returnInfo() {
  msg="$1"
  echo "{\"result\": \"Success\", \"message\": \"$msg\"}" |${JQ} '.'
  exit 0
}

########################## main ##############################
## declare input parameters
P1=""
P2=""

## check script parameters
if [[ $# -eq 0 ]]; then
  msg="No command-line arguments were specified..."
  returnError "$msg"
fi

while [[ $# -gt 0 ]]
do
  case "$1" in
    -p1|--p11)
       shift
       P1=$1
       shift;;

    -p2|--p22)
       shift
       P2=$1
       shift;;

    *) msg="$0: Bad Usage..."
       returnError "$msg"
  esac
done

## double check or restrict parameters
if [[ "X${P1}" = "X" ]]; then
  msg="-p1 was not specified"
  returnError "$msg"
elif [[ "X${P2}" = "X" ]]; then
  msg="-p2 was not specified"
  returnError "$msg"
fi

## then do the job, when you run some commands in script 
## you can redirect the normal output to ${OUT_FILE}.out
## and error output to ${OUT_FILE}.err by:
command 1>>${OUT_FILE}.out 2>>${OUT_FILE}.err

if [[ $? = 0 ]]; then
    msg="Run command successfully..."
    returnInfo "$msg"
else
    ## parse err log file and return message
    msg=$(cat ${OUT_FILE}.err|sed -e 's/"/\\"/g')
    returnError "$msg"
fi
```
]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Daily Talk</title>
    <url>/2019/04/11/softskill-daily-talk/</url>
    <content><![CDATA[
> 04/30/2020 目前来看，感受是我的职业发展一大障碍并不是技术，而是soft skills，比如如何高效
直接又不失礼貌的**沟通** (not just what you say but how you say it!!!)，how to
email, phone call effectively，如何协作多个团队以及个人，以及管理一个团队(interpersonal
skill). PluralSight上这方面的课程很不错，也有一些BQ相关的书籍需要阅读。总之，这非常重要！

# Dialog
## Dail in
I can barely hear you.

(not hear clearly) Sorry, I miss some points. I didn't hear you.
Are your voice broken?

Who do we have on the call?
Do you want to go first?

This is from SVL, we have x1 x2 and myself, we are waiting for x3 to dail/dail/
in. only three of us here

go ahead ...
we are waiting for more folks to join.

xx is on her way.
Even I saw her.

## Meeting
The first option seems a **non-starter**, since we cannot get blank access to
all clouds.

Get started in a minute or 2, just give others the opportunity to join the call.

Why don't we get started? We have a bunch of material to cover and we need to
leave time for questions.
1. Introduce yourself
2. Articulate High level design and questions
* feel free to interrupt if you have any questions.
* let me know if you have questions so far.
* I will be happy to answer your questions.
3. If you can provided **code fragment**, resources
4. Chat normally
5. Bye

Do you **go by** xxx: is used when asking someone if they prefer to be called or
known by a particular name.

Something **came up at last minute**, sorry (to cancel the meeting)

He is **running late**, let's wait 5 more minutes
have something unplanned/ˌʌn'plænd/...

## Describe Problem
obscure problems **came up**.

some of the files must have setuid bit set in order to function properly.
Jaijeet can **take the topic**.
Kannu covered a lot we did last week.

(别再说 I don't know了😂)
it's unknown to me.
It's a complete black box to me.
I don't have answer to this question.
I don't have answer to all of these now.
I cannot make any promise now.
It's hard to have a binary answer yes or no.

I **ran into** the same issue.
I don't want to clutter/mess up the cluster...
Is there any workaround for this? Run something **without killing** my cluster? 

let us talk over on this on monday - seems convoluted (费解的).

not sure it's possible or not

So what happens **right now** ...
what happening is that ...
**How come** the output file is missing ..

Let me know if any issues.
**What is your opinion** Badal?

Elaborate the context, open the git issue if necessary. We need to understand
the problem exactly(without discrepancy).

I would say don't do that.
we do have this fix.
I will take a look at it today.

They didn't articulate what need to test.

I **haven't got time to stay with** Deep to discuss...
That's what I am discussing with you.
We don't have anything to **lose**.

we are still facing the issue.
Something is failing **intermittently**, hard to figure it out what is the
problem.

What am I supposed to conclude?

**I take what I said back.**
I am still struggleing myself for this issue.

Nothing is working, we need to engage UI team, API team.
I am able to open the defect (ticket/issue) but not able to assign.

**It is much easier said than done.**
It is a big list **but by no means** it is a exhausted list

Make sure that it **stays** a useful tool and doesn't become a maintenance
headache/nightmare.

check out(pull)/in(push) the code (github).

Other things are more or less similar.
There are a few notable differences from ...

go/read/step through ..
sth is populated from ... 什么被什么填充

analogy /əˈnælədʒi/ to sth. 类比什么
After updates, take the **appropriate** next steps.

This is an **evolving project**. As such就这点而论, be sure to **consult** its
GitHub repository to find the latest installation instructions

It is commonly understood as the **elapsed time** or **lag time** between a
request and its completion.

Let's give it some **soak/soʊk/ time**.
I don't get your question?

From the technical perspective,...
**As oppose to** running locally using docker compose, use kubernetes right
away so you can **spot** the problems early.

Sorry for confusion, ...

**If there is no task with high variety**, I can finish it by ....

we have some technical difficulties. 

## Progress
Ack, give me 1 hour please, in the middile of something.

I have some pending items in my plate.

It's a **overkill** for me.

I was primarily working on ...
I would suggest...

I did't make much progress yesterday.

I am going to **mimic** what you did ...
I will continue on the effort to ...

we need to do that **standalone** ...
I haven't started, it will not take too long ...

This is a **ad hoc task**. (Ad hoc tasks are work items that can be created
unprepared that are not initially part of a modeled process flow)

Anything else on your side?

That's my status.
That's all in my side.

(什么时候完成?)
When do you think you can put these in place?
When do you expect to have a draft for review?

targeting/ETA mondya EOD.
I am going to close it
push/slip it to next sprint

definitely (明确地)

This is our major block.
We plan to finish it by 8/28 or schedule **need to be extended**.
Please let me know ASAP as I have to report to ...

I am done, can I go/drop?
The things left for me are ...
it's not **urgent**.

I will let someone know and ask .. so we get **immediate attention**.
more or less the same/similar
Is this still something that needs attention? It is unclear from the comments.
Please advise.

Also DM(direct message) me the cluster info if this still needs attention.

**I need to go back and see all the notes.**
go over your notes

There may be some **potential issues** that we cannot see right now.
To be complete, let me mention ...

XX seems to be waiting on some inputs.
Do you think a discuss with XX will help?

I am not sure what do we need to proceed here.

Let's put debug details aside (放一边) and proceed with ...
For now we leave the .. outside until we figure it out.

Meanwhile(in parallel) 与此同时做什么

Not sure how far did you get with debugging?
Need some time, since I am not admin.

I am taking most of my time doing sth...
could you please support about the ... I cannot find ...

I am actively working on x,y and c and I don't have bandwith to do
I have taken over the ownership of ...
I am not fully aware of the process.

its structure makes absolutely no sense to a new user.

it **has been a while** since I checked it... 做了什么事有一段时间了...
= **it's been a while**

since then, ...
it is on the roadmap:  it is part of a planned strategy or schedule for
development or implementation.

## Office Talk
examine/outline sth.
iterating over time 如此往复..
sth is discernable/di'sə:nəbl/ in .. 有迹可循

It seems we have different readings/understanding on ....
I will sync up/ set with you later...
**We are on the same page now.**
Hopefully we can walk through the doc and fill in the  gaps and identify any
others.

I'm not familiar with this.. I'll have a look later today and will let you know
if I figure out how it works.

强硬催人:
Sorry for the push. When you have some time - please do check and let us know.
Please let know.
Any luck with xxx?

I just **stop by**.
can you come over?

Hi chengdol, can you come to my office for a sec?

My bad for not being clear, I was specifically asking about
Apologize for the long delay...

One quick question, ...

if you have bandwidth you can start to work on ....

Can you give me a quick overview about ...

Distracted by the conversations/oncalls ...
will find comfort room to dail in

Thanks, this is a big call.
we have a **hard stop** at 11:00 AM (for meeting)

you raise/give a bunch of information..

can I drop off（挂断）or I need to stay here?

Is the system back on line?
Getting system up today.

users are allowed to elevate their privileges to root.
we don't want to escalate this issue.
we will raise this question to XX team and get approval for not using it.

I **get lost** at this part.. I feel a little bit abstract on ...
please don't deviate my question.
I don't know whether .. will fit in.

Make sense?
He can leverage me when thing is not going

Nevermind I got what I needed (问了问题别人还没回答自己找到答案了)

who may know better on this.

(别人说了sorry什么的)
**That's OK.**

How can I get plugged/plʌɡ/ into the project/team/community?

Get educated ourselves
This is a necessity

Use the navigation/ˌnævɪˈɡeɪʃn/ on the left to ... 左边的菜单栏

Just in case the above link **vanishes** some day, I am **capturing** the
**main piece** of the solution below.

Sorry if it's **off topic**, ...

A rather unusual situation, ....

kubernetes and docker will **part ways** (分道扬镳)

out-of-hours, outside working hours, in-hours, work time. 描述工作，非工作时间.

## Issue
It might has slipped from my TODO

We will not ba able to immediately pick this up, I will add it to our backlog/
hotlist.

I wonder what the implications might be for us?

This issue doesn't appear to be related to xxx,  but an underlying xxx issue with networking..

We should `investigate` why the network performance is so bad.

This likely also explains some of the poor performance seen for the NFS mounts.

I'm going to remove my name from this one as the software appears to be
functioning as designed, but is limited by the underlying infrastructure. 

If additional information comes up and anything else you want me to look at let
me know. Per previous discussion, assigning this to you for now.

Not mission critical.

xxx incurs/introduces overhead.

Provide context and a shared language to describe ...

pinpoint the problem.

There is a high likelihood that ...

can easily carry out the tasks **by their own**.

Be on the lookout for user requests...

The **temptation** will be ..., but probably to ...

On the flip side, ...

One-off 一次性

Countering security breaches, reliable defenses.

I am not in the context...（没明白对方说的什么) 

Sorry I don't come from this background..

## Sick
I was "under the weather"(身体不适) yesterday and could not make it to our call.
Thanks for capturing this.

AFK for 1 hour.

have some personal errands/ˈerənd/

Departure for doctor appointment, will be off 4 hours

## Discussion/Analysis
We have different reading on ...let's have xxx to explain on it

let's put one step back...

Here is a suspect.

Looks like we **once again** run into the ... issue caused by ...

The theory didn't hold.

... is not designed in that way, I suggest to read to recap what is the purpose
of ... and how does it **interplay** with ...

IMO,IMHO ... The doc is based on incorrect assumption, specifically, see ...
Consequently, I don't believe ... is a most **plausible** alternative solving the
issue of ...


# Leaving
## For other people
was very nice working with you, all the best to your next stage!

People will miss you!

please carry on the good work!

whom can I reach to now, I mean your replacement.

We will be having farewell lunch for xx at xx on Monday, Febr 3rd. Let's meet
and wish him all the best in his new role and Thank him for all the GREAT work
he did for our team. Please RSVP by Thursday.

Thanks for all the work you did... I will surely miss your expertise.

## Myself
**Resignation letter to manager**
Dear XXX,
 
Please accept this message as notification that I am going to leaving my position and my last day will be July 24, two weeks from today.
 
I have enjoyed my time at XXX and will miss working with you and the team. I’m proud of the work we’ve done. Thank you for your professional guidance and support, I wish you and XXX the best success in the future. 
 
Please let me know what to expect as far as my final work schedule, accrued vacation leave, and my employee benefits.
 
I’m also happy to help assist in training my replacement during the transition and make it as smooth as possible.
 
Sincerely,

- https://www.thebalancecareers.com/resignation-email-message-example-and-tips-2063055
- https://www.indeed.com/career-advice/starting-new-job/how-to-write-a-resignation-letter
- https://howtowiki.net/resignation-email-to-manager/

I was previously working at XXX in San Jose as a software engineer, I’m particularly interested in Linux, Cloud Computing, especially the container orchestration. In my free time I enjoy hiking, playing piano, and writing on my blog.

**Resignation letter to peers**
Dear colleagues and friends, this Friday, Jan 24th will be my last day at xx, but this Is not a note to say good bye. This is a note to say thank you.
I am so thankful for my time here at xxx and for all of the wonderful people who have I have had the privilege to work with.
I have learned so much and been inspired by many of you over the years. I am so grateful for all the gifts the people of xx have given me and I can only hope in some way I have inspired some of you as well.
I take my leave from xx because of a unique opportunity to leverage all I have learned, and build some really cool xx solutions right here in my beloved Chicago. Leaving IBM and all the people I love here was the hard part, even with the exciting opportunity ahead of me.


# from Book
This book is a `broad overview of` “living” on the Linux command line
Another goal is to `acquaint` you with the Unix way of thinking
This book is divided into four parts

`ordinary tasks` that are commonly performed from the command line
working directory = current directory
You will thank yourself later
reveal more detail
examine text files (这个词要用起来)
here are some all-time favorites: ...

As we gain Linux experience，....
Without a proper understanding of expansion, the shell will always be a source of mystery and confusion.
... does the trick..

# How to go far
1. Early draft
2. Comments and discussion about content, presentation
3. synthesizing many ideas
4. respectful discussion and disagreement
5. lose the ego
6. learn to take criticism
7. learn from mistakes

# How to feedback
1. Show the intent(do you have a few minutes)
2. Show the data
3. Show the impact
4. Leave room for questions
4. Ask for feedback proactively (peers, manager, team)

# Effective No
Some "P0" tasks you must say yes, deponds on your priorities.
NACK, I am overloaded, overwhelmed, no bandwidth, occupied, overlapped.

This time does not work for me.
It is not clear whay you are asking.
I am not qualified to do that.
Here's the document to figure it out, can you try it for me?
 
]]></content>
      <categories>
        <category>Soft Skill</category>
      </categories>
      <tags>
        <tag>soft skill</tag>
        <tag>daily talk</tag>
      </tags>
  </entry>
  <entry>
    <title>Socket Quick Start</title>
    <url>/2020/09/13/socket-learn/</url>
    <content><![CDATA[
//TODO:
1. python `socket` module demos: https://pymotw.com/2/socket/uds.html


最近在做Proxy的工作，重新回顾和学习了很多相关的东西，这里把Socket的分类和概念梳理一下。
# Socket
https://en.wikipedia.org/wiki/Socket
- `Network socket`, an end-point in a bidirectional communication across a network or the Internet
- `Unix domain socket`, an end-point in local bidirectional inter-process communication
- `socket()`, a system call defined by the Berkeley sockets API

[Unix socket vs Network socket](https://serverfault.com/a/124518)

## Unix Domain Socket
https://en.wikipedia.org/wiki/Unix_domain_socket
A **Unix domain socket** or **IPC socket** (inter-process communication socket) is a data communications endpoint for exchanging data between processes executing on the same host operating system. 

> `IPC` 也是一种概念，有多种实现的方式。

The API for Unix domain sockets is similar to that of an Internet socket, but rather than using an underlying network protocol, all communication occurs entirely within the operating system kernel. Unix domain sockets may use the file system as their address name space. (Some operating systems, like Linux, offer additional namespaces.) Processes reference Unix domain sockets as file system inodes, so two processes can communicate by opening the same socket.

Valid socket types in the UNIX domain are:
- SOCK_STREAM (compare to TCP) – for a stream-oriented socket
- SOCK_DGRAM (compare to UDP) – for a datagram-oriented socket that preserves message boundaries (as on most UNIX implementations, UNIX domain datagram sockets are always reliable and don't reorder datagrams)
- SOCK_SEQPACKET (compare to SCTP) – for a sequenced-packet socket that is connection-oriented, preserves message boundaries, and delivers messages in the order that they were sent


## Network Socket
仔细读一下维基, network socket是指一种连接，这种连接有很多具体的实现方法。
https://en.wikipedia.org/wiki/Network_socket
Also referred to as **Internet socket**.

A network socket is a software structure within a network node of a computer network that serves as an endpoint for sending and receiving data across the network. The structure and properties of a socket are defined by an application programming interface (API) for the networking architecture. Sockets are created only during the lifetime of a process of an application running in the node.

`Socket Address` is comprised of:
- protocol type
- IP address
- port number

On Unix-like operating systems and Microsoft Windows, the command-line tools `netstat` or `ss` are used to list established sockets and related information.

Several **types** of Internet socket are available:
- Datagram sockets: connectless sockets, which use UDP.
- Stream sockets: connection-oriented sockets, which use TCP.
- Raw sockets: IP packet.


## Berkeley Sockets
https://en.wikipedia.org/wiki/Berkeley_sockets
Berkeley sockets is an application programming interface (API) for **Internet sockets** and **Unix domain sockets**, used for inter-process communication (IPC). 是以上几个socket的一种统一的抽象表示。

The term **POSIX sockets** is essentially synonymous with **Berkeley sockets**, but they are also known as **BSD sockets**.
]]></content>
      <categories>
        <category>Socket</category>
      </categories>
      <tags>
        <tag>socket</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell Process Demystify</title>
    <url>/2019/06/02/shell-subshell/</url>
    <content><![CDATA[
Understanding how `SHELL` works under the hood is a must to me, I have encountered several interesting and confusing issues in my daily work about `SHELL`. Let's dive deeply into shell process and its relationships to explore how subshells are created and the relationship between parent and child shell.

# Shell Type
Due to the `bash` shell's popularity, it's rare to use any other shell as a default shell.

The `default interactive shell` starts whenever a user logs into a virtual console terminal or starts a terminal emulator in the GUI. Another default shell, `/bin/sh`, is the `default system shell`. The `default system shell` is used for system shell scripts, such as those needed at startup.

In my Redhat and CentOS system, they are the same:
```bash
lrwxrwxrwx. 1 root root 4 Apr 13  2018 /bin/sh -> bash
```

To see the user default login shell, go to see `/etc/passwd`, for example:
```bash
fyre:x:1000:1000::/home/fyre:/bin/bash
demo:x:1001:1001::/home/demo:/bin/bash
```

# Shell Relationships
You can use `ps -f` to see difference before you run several times `bash`(child) in a shell(parent):
```bash
# run bash
# then run ps -f

UID        PID  PPID  C STIME TTY          TIME CMD
root      7762  7758  0 Jun03 pts/0    00:00:00 -bash
root     14957  7762  0 17:12 pts/0    00:00:00 bash
root     15028 14957  0 17:13 pts/0    00:00:00 ps -f
```
Here PID `14957` has parent `7762`.

A child shell is also called a **subshell**. A subshell can be created from a parent shell or from another subshell. For example, run `bash` 3 times:
```bash
# bash bash bash
ps --forest -f

UID        PID  PPID  C STIME TTY          TIME CMD
root      7762  7758  0 Jun03 pts/0    00:00:00 -bash
root      2264  7762  0 23:52 pts/0    00:00:00  \_ bash
root      2467  2264  0 23:55 pts/0    00:00:00      \_ bash
root      2487  2467  0 23:55 pts/0    00:00:00          \_ bash
root      2510  2487  0 23:55 pts/0    00:00:00              \_ ps --forest -f
```

# Constructs Create SubShell
Refer to this article [what is a subshell](https://unix.stackexchange.com/questions/442692/is-a-subshell)

> Note subshells are often used for multi-processing in shell scripts. However, entering into a subshell is an expensive method and can significantly slow down processing.

A subshell is typically implemented by `forking` a new process (but some shells may optimize this in some cases).

- Subshell for grouping: `(...)` does nothing but create a subshell and **wait** for it to terminate. Contrast with `{...}` which groups commands purely for syntactic purposes and does not create a subshell.
- Background `&`: creates a subshell and does not wait for it to terminate.
- Pipeline: `|` creates two subshells, one for the left-hand side and one for the right-hand side, and waits for both to terminate. The shell creates a pipe and connects the left-hand side's standard output to the write end of the pipe and the right-hand side's standard input to the read end. In some shells (ksh88, ksh93, zsh, bash with the lastpipe option set and effective), the right-hand side runs in the original shell, so the pipeline construct only creates one subshell.
- Command substitution: `$()` creates a subshell with its standard output set to a pipe, collects the output in the parent and expands to that output, minus its trailing newlines. (And the output may be further subject to splitting and globbing, but that's another story.)
- Process substitution: `<(cmd)` creates a subshell with its standard output set to a pipe and expands to the name of the pipe. The parent (or some other process) may open the pipe to communicate with the subshell. `>(cmd)` does the same but with the pipe on standard input.
- Coprocess: `coproc` creates a subshell and does not wait for it to terminate. The subshell's standard input and output are each set to a pipe with the parent being connected to the other end of each pipe.

## Process List
For a command list to be considered a `process list` (a grouping), the commands must be encased in parentheses `()`. Adding parentheses and turning the command list into a process list created a subshell to execute the commands.
```bash
# echo $BASH_SUBSHELL
0
# (echo $BASH_SUBSHELL)
1
# ( (echo $BASH_SUBSHELL) ) 
2
```

For parent variables act in subshell `()`, from [this](https://stackoverflow.com/questions/26079488/bash-subshell-mystery), [this](https://unix.stackexchange.com/questions/157957/why-is-a-variable-visible-in-a-subshell?noredirect=1&lq=1) and [this](https://unix.stackexchange.com/questions/138463/do-parentheses-really-put-the-command-in-a-subshell?noredirect=1&lq=1) posts, long story short: subshell `()` inherit all variables. Even `$$` (the PID of the original shell) is kept. The reason is that for a subshell, the shell just **forks** and doesn't execute a new shell (such as run a script `./xx`)

> Note, usually use subshell `()` with `&`.

## Background mode
Background mode is very handy. And it provides a method for creating useful subshells at the CLI.
```bash
# jobs -l
[1]+  7552 Running                 sleep 40 &
```
`[1]` is job number, `7552` is PID, then `Running` is job status. The `jobs` command displays **any** user's processes (jobs) currently running in background mode:

Using a `process list` in background mode is one creative method for using subshells at the CLI. Remember we start Jetty in conductor container? `docker load` and `scp` are also suitable for background execution sometimes.

## Co-processing
Co-processing performs almost identically to putting a command in background mode, **except** for the fact that it creates a subshell.
```bash
# coproc sleep 2
[1] 8174
[1]+  Done                    coproc COPROC sleep 2
```
it the same as:
```bash
# (sleep 2) &
```

The `COPROC` is a name given to the porcess, you can change it:
```
# coproc My_Job { sleep 10; }
```
> The only time you need to name a co-process is when you have multiple co-processes running, and you need to communicate with them all. Otherwise, just let the coproc command set the name to the default, `COPROC`.

This will create a nested subshell:
```
# coproc ( sleep 10; sleep 2 )
```

## My question
Remember in conductor container we start Jetty using the `(...) &`. We want to run it in a separate process in background. Why not just `&`? So If I want to run something in background, should I use `&` to put command in background or `()&` to put subshell in background?

Referring to my [question](https://unix.stackexchange.com/questions/523675/run-script-in-background-using-or). I am testing in these 2 cases but did not see difference:
```bash
sleep 1 & ps -f
(sleep 1)& ps -f
```
May be different Linux distro has different result, check it first. For now, using `&` directly on command is fine.

# Shell Build-in Commands
An external command, sometimes called a filesystem command, is a program that exists outside of the bash shell. They are not built into the shell program. An external command program is typically located in `/bin`, `/usr/bin`, `/sbin`, or `/usr/sbin`.
```bash
# which ps
/usr/bin/ps

# type -a ps
ps is /usr/bin/ps
```
Whenever an external command is executed, a child process is created. This action is termed `forking`. It takes time and effort to set up the new child process's environment. Thus, external commands can be a little expensive.

When using a built-in command, no forking is required. Therefore, built-in commands are less expensive.

Built-in commands are different in that they do not need a child process to execute. They were compiled into the shell and thus are part of the shell's toolkit.


]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Effective Email</title>
    <url>/2020/05/24/softskill-email/</url>
    <content><![CDATA[
# Email Structure
Understand the purpose of every email, for example:
- Educate or inform
- Make a request
- Introduction
- Respond

Before sending, review for purpose alignment. If not, adjust the message.

Key elements for good email:
- Subject line: keep email from getting deleted
- Introduction: create context, build trust, remind who you are
- Message: bulk of the email
- Call to action: request, the last part of the body
- Signature: provide contact information, your brand

Adjust the `From` name and email address, for example `chengdol <email address>`
, this can be done through `Send email as` in Gmail settings.

The `Signature` helps people understand your skill set, value proposition, may
include:
- your name
- tagline: directly under your name, a few words, can be your title.
- phone, address, links

# To, CC and BCC

* `To`: directly talk to a person.
* `CC`: who can hear the conversation, don't have to act on it.
* `BCC`: blind from others, only the person put you in BCC field knows you are
listening, no one else know you are on recipient list, fine to reply to sender
but not reply all.

# Communicating Better
Visuals may help, but should not make it distracting, `bold or highlighting` is
good to make response stand out.

When many people involved, use `Reply All`, unless you intend to start a side
conversation. Move to `Reply`, for example:

```
Thank again for the introduction, I'll move this conversation with xx to another
thread so we don't clutter your inbox.
```

# Vacation Responder
Set `out-of-office auto reply`, for example, at a conference, on leave. A good
example:

```
I’m currently consumed with a project that is taking almost all of my time. If
you are emailing me with a product or support question, please email Liz.
Otherwise, I will try to respond within 24 hours.
```

# Send Report 
This is usually for project progress report weekly to stakeholders. Draft them
in spreadsheet and copy to email.

There are 5 stages for a project (Not Started, In Progress, Complete):

* Concept Exit
* Design & Planning
* Execution/Implementation
* Preview
* GA

You can also point out weekly:

* Highlights
* Lowlights

The risk status of `milestone` for different teams or components:

* On Track(green): good, healthy
* At Risk(amber): signal caution
* Off Track(red): unlikely to be successful or correct

You can also add `Next Steps` and `ETA`, etc.

There is an example of the table(generated from https://tableconvert.com/markdown-generator),
for status, using color to highlights:

| Overall Risk Status: ON TRACK |           |                |          |                       |          |
|:-----------------------------:|:---------:|:--------------:|:--------:|:---------------------:|:--------:|
| Highlights:                   |           |                |          |                       |          |
| Lowlights:                    |           |                |          |                       |          |
| Workstreams                   | Exit Date | Weekly Updates | Owner(s) | Open Risks/Mitigation | Status   |
| ...                           | ...       | ...            | ...      | ...                   | ON TRACK |
| Key Decisions made this week: |           |                |          |                       |          |



# Schedule Send
This feature is helpful.

# Proofread
- purpose aligned
- body
- distraction
- call to action is clear
- spelling and grammar
- proper audience
- any attachment

When the stakes are high, and your email can have a major impact on the outcome,
it can pay to invest your time in proper proofreading, you can ask LLM to help
proofread.

# Demos

> You can also generate draft or template from bard or charGPT, with the input
from you for your purpose.  
## The First Communication
```ini
## subject
Reaching out from Twitter

## introduction
Hi Chris, I have been following you on Twitter for a while, and have interacted
a little with you over the last few weeks. I wanted to bring the conversation
over to email.

## message body

## call to action
Can we get on a call in the next week or so? I am open all day Thursday, just
let me know what works for you.

## signature
```

## Virtual Introduction
```ini
## subject
Virtual introduction: Matt and Jesse

## introduction
Hi Matt and Jesse, as per our previous conversations I wanted to introduce you
to one another.

## message
Matt, I’ve known Jesse for a few years and know him to be a very clever
developer, and a loyal friend. I know he can help you with some of the coding
challenges you are facing right now.

Jesse, Matt is my friend and colleague, and can better explain his challenges
than I can, but I think you are the right person for him to talk to.

## call to action
I hope you two can get together soon. I’ll let you take it from here.

## signature
```

## Information Heavy
```ini
Follow-up on job search information

Hi Laurie, you had asked for information to help you with your job search
Thursday morning when we spoke.

Below are a few of my favorite blog posts which I think are relevant to where
you are, based on our conversation. I am happy to talk about any of this with
you, over email or on a call. Just let me know what works best for you.

## some links here

I have been blogging for over 14 years, and have plenty to share, but I thought
these would be the most interesting and meaningful to you.

I would love to jump on a call this week to talk about your next steps. Are you
available for a call Friday before 2?

## signature
```

## Respond to Questions
```ini
Hi Jim, thank you for your thoughtful email. You have a lot of questions and
ideas in there. Please scroll down and see my comments in `yellow`.

## copy the original email and answer right after each question and highlight
## with yellow
```

## Negative Situation
```ini
Team performance

Mike, I promised you an email follow-up from our conversation this morning. I
know this is an uncomfortable conversation and I appreciate your willingness to
address this with me.

There are two issues we need to address. 
```

```ini
Project Foo meeting this morning

Hi Carlos, let me first apologize for how the meeting went this afternoon. I
could tell that you were uncomfortable. I wanted to share my perspective on what
was happening.

While I knew there was a chance your Project Foo was going to be killed, I was
not aware of the reasons stated in the meeting. I have seen what you and your
team have done with Project Foo and I have been very impressed.
```

## Seek Help
```ini
Hi xx,
Hope you are doing well!

We have started the work related to ... 
## body

At this point it is not clear for us how to proceed ...
Can you please provide guidance, or point us to someone who can assist?

Appreciate your help!
```



]]></content>
      <categories>
        <category>Soft Skill</category>
      </categories>
      <tags>
        <tag>soft skill</tag>
        <tag>email</tag>
      </tags>
  </entry>
  <entry>
    <title>Variables in Shell</title>
    <url>/2019/06/02/shell-variables/</url>
    <content><![CDATA[
This article talks about the `variable scope` in shell, especially how to access variables between different scripts.

# Local Variables
The `set` command displays **all** functions and local variables defined for a specific process:
```bash
# this definition will be in `set`, not in `env`
HELLO=999
# run `set` will see `HELLO`
set
```
It also sorts the display alphabetically.

注意`set`中的variables 不一定出现在`env`中，顾名思义, `env`中显示的才是当前运行shell会被采用的variables.

# ENV Variables
Environment variables are visible from the shell session and from **any** spawned child subshells, and also visable by the command launched from that shell. This makes environment variables useful in applications that create child subshells, which require parent shell information.

To view environment variables, use the `env` or the `printenv` command:
```bash
env | grep ^HOME
printenv HOME
```

You can use `export` to create environment variable.
```bash
export demo='hello world'
```
You can use `unset` to remove an existing environment variable.
```bash
unset demo
```

If you want to only set the variable for one command in one use, just prefix it, eg:
```bash
# EDITOR will only works for crontab command
EDITOR=vim crontab -e
# hot KUBECONFIG env variable
KUBECONFIG=xx.yml kubectl get nodes
```

A common trick for programmers is to include the `single dot` symbol in their `PATH` environment variable. The single dot symbol represents the current directory:
```bash
PATH=$PATH:.
```

# Declare Command
Note that `declare` can print both `set` and `env` variables:
```bash
declare | grep ^HOME=
```

`declare` command can be used to create local variables in `set` or export to `env`:
```bash
# -i: integer
declare -i n=34
# can do arithmetic operations directly without let or expr
n=n/2
# the result is 17
echo $n

# -l: convert to lower case
declare -l hello=WORLD
# -u: convert to upper case
declare -u hello=world
# if reassign value to hello, will be convert to lower/upper case automatically

hello=world
# -p: display the attributes and values of each name
declare -p hello
## result is below, -- means local variable with no options compare to others like -x -i, etc
declare -- hello="world"

## declare can also export variable
declare -x apple=123
## will see apple in env
env | grep apple

declare -p apple
## result is below, -x means export variable
declare -x apple="123"

## remove from env
declare +x apple
```

You can mix the option:
```bash
declare -x -l EDITOR
EDITOR=vIM

declare -p EDITOR
## auto fix it to lower case
declare -xl EDITOR="vim"
```
Other ways to convert char to lower/upper case:
```bash
val=abcdEFG
# pattern is optional, eg [A-F]
# all to lower cases
echo ${val,,pattern}
# only first char to lower case
echo ${val,pattern}
# all to upper cases
echo ${val^^pattern}
# only first char to upper case
echo ${val^pattern}
```

Create constant (read-only) variable, cannot be unset and sustain for the shell session:
```bash
## set name as readonly
declare -r name=bob
## export it and readonly
declare -xr name=alice
## define a readonly array
declare -ra array=(1 2 3 4)
```

Declare different kind of arrays, see my other blog [`Array in Script`](https://chengdol.github.io/2019/08/13/shell-array/):
```bash
## index-based
declare -a user_name
## associated-based
declare -A user_name
```

# Shell Variables
Shell variables (actually the local variable) are available **only** in the shell that creates them. In fact, the Linux system also defines standard shell environment variables for you by default. 

How to define a Shell environment variable:
**No** space can appear between the variable, the equal sign, and the value:
```bash
var1=value
```
Variables defined within the shell script maintain their values throughtout the life of the shell script but are vanished when the shell script completes.

If you want to assign the value of one variable to another, must use `$`:
```bash
var2=${var1}
```
otherwise, `var1` will be interpreted as text string.
```bash
var2=var1
```

# Variable Scope
My question is how to pass variables from one script to another?
Basically there are 3 options:

1. Make the variable an environment variable (`export` it) before calling the child script (`./script2`).

2. `source` the child script and it will run in the same shell, This would let you share more complex variables like arrays easily, but also means that the other script could modify variables in the caller shell.

3. pass as parameters to child script: `./script2 var1 var2`

4. if use `exec` to replace current process, still need export variable.


# Preserve ENV variables
Refer to [this](https://unix.stackexchange.com/questions/188144/why-is-a-variable-passed-to-the-su-command-but-not-an-array-from-the-same-scope) question. This is interesting and easy to make mistake, if I want to preserve global ENV variable when switch user in shell, use `su xxx` instead of `su - xxx`, `-` option will do:

1. clears all environment variables except for TERM
2. initializes the environment variables HOME, SHELL, USER, LOGNAME, PATH
3. changes to the target user's home directory
4. sets argv[0] of the shell to '-' in order to make the shell a login shell

But be very careful not let shell do variable expanding in command option:
```bash
su xxx -c "... echo $a"
```
Here `$a` will be expanded before executing, use single quote or escape the `$` sign:
```bash
su xxx -c '... echo $a'
su xxx -c "... echo \$a"
```

]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Google Form for Survery</title>
    <url>/2023/03/14/softskill-google-form/</url>
    <content><![CDATA[
It is important to collect data or feedback by surverying, Google Form is a
handy tool for questionnaire, you can create it through:

* Google drive `+ New`
* [forms.google.com](https://forms.google.com)

Some **highlights**:

* There are tons of template for reference.
* For widgets such as `Multiple choice` and `Dropdown`, you can set up logic to
go to specified sections based on answer selected.
* Preview mode to test form before release.
* Disable accepting response from Response section to close the survey when
necessary.
]]></content>
      <categories>
        <category>Soft Skill</category>
      </categories>
      <tags>
        <tag>soft skill</tag>
        <tag>google form</tag>
      </tags>
  </entry>
  <entry>
    <title>SSL/TLS Demystify</title>
    <url>/2019/11/29/ssl-tls-learn/</url>
    <content><![CDATA[
This is all about securing servers with `SSL/TLS certificates` from udemy course **SSL complete guide**.

The quality of SSL varies, you have SSL setup doesn't mean your site is good secured. The HTTPS may not work correctly, sub-optimal, you can test it here:
https://www.ssllabs.com/index.html

If you click the `lock` icon at left of the website address, it will show you if the connection is secured or not, it's certificates, cookies and so on. further click the certificate icon, you will see `root CA`, `intermediate CA` and `certificate`.

> Install wireshark on Mac, go to download the `stable` version dmg package and double click to install.

> You can use Chrome inspect -> Network to see traffic or use wireshark

For example, from `Network` select one item, check `HEADER` information you can get IP address of remote server, or just use `host`, `nslookup` commands to get IP address.

> Interesting, from `Network` I see the the chrome browser sometime uses IPV6 address talk to server, for example facebook and some other sites. see this [question](https://superuser.com/questions/1199129/how-web-browser-determines-when-to-use-ipv4-or-ipv6-to-connect-to-the-destinatio)


**大概总结一下:**
openssl 目前有command 一次性生成(或分开生成 private key -> CSR(certificate signing request) -> self-signed certificate) private key 和 (self-signed) certificate, 一般用pem 的格式。这2个东西是放在web server上的，也可以把private key 和 certificate 合并到一个pem 文件中。还要注意的是，这里没有用到public key，但public key可以从private key中生成(其实里面已经包含了public key的信息)。此外，web server给 client的certificate 就只是certificate，不会有private key尽管它们可能在一个pem file中。

在client部分，对于self-signed certificate, 需要设置操作系统trust it，但如果是用的let's encrypt（或其他well-known CA) 则很可能已经支持了。联想一下TLS handshake, 得到web server的certificate 后，会逐层验证到root CA(会下载所有相关的certificates), 由于client自身已经携带了well-known root CA的证书了，并且也知道CA的public key，所以就会知道这个root CA是否合法。如果合法，就会进行symmetric key的生成和交换，用于之后的数据传输。

具体命令操作可以参考: `<<Set up Secure Docker Registry Container>>`
如果要把这个过程自动化，比如自动给网站request安排证书，renew更新，revoke撤销等，需要用到一些automation，比如certbot, or cert-manager in K8s，见下面一章:

然后再来看看crt/key 文件和它们的内容, 一般来说会用到形如`tls.crt`, `tls.key`的文件，其中`tls.key` 一般就是private key的内容了, 比如:
```json
-----BEGIN RSA PRIVATE KEY-----
///xxxxx
-----END RSA PRIVATE KEY-----
```

`tls.crt` 就是certificate, 它是一种`PEM` formatted file, `PEM` means `Privacy Enhanced Mail` (concatenated certificate container files), can have different extension like: `tls.cert`, `tls.cer`, `tls.pem`, etc. `PEM` is a container format that may include just the public certificate, or may include an entire certificate chain including public key, private key, and root certificates. Confusingly, it may also encode a CSR, for example, here it contains 2 block certificates:
```json
// can also contain private key
-----BEGIN RSA PRIVATE KEY-----
(Your Private Key: your_domain_name.key)
-----END RSA PRIVATE KEY-----
// trust chain
// 一般顺序是从上到下: your domain cert -> intermediate cert -> root cert
-----BEGIN CERTIFICATE-----
(Your Primary SSL certificate: your_domain_name.crt)
-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
(Your Intermediate certificate: DigiCertCA.crt)
-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
(Your Root certificate: TrustedRoot.crt)
-----END CERTIFICATE-----
```

阅读以下这些文章:
[PEM, DER, CRT, and CER: X.509 Encodings and Conversions](https://www.ssl.com/guide/pem-der-crt-and-cer-x-509-encodings-and-conversions/)
[Creating a .pem File for SSL Certificate Installations](https://www.digicert.com/kb/ssl-support/pem-ssl-creation.htm)

Decode PEM encoded ssl/tls certificate to verify it contains correct information:
```bash
# 如果有多个certificate block in one file, 则解析的是第一个
openssl x509 -in <crt file> -text -noout
```

# Cert-manager
Used in K8s to secure for example ingress, search this post: `<<Cert-manager Light Note>>`.

# Certbot
Get free HTTPs certificates forever, it utilizes `Let's Encrypt` CA to automatically refresh the certificate for your web site:
https://letsencrypt.org/docs/
https://certbot.eff.org/

How it works in detail:
https://letsencrypt.org/how-it-works/
This is accomplished by running a certificate management agent on the web server.

# Encryption
`symmetric` encryption, the same key is used by both sides, for example: `AES`. This algorithm is embedded in SSL with HTTPS protocol.
`asymmetric` encryption, for example: `RSA`.

## Hash
How does hash work to verify data integrity:
```js
data + hash(data)  ---------> data + hash(data)
                                |         |
                                |--hash-->| (compare if they are the same)
```
Notice that in database the password is hashed, not plain text.

Hash algorithms: `MD5`, `SHA`...
- MD5: 128 bits, `echo 123 | md5`

for `SHA`, use `shasum` command in linux or use inline tool.
- SHA-1: 160 bits
- SHA-256: 256 bits
- SHA-512: 512 bits

```bash
## SHA-256
shasum -a 256 -t test.txt
```

- HMAC: can be used with md5 or sha. In cryptography, an HMAC (sometimes expanded as either keyed-hash message authentication code or hash-based message authentication code) is a specific type of message authentication code (MAC) involving a cryptographic hash function and a secret cryptographic `key`. It may be used to simultaneously verify `both` the data integrity and the authenticity of a message, as with any MAC. Any cryptographic hash function, such as SHA-256 or SHA-3, may be used in the calculation of an HMAC.

## Asymmetric Keys
`Encryption`
```js
data -------------> code =========>  code -------------> data  (owner side)
      public key                          private key
      encryption                          decryption
```
Usually(but not necessarily), the keys are `interchangeable`, in the sense that if key A encrypts a message, then B can decrypt it, and if key B encrypts a message, then key A can decrypt it. While common, this property is not essential to asymmetric encryption.


`Signature`
```js
            data                          |----------> hash value
             | (hash)     ==========>     |  compare      /|\
             |                            |                |
         private key encrypt              |           public key decrypte
            \|/                           | (hash)         |
data   +   encrypted hash              data    +   encrypted hash
```
Signing ensures the data is sent by the `owner of private key` and not has been modified inbetween.

What is the difference of digest and signature?
https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.2.0/com.ibm.mq.sec.doc/q009810_.htm
A message `digest` is a fixed size numeric representation of the contents of a message, computed by a hash function. A message digest can be encrypted by the sender's private key, forming a `digital signature`.

More readings:
[How does a public key verify a signature?](https://stackoverflow.com/questions/18257185/how-does-a-public-key-verify-a-signature)


## PKI
`public key infrastructure` is a set of roles, policies, hardware, software and procedures needed to create, manage, distribute, use, store and revoke digital certificates and manage public-key encryption. The purpose of a PKI is to facilitate the secure electronic transfer of information for a range of network activities such as e-commerce, internet banking and confidential email.

## Certificate
A file with some contents:
1. certificate owner
2. certificate issuer
3. signature (RSA created, made by issuer)
4. public key (from owner, we then use this public key to HTTPS)

`Self-signed certificate`: issued and signed by the owner.
The basic rule is we trust the CA (the issuer) so on the certificate owner.


## Why we need intermediary CAs?
There are not so much public root CAs because of problem of trust. Actually anybody can create own root CA but nobody will trust it. That's why there is limited set of global root CAs that are trusted worldwide by operating systems and browsers. You can view list of such global CAs with their root certificates in any browser or OS.

Such root CAs have certificates with long period of validity and their main responsibility is simple create "source of trust". That's why they don't issue certificates to end users to avoid additional work and minimize risk that their private keys will be compromised. Intermediate CAs certificates don't necessarily need to be in the list of trusted certificates in the OS or browser. They need simply be issued by trusted root CA.


# Chain of Trust
Let's see `openssl` command, generate RSA private key and public key:
这里没有谈到self-signed certificate,见后面
```bash
## check help for sub-command genrsa
openssl genrsa -h

## generate private.pem file private key with aes256 encryption method
## will ask you input pass phrase 
## 实际上private.pem 包含了public key 的信息了！
openssl genrsa -aes256 -out private.pem

## generate public key from above private key
## will ask you pass phrase from private key
openssl rsa -in private.pem -outform PEM -pubout -out public.pem
```

## Root CAs in OS
How does web browser trust the root CAs and certificates?
The OS ships a list of trusted certificates, in Mac search `Keychain Access`. 

In Linux, see this [link](https://blog.confirm.ch/adding-a-new-trusted-certificate-authority/): On Red Hat/Centos, It includes all trusted certificate authorities under `/etc/pki/ca-trust/extracted/openssl/ca-bundle.trust.crt`. Just add your new certificate authority file(s) to the directory `/etc/pki/ca-trust/source/anchors`, then run `/bin/update-ca-trust` to update the certificate authority file.

## Verify chain of trust
```yaml
           root CA         |     intermediate CA       |        end user
----------------------------------------------------------------------------------------
  self-singed certificate  |     signed by root CA     |   signed by intermediate CA
----------------------------------------------------------------------------------------
   signature: encrpted     |    signature: encrpted    |       signature: encrpted
 by private key of root CA | by private key of root CA |  by private key of intermediate
                           |                           |  CA
```

`CSR`: certificate signing request (the root CA receive CSR from intermediate CA, the signature of intermediate CA is signed by root CA, the root CA also provide issuer info for intermediate CA. Similarly, end user is signed by intermediate CA)

Web server (the end user) sends you its certificate and all intermediate certificates. Then on your side start the verification process from end user certificates back to top intermediate certificate then root certificate. 

这里如何验证的: To verify a certificate, a browser will obtain a sequence of certificates, each one having signed the next certificate in the sequence, connecting the signing CA's root to the server's certificate.

There is a online tool to check the certificates chain:
https://www.geocerts.com/ssl-checker.


# Create Self-signed Certificate
[Openssl Essential, several ways to generate certificates](https://www.digitalocean.com/community/tutorials/openssl-essentials-working-with-ssl-certificates-private-keys-and-csrs#generating-csrs)

Before creating certificate, you need `CSR`, and before `CSR`, you need first generate asymmetric keys. (Becuase certificate needs to include signature from upstream and also your public key)

> Choose common name (CN) according to the main domain where certificate will be used. (for example, in secure docker registry, CN is the registry address), actually CN is deprecated and Alternative Name(SAN) is used instead.

> What is `/etc/ssl/certs` directory? Actually this is a softlink to `/etc/pki/tls/certs`.

Generate self-signed certificate, see this [post](https://stackoverflow.com/questions/10175812/how-to-create-a-self-signed-certificate-with-openssl)
```bash
# this is CN certificate, if you want to have SAN(Subject Alternative Name), see below
openssl req \
        -newkey rsa:4096 -nodes -x509 -sha256\
        -keyout key.pem -out cert.pem -days 365 \
        -subj "/C=US/ST=CA/L=San Jose/O=Company Name/OU=Org/CN=<domain>"
```
* -nodes: short for No DES, if you don't want to protect your private key with a passphrase.
* Add `-subj '/CN=localhost'` to suppress questions about the contents of the certificate (replace localhost with your desired domain).
* For anyone else using this in automation, here's all of the common parameters for the subject: `-subj "/C=US/ST=CA/L=San Jose/O=Company Name/OU=Org/CN=<domain>"`
* Remember to use `-sha256` to generate SHA-256-based certificate.

> 注意有时遇到的cert 文件中可能包含多个CERTIFICATE 块，其中有intermediate CA. 在kubernetes 中构造tls secret的时候，直接使用即可。

To generate `SAN` certificate, see this [post](https://geekflare.com/san-ssl-certificate/). 主要是注意如何构造 san.cnf 文件给 openssl 使用. `SAN`主要是为了一证多用，注意这个和wildcard tls certificate不一样, wildcard tls certificate 主要是为了包容subdomains.

# SSL/TLS and HTTPS
Both are cryptographic protocols used in HTTPS:
- `SSL`: Secure Socket Layer
- `TLS`: Transport Layer Security. 

Difference between `SSL` vs `TLS`: `TLS` is an update and secure version of SSL.
https://www.globalsign.com/en/blog/ssl-vs-tls-difference/
It’s important to note that certificates are not dependent on protocols. Sometimes you hear `SSL/TLS certificate`, it may be more accurate to call them `Certificates for use with SSL and TLS`, since the protocols are determined by your server configuration, not the certificates themselves.

Go to [`ssllab`](https://www.ssllabs.com/ssltest/), you can check which version of TLS the web server use, input the web server address and scan, then click IP icon.

Why `RSA` is not used in data encryption?
1. too slow.
2. bi-directional data encryption requires RSA key pairs on both sides.
We encrypt data use symmetic key after setup secure connection.

> Why need rsa key pairs on both sides? Because the key is interchangeable, everyone has public key can decrypt data encrypted by private key!


## Establish TLS Session
You can see verbose from curl command
```bash
# -I: fetch header only
# -v: verbose
curl -Iv https://www.google.com
```
1. establish tcp session
2. establish tls session (negotiate protocol)
3. web server sends its **certifiate** (intermediate and others) to browser
4. browser generate symmetic key secured by public key from server and send to server. Or use [`Diffie–Hellman key exchange`](https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange).

Let's see wireshark for wikipedia connection:
The top 3 are TCP handshakes, then you see TLS client hello, then TLS server hello.
![](https://drive.google.com/uc?id=1BqEQwaHXUIBIJguBGifnGIc_AnHJtoBB)

In client hello, there are lots of information to negoitate with server, here you see some supported version of TLS, also cipher suites.
![](https://drive.google.com/uc?id=1eLDoipLHyJ72aXeUPv5ERcSLHlAYNU1N)

In server hello, you see the the server has selected one of cipher suites. The `TLS_ECDHE.._SHA256` means it uses Diffie–Hellman key exchange and sha256 as hash.
![](https://drive.google.com/uc?id=1lC9yZ5vy1T-geCDnJoN1aq--QZIe3eOm)

### Other Readings
这个网站上的资源可以好好看看:
https://www.cloudflare.com/learning/

-->> [How Does SSL Work? ](https://www.cloudflare.com/learning/ssl/how-does-ssl-work/)
The main use case for SSL/TLS is securing communications between a client and a server, but it can also secure email, VoIP, and other communications over unsecured networks.

-->> [TLS handshakes](https://www.cloudflare.com/learning/ssl/what-happens-in-a-tls-handshake/)
During a TLS handshake, the two communicating sides exchange messages to acknowledge each other, verify each other, establish the encryption algorithms they will use, and agree on session keys.

SSL handshakes are now called TLS handshakes, although the "SSL" name is still in wide use.

A TLS handshake also happens whenever any other communications use HTTPS, including API calls and DNS over HTTPS queries.

这里讲到了2种不同的TLS handshake过程，一个是public-private key参与，另一个是Diffie–Hellman handshake. 对于第一种方式，提到了client从server cerficiate extract public key:
https://stackoverflow.com/questions/17143606/how-to-save-public-key-from-a-certificate-in-pem-format

-->> [How does proxy handle TLS handshakes](https://security.stackexchange.com/questions/115762/how-is-it-possible-to-do-tls-through-proxy-without-anyone-noticing/115764) (还记得envoy吗?)
HTTPS knows how to tunnel the TLS handshake even through the proxy.
也就说TLS/SSL through proxy就是通过HTTP CONNECT tunnel去实现的, 特别是看一下第二个回答, comments:

So, the proxy is not MITM'ing the HTTPS connection, by replacing the server's certificate with its own - it's simply passing the HTTPS connection straight through between the client and the server. Is that right?

Normally, when HTTPS is done through a proxy, this is done with the CONNECT mechanism: the client talks to the proxy and asks it to provide a bidirectional tunnel for bytes with the target system. In that case, the certificate that the client sees is really from the server, not from the proxy. In that situation, the proxy is kept on the outside of the SSL/TLS session -- it can see that some SSL/TLS is taking place, but it has no access to the encryption keys.

## Diffie–Hellman
Diffie–Hellman uses one-way function, for example, mod operation.
![](https://drive.google.com/uc?id=1lXCys7VAEXJxZ-sytIWUVQGXgxxRTCTi)
See, here `a`, `b` are priviate keys on both side, `g`, `p` are public key. `A`, `B` are mod result, `K` is the final result that both side can get use to encrypt the data.

Elliptic-curve cryptography is used in Diffie–Hellman.


# Custom Domain
Purchase custom domain and use free hosting to setup our website.












]]></content>
      <categories>
        <category>SSL/TLS</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ssl</tag>
        <tag>tls</tag>
      </tags>
  </entry>
  <entry>
    <title>Design Document Layout</title>
    <url>/2020/10/24/softskill-design-doc/</url>
    <content><![CDATA[
## Reviewer Sheet
Right after the title, list stakeholders in sheet:
```
Username | Role      | Status   | Last Change
         | Reviewer  | Pending  | 
         | Approver  | Approved |
         |           | Waiting  |
```

## Document profile
* Author: xxx
* Contributors: xxx
* Intended audience: xxx
* Created: xxx
* Latest update date: xxx
* Visibility: public| confidential | need to know
* Status: draft | review | current | needs update | obsolete
* Self link: xxx
* Team: xxx
* Hotlist/ticket: xxxx

## Sections
```
# Context
  ## Objective
  ## Background

# Requirements
# Design/Proposal
  ## Preferred Approach
    chart: Pros | Cons | Effort
  ## Alternative Considered
    chart: Pros | Cons | Effort

# Compliance
# Implementation Plan
# Reference
```

## Diagram
Open source versatile tools to help with diagram:
* https://bramp.github.io/js-sequence-diagrams/
* https://github.com/mermaid-js/mermaid]]></content>
      <categories>
        <category>Soft Skill</category>
      </categories>
      <tags>
        <tag>soft skill</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell timeout Command</title>
    <url>/2019/09/04/shell-timeout/</url>
    <content><![CDATA[
`timeout` command used to run command with a time limit. One case is when you know exactly how long you want a process to run for. A common use-case is to have timeout control a logging or data-capture program so that the log files don’t relentlessly devour your hard drive space.

Another case is when you don’t know how long you want a process to run for, but you do know you don’t want it to run indefinitely. You might have a habit of setting processes running, minimizing the terminal window, and forgetting about them. For example:
```bash
timeout 12 ping 127.0.0.1
```
By default 12 is second unit, to use a time value measured in minutes, hours or days append an `m`, `h` or a `d`.

# Send Right Signal
When `timeout` wants to stop a program it sends the `SIGTERM` signal. This politely asks the program to terminate. Some programs may choose to ignore the `SIGTERM` signal. When that happens, we need to tell timeout to be a little more forceful.

Send `KILL` signal if program is still running:
```bash
# -s: signal
timeout -s SIGKILL 10 tcpdump -i eth0 host 172.18 and tcp 32050
timeout -s SIGKILL 10 tcpdump -w chengdol.pcap &> /dev/null &
```
We can use the `-s (signal)` option to tell timeout to send the `SIGKILL` signal.

Or give it a buffer:
```bash
# -k 20: kill with 20 tolerance
timeout -k 20 10 tcpdump -w chengdol.pcap &> /dev/null &
```
we use the `-k (kill after)` option. The `-k` option requires a time value as a parameter. If tcpdump is still running after 20 seconds, it means the `SIGTERM` was ignored and timeout should send in `SIGKILL` to finish the job.

# Retrieving the Program’s Exit Code
`timeout` provides its own exit code, The exit code is `124`. This is the value timeout uses to indicate the program was terminated using SIGTERM.

But we may not care about that. We are probably more interested in the exit code from the process that timeout is controlling.

If the execution of the program ends **before** timeout terminates it, timeout **can** pass the exit code from the program back to the shell. (no need `--preserve-status`)

we must use the `--preserve-status` option if the program timeout but we still want to get its status exit code! For example:
```bash
timeout --preserve-status 10 <program>
```

```bash
# Start Engine Conductor
LogMsg "Starting Engine Conductor Pod..."
(cd $BASE/Engine/engine-conductor; pwd; ./createConductorDepAndSvc.sh $NAME_SPACE $ENGINE_HOST)
check_return_code $?
conductor_pod=$(kubectl get pods -n $NAME_SPACE | grep -i -E $POD_STATES | grep -v NAME|awk '{print $1}'|grep $ENGINE_HOST)
# Check for Engine background processes to complete
# <() is process substitution and redirect the command output to cat
# 其实这里没必要用这种形式 cat <()
timeout --preserve-status 3.0m cat <(check_k8s_start_loop $conductor_pod 7 $NAME_SPACE)
check_timeout_return $? $conductor_pod
```]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Test Dive Deep</title>
    <url>/2023/10/13/test-dive-deep/</url>
    <content><![CDATA[
This is the abridged summary of team tech talk ;-)

# Type of Testing
Amount of tests from high to low:

* **Unit tests**: functions in your code, you use Mock/Fake for external
dependencies, focus on good coverage.

* **Functional tests**: isolated functionality of single component, whthin its
boundaries, focus on business logic, every path should have a test.

* **Integration tests**:** components are being actively developed are involved, focus
on different combinations, not all the functions.

* **E2E tests**: often the first "real" env being tested, inevitable for a product,
need to exercise main product features(happy path), for example: create, update,
delete a PC, focus on main use cases.

* **CUJ(critical user journey) tests**: usually manual process, like UAT(user
acceptance test), focus on common user cases, for example, not just
only create a PC but also deploy and run the workload.

CUJ can help to improve the user experience, while UAT can help to ensure that
the system meets the user's requirements.

Regression test(can be of anyone above) ensure new feature does not break
exsiting code.
]]></content>
      <categories>
        <category>Test</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title>WebSocket Quick Start</title>
    <url>/2020/09/13/websocket-learn/</url>
    <content><![CDATA[
# Demo
Git repo, create basic and secure websocket server and client via Python websockets module:
https://github.com/chengdol/websocket-demo


# Introduction
[Difference between socket and websocket?](https://stackoverflow.com/a/4973689/5282011)

如果你已经理解了HTTP, HTTP CONNECT method 以及Proxy的知识，那么维基百科的描述已经比较清楚了.
[WebSocket Wiki](https://en.wikipedia.org/wiki/WebSocket)
WebSocket is a computer communications **protocol**, providing **full-duplex** communication channels over a single TCP connection.

WebSocket is distinct from HTTP (**half-duplex**, clients issue request). Both protocols are located at layer 5 in the OSI model and depend on TCP at layer 4. Although they are different, RFC 6455 states that WebSocket "is designed to work over HTTP ports 443 and 80 as well as to support HTTP proxies and intermediaries," thus making it **compatible** with the HTTP protocol. To achieve compatibility, the WebSocket handshake uses the HTTP(1.1 version) Upgrade header to change from the HTTP protocol to the WebSocket protocol.

> Some proxies also support WebSocket, for example, Envoy, some may not.

WebSocket handshake `ws://` or `wss:// (WebSocket Secure)`:
```js
// client request
GET /chat HTTP/1.1
Host: server.example.com
Upgrade: websocket
Connection: Upgrade
Sec-WebSocket-Key: x3JJHMbDL1EzLkh9GBhXDw==
Sec-WebSocket-Protocol: chat, superchat
Sec-WebSocket-Version: 13
Origin: http://example.com

// server response
HTTP/1.1 101 Switching Protocols
Upgrade: websocket
Connection: Upgrade
Sec-WebSocket-Accept: HSmrc0sMlYUkAGmm5OPpG2HaGWk=
Sec-WebSocket-Protocol: chat
```

WebSockets use cases:
- chatting
- live feed
- multiplayer gaming
- showing client progress/logging

Pros:
- full-duplex (no polling)
- HTTP compatible
- firewall friendly

Cons:
- lots of proxies and transparent proxies don’t support it yet
- L7, LB challenging (timeouts)
- stateful, difficult to horizontal scale

Do you have to use Web Sockets ? (alternatives) 
It is important to note that WebSockets is not the only HTTP realtime based solution, there are other ways to achieve real time such as **eventsource**, and **long polling**. 


]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>websocket</tag>
      </tags>
  </entry>
  <entry>
    <title>Leadership Principle</title>
    <url>/2020/05/23/softskill-leadership/</url>
    <content><![CDATA[
今天决定把Soft Skill相关的总结单独分出来。除了日常工作对话，后续重要的技能包括email, phone, technical writing 以及 negotiation, management等.

关注并接触Leadership 是始于Amazon的面试准备，虽然我很烦在面试中考察这种形式化的东西，但我不得不说它非常重要，特别是在英文背景中我这方面欠缺挺大的，必须要系统性的总结一下。

# Leadership View
I think you have been doing a nice job.

figure out customer requirements
hold meetings
write agendas
status reports

Critical skill of being a leader:
- Communication
- Effective management skill
- Emotional intelligence (情商) and Empathy

Communicate in clear, credible and authentic way.
Use passion and confidence to enhance the message.(口音，表情，肢体语言)
Inspire, motivate others
Informs, persuades, guides, assures

when you be a leader in your new team:
- devote time and energy to establishing how you want your team to work
- first few weeks are critical
- get to know your team members
- showcase your values
- explain how you want the team to work
- set and clarify goals, walk the talk
- don't be afraid to over communicate

Don't:
- 不要认为没建立关系也能完成工作
- 不要假设成员理解你的工作模式和期望
- 不要担心在开始阶段过多的重复谈话(reiterate the stragety over and over)

What does leadership mean to you?
connection:
- focus on the person
- influence
- words

changes:
- vision
- action
- drive change

motivate:
- inspire motivation
- long-lasting motivation

Why they will follow you?
make them feel comfortable, configent and satisfied:
- trust
  - be open, fair and listen
  - admit mistake
  - be decisive
  - respect the opinions of others
- compassion
- stability
- hope

# Effective Leader
The more it becomes about people.
The less it becomes about your personal tech expertise.
The broader your domain becomes, making you even more removed.

Always be deciding(identify ambiguity, make tradeoff)
Always be leaving(make the team self-drive without you)
Always be scaling]]></content>
      <categories>
        <category>Soft Skill</category>
      </categories>
      <tags>
        <tag>soft skill</tag>
        <tag>leadership</tag>
      </tags>
  </entry>
  <entry>
    <title>VSC Developing inside a Container</title>
    <url>/2023/10/28/vscode-dev-container/</url>
    <content><![CDATA[
This is a convenient function to create isolated, self-contained environment
for development.

For example, I want to test my Go/Python app in different versions w/wo
external service dependencies. Previsouly I need to set up virtual env for
Python or managing multiple versions for Go. Now by taking the advantage of dev
container I can easily develop code inside the full-feature container(or docker
compose) with SVC editor even in parallel, the setup is as simple as below.

## Prerequisite
The dev container [overview](https://code.visualstudio.com/docs/devcontainers/containers).

* Docker installed at target machine(local or remote)
* VSC Dev Container extension installed

## Setup
### [How to Create a Dev Container](https://code.visualstudio.com/docs/devcontainers/create-dev-container#_automate-dev-container-creation)

1. create a `.devcontainer` folder in your project root directory.
2. create a `devcontainer.json` file inside `.devcontainer` folder and add
settings, for example:

```json
// For format details, see https://aka.ms/devcontainer.json. For config options, see the
// README at: https://github.com/devcontainers/templates/tree/main/src/go
{
  // A name for the dev container displayed in VSC left corner status bar
  "name": "Demo",
  // other pre-build image to use, it has better support and features.
  // https://github.com/devcontainers/images/tree/main/src
  "image": "golang:1.21.3-bullseye",

  // Dockerfile is most suitable for installing packages and tools independent of your workspace files.
  // https://code.visualstudio.com/docs/devcontainers/create-dev-container#_dockerfile
  // "build": { "dockerfile": "Dockerfile" },

  // Docker compose
  // https://code.visualstudio.com/docs/devcontainers/create-dev-container#_use-docker-compose

  // Features to add to the dev container. More info: https://containers.dev/features.
  // For example, you can install go, git here instead of using post create command
  // "features": {
  // 	"ghcr.io/devcontainers/features/common-utils:2": {},
  // 	"ghcr.io/devcontainers/features/sshd:1": {}
  // },

  // Configure tool-specific properties.
  "customizations": {
    // Configure properties specific to VS Code.
    "vscode": {
      "settings": {},
      "extensions": [
        "streetsidesoftware.code-spell-checker"
      ]
    }
  },

  // Use 'forwardPorts' to make a list of ports inside the container available locally.
  // "forwardPorts": [9000],

  // Use 'portsAttributes' to set default properties for specific forwarded ports. 
  // More info: https://containers.dev/implementors/json_reference/#port-attributes
  // "portsAttributes": {
  // 	"9000": {
  // 		"label": "Hello Remote World",
  // 		"onAutoForward": "notify"
  // 	}
  // },

  // Use 'postCreateCommand' to run commands after the container is created.
  // you can have a utility script in your workspace folder to indicate what
  // to install here, but a Dockerfile may be more efficient.
  // "postCreateCommand": "bash -i scripts/install-dependencies.sh",

  // Uncomment to connect as root instead. More info: https://aka.ms/dev-containers-non-root.
  "remoteUser": "root"
}
```

3. start dev container:
```
Dev Containers: Reopen in Container
```

4. if any change is made in `.devcontainer.json`, rebuild it:
```
Dev Containers: Rebuild Containers
```

### [How to Attach to a Running Container](https://code.visualstudio.com/docs/devcontainers/attach-container)
Visual Studio Code can create and start containers for you but that may not
match your workflow and you may prefer to "attach" VS Code to an already running
Docker container - regardless of how it was started.

Once attached, you can install extensions, edit, and debug like you can when you
open a folder in a container using `devcontainer.json`.

This is useful to modify and test a running docker container on the fly without
codebase on your local.

1. attach VSC to a running container
```
Dev Containers: Attach to Running Container
```

If you need to repeatedly attach to a running container, you can set up 
[configuration](https://code.visualstudio.com/docs/devcontainers/attach-container#_attached-container-configuration-files)
in image or container-name level, so next time the attach will start in the right
step and place.

## Templates
There are a bunch of exisitng dev container templates to pick, please check:
```
Dev Containers: Add Dev Container Configuration Files
```

For example, [Go & PostgreSQL](https://github.com/devcontainers/templates/tree/main/src/go-postgres)
Docker compose dev container setup.

## Other Use Cases
1.[Open a folder on a remote SSH host in a container](https://code.visualstudio.com/docs/devcontainers/containers#_open-a-folder-on-a-remote-ssh-host-in-a-container)

* install Docker on remote host.
* ssh to the remote host and open the target folder by VSC.
* create and set up the dev container config file.
* start the remote dev container from VSC.

2.[Open a folder on a remote Tunnel host in a container](https://code.visualstudio.com/docs/devcontainers/containers#_open-a-folder-on-a-remote-tunnel-host-in-a-container)

3.[Attach to a container in a Kubernetes cluster](https://code.visualstudio.com/docs/devcontainers/attach-container#_attach-to-a-container-in-a-kubernetes-cluster)




]]></content>
      <categories>
        <category>Visual Studio Code</category>
      </categories>
      <tags>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title>Visual Studio Code Setup</title>
    <url>/2020/05/28/vscode-setup/</url>
    <content><![CDATA[
# Example User Setting.json
Not a exhausted list, but useful:
```json
{
  // The VSC font size.
  "editor.fontSize": 15,
  // 2 spaces when indent
  "editor.tabSize": 2,
  // Column ruler and color.
  "editor.rulers": [80],
  // Stay the same with iTerm2 ohmyzsh font.
  "terminal.integrated.fontFamily": "MesloLGS NF",
  // This can be customized as needed.
  "python.defaultInterpreterPath": "/usr/bin/python3",
  "files.autoSave": "afterDelay",
  "workbench.iconTheme": "material-icon-theme",
  "workbench.colorCustomizations": {
      "editorRuler.foreground": "#5c7858"
  },
  // After installing go plugin
  "go.useLanguageServer": true,
  "go.toolsManagement.autoUpdate": true,
  // To make VSC compile code correctly with file that has below build tags
  // otherwise VSC cannot find them, for example, the struct in Mock file.
  "go.buildTags": "integration test mock lasting",
  // For Markdown Preview Github Styling plugin.
  "markdown-preview-github-styles.colorTheme": "light",
  // For Markdown Preview Mermaid Support plugin.
  "markdown-mermaid.darkModeTheme": "neutral"
}
```

The pictures referred in blog are still in google drive, linked by
`![](https://drive.google.com/uc?id=xxx)`

# Recommended Extensions
The following VSC plugins are highly recommended.
- `Material Icon Theme`, better view for icons

- `Remote Development`, it includes:
  - Remote - SSH
  - Remote - SSH: Editing Configuration Files
  - WSL

- `Docker`

- `Python`
- `Pylint`, you need to customize the configuration
https://code.visualstudio.com/docs/python/linting
https://dev.to/eegli/quick-guide-to-python-formatting-in-vs-code-2040

- `Go`: must have for go development
- `YAML`
- `HashiCorp Terrform`
- `Ansible`

- `gRPC Clicker`, grpcurl under the hood
- `vscode-proto3`, proto syntax highlight
- `Thunder Client`, the VSC postman

- `TODO Highlight`
- `Better Comments`
- `Indenticator`, highlights indent depth

- `GitLens`, code blame, heatmaps and authorship, etc.

- `Markdown Preview Github Styling`, rendering markdown file as Github style
- `Markdown Preview Mermaid Support`, draw sequence diagrams, see
[Example](https://github.com/mermaid-js/mermaid) and [Live Editor](https://mermaid.live/edit#pako:eNpVkE1rwzAMhv-K0WmDtlmSLW1zGKwfsMNg0PXW5ODaSmJI7ODI60qS_z5nZbDpZD3PayG7B2EkQgpFbS6i4pbY2yHTzNfL6ZVbmbP5_Hk44hcNbHN3ME7L-5vfTIZt-x0K1Smjxxve_lx41ziw3emAnauJhflfd7yYge1_XZTDDBq0DVfS79FPyQyowgYzSP1RYsF9MINMjz7KHZmPqxaQknU4A9dKTrhTvLS8-Q_3UpGxkBa87jysDZfo2x7o2k5vLlVHfqIwulDlxJ2tPa6I2i4NgkkvSkWVOy-EaYJOyemDqs91EiRRsuJRjMky5k9xLMU5XK-K6DEs5PIhjDiM4_gNmllwJg)

> NOTE: The Mermaid plugin here is for VSCode, not for Hexo deployment, to
enable Mermaid in Hexo rendering, please check Hexo setup blog.

If you are working remotely by SSH, installed these plugins on remote as well on
demands.

I have encountered a issue that the `go` extension does not work properly, for
example the source code back tracing is not working, the troubleshooting please
see [here](https://github.com/golang/vscode-go/blob/master/docs/troubleshooting.md).
Most likely you need to run `go vendor` to download the dependencies locally.]]></content>
      <categories>
        <category>Visual Studio Code</category>
      </categories>
      <tags>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title>Ansible Up and Running, 2nd Edition</title>
    <url>/2019/03/13/book-ansible-up-and-running/</url>
    <content><![CDATA[
这本书其实挺不错的，我还总结了在工作中ansible的常用模块在另一个blog里面。

Could we remove major architectural components from the IT automation stack? Eliminating management daemons and relying instead on OpenSSH meant the system could start managing a computer fleet immediately, without having to set up anything on the managed machines.

the "Making Ansible Go Even Faster" chapter now covers asynchronous tasks, and the "Debugging Ansible Playbooks" chapter now covers the debugger that was introduced in version 2.1.

we are all slowly turning into system engineers.

# Chapter 1. Introduction
When we talk about configuration management, we are typically talking about writing some kind of state description for our servers, and then using a tool to enforce that the servers are, indeed, in that state: the right packages are installed, configuration files contain the expected values and have the expected permissions, the right services are running, and so on.

Ansible is a great tool for deployment as well as configuration management. Using a single tool for both configuration management and deployment makes life simpler for the folks responsible for operations.

Some people talk about the need for *orchestration* of deployment. This is where multiple remote servers are involved, and things have to happen in a specific order.

## How Ansible works
In Ansible, a script is called a playbook. A playbook describes which hosts (what Ansible calls remote servers) to configure, and an ordered list of tasks to perform on those hosts.

Ansible will make SSH connections in parallel to web1, web2, and web3. It will execute the first task on the list on all three hosts simultaneously

To manage a remote server with Ansible, the server needs to have SSH and Python 2.5 or later installed, or Python 2.4 with the Python `simplejsonlibrary` installed. There’s no need to preinstall an agent or anyother software on the host.

The control machine (the one that you use to control remote machines) needs to have Python 2.6 or later installed.

Ansible is **push based**, and has been used successfully in production with thousands of nodes, and has excellent support for environments where servers are dynamically added and removed.

Ansible modules are declarative; you use them to describe the state you want the server to be in. Modules are also idempotent.

Ansible has excellent support for templating, as well as defining variables at different scopes. Anybody who thinks Ansible is equivalent to working with shell scripts has never had to maintain a nontrivial program written in shell. I’ll always choose Ansible over shell scripts for config management tasks if given a choice.

To be productive with Ansible, you need to be familiar with basic Linux system administration tasks. Ansible makes it easy to automate your tasks, but it’s not the kind of tool that “automagically” does things that you otherwise wouldn’t know how to do.

Ansible uses the YAML file format and the Jinja2 templating languages, so you’ll need to learn some YAML and Jinja2 to use Ansible, but both technologies are easy to pick up.

If you prefer not to spend the money on a public cloud, I recommend you install Vagrant on your machine. Vagrant is an excellent open source tool for managing virtual machines. You can use Vagrant to boot a Linux virtual machine inside your laptop, and you can use that as a test server.

Vagrant needs the VirtualBox virtualizer to be installed on your machine. Download [VirtualBox](http://www.virtualbox.org/) and then download [Vagrant](http://www.vagrantup.com/).

```
mkdir playbooks
cd playbooks
vagrant init ubuntu/trusty64
vagrant up
```
The first time you use vagrant up, it will download the virtual machine image file, which might take a while, depending on your internet connection.

You should be able to SSH into your new Ubuntu 14.04 virtual machine by running the following:
```
vagrant ssh
```

This approach lets us interact with the shell, but Ansible needs to connect to the virtual machine by using the regular SSH client, not the vagrant ssh command.

Tell Vagrant to output the SSH connection details by typing the following:
```
vagrant ssh-config
```
```
ssh vagrant@127.0.0.1 -p 2222 -i /Users/lorin/dev/ansiblebook/ch01/
playbooks/.vagrant/machines/default/virtualbox/private_key
```

```
testserver ansible_host=127.0.0.1 ansible_port=2222 ansible_user=vagrant ansible_private_key_file=.vagrant/machines/default/virtualbox/private_key

```
```
~/.ansible.cfg
```
Ansible supports the ssh-agent program, so you don’t need to explicitly specify SSH key files in your inventory files. See [“SSH Agent”](https://learning.oreilly.com/library/view/Ansible:+Up+and+Running,+2nd+Edition/9781491979792/app01.html#SSH_AGENT) for more details if you haven’t used ssh-agent before

If Ansible did not succeed, add the `-vvvv` flag to see more details about the error:
```
ansible testserver -i hosts -m ping -vvvv
```

## Simplify by *ansible.cfg* file
we’ll use one such mechanism, the `ansible.cfg` file, to set some defaults so we don’t need to type as much.

Ansible looks for an `ansible.cfg` file in the following places, in this order:

* File specified by the ANSIBLE_CONFIG environment variable
* ./ansible.cfg (ansible.cfg in the current directory)
* ~/.ansible.cfg (.ansible.cfg in your home directory)
* /etc/ansible/ansible.cfg

I typically put *ansible.cfg* in the current directory, alongside my playbooks. That way, I can check it into the same version-control repository that my playbooks are in.

```ini
[defaults]
inventory = hosts
remote_user = vagrant
private_key_file = .vagrant/machines/default/virtualbox/private_key
host_key_checking = False
```
Disables SSH host-key checking. Otherwise, we need to edit our *~/.ssh/known_hosts* file every time we destroy and re-create a nodes.

Ansible uses `/etc/ansible/hosts` as the default location for the inventory file. However, I never use this because I like to keep my inventory files version-controlled alongside my playbooks.

The `command` module is so commonly used that it’s the default module, so we can omit it
```bash
ansible testserver -a uptime
## spaces in command use quotes
ansible testserver -a "tail /var/log/dmesg"
## -b becomes root user
ansible testserver -b -a "tail /var/log/syslog"
```

# Chapter 2. Playbooks: A Beginning
Most of your time in Ansible will be spent writing *playbooks*. A playbook is the term that Ansible uses for a configuration management script.

vargant virtual machine ports mapping in *vagrantfile*:
```ruby
VAGRANTFILE_API_VERSION = "2"

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  config.vm.box = "ubuntu/trusty64"
  config.vm.network "forwarded_port", guest: 80, host: 8080
  config.vm.network "forwarded_port", guest: 443, host: 8443
end
```
```
vagrant reload

==> default: Forwarding ports...
    default: 80 => 8080 (adapter 1)
    default: 443 => 8443 (adapter 1)
    default: 22 => 2222 (adapter 1)
```

## TLS VERSUS SSL
You might be familiar with the term *SSL* rather than *TLS*(Transport Layer Security) in the context of secure web servers. SSL is an older protocol that was used to secure communications between browsers and web servers, and it has been superseded by a newer protocol named TLS. Although many continue to use the term *SSL* to refer to the current secure protocol, in this book, I use the more accurate *TLS*.

## WHY DO YOU USE TRUE IN ONE PLACE AND YES IN ANOTHER?
Strictly speaking, module arguments (for example, update_cache=yes) are treated differently from values elsewhere in playbooks (for example, sudo: True). Values elsewhere are handled by the YAML parser and so use the YAML conventions of truthiness:

YAML truthy
> true, True, TRUE, yes, Yes, YES, on, On, ON, y, Y

YAML falsey
> false, False, FALSE, no, No, NO, off, Off, OFF, n, N

Module arguments are passed as strings and use Ansible’s internal conventions:

module arg truthy
> yes, on, 1, true

module arg falsey
> no, off, 0, false

I tend to follow the examples in the official Ansible documentation. These typically use yes and no when passing arguments to modules (since that’s consistent with the module documentation), and True and False elsewhere in playbooks.

## NOTE
An Ansible convention is to keep files in a subdirectory named *files*, and Jinja2 templates in a subdirectory named *templates*. I follow this convention throughout the book.

For `.j2` file, when Ansible renders this template, it will replace this variable with the real value.

Inventory files are in the `.ini` file format.

## [COWSAY](https://michaelheap.com/cowsay-and-ansible/)

If you have the *cowsay* program installed on your local machine, Ansible output will look like this instead:

you can download *cowsay* rpm and install it:
```
cowsay -l

Cow files in /usr/share/cowsay:
beavis.zen blowfish bong bud-frogs bunny cheese cower default dragon
dragon-and-cow elephant elephant-in-snake eyes flaming-sheep ghostbusters
head-in hellokitty kiss kitty koala kosh luke-koala mech-and-cow meow milk
moofasa moose mutilated ren satanic sheep skeleton small sodomized
stegosaurus stimpy supermilker surgery telebears three-eyes turkey turtle
tux udder vader vader-koala www
```
set what animal you like:
```
export ANSIBLE_COW_SELECTION=tux
```
enable cowsay:
```
export ANSIBLE_NOCOWS=0
```
then if you run playbook:
```_______________________________________
< PLAY [Configure webserver with nginx] >
 ---------------------------------------
        \   ^__^
         \  (oo)\_______
            (__)\       )\/\
                ||----w |
                ||     ||
```
If you don’t want to see the cows, you can disable cowsay by setting the `ANSIBLE_NOCOWS` environment variable like this:

```
export ANSIBLE_NOCOWS=1
```

You can also disable cowsay by adding the following to your *ansible.cfg* file:
```
[defaults]
nocows = 1
```

## TIP
If your playbook file is marked as executable and starts with a line that looks like this
```
#!/usr/bin/env ansible-playbook
```
then you can execute it by invoking it directly, like this:
```
./playbook-file.yml
```

## YAML syntax
### Start of file
```
---
```
However, if you forget to put those three dashes at the top of your playbook files, Ansible won’t complain.

### String
In general, YAML strings don’t have to be quoted, although you can quote them if you prefer. Even if there are spaces, you don’t need to quote them.

In some scenarios in Ansible, you will need to quote strings. These typically involve the use of {{ braces }} for variable substitution. 

### Boolean
YAML has a native Boolean type, and provides you with a wide variety of strings that can be interpreted as true or false.

### List
```
- My Fair Lady
- Oklahoma
- The Pirates of Penzance
```
### Dictionary
```
address: 742 Evergreen Terrace
city: Springfield
state: North Takoma
```
### Line folding
When writing playbooks, you’ll often encounter situations where you’re passing many arguments to a module. For aesthetics, you might want to break this up across multiple lines in your file, but you want Ansible to treat the string as if it were a single line.

You can do this with YAML by using line folding with the greater than (`>`) character. The YAML parser will replace line breaks with spaces. For example:
```
address: >
    Department of Computer Science,
    A.V. Williams Building,
    University of Maryland
city: College Park
state: Maryland
```
## [Anatomy of a Playbook](https://learning.oreilly.com/library/view/ansible-up-and/9781491979792/ch02.html#playbooks_a_beginning)
A playbook is a list of plays:
```
---
- name: Configure webserver with nginx
  hosts: webservers
  become: True
  tasks:
  - name: install nginx
      apt: name=nginx update_cache=yes

  - name: copy nginx config file
    copy: src=files/nginx.conf dest=/etc/nginx/sites-available/default

  - name: enable configuration
    file: >
      dest=/etc/nginx/sites-enabled/default
      src=/etc/nginx/sites-available/default
      state=link

  - name: copy index.html
    template: src=templates/index.html.j2  dest=/usr/share/nginx/html/index.html mode=0644

  - name: restart nginx
    service: name=nginx state=restarted
```
you’ll see in [Chapter 16](https://learning.oreilly.com/library/view/ansible-up-and/9781491979792/ch16.html#DEBUGGING), you can use the `--start-at-task <task name>` flag to tell `ansible-playbook` to start a playbook in the middle of a play, but you need to reference the task by name.

Every task must contain a key with the name of a module and a value with the arguments to that module. In the preceding example, the module name is `apt` and the arguments are `name=nginx update_cache=yes`.

The arguments are treated as a string, not as a dictionary. This means that if you want to break arguments into multiple lines, you need to use the YAML folding syntax, like this:
```
- name: install nginx
  apt: >
      name=nginx
      update_cache=yes
```
Ansible also supports a task syntax that will let you specify module arguments as a YAML dictionary, which is helpful when using modules that support complex arguments. For example:
```
- name: Install docker
  any_errors_fatal: true
  yum:
    name: docker-ce
    state: present
    enablerepo: docker-local
```
## Modules
Modules are scripts that come packaged with Ansible and perform some kind of action on a host.

commonly used modules:
* yum
* copy
* file
* service
* template

### VIEWING ANSIBLE MODULE DOCUMENTATION
Ansible ships with the `ansible-doc` command-line tool, which shows documentation about modules. Think of it as man pages for Ansible modules.
```
ansible-doc yum
```
Recall from the first chapter that Ansible executes a task on a host by generating a custom script based on the module name and arguments, and then copies this script to the host and runs it.

More than 200 modules ship with Ansible, and this number grows with every release. You can also find third-party Ansible modules out there, or write your own.

## Putting It All Together
To sum up, a playbook contains one or more plays. A play associates an unordered set of hosts with an ordered list of tasks. Each task is associated with exactly one module.
```yaml
+-------------+         +------------+          +-------------+
|             |     +--->            |      +--->             |
|  playbook   +--------->    play    +---------->    hosts    |
|             |     +--->            |      +--->             |
+-------------+         +------+-----+          +-------------+
                               |
                               |
                               |
                             +---+
                             | | |
                        +----v-v-v----+         +-------------+
                        |             |         |             |
                        |   task      +-------->+   module    |
                        |             |         |             |
                        +-------------+         +-------------+
```

## Did Anything Change? Tracking Host State
Ansible modules will first check to see whether the state of the host needs to be changed before taking any action. If the state of the host matches the arguments of the module, Ansible takes no action on the host and responds with a state of `ok`.

Ansible’s detection of state change can be used to trigger additional actions through the use of *handlers*. But, even without using handlers, it is still a useful form of feedback to see whether your hosts are changing state as the playbook runs.

## Variables and Handlers
```
- name: Configure webserver with nginx and tls
  hosts: webservers
  become: True
  vars:
    key_file: /etc/nginx/ssl/nginx.key
    cert_file: /etc/nginx/ssl/nginx.crt
    conf_file: /etc/nginx/sites-available/default
    server_name: localhost
  tasks:
    - name: Install nginx
      apt: name=nginx update_cache=yes cache_valid_time=3600

    - name: create directories for ssl certificates
      file: path=/etc/nginx/ssl state=directory

    - name: copy TLS key
      copy: src=files/nginx.key dest={{ key_file }} owner=root mode=0600
      notify: restart nginx

    - name: copy TLS certificate
      copy: src=files/nginx.crt dest={{ cert_file }}
      notify: restart nginx

    - name: copy nginx config file
      template: src=templates/nginx.conf.j2 dest={{ conf_file }}
      notify: restart nginx

    - name: enable configuration
      file: dest=/etc/nginx/sites-enabled/default src={{ conf_file }} state=link
      notify: restart nginx

    - name: copy index.html
      template: src=templates/index.html.j2 dest=/usr/share/nginx/html/index.html
             mode=0644

  handlers:
    - name: restart nginx
      service: name=nginx state=restarted
```

### Generating a TLS Certificate
In a production environment, you’d purchase your TLS certificate from a certificate authority, or use a free service such as [Let’s Encrypt](https://letsencrypt.org/), which Ansible supports via the letsencrypt module.

Here we use self-signed certificate generated free of charge:
```
mkdir files
openssl req -x509 -nodes -days 3650 -newkey rsa:2048 \
    -subj /CN=localhost \
    -keyout files/nginx.key -out files/nginx.crt
```
Any valid YAML can be used as the value of a variable. You can use lists and dictionaries in addition to strings and Booleans.

### Variables
Variables can be used in tasks, as well as in template files. You reference variables by using the `{{ braces }}` notation. Ansible replaces these braces with the value of the variable.

### WHEN QUOTING IS NECESSARY
bad:
```
- name: perform some task
  command: {{ myapp }} -a foo
```
good:
```
- name: perform some task
  command: "{{ myapp }} -a foo"
```
A similar problem arises if your argument contains a colon. For example, bad:
```
- name: show a debug message
  debug: msg="The debug module will print a message: neat, eh?"
```
good:
```
- name: show a debug message
  debug: "msg='The debug module will print a message: neat, eh?'"
```


## Generating the Template
We put templates in `templates` folder, we use the `.j2` extension to indicate that the file is a Jinja2 template. However, you can use a different extension if you like; Ansible doesn’t care.

You can use all of the Jinja2 features in your templates, you probably won’t need to use those advanced templating features, though. One Jinja2 feature you probably will use with Ansible is filters: [Jinja2 Template Designer Documentation](http://jinja.pocoo.org/docs/dev/templates/).

### Handlers
Handlers are one of the conditional forms that Ansible supports. A handler is similar to a task, but it runs only if it has been notified by a task. A task will fire the notification if Ansible recognizes that the task has changed the state of the system. A task notifies a handler by passing the handler’s name as the argument. 

### A FEW THINGS TO KEEP IN MIND ABOUT HANDLERS
Handlers usually run after all of the tasks are run at the end of the *play*. They run only *once*, even if they are notified multiple times. If a play contains multiple handlers, the handlers always run in the order that they are *defined* in the handlers section, not the notification order.

The official Ansible docs mention that the only common uses for handlers are for restarting services and for reboots. Personally, I’ve always used them only for restarting services.Even then, it’s a pretty small optimization, since we can always just unconditionally restart the service at the end of the playbook instead of notifying it on change, and restarting a service doesn’t usually take very long.

# Chapter 3. Inventory: Describing Your Servers
The collection of hosts that Ansible knows about is called the inventory. In this chapter, you will learn how to describe a set of hosts as an Ansible inventory.

Ansible automatically adds one host to the inventory by default: *localhost*. Ansible understands that localhost refers to your local machine, so it will interact with it directly rather than connecting by SSH.

## Preliminaries: Multiple Vagrant Machines
Before you modify your existing Vagrantfile, make sure you destroy your existing virtual machine by running the following:
```bash
vagrant destroy --force
```
*vagrantfile* for three servers:
```ruby
VAGRANTFILE_API_VERSION = "2"

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  # Use the same key for each machine
  config.ssh.insert_key = false

  config.vm.define "vagrant1" do |vagrant1|
    vagrant1.vm.box = "ubuntu/trusty64"
    vagrant1.vm.network "forwarded_port", guest: 80, host: 8080
    vagrant1.vm.network "forwarded_port", guest: 443, host: 8443
  end
  config.vm.define "vagrant2" do |vagrant2|
    vagrant2.vm.box = "ubuntu/trusty64"
    vagrant2.vm.network "forwarded_port", guest: 80, host: 8081
    vagrant2.vm.network "forwarded_port", guest: 443, host: 8444
  end
  config.vm.define "vagrant3" do |vagrant3|
    vagrant3.vm.box = "ubuntu/trusty64"
    vagrant3.vm.network "forwarded_port", guest: 80, host: 8082
    vagrant3.vm.network "forwarded_port", guest: 443, host: 8445
  end
end
```
Using the same key on each host simplifies our Ansible setup because we can specify a single SSH key in the *ansible.cfg* file:
```ini
[defaults]
inventory = inventory
remote_user = vagrant
private_key_file = ~/.vagrant.d/insecure_private_key
host_key_checking = False
```
In the inventory file, you can use `ansible_host` to explicitly specify IP and `ansible_port` indicates SSH port number:
```
vagrant1 ansible_host=127.0.0.1 ansible_port=2222
vagrant2 ansible_host=127.0.0.1 ansible_port=2200
vagrant3 ansible_host=127.0.0.1 ansible_port=2201
```

## Behavioral Inventory Parameters
[Markdown table generator](https://www.tablesgenerator.com/markdown_tables)
```
|            Name            	|     Default     	|                                   Description                                   	|
|:--------------------------:	|:---------------:	|:-------------------------------------------------------------------------------:	|
| ansible_host               	| Name of host    	| Hostname or IP address to SSH to                                                	|
| ansible_port               	| 22              	| Port to SSH to                                                                  	|
| ansible_user               	| Root            	| User to SSH as                                                                  	|
| ansible_password           	| (None)          	| Password to use for SSH authentication                                          	|
| ansible_connection         	| smart           	| How Ansible will connect to host (see the following section)                    	|
| ansible_private_key_file   	| (None)          	| SSH private key to use for SSH authenticatio                                    	|
| ansible_shell_type         	| sh              	| Shell to use for commands (see the following section)                           	|
| ansible_python_interpreter 	| /usr/bin/python 	| Python interpreter on host (see the following section)                          	|
| ansible_*_interpreter      	| (None)          	| Like ansible_python_interpreter for other languages (see the following section) 	|

```
Explanation:
* ansible_connection
Ansible supports multiple *transports*, which are mechanisms that Ansible uses to connect to the host. The default transport, `smart`, will check whether the locally installed SSH client supports a feature called *ControlPersist*. If the SSH client supports ControlPersist, Ansible will use the local SSH client. If the SSH client doesn’t support ControlPersist, the smart transport will fall back to using a Python-based SSH client library called *Paramiko*.

* ansible_shell_type
Ansible works by making SSH connections to remote machines and then invoking scripts. By default, Ansible assumes that the remote shell is the Bourne shell located at */bin/sh*, and will generate the appropriate command-line parameters that work with Bourne shell.

  Ansible also accepts `csh`, `fish`, and (on Windows) `powershell` as valid values for 
  this parameter. I’ve never encountered a need for changing the shell type.

* ansible_python_interpreter
Because the modules that ship with Ansible are implemented in Python 2, Ansible needs to know the location of the Python interpreter on the remote machine. You might need to change this if your remote host does not have a Python 2 interpreter at */usr/bin/python*. For example, if you are managing hosts that run Arch Linux, you will need to change this to */usr/bin/python2*, because Arch Linux installs Python 3 at */usr/bin/python*, and Ansible modules are not (yet) compatible with Python 3.

* ansible_*_interpreter
If you are using a custom module that is not written in Python, you can use this parameter to specify the location of the interpreter (e.g., /usr/bin/ruby).

**Note**: You can override some of the behavioral parameter default values in the defaults section of the `ansible.cfg` file
```
| Behavioral inventory parameter 	| ansible.cfg option 	|
|:------------------------------:	|:------------------:	|
| ansible_port                   	| remote_port        	|
| ansible_user                   	| remote_user        	|
| ansible_private_key_file       	| private_key_file   	|
| ansible_shell_type             	| executable         	|
```
## Groups
Ansible automatically defines a group called `all` (or `*`), which includes all of the hosts in the inventory. For example:
```
ansible all -a "date"
ansible '*' -a "date"
```
We can define our own groups in the inventory file. Ansible uses the *.ini* file format for inventory files. In the *.ini* format, configuration values are grouped together into sections. For example:
```ini
[master]

[workers]

[nodes]

[all:vars]
ansible_connection=ssh
ansible_user=root
ansible_ssh_private_key_file=null
gather_facts=True
gathering=smart
host_key_checking=False
```

## Alias and Ports:
In inventory file:
```
[master]
<hostname>
<hostname>:<port number>
<alias> ansible_host=<IP> ansible_port=<port number>
```
## Groups of groups
Ansible also allows you to define groups that are made up of other groups. Here web and task are groups, diango subgroup wrap them up.
```
[django:children]
web
task
```
## Numbered hosts
```
[host]
web[1:20].example.com
## leading 0
web[01:20].example.com
web-[a-t].example.com
```
## Host and Group Variables
Format: [<group name>:vars]
```
[all:vars]

[production:vars]

[staging:vars]
```
Additionally, though Ansible variables can hold Booleans, strings, lists, and dictionaries, in an inventory file, you can specify only Booleans and strings.

Ansible offers a more scalable approach to keep track of host and group variables: you can create a separate variable file for each host and each group. Ansible expects these variable files to be in YAML format.

Ansible looks for host variable files in a directory called `host_vars` and group variable files in a directory called `group_vars`. Ansible expects these directories to be either in the directory that contains your playbooks or in the directory adjacent to your inventory file. For example:
```yaml
playbooks folder:
  - playbook.yml
  - group_vars folder
    - production
```
If we use YAML dictionary format in group variable file:
```yaml
db:
    user: widgetuser
    password: pFmMxcyD;Fc6)6
    name: widget_production
    primary:
        host: rhodeisland.example.com
        port: 5432
    replica:
        host: virginia.example.com
        port: 5432

rabbitmq:
    host: pennsylvania.example.com
    port: 5672
```
when reference in playbook:
```
{{ db.primary.host }}
```
##  Dynamic Inventory
If the inventory file is marked executable, Ansible will assume it is a dynamic inventory script and will execute the file instead of reading it.

If some other system, such as AWS EC2, will keep track of the virtual machine information for us, we don't necessarily need to write the inventory file manually, we can use dynamic inventory script to query about which machines are running and use them.

[preexisting inventory scripts](https://github.com/ansible/ansible/blob/devel/contrib/inventory/vagrant.py)

## Adding Entries at Runtime with add_host and group_by
Ansible will let you add hosts and groups to the inventory during the execution of a playbook.
**not use yet**

# Chapter 4. Variables and Facts
Ansible is not a full-fledged programming language, but it does have several programming language features, and one of the most important of these is **variable substitution**. This chapter presents Ansible’s support for variables in more detail, including a certain type of variable that Ansible calls a *fact*.

## Defining Variables in Playbooks
There are two scenarios, for example:
```yaml
- name: Configure Kubeadm
  hosts: master
  become: true
  any_errors_fatal: true
  vars:
    NODE_COUNT: "{{ groups['nodes'] | length }}"
  roles:
    - setup.master

## or
- name: Configure Kubeadm
  hosts: master
  become: true
  any_errors_fatal: true
  vars_files:
    - config.yml
  roles:
    - setup.master
```
## Viewing the Values of Variables
We use the `debug` module to print out an arbitrary message. We can also use it to output the value of the variable.
```
- debug: var=<myvarname>
```
## Registering Variables
Often, you’ll find that you need to set the value of a variable based on the result of a task.To do so, we create a *registered variable* using the `register` clause when invoking a module. Below shows how to capture the output of the `whoami` command to a variable named `login`.
```
- name: capture output of whoami command
  command: whoami
  register: login
```
**Note**, if you want to use `login` variable registered here, it's not like that you can call it as `{{ login }}`

In order to use the login variable later, we need to know the type of value to expect. The value of a variable set using the register clause is always a dictionary, but the specific keys of the dictionary are different, depending on the module that was invoked.

Unfortunately, the official Ansible module documentation doesn’t contain information about what the return values look like for each module. The module docs do often contain examples that use the register clause, which can be helpful. I’ve found the simplest way to find out what a module returns is to register a variable and then output that variable with the debug module.
```
- name: show return value of command module
  hosts: server1
  tasks:
    - name: capture output of id command
      command: id -un
      register: login
    - debug: var=login
```
The `shell` module has the same output structure as the `command` module, but other modules contain different keys, the output here is:
```
TASK: [debug var=login] *******************************************************
ok: [server1] => {
    "login": {
        "changed": true, 1
        "cmd": [ 2
            "id",
            "-un"
        ],
        "delta": "0:00:00.002180",
        "end": "2015-01-11 15:57:19.193699",
        "invocation": {
            "module_args": "id -un",
            "module_name": "command"
        },
        "rc": 0, 3
        "start": "2015-01-11 15:57:19.191519",
        "stderr": "", 4
        "stdout": "vagrant", 5
        "stdout_lines": [ 6
            "vagrant"
        ],
        "warnings": []
    }
}
```
* The `changed` key is present in the return value of all Ansible modules, and Ansible uses it to determine whether a state change has occurred. For the `command` and `shell`module, this will always be set to `true` unless overridden with the `changed_when`clause.

* The `cmd` key contains the invoked command as a list of strings.

* The `rc` key contains the return code. If it is nonzero, Ansible will assume the task failed to execute.

* The `stdout` key contains any text written to standard out, as a single string.

* The `stdout_lines` key contains any text written to split by newline. It is a list, and each element of the list is a line of output.

So now you can access `login` with:
```
- name: capture output of id command
  command: id -un
  register: login
- debug: msg="Logged in as user {{ login.stdout }}"
```
### ACCESSING DICTIONARY KEYS IN A VARIABLE
If a variable contains a dictionary, you can access the keys of the dictionary by using either a dot (.) or a subscript ([]).
```
{{ login.stdout }}
{{ login['stdout'] }}

ansible_eth1['ipv4']['address']
ansible_eth1['ipv4'].address
ansible_eth1.ipv4['address']
ansible_eth1.ipv4.address
```
### CAUTION
If your playbooks use registered variables, make sure you know the content of those variables, both for cases where the module changes the host’s state and for when the module doesn’t change the host’s state. Otherwise, your playbook might fail when it tries to access a key in a registered variable that doesn’t exist.

## Facts
As you’ve already seen, when Ansible runs a playbook, before the first task runs, this happens:
```
GATHERING FACTS **************************************************
ok: [servername]
```
When Ansible gathers facts, it connects to the host and queries it for all kinds of details about the host: CPU architecture, operating system, IP addresses, memory info, disk info, and more. This information is stored in variables that are called *facts*, and they behave just like any other variable. For example:
```
- name: print out operating system
  hosts: all
  gather_facts: True
  tasks:
  - debug: var=ansible_distribution
```
List of facts variable can be found [here](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#information-discovered-from-systems-facts)

### Viewing All Facts Associated with a Server
Ansible implements fact collecting through the use of a special module called the `setup`module. You don’t need to call this module in your playbooks because Ansible does that automatically when it gathers facts.
```
ansible server1 -m setup
```
```
server1 | success >> {
    "ansible_facts": {
        "ansible_all_ipv4_addresses": [
            "10.0.2.15",
            "192.168.4.10"
        ],
        "ansible_all_ipv6_addresses": [
            "fe80::a00:27ff:fefe:1e4d",
            "fe80::a00:27ff:fe67:bbf3"
        ],
(many more facts)
```
Note that the returned value is a dictionary whose key is `ansible_facts` and whose value is a dictionary that contains the name and value of the actual facts.

### Viewing a Subset of Facts
The `setup` module supports a `filter` parameter that lets you filter by fact name by specifying a glob.
```
ansible web -m setup -a 'filter=ansible_eth*'
```
### Any Module Can Return Facts
The use of `ansible_facts` in the return value is an Ansible idiom. If a module returns a dictionary that contains `ansible_facts` as a key, Ansible will create variable names in the environment with those values and associate them with the active host.

For modules that return facts, there’s no need to register variables, since Ansible creates these variables for you automatically.
```
- name: get ec2 facts
  ec2_facts:

- debug: var=ansible_ec2_instance_id
```
Several modules ship with Ansible that return facts. You’ll see another one of them, the `docker` module.

### Local Facts
Ansible provides an additional mechanism for associating facts with a host. You can place one or more files on the remote host machine in the `/etc/ansible/facts.d` directory.
**not used yet**

### Using set_fact to Define a New Variable
Ansible also allows you to set a fact (effectively the same as defining a new variable) in a task by using the `set_fact` module. I often like to use `set_fact` immediately after `register` to make it simpler to refer to a variable.
```
- name: get snapshot id
  shell: >
    aws ec2 describe-snapshots --filters
    Name=tag:Name,Values=my-snapshot
    | jq --raw-output ".Snapshots[].SnapshotId"
  register: snap_result

- set_fact: snap={{ snap_result.stdout }}

- name: delete old snapshot
  command: aws ec2 delete-snapshot --snapshot-id "{{ snap }}"
```
## Built-in Variables
Ansible defines several variables that are always available in a playbook
```
| Parameter                	| Description                                                                                                                                                                                 	|
|--------------------------	|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	|
| hostvars                 	| A dict whose keys are Ansible hostnames and values are dicts  that map variable names to values                                                                                             	|
| inventory_hostname       	| Fully qualified domain name of the current host as known by Ansible  (e.g., myhost.example.com)                                                                                             	|
| inventory_hostname_short 	| Name of the current host as known by Ansible, without the domain name  (e.g., myhost)                                                                                                       	|
| group_names              	| A list of all groups that the current host is a member of                                                                                                                                   	|
| groups                   	| A dict whose keys are Ansible group names and values are a list of  hostnames that are members of the group. Includes all and ungrouped groups:  {"all": […], "web": […], "ungrouped": […]} 	|
| ansible_check_mode       	| A boolean that is true when running in check mode                                                                                                                                           	|
| ansible_play_batch       	| A list of the inventory hostnames that are active in the current batch                                                                                                                      	|
| ansible_play_hosts       	| A list of all of the inventory hostnames that are active in the current  play                                                                                                               	|
| ansible_version          	| A dict with Ansible version info:  {"full": 2.3.1.0", "major": 2, "minor": 3, "revision": 1, "string": "2.3.1.0"}                                                                           	|
```

* **hostvars**
This is a dictionary that contains all of the variables defined on all of the hosts, keyed by the hostname as known to Ansible. If Ansible has not yet gathered facts on a host, you will not be able to access its facts by using the hostvars variable, unless fact caching is enabled.
  ```
  {{ hostvars['db.example.com'].ansible_eth1.ipv4.address }}
  ```

*  **inventory_hostname**
The inventory_hostname is the hostname of the current host, as known by Ansible.

* **groups**
The groups variable can be useful when you need to access variables for a group of hosts.

I encounter this in playbook vars section:
```
- name: xxx
  hosts: master
  become: true
  any_errors_fatal: true
  vars:
    NODE_COUNT: "{{ groups['nodes'] | length }}"
  roles:
    - ...

- name: xxx
  any_errors_fatal: true
  shell: "..."
  when: "inventory_hostname == groups.master[0]"
```

## Setting Variables on the Command Line
Variables set by passing `-e var=value` to `ansible-playbook` have the highest precedence, which means you can use this to override variables that are already defined.
```
ansible-playbook greet.yml -e 'greeting="hi there"'
```
Ansible also allows you to pass a file containing the variables instead of passing them directly on the command line by passing @filename.yml as the argument to `-e`
```
ansible-playbook greet.yml -e @greetvars.yml
```
content in `greetvars.yml`:
```
greeting: hiya
```
## Precedence
We’ve covered several ways of defining variables, and it can happen that you define the same variable multiple times for a host, using different values. Avoid this when you can, but if you can’t, then keep in mind Ansible’s precedence rules.
```
1. (Highest) ansible-playbook -e var=value
2. Task variables
3. Block variables
4. Role and include variables
5. set_fact
6. Registered variables
7. vars_files
8. vars_prompt
9. Play variables
10. Host facts
11. host_vars set on a playbook
12. group_vars set on a playbook
13. host_vars set in the inventory
14. group_vars set in the inventory
15. Inventory variables
16. In defaults/main.yml of a role
```

# Chapter 5. Introducing Mezzanine: Our Test Application
Let’s take a little detour and talk about the differences between running software in development mode on your laptop versus running the software in production. Mezzanine is a great example of an application that is much easier to run in development mode than it is to deploy.
```
virtualenv venv
source venv/bin/activate
pip install mezzanine
mezzanine-project myproject
cd myproject
sed -i 's/ALLOWED_HOSTS = \[\]/ALLOWED_HOSTS = ["127.0.0.1"]/' settings.py
python manage.py createdb
python manage.py runserver
```
You’ll be prompted to answer several questions. I answered “yes” to each yes/no question, and accepted the default answer whenever one was available.

Now, let’s look at what happens when you deploy to production.

* **PostgreSQL: The Database**
In production, we want to run a server-based database, because those have better support for multiple, concurrent requests, and server-based databases allow us to run multiple HTTP servers for load balancing. This means we need to deploy a database management system such as MySQL or PostgreSQL (aka Postgres). 
  ```
  Install the database software.
  Ensure the database service is running.
  Create the database inside the database management system.
  Create a database user who has the appropriate permissions for the database system.
  Configure our Mezzanine application with the database user credentials and 
  connection information.
  ```
* **Gunicorn: The Application Server**

* **Nginx: The Web Server**

> Note: Application server and Web server, their usage is different. Here Nginx is like a [reverse proxy](https://www.cnblogs.com/Anker/p/6056540.html) for Gunicorn.

* **Supervisor: The Process Manager**



# Chapter 7. Roles: Scaling Up Your Playbooks
Ansible scales down well because simple tasks are easy to implement. It scales up well because it provides mechanisms for decomposing complex jobs into smaller pieces.

In Ansible, the `role` is the primary mechanism for breaking a playbook into multiple files. This simplifies writing complex playbooks, and it makes them easier to reuse.

## Basic Structure of a Role
An Ansible role has a name, such as `database`. Files associated with the `database` role go in the *roles/database* directory, which contains the following files and directories:
```
| roles/database/tasks/main.yml    	| Tasks                                    	|
| roles/database/files/            	| Holds files to be uploaded to hosts      	|
| roles/database/templates/        	| Holds Jinja2 template files              	|
| roles/database/handlers/main.yml 	| Handlers                                 	|
| roles/database/vars/main.yml     	| Variables that shouldn’t be overridden   	|
| roles/database/defaults/main.yml 	| Default variables that can be overridden 	|
| roles/database/meta/main.yml     	| Dependency information about a role      	|
```
Each individual file is optional; if your role doesn’t have any handlers, there’s no need to have an empty handlers/main.yml file.

## WHERE DOES ANSIBLE LOOK FOR MY ROLES?
Ansible looks for roles in the *roles* directory alongside your playbooks. It also looks for systemwide roles in */etc/ansible/roles*. You can customize the systemwide location of roles by setting the `roles_path` setting in the `defaults` section of your *ansible.cfg* file.
```
[defaults]
roles_path = ~/ansible_roles
```
## Using Roles in Your Playbooks
```
- name: deploy mezzanine on vagrant
  ## target hosts
  hosts: web
  vars_files:
    - secrets.yml
  ## role section
  roles:
    - role: database
       ## pass variables into role task
      database_name: "{{ mezzanine_proj_name }}"
      database_user: "{{ mezzanine_proj_name }}"

    - role: mezzanine
      live_hostname: 192.168.33.10.xip.io
      domains:
        - 192.168.33.10.xip.io
        - www.192.168.33.10.xip.io
```
Note that we can pass in variables when invoking the roles. If these variables have already been defined in the role (either in vars/main.yml or defaults/main.yml), then the values will be overridden with the variables that were passed in.

## Pre-Tasks and Post-Tasks
Sometimes you want to run tasks before or after you invoke your roles. Let’s say you want to update the apt cache before you deploy Mezzanine, and you want to send a notification to a Slack channel after you deploy.

Ansible allows you to define a list of tasks that execute before the roles with a `pre_tasks` section, and a list of tasks that execute after the roles with a `post_tasks`section.

```yaml
- name: deploy mezzanine on vagrant
  hosts: web
  vars_files:
    - secrets.yml
  pre_tasks:
    - name: update the apt cache
      apt: update_cache=yes
  roles:
    - role: mezzanine
  post_tasks:
    - name: notify Slack that the servers have been updated
      local_action: >
        slack
        domain=acme.slack.com
        token={{ slack_token }}
        msg="web server {{ inventory_hostname }} configured"
```
> Note: you need to define and put variables in right place, for example, the variables that would be used by multiple roles or playbooks should be put in `group_vars/all` file.

## WHY ARE THERE TWO WAYS TO DEFINE VARIABLES IN ROLES?
When Ansible first introduced support for roles, there was only one place to define role variables, in *vars/main.yml*. Variables defined in this location have a higher precedence than those defined in the `vars` section of a play, which meant you couldn’t override the variable unless you explicitly passed it as an argument to the role.

Ansible later introduced the notion of default role variables that go in *defaults/main.yml*.This type of variable is defined in a role, but has a low precedence, so it will be overridden if another variable with the same name is defined in the playbook.

If you think you might want to change the value of a variable in a role, use a default variable. If you don’t want it to change, use a regular variable.

## Some role practices
**Note** that if for role variables, it's better to add prefix like `<role name>_<var name>`. It’s good practice to do this with role variables because Ansible doesn’t have any notion of namespace across roles. This means that variables that are defined in other roles, or elsewhere in a playbook, will be accessible everywhere. This can cause some unexpected behavior if you accidentally use the same variable name in two different roles. For example, for the role called `mezzanine`, in `roles/mezzanine/vars/main.yml` file:
```
mezzanine_user: "{{ ansible_user }}"
mezzanine_venv_home: "{{ ansible_env.HOME }}"
mezzanine_venv_path: "{{ mezzanine_venv_home }}/{{ mezzanine_proj_name }}"
mezzanine_repo_url: git@github.com:lorin/mezzanine-example.git
mezzanine_proj_dirname: project
```
For role variables in *default/main.yml*, no need to add prefix because we may intentionally override them elsewhere.

If the role task is very long, you can break it into several task files, for example:
```
roles/mezzanine/tasks/django.yml
roles/mezzanine/tasks/main.yml
roles/mezzanine/tasks/nginx.yml
```
In the `main.yml` task file you can invoke other tasks by using `include` statement:
```
- name: install apt packages
  apt: pkg={{ item }} update_cache=yes cache_valid_time=3600
  become: True
  with_items:
    - git
    - supervisor

- include: django.yml
- include: nginx.yml
```

**Note**: there’s one important difference between tasks defined in a role and tasks defined in a regular playbook, and that’s when using the `copy` or `template` modules.

When invoking `copy` in a task defined in a role, Ansible will first check the *rolename/files/* directory for the location of the file to copy. Similarly, when invoking `template` in a task defined in a role, Ansible will first check the *rolename/templates* directory for the location of the template to use.

This means that a task that used to look like this in a playbook:
```
- name: set the nginx config file
  template: src=templates/nginx.conf.j2 \
  dest=/etc/nginx/sites-available/mezzanine.conf
```
now looks like this when invoked from inside the role (note the change of the src parameter):
```
- name: set the nginx config file
  template: src=nginx.conf.j2 dest=/etc/nginx/sites-available/mezzanine.conf
  notify: restart nginx
```

## Creating Role Files and Directories with ansible-galaxy
Ansible ships with another command-line tool we haven’t talked about yet, `ansible-galaxy`. Its primary purpose is to download roles that have been shared by the Ansible community. But it can also be used to generate *scaffolding*, an initial set of files and directories involved in a role:
```
ansible-galaxy init <path>/roles/web
```
```playbooks
└── roles
    └── web
        ├── README.md
        ├── defaults
        │   └── main.yml
        ├── files
        ├── handlers
        │   └── main.yml
        ├── meta
        │   └── main.yml
        ├── tasks
        │   └── main.yml
        ├── templates
        ├── tests
        │   ├── inventory
        │   └── test.yml
        └── vars
            └── main.yml
```
## Dependent Roles
Ansible supports a feature called [dependent roles](https://docs.ansible.com/ansible/latest/user_guide/playbooks_roles.html#role-dependencies) to deal with this scenario. When you define a role, you can specify that it depends on one or more other roles. Ansible will ensure that roles that are specified as dependencies are executed first.

Let’s say that we create an`ntp` role that configures a host to synchronize its time with an NTP server. Ansible allows us to pass parameters to dependent roles, so let’s also assume that we can pass the NTP server as a parameter to that role.

We specify that the `web` role depends on the `ntp` role by creating a *roles/web/meta/main.yml* file and listing `ntp` as a role, with a parameter:
```
dependencies:
    - { role: ntp, ntp_server=ntp.ubuntu.com }
```

## Ansible Galaxy
Whether you want to reuse a role somebody has already written, or you just want to see how someone else solved the problem you’re working on, [Ansible Galaxy](https://galaxy.ansible.com/) can help you out. Ansible Galaxy is an open source repository of Ansible roles contributed by the Ansible community. The roles themselves are stored on GitHub.

# Chapter 8. Complex Playbooks
This chapter touches on those additional features, which makes it a bit of a grab bag.

## Dealing with Badly Behaved Commands
What if we didn’t have a module that could invoke equivalent commands (wasn’t idempotent)? The answer is to use `changed_when` and `failed_when` clauses to change how Ansible identifies that a task has changed state or failed.

First, we need to understand the output of this command the first time it’s run, and the output when it’s run the second time.
```
- name: initialize the database
  django_manage:
    command: createdb --noinput --nodata
    app_path: "{{ proj_path }}"
    virtualenv: "{{ venv_path }}"
  failed_when: False
  register: result

- debug: var=result

- fail:
```
`failed_when: False` is to close task fail, so ansible play will continue to execute. We can run several times of the playbook and see different register variable output.
`fail` statement here is to stop the execution.

Some module may not report changed state even though it did make change in target machine, so we can check if state changed ourselves by using `changed_when` clause:
```
- name: initialize the database
  django_manage:
    command: createdb --noinput --nodata
    app_path: "{{ proj_path }}"
    virtualenv: "{{ venv_path }}"
  register: result
  changed_when: '"Creating tables" in result.out|default("")'
```
We use filter here in `changed_when` since register variable sometimes doesn't have `out` field. Alternatively, we could provide a default value for `result.out` if it doesn’t exist by using the Jinja2 default filter.

## Filter
*Filters* are a feature of the Jinja2 templating engine. Since Ansible uses Jinja2 for evaluating variables, as well as for templates, you can use filters inside `{{ braces }}` in your playbooks, as well as inside your template files. Using filters resembles using Unix pipes, whereby a variable is piped through a filter. Jinja2 ships with a set of [built-in filters](http://bit.ly/1FvOGzI). In addition, Ansible ships with its own filters to augment the [Jinja2 filters](http://bit.ly/1FvOIrj).

### The Default Filter
```
"HOST": "{{ database_host | default('localhost') }}"
```
If the variable `database_host` is defined, the braces will evaluate to the value of that variable. If the variable `database_host` is not defined, the braces will evaluate to the string localhost.

### Filters for Registered Variables
Let’s say we want to run a task and print out its output, even if the task fails. However, if the task does fail, we want Ansible to fail for that host after printing the output.
```
- name: Run myprog
  command: /opt/myprog
  register: result
  ignore_errors: True

- debug: var=result

- debug: msg="Stop running the playbook if myprog failed"
  failed_when: result|failed
```
 a list of filters you can use on registered variables to check the status:
```
| Name    	| Description                                           	|
|---------	|-------------------------------------------------------	|
| failed  	| True if a registered value is a task that failed      	|
| changed 	| True if a registered value is a task that changed     	|
| success 	| True if a registered value is a task that succeeded   	|
| skipped 	| True if a registered value is a task that was skipped 	|
```
The basename filter will let us extract the index.html part of the filename from the full path, allowing us to write the playbook without repeating the filename:
```
vars:
    homepage: /usr/share/nginx/html/index.html
  tasks:
  - name: copy home page
    copy: src=files/{{ homepage | basename }} dest={{ homepage }}
```

## Lookups
Sometimes a piece of configuration data you need lives somewhere else. Maybe it’s in a text file or a *.csv* file, and you don’t want to just copy the data into an Ansible variable file because now you have to maintain two copies of the same data. 

Ansible has a feature called `lookups` that allows you to read in configuration data from various sources and then use that data in your playbooks and template.
```
| Name     	| Description                        	|
|----------	|------------------------------------	|
| file     	| Contents of a file                 	|
| password 	| Randomly generate a password       	|
| pipe     	| Output of locally executed command 	|
| env      	| Environment variable               	|
| template 	| Jinja2 template after evaluation   	|
| csvfile  	| Entry in a .csv file               	|
| dnstxt   	| DNS TXT record                     	|
| redis_kv 	| Redis key lookup                   	|
| etcd     	| etcd key lookup                    	|
```
You can invoke lookups in your playbooks between `{{ braces }}`, or you can put them in templates. 

**Note** all Ansible lookup plugins execute on the control machine, not the remote host.

### file
Let’s say you have a text file on your control machine that contains a public SSH key that you want to copy to a remote server.
```
- name: Add my public key as an EC2 key
  ec2_key: name=mykey key_material="{{ lookup('file',  '/Users/lorin/.ssh/id_rsa.pub') }}"
```
### pipe
The `pipe` lookup invokes an external program on the control machine and evaluates to the program’s output on standard out.

For example, if our playbooks are version controlled using `git`, and we want to get the `SHA-1` value of the most recent `git commit`, we could use the `pipe` lookup
```
- name: get SHA of most recent commit
  debug: msg="{{ lookup('pipe', 'git rev-parse HEAD') }}"
```
```
TASK: [get the sha of the current commit] *************************************
ok: [myserver] => {
    "msg": "e7748af0f040d58d61de1917980a210df419eae9"
}
```

### env
The `env` lookup retrieves the value of an environment variable set on the control machine.For example, we could use the lookup like this:
```
- name: get the current shell
  debug: msg="{{ lookup('env', 'SHELL') }}"
```
```
TASK: [get the current shell] *************************************************
ok: [myserver] => {
    "msg": "/bin/zsh"
}
```

### password
The `password` lookup evaluates to a random password, and it will also write the password to a file specified in the argument. For example, if we want to create a Postgres user named `deploy` with a random password and write that password to *deploy-password.txt*on the control machine, we can do this:
```
- name: create deploy postgres user
  postgresql_user:
    name: deploy
    password: "{{ lookup('password', 'deploy-password.txt') }}"
```

### template
The `template` lookup lets you specify a Jinja2 template file, and then returns the result of evaluating the template.

### csvfile
The `csvfile` lookup reads an entry from a *.csv* file.
For example, we have a `users.csv` file:
```
username,email
lorin,lorin@ansiblebook.com
john,john@example.com
sue,sue@example.org
```
```
lookup('csvfile', 'sue file=users.csv delimiter=, col=1')
```
In the case of `csvfile`, the first argument is an entry that must appear exactly once in column 0 (the first column, 0-indexed) of the table.

In our example, we want to look in the file named `users.csv` and locate where the fields are delimited by commas, look up the row where the value in the first column is `sue`, and return the value in the second column (column 1, indexed by 0). This evaluates to sue@example.org.

### etcd
Etcd is a distributed key-value store, commonly used for keeping configuration data and for implementing service discovery. You can use the `etcd` lookup to retrieve the value of a key.

For example, let’s say that we have an etcd server running on our control machine, and we set the key weather to the value cloudy by doing something like this:
```
curl -L http://127.0.0.1:4001/v2/keys/weather -XPUT -d value=cloudy
```
```
- name: look up value in etcd
  debug: msg="{{ lookup('etcd', 'weather') }}"

TASK: [look up value in etcd] *************************************************
ok: [localhost] => {
    "msg": "cloudy"
}
```
By default, the etcd lookup looks for the etcd server at http://127.0.0.1:4001, but you can change this by setting the `ANSIBLE_ETCD_URL` environment variable before invoking ansible-playbook.

## More Complicated Loops
Up until this point, whenever we’ve written a task that iterates over a list of items, we’ve used the `with_items` clause to specify a list of items. Although this is the most common way to do loops, Ansible supports other mechanisms for iteration.
```
| Name                     	| Input                	| Looping strategy                  	|
|--------------------------	|----------------------	|-----------------------------------	|
| with_items               	| List                 	| Loop over list elements           	|
| with_lines               	| Command to execute   	| Loop over lines in command output 	|
| with_fileglob            	| Glob                 	| Loop over filenames               	|
| with_first_found         	| List of paths        	| First file in input that exists   	|
| with_dict                	| Dictionary           	| Loop over dictionary elements     	|
| with_flattened           	| List of lists        	| Loop over flattened list          	|
| with_indexed_items       	| List                 	| Single iteration                  	|
| with_nested              	| List                 	| Nested loop                       	|
| with_random_choice       	| List                 	| Single iteration                  	|
| with_sequence            	| Sequence of integers 	| Loop over sequence                	|
| with_subelements         	| List of dictionaries 	| Nested loop                       	|
| with_together            	| List of lists        	| Loop over zipped list             	|
| with_inventory_hostnames 	| Host pattern         	| Loop over matching hosts          	|
```
The [official documentation](http://bit.ly/1F6kfCP) covers these quite thoroughly, so I’ll show examples from just a few of them to give you a sense of how they work.

### with_lines
The with_lines looping construct lets you run an arbitrary command on your control machine and iterate over the output, one line at a time. For example, read a file and iterate over its contents line by line.
```
- name: Send out a slack message
  slack:
    domain: example.slack.com
    token: "{{ slack_token }}"
    msg: "{{ item }} was in the list"
  with_lines:
    - cat files/turing.txt
```

### with_fileglob
The with_fileglob construct is useful for iterating over a set of files on the control machine.

For example, iterate over files that end in `.pub` in the /var/keys directory, as well as a keys directory next to your playbook. It then uses the `file` lookup plugin to extract the contents of the file, which are passed to the authorized_key module.
```
- name: add public keys to account
  authorized_key: user=deploy key="{{ lookup('file', item) }}"
  with_fileglob:
    - /var/keys/*.pub
    - keys/*.pub
```

### with_dict
The `with_dict` construct lets you iterate over a dictionary instead of a list. When you use this looping construct, the `item` loop variable is a dictionary with two fields:
```
 - name: iterate over ansible_eth0
    debug: msg={{ item.key }}={{ item.value }}
    with_dict: "{{ ansible_eth0.ipv4 }}"
```
### Looping Constructs as Lookup Plugins
Ansible implements looping constructs as lookup plugins. That means you can alter the form of lookup to perform as a loop:
```
- name: Add my public key as an EC2 key
  ec2_key: name=mykey key_material="{{ item }}"
  with_file: /Users/lorin/.ssh/id_rsa.pub
```
Here we prefix `with_` with `file` lookup plugin. Typically, you use a lookup plugin as a looping construct only if it returns a list,

## Loop Controls
With version 2.1, Ansible provides users with more control over loop handling.

### Setting the Variable Name
The `loop_var` control allows us to give the iteration variable a different name than the default name, `item`:
```
- user:
    name: "{{ user.name }}"
  with_items:
  ## list of dict
    - { name: gil }
    - { name: sarina }
    - { name: leanne }
  loop_control:
    loop_var: user
```
Next one is a advanced usage, use `include` with `with_items`, we loop over multiple task at once, in current task we include a task called `vhosts.yml` which will be executed 3 times with different parameters passed in:
```
- name: run a set of tasks in one loop
  include: vhosts.yml
  with_items:
    - { domain: www1.example.com }
    - { domain: www2.example.com }
    - { domain: www3.example.com }
  loop_control:
    loop_var: vhost
```
The `vhosts.yml` file that is going to be included may also contain `with_items` in some tasks. This would produce a conflict, as the default loop_var `item` is used for both loops at the same time.

To prevent a naming collision, we specify a different name for loop_var in the outer loop.
```
- name: create nginx directories
  file:
    path: /var/www/html/{{ vhost.domain }}/{{ item }}
  state: directory
  with_items:
    - logs
    - public_http
    - public_https
    - includes

- name: create nginx vhost config
  template:
    src: "{{ vhost.domain }}.j2"
    dest: /etc/nginx/conf.d/{{ vhost.domain }}.conf
```

### Labeling the Output
The `label` control was added in Ansible 2.2 and provides some control over how the loop output will be shown to the user during execution.
```
- name: create nginx vhost configs
  template:
    src: "{{ item.domain }}.conf.j2"
    dest: "/etc/nginx/conf.d/{{ item.domain }}.conf"
  with_items:
    - { domain: www1.example.com, ssl_enabled: yes }
    - { domain: www2.example.com }
    - { domain: www3.example.com,
      aliases: [ edge2.www.example.com, eu.www.example.com ] }
  loop_control:
    label: "for domain {{ item.domain }}"
```
```
TASK [create nginx vhost configs] **********************************************
ok: [localhost] => (item=for domain www1.example.com)
ok: [localhost] => (item=for domain www2.example.com)
ok: [localhost] => (item=for domain www3.example.com)
```

## Includes
The `include` feature allows you to include tasks or even whole playbooks, depending on where you define an include. It is often used in roles to separate or even group tasks and task arguments to each task in the included file.

For example, you can extract different part of tasks, put them into a separate yml file and include it into another task along with common arguments:
```
# nginx_include.yml file
- name: install nginx
  package:
    name: nginx

- name: ensure nginx is running
  service:
    name: nginx
    state: started
    enabled: yes
```
```
- include: nginx_include.yml
  tags: nginx
  become: yes
  when: ansible_os_family == 'RedHat'
```
Ansible [Tags](https://docs.ansible.com/ansible/latest/user_guide/playbooks_tags.html): If you have a large playbook, it may become useful to be able to run only a specific part of it rather than running everything in the playbook. Ansible supports a `tags:` attribute for this reason.

### Dynamic includes
A common pattern in roles is to define tasks specific to a particular operating system into separate task files.
```
- include: Redhat.yml
  when: ansible_os_family == 'Redhat'

- include: Debian.yml
  when: ansible_os_family == 'Debian'
```
Since version 2.0, Ansible allows us to dynamically include a file by using variable substitution:
```
- include: "{{ ansible_os_family }}.yml"
  static: no
```
However, there is a drawback to using dynamic includes: `ansible-playbook --list-tasks` might not list the tasks from a dynamic include if Ansible does not have enough information to populate the variables that determine which file will be included.

You can use `ansible-playbook <playbook> --list-tasks` to list all the tasks in it.

### Role includes
A special include is the `include_role` clause. In contrast with the `role` clause, which will use all parts of the role, the `include_role` not only allows us to selectively choose what parts of a role will be included and used, but also where in the play.
```
- name: install php
  include_role:
    name: php
```
This will include and run main.yml from the php role, remember a role can have multiple tasks yml files: main.yml and others.
```
- name: install php
  include_role:
    name: php
    tasks_from: install
```
This will include and run install.yml from php role.

## Blocks
Much like the `include` clause, the `block` clause provides a mechanism for grouping tasks. The `block` clause allows you to set conditions or arguments for all tasks within a block at once:
```
- block:
  - name: install nginx
    package:
      name: nginx
  - name: ensure nginx is running
    service:
      name: nginx
      state: started
      enabled: yes
  become: yes
  when: "ansible_os_family == 'RedHat'"
```
The `become` and `when` apply for both tasks.

### Error Handling with Blocks
Dealing with error scenarios has always been a challenge. Historically, Ansible has been error agnostic in the sense that errors and failures may occur on a host. Ansible’s default error-handling behavior is to take a host out of the play if a task fails and continue as long as there are hosts remaining that haven’t encountered errors.
```
- block:
  - debug: msg="You will see a failed tasks right after this"
  - command: /bin/false
  - debug: "You won't see this message"
  rescue: # Tasks to be executed in case of a failure in block clause
  - debug: "You only see this message in case of an failure in the block"
  always: # Tasks to always be executed
  - debug: "This will be always executed"
```
If you have some programming experience, the way error handling is implemented may remind you of the `try-catch-finally` paradigm, and it works much the same way.

## Encrypting Sensitive Data with Vault
Ansible provides an alternative solution: instead of keeping the `secrets.yml` file out of version control, we can commit an encrypted version. That way, even if our version-control repository were compromised, the attacker would not have access to the contents of the `secrets.yml` file unless he also had the password used for the encryption.

The `ansible-vault` command-line tool allows you to create and edit an encrypted file that `ansible-playbook` will recognize and decrypt automatically, given the password.
```
ansible-vault create secrets.yml
ansible-vault encrypt secrets.yml
```
You will be prompted for a password, and then `ansible-vault` will launch a text editor so that you can populate the file. It launches the editor specified in the `$EDITOR` environment variable. If that variable is not defined, it defaults to `vim`.
```
ansible-playbook <playbook> --ask-vault-pass
ansible-playbook <playbook> --vault-password-file ~/password.txt
```
If the argument to `--vault-password-file` has the executable bit set, Ansible will execute it and use the contents of standard out as the vault password. This allows you to use a script to provide the password to Ansible.
```
| Command                        	| Description                                       	|
|--------------------------------	|---------------------------------------------------	|
| ansible-vault encrypt file.yml 	| Encrypt the plain-text file.yml file              	|
| ansible-vault decrypt file.yml 	| Decrypt the encrypted file.yml file               	|
| ansible-vault view file.yml    	| Print the contents of the encrypted file.yml file 	|
| ansible-vault create file.yml  	| Create a new encrypted file.yml file              	|
| ansible-vault edit file.yml    	| Edit an encrypted file.yml file                   	|
| ansible-vault rekey file.yml   	| Change the password on an encrypted file.yml file 	|
```

# Chapter 9. Customizing Hosts, Runs, and Handlers
In this chapter, we cover Ansible features that provide customization by controlling which hosts to run against, how tasks are run, and how handlers are run.

## Patterns for Specifying Hosts
Instead of specifying a single host or group for a play, you can specify a *pattern*. You’ve already seen the `all` pattern, which will run a play against all known hosts:
```
hosts: all
```
You can specify a union of two groups with a colon. You specify all dev and staging machines as follows:
```
hosts: dev:staging
```
```
| Action                    	| Example Usage               	|
|---------------------------	|-----------------------------	|
| All hosts                 	| all                         	|
| All hosts                 	| *                           	|
| Union                     	| dev:staging                 	|
| Intersection              	| dev:&database               	|
| Exclusion                 	| dev:!queue                  	|
| Wildcard                  	| *.example.com               	|
| Range of numbered servers 	| web[5:12]                   	|
| Regular expression        	| ~web\d+\.example\.(com|org) 	|
```
Ansible supports multiple combinations of patterns—for example:
```
hosts: dev:staging:&database:!queue
```

## Limiting Which Hosts Run
Use the `-l hosts` or `--limit hosts` flag to tell Ansible to limit the hosts to run the playbook against the specified list of hosts
```
ansible-playbook -l hosts playbook.yml
ansible-playbook --limit hosts playbook.yml
ansible-playbook -l 'staging:&database' playbook.yml
```

## Running a Task on the Control Machine
Sometimes you want to run a particular task on the control machine instead of on the remote host. Ansible provides the `local_action` clause for tasks to support this. For example, when we check the node ready status in k8s cluster.

Imagine that the server we want to install Mezzanine onto has just booted, so that if we run our playbook too soon, it will error out because the server hasn’t fully started up yet. We could start off our playbook by invoking the `wait_for` module to wait until the SSH server is ready to accept connections before we execute the rest of the playbook.
```
- name: wait for ssh server to be running
  local_action: wait_for port=22 host="{{ inventory_hostname }}" search_regex=OpenSSH
```
Note that `inventory_hostname` evaluates to the name of the remote host, not `localhost`. That’s because the scope of these variables is still the remote host, even though the task is executing locally.

If your play involves multiple hosts, and you use `local_action`, the task will be executed multiple times, one for each host. You can restrict this by using `run_once`

## Running a Task on a Machine Other Than the Host
Sometimes you want to run a task that’s associated with a host, but you want to execute the task on a different server. You can use the `delegate_to` clause to run the task on a different host.
```
- name: enable alerts for web servers
  hosts: web
  tasks:
    - name: enable alerts
      nagios: action=enable_alerts service=web host={{ inventory_hostname }}
      delegate_to: nagios.example.com
```
In this example, Ansible would execute the `nagios task` on nagios.example.com, but the `inventory_hostname` variable referenced in the play would evaluate to the web host.

> Note: if you specify `delegate_to: localhost` to control machine, it's the same as `local_action`, also the same as `connection: local`

## Running on One Host at a Time
By default, Ansible runs each task in parallel across all hosts. Sometimes you want to run your task on one host at a time. The canonical example is when upgrading application servers that are behind a load balancer. Typically, you take the application server out of the load balancer, upgrade it, and put it back. But you don’t want to take all of your application servers out of the load balancer, or your service will become unavailable.

You can use the `serial` clause on a play to tell Ansible to restrict the number of hosts that a play runs on.
```
- name: upgrade packages on servers behind load balancer
  hosts: myhosts
  serial: 1
  tasks:
    - name: get the ec2 instance id and elastic load balancer id
      ec2_facts:

    - name: take the host out of the elastic load balancer
      local_action: ec2_elb
      args:
        instance_id: "{{ ansible_ec2_instance_id }}"
        state: absent

    - name: upgrade packages
      apt: update_cache=yes upgrade=yes

    - name: put the host back in the elastic load balancer
      local_action: ec2_elb
      args:
        instance_id: "{{ ansible_ec2_instance_id }}"
        state: present
        ec2_elbs: "{{ item }}"
      with_items: ec2_elbs
```
In our example, we pass 1 as the argument to the serial clause, telling Ansible to run on only one host at a time. If we had passed 2, Ansible would have run two hosts at a time.

Normally, when a task fails, Ansible stops running tasks against the host that fails, **but** continues to run against other hosts. In the load-balancing scenario, you might want Ansible to fail the entire play before all hosts have failed a task. Otherwise, you might end up with the situation where you have taken each host out of the load balancer, and have it fail, leaving no hosts left inside your load balancer.

You can use a `max_fail_percentage` clause along with the `serial` clause to specify the maximum percentage of failed hosts before Ansible fails the entire play. For example, assume that we specify a maximum fail percentage of 25%, as shown here:
```
- name: upgrade packages on servers behind load balancer
  hosts: myhosts
  serial: 1
  max_fail_percentage: 25
  tasks:
    # tasks go here
```
If you want Ansible to fail if any of the hosts fail a task, set the `max_fail_percentage` to 0.

> Note: `any_errors_fatal: true` is just like set `max_fail_percentage` to 0, with the `any_errors_fatal` option, any failure on any host in a multi-host play will be treated as fatal and Ansible will exit immediately without waiting for the other hosts.

We can get even more sophisticated. For example, you might want to run the play on one host first, to verify that the play works as expected, and then run the play on a larger number of hosts in subsequent runs. 
```
- name: configure CDN servers
  hosts: cdn
  serial:
    - 1
    - 30%
  tasks:
    # tasks go here
```
In the preceding play with 30 CDN hosts, on the first batch run Ansible would run against one host, and on each subsequent batch run it would run against at most 30% of the hosts (e.g., 1, 10, 10, 9).

## Running Only Once
Using `run_once` can be particularly useful when using `local_action` if your playbook involves multiple hosts, and you want to run the local task only once:
```
- name: run the task locally, only once
  local_action: command /opt/my-custom-command
  run_once: true
```

## Running Strategies
The strategy clause on a play level gives you additional control over how Ansible behaves per task for all hosts.

The default behavior we are already familiar with is the `linear` strategy. This is the strategy in which Ansible executes one task on all hosts and waits until the task has completed (of failed) on all hosts before it executes the next task on all hosts. As a result, a task takes as much time as the slowest host takes to complete the task.

### Linear
**Note**: I forget that host file can define variable, here `sleep_seconds` can be referred in task:
```
one   sleep_seconds=1
two   sleep_seconds=6
three  sleep_seconds=10
```
**Note** that the orders show up is the complete order in target host, first done at top.
```
TASK [setup] *******************************************************************
ok: [two]
ok: [three]
ok: [one]
```
### Free
Another strategy available in Ansible is the `free` strategy. In contrast to `linear`, Ansible will not wait for results of the task to execute on all hosts. Instead, if a host completes one task, Ansible will execute the next task on that host.
```
- hosts: all
  strategy: free
  tasks:
     ...
```

## Advanced Handlers
When we covered handlers, you learned that they are usually executed after all tasks, once, and only when they get notified. But keep in mind there are not only `tasks`, but `pre_tasks`, `tasks`, and `post_tasks`.

Each tasks section in a playbook is handled separately; any handler notified in `pre_tasks`, `tasks`, or `post_tasks` is executed at the end of each section. As a result, it is possible to execute one handler several times in one play:

**Note**: the rest of Chapter 9 is useless for me now, just skip it.



# Chapter 16. Debugging Ansible Playbooks
## Humane Error Messages
Enable the plugin by adding the following to the `defaults` section of *ansible.cfg*:
```
[defaults]
stdout_callback = debug
```
the `debug` callback plugin makes this output much easier for a human to read, for example the format is like this:
```
TASK [check out the repository on the host] *************************************
fatal: [web]: FAILED! => {
    "changed": false,
    "cmd": "/usr/bin/git clone --origin origin '' /home/vagrant/mezzanine/mezzani
...
}

STDERR:

Cloning into '/home/vagrant/mezzanine/mezzanine_example'...
Permission denied (publickey).
fatal: Could not read from remote repository.
...
MSG:

Cloning into '/home/vagrant/mezzanine/mezzanine_example'...
Permission denied (publickey).
fatal: Could not read from remote repository.
...
```

## Debugging SSH Issues
Sometimes Ansible fails to make a successful SSH connection with the host. When this happens, it’s helpful to see exactly what arguments Ansible is passing to the underlying SSH client so you can reproduce the problem manually on the command line.

If you invoke `ansible-playbook` with the `-vvv` argument, you can see the exact SSH commands that Ansible invokes. This can be handy for debugging.

> Note that usually I use `-v` flag

Sometimes you might need to use -vvvv when debugging a connection issue, in order to see an error message that the SSH client is throwing. 

## The Debug Module
We’ve used the `debug` module several times in this book. It’s Ansible’s version of a `print` statement.
```
- debug: var=myvariable
- debug: msg="The value of myvariable is {{ var }}"
- debug: var=hostvars[inventory_hostname]
```

## Playbook Debugger
Ansible 2.1 added support for an **interactive debugger**. To enable debugging, add `strategy: debug` to your play; for example:
```
- name: an example play
  strategy: debug
  tasks:
    ...
```
If debugging is enabled, Ansible drops into the debugger when a task fails, for example, I write a task like this:
```
- name: install xxx package
  yum:
    name: xxx
    state: latest
```
```
TASK [install.components : interactive debug] ************************************************************************
fatal: [myk8s2.fyre.ibm.com]: FAILED! => {
    "changed": false,
    "rc": 126,
    "results": [
        "No package matching 'xxx' found available, installed or updated"
    ]
}

MSG:

No package matching 'xxx' found available, installed or updated

Debugger invoked
(debug) p task
```
Let's see the command list
```
| Command              | Description                                 |
|----------------------|---------------------------------------------|
| p var                | Print out the value of a supported variable |
| task.args[key]=value | Modify an argument for the failed task      |
| vars[key]=value      | Modify the value of a variable              |
| r                    | rerun the failed task                       |
| c                    | continue execute next                       |
| q                    | abort the play                              |
| help                 | show help message                           |
```
variables supported by the debugger
```
| Command     | Description                            |
|-------------|----------------------------------------|
| p task      | the name of the failed task            |
| p task.args | The module arguments                   |
| p result    | The result returned by the failed task |
| p vars      | Value of all known variables           |
| p vars[key] | Value of one variable                  |
```
## The Assert Module
The `assert` module will fail with an error if a specified condition is not met. For example, to fail the playbook if there’s no `eth1` interface:
```
- name: assert that eth1 interface exists
  assert:
    that: ansible_eth1 is defined
```
When debugging a playbook, it can be helpful to insert assertions so that a failure happens as soon as any assumption you’ve made has been violated.

Keep in mind that the code in an `assert` statement is Jinja2, not Python.

## Checking Your Playbook Before Execution
The `ansible-playbook` command supports several flags that allow you to sanity check your playbook before you execute it.

### Syntax Check
The `--syntax-check` flag checks that your playbook’s syntax is valid, but it does not execute it.
```
ansible-playbook --syntax-check -i <host file> playbook.yml
```
### List Hosts
The --list-hosts flag outputs the hosts that the playbook will run against, but it does not execute the playbook.
```
ansible-playbook --list-hosts -i <host file> playbook.yml
```
### List Tasks
Outputs the tasks that the playbook will run against. It does not execute the playbook.
```
ansible-playbook --list-tasks -i <host file> playbook.yml
```
### Check Mode
The `-C` and `--check` flags run Ansible in check mode (sometimes known as *dry-run*), which tells you whether each task in the playbook will modify the host, but does not make any changes to the server.

```
ansible-playbook --check -i <host file> playbook.yml
```
One of the challenges with using check mode is that later parts of a playbook might succeed only if earlier parts of the playbook were executed.

### Diff (Show File Changes)
The `-D` and `-diff` flags output differences for any files that are changed on the remote machine. It’s a helpful option to use in conjunction with `--check` to show how Ansible would change the file if it were run normally:
```
ansible-playbook --diff --check -i <host file> playbook.yml
```
If Ansible would modify any files (e.g., using modules such as `copy`, `template`, and `lineinfile`), it will show the changes in .diff format, like this:
```
TASK: [set the gunicorn config file] ******************************************
--- before: /home/vagrant/mezzanine-example/project/gunicorn.conf.py
+++ after: /Users/lorin/dev/ansiblebook/ch06/playbooks/templates/gunicor
n.conf.py.j2
@@ -1,7 +1,7 @@
 from __future__ import unicode_literals
 import multiprocessing

 bind = "127.0.0.1:8000"
 workers = multiprocessing.cpu_count() * 2 + 1
-loglevel = "error"
+loglevel = "warning"
 proc_name = "mezzanine-example"
```

## Limiting Which Tasks Run
Sometimes you don’t want Ansible to run every single task in your playbook, particularly when you’re first writing and debugging the playbook. Ansible provides several command-line options that let you control which tasks run.

### Step
The `--step` flag, shown in [Example 16-7](https://learning.oreilly.com/library/view/ansible-up-and/9781491979792/ch16.html#step-example), has Ansible prompt you before running each task, like this:
```
Perform task: install packages (y/n/c):
```
You can choose to execute the task (y), skip it (n), or tell Ansible to continue running the rest of the playbook without prompting you (c).
```
ansible-playbook -i <host file> --step playbook.yml
```

### Start-at-Task
The `--start-at-task taskname` flag tells Ansible to start running the playbook at the specified task, instead of at the beginning. This can be handy if one of your tasks failed because there was a bug in one of your tasks, and you want to rerun your playbook starting at the task you just fixed.
```
ansible-playbook -i <host file> --start-at-task="install packages" playbook.yml
```
### Tags
Ansible allows you to add one or more tags to a task or a play. For example, here’s a play that’s tagged with `foo` and a task that’s tagged with `bar` and `quux`:
```
- hosts: myservers
  tags:
   - foo
  tasks:
   - name: install editors
     apt: name={{ item }}
     with_items:
       - vim
       - emacs
       - nano

   - name: run arbitrary command
     command: /opt/myprog
     tags:
       - bar
       - quux
```
Use the `-t tagnames` or `--tags tagnames` flag to tell Ansible to run only plays and tasks that have certain tags. Use the `--skip-tags tagnames` flag to tell Ansible to skip plays and tasks that have certain tags.
```
ansible-playbook -t foo,bar playbook.yml
ansible-playbook --tags=foo,bar playbook.yml
ansible-playbook --skip-tags=baz,quux playbook.yml
```


]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>ansible</tag>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>How Linux Works, 2nd Edition</title>
    <url>/2019/04/19/book-how-linux-works/</url>
    <content><![CDATA[Great book for all Linux developers and administrators! Just note for future quick revisit!

## Chapter1. The Big picture
The most effective way to understand how an operating system works is through abstraction—a fancy way of saying that you can ignore most of the details.

The `kernel` is software residing in memory that tells the CPU what to do. The kernel manages the hardware and acts primarily as an interface between the hardware and any running program.

Processes—the running programs that the kernel manages—collectively make up the system’s upper level, called `user space`.

```
+-------------------------------------------------------------------------+
|                                                                         |
|  User process                                                           |
|                                                                         |
|   +-------------------+   +-----------------+    +---------------+      |
|   |  GUI              |   |  Servers        |    |  Shell        |      |
|   |                   |   |                 |    |               |      |
|   +-------------------+   +-----------------+    +---------------+      |
|                                                                         |
+-------------------------------------------------------------------------+


+-------------------------------------------------------------------------+
|                                                                         |
|  Linux kernel                                                           |
|  +--------------+     +--------------------------+                      |
|  |  system calls|     |   process management     |                      |
|  +--------------+     +--------------------------+                      |
|                                                                         |
|  +---------------------+        +-------------------------------+       |
|  |  device driver      |        |   memory management           |       |
|  +---------------------+        +-------------------------------+       |
+-------------------------------------------------------------------------+

+-------------------------------------------------------------------------+
|                                                                         |
|  Hardware                                                               |
|                                                                         |
|  +-------------------+   +-------------------+   +---------------+      |
|  |   CPU             |   |    RAM            |   |  Disk         |      |
|  +-------------------+   +-------------------+   +---------------+      |
|  +---------------------+                                                |
|  |   Network           |                                                |
|  +---------------------+                                                |
+-------------------------------------------------------------------------+

```
There is a critical difference between the ways that the kernel and user processes run: The kernel runs in kernel mode, and the user processes run in user mode. Code running in kernel mode has unrestricted access to the processor and main memory. This is a powerful but dangerous privilege that allows a kernel process to easily crash the entire system. The area that only the kernel can access is called kernel space.

User mode, in comparison, restricts access to a (usually quite small) subset of memory and safe CPU operations. User space refers to the parts of main memory that the user processes can access. If a process makes a mistake and crashes, the consequences are limited and can be cleaned up by the kernel. This means that if your web browser crashes, it probably won’t take down the scientific computation that you’ve been running in the background for days.

### Hardware
A CPU is just an operator on memory; it reads its instructions and data from the memory and writes data back out to the memory.

You’ll often hear the term `state` in reference to memory, processes, the kernel, and other parts of a computer system. Strictly speaking, a state is a particular arrangement of bits. For example, if you have four bits in your memory, 0110, 0001, and 1011 represent three different states.

The term `image` refers to a particular physical arrangement of bits.

### Kernel
Nearly everything that the kernel does revolves around main memory. One of the kernel’s tasks is to split memory into many subdivisions, and it must maintain certain state information about those subdivisions at all times. Each process gets its own share of memory, and the kernel must ensure that each process keeps to its share.

The kernel is in charge of managing tasks in four general system areas:
**Processes**. The kernel is responsible for determining which processes are allowed to use the CPU.

**Memory**. The kernel needs to keep track of all memory—what is currently allocated to a particular process, what might be shared between processes, and what is free.

**Device drivers**. The kernel acts as an interface between hardware (such as a disk) and processes. It’s usually the kernel’s job to operate the hardware.

**System calls and support**. Processes normally use system calls to communicate with the kernel.

The act of one process giving up control of the CPU to another process is called a **context switch**.

The kernel is responsible for context switching. To understand how this works, let’s think about a situation in which a process is running in user mode but its time slice is up. Here’s what happens:
1.  The CPU (the actual hardware) interrupts the current process based on an internal timer, switches into kernel mode, and hands control back to the kernel.
2.  The kernel records the current state of the CPU and memory, which will be essential to resuming the process that was just interrupted.
3.  The kernel performs any tasks that might have come up during the preceding time slice (such as collecting data from input and output, or I/O, operations).
4.  The kernel is now ready to let another process run. The kernel analyzes the list of processes that are ready to run and chooses one.
5.  The kernel prepares the memory for this new process, and then prepares the CPU.
6.  The kernel tells the CPU how long the time slice for the new process will last.
7.  The kernel switches the CPU into user mode and hands control of the CPU to the process.

The context switch answers the important question of **when** the kernel runs. The answer is that it runs **between** process time slices during a context switch.

Modern CPUs include a `memory management unit (MMU)` that enables a memory access scheme called `virtual memory`. When using virtual memory, a process does not directly access the memory by its physical location in the hardware. Instead, the kernel sets up each process to act as if it had an entire machine to itself. When the process accesses some of its memory, the MMU intercepts the access and uses a memory address map to translate the memory location from the process into an actual physical memory location on the machine. The kernel must still initialize and continuously maintain and alter this memory address map. For example, during a context switch, the kernel has to change the map from the outgoing process to the incoming process.

> The implementation of a memory address map is called a page table.

The kernel’s role with devices is pretty simple. A device is typically accessible only in kernel mode because improper access (such as a user process asking to turn off the power) could crash the machine. Another problem is that different devices rarely have the same programming interface, even if the devices do the same thing, such as two different network cards. Therefore, device drivers have traditionally been part of the kernel.

There are several other kinds of kernel features available to user processes. For example, `system calls` (or syscalls) perform specific tasks that a user process alone cannot do well or at all. For example, the acts of opening, reading, and writing files all involve system calls.

Other than `init`, **all** user processes on a Linux system start as a result of `fork()`, and most of the time, you also run `exec()` to start a new program instead of running a copy of an existing process.

### User Space
As mentioned earlier, the main memory that the kernel allocates for user processes is called `user space`. Because a process is simply a state (or image) in memory, user space also refers to the memory for the entire collection of running processes. 

### Users
A `user` is an entity that can run processes and own files. A user is associated with a username. For example, a system could have a user named billyjoe. However, the kernel does not manage the usernames; instead, it identifies users by simple numeric identifiers called userids.

Users exist primarily to support permissions and boundaries.

In addition, as powerful as the **root** user is, it still runs in the operating system’s user mode, not kernel mode.

`Groups` are sets of users. The primary purpose of groups is to allow a user to share file access to other users in a group.

## Chapter 2. Basic Commands and Directory Hierarchy
Some resources:
`<<UNIX for the Impatient>>`
`<<Learning the UNIX Operating System>>`

The `shell` is one of the most important parts of a Unix system. A shell is a program that runs commands. The shell also serves as a small programming environment.

Many important parts of the system are actually `shell scripts`—text files that contain a sequence of shell commands. 

There are many different Unix shells, but all derive several of their features from the `Bourne shell` (/bin/sh), a standard shell developed at Bell Labs for early versions of Unix. Every Unix system needs the Bourne shell in order to function correctly, as you will see throughout this book.

Linux uses an enhanced version of the Bourne shell called `bash` or the “Bourne-again” shell. The bash shell is the default shell on most Linux distributions, and /bin/sh is normally a link to bash on a Linux system. 

> `cat` command: The command is called cat because it performs concatenation when it prints the contents of more than one file.

Pressing `CTRL-D` on an empty line stops the current standard input entry from the terminal (and often terminates a program). Don’t confuse this with `CTRL-C`, which terminates a program regardless of its input or output.

> Unix filenames do not need extensions and often do not carry them.

`shell globs` don’t match dot files unless you explicitly use a pattern such as `.*`. This is why `rm -rf ./*` doesn't remove hidden objects.

> You can run into problems with globs because `.*` matches `.` and `..` (the current and parent directories)

The shell can store temporary variables, called `shell variables`, containing the values of text strings. Shell variables are very useful for keeping track of values in scripts, and some shell variables control the way the shell behaves.

An `environment variable` is like a shell variable, but it’s not specific to the shell. All processes on Unix systems have environment variable storage. The main difference between environment and shell variables is that the operating system passes all of your shell’s `environment variables` to programs that the shell runs (for example, the sub-script), whereas shell variables cannot be accessed in the commands that you run.

Assign an environment variable with the shell’s `export` command. For example, if you’d like to make the `$STUFF` shell variable into an environment variable, use the following:
```
STUFF=123
export STUFF
```
`PATH` is a special environment variable that contains the `command path` (or path for short). A command path is a list of system directories that the shell searches when trying to locate a command.

resource:
`<<Learning the vi and Vim Editor>>`

Some `kill process` ways.There are many types of signals. The default is `TERM`, or terminate.
```
kill -STOP pid
kill -CONT pid
kill -KILL pid  # the same as kill -9 pid
```

To see if you’ve accidentally suspended any processes on your current terminal, run the `jobs` command.

You can detach a process from the shell and put it in the “background” with the ampersand `&`. The best way to make sure that a background process doesn’t bother you is to redirect its output (and possibly input).

Some executable files have an `s` in the `owner permissions` listing instead of an x. This indicates that the executable is `setuid`, meaning that when you execute the program, it runs as though the file owner is the user instead of you. Many programs use this `setuid` bit to run as root in order to get the privileges they need to change system files. One example is the `passwd` program, which needs to change the `/etc/passwd` file.

Directories also have permissions. You can list the contents of a directory if it’s readable, but you can only access a file in a directory if the directory is `executable`. (One common mistake people make when setting the permissions of directories is to accidentally remove the execute permission when using absolute modes.)

You can specify a set of default permissions with the `umask (user file-creation mode mask) `shell command, which applies a predefined set of permissions to any new file you create. In general, use `umask 022` if you want everyone to be able to see all of the files and directories that you create, and use `umask 077` if you don’t. (You’ll need to put the umask command with the desired mode in one of your startup files to make your new default permissions apply to later sessions).

> How to calculate the `umask`?
> 
> For directories, the base permissions are (rwxrwxrwx) `0777` and for files they are `0666` (rw-rw-rw).
> 
> You can simply subtract the umask from the base permissions to determine the final permission for file as follows:
666 – 022 = 644
> subtract to get permissions of new file (666-022) : 644 (rw-r–r–)
>
> You can simply subtract the umask from the base permissions to determine the final permission for directory as follows:
777 – 022 = 755
> Subtract to get permissions of new directory (777-022) : 755 (rwxr-xr-x)

Another compression program in Unix is `bzip2`, whose compressed files end with `.bz2`. While marginally slower than gzip, bzip2 often compacts text files a little more, and it is therefore increasingly popular in the distribution of source code. 

The `bzip2` compression/decompression option for tar is `j`:
```
tar jcvf xx.bz2 file...
tar jxvf xx.bz2
```

### Linux Directory Hierarchy Essentials
Simplified overview of the hierarchy
```
                                                  +---------+
                                                  |   /     |
                                                  +-----+---+
                                                        |
    +-------------+-------------+-----------+----------------------+-----------+-----------+----------+
    |             |             |           |           |          |           |           |          |
    |             |             |           |           |          |           |           |          |
    |             |             |           |           |          |           |           |          |
    v             v             v           v           v          v           v           v          v
+---+----+    +---+----+   +----+---+  +----+---+  +----+---+  +---+---+  +----+---+  +----+----+ +---------+
|  /bin  |    |  /dev  |   |  /etc  |  |   /usr |  | /home  |  | /lib  |  |  /sbin |  |   /tmp  | |  /var   |
+--------+    +--------+   +--------+  +----+---+  +--------+  +-------+  +--------+  +---------+ +----+----+
                                            |                                                          |
                                            |                                                     +----+-----+
                                            |                                                     |          |
              +---------+----------+--------------------+----------+                              |          |
              |         |          |        |           |          |                              |          |
              v         v          v        v           v          v                              v          v
         +----+--+  +---+--+  +----+---+  +-+----+  +---+---+  +---+---+                     +----+---+  +---+----+
         | bin/  |  | man/ |  |  lib/  |  |local/|  | sbin/ |  | share/|                     | log/   |  |  /tmp  |
         +-------+  +------+  +--------+  +------+  +-------+  +-------+                     +--------+  +--------+

```

* `/bin` Contains ready-to-run programs (also known as an executables), including most of the basic Unix commands such as ls and cp. Most of the programs in /bin are in binary format, having been created by a C compiler, but some are shell scripts in modern systems.

* `/dev` Contains device files.

* `/etc` This core system configuration directory contains the user password, boot, device, networking, and other setup files. Many items in /etc are specific to the machine’s hardware. 

* `/home` Holds personal directories for regular users. 

* `/lib` An abbreviation for library, this directory holds library files containing code that executables can use.

* `/proc` Provides system statistics through a browsable directory-and-file interface. The `/proc` directory contains information about currently running processes as well as some kernel parameters.

* `/sys` This directory is similar to /proc in that it provides a device and system interface.

* `/sbin` The place for system executables. Programs in /sbin directories relate to system management.

* `/tmp` A storage area for smaller, temporary files that you don’t care much about. If something is extremely important, don’t put it in /tmp because most distributions clear /tmp when the machine boots and some even remove its old files periodically. Also, don’t let /tmp fill up with garbage because its space is usually shared with something critical  

* `/usr` Although pronounced “user,” this subdirectory has no user files. Instead, it contains a large directory hierarchy, including the bulk of the Linux system. Many of the directory names in /usr are the same as those in the root directory (like /usr/bin and /usr/lib), and they hold the same type of files. (The reason that the root directory does not contain the complete system is primarily historic—in the past, it was to keep space requirements low for the root.)

* `/var` The variable subdirectory, where programs record runtime information. System logging, user tracking, caches, and other files that system programs create and manage are here.

* `/boot` Contains kernel boot loader files. These files pertain only to the very first stage of the Linux startup procedure.

* `/media` A base attachment point for removable media such as flash drives that is found in many distributions.

* `/opt` This may contain additional third-party software. 

### Kernel Location
On Linux systems, the kernel is normally in `/vmlinuz` or `/boot/vmlinuz`. A boot loader loads this file into memory and sets it in motion when the system boots.

Once the boot loader runs and sets the kernel in motion, the main kernel file is no longer used by the running system. However, you’ll find many modules that the kernel can load and unload on demand during the course of normal system operation. Called loadable kernel modules, they are located under `/lib/modules`.

## Chapter 3. Devices
It’s important to understand how the kernel interacts with user space when presented with new devices. The `udev` system enables user-space programs to automatically configure and use new devices. 

> `udev` (userspace /dev) is a device manager for the Linux kernel. As the successor of devfsd and hotplug, udev primarily manages device nodes in the /dev directory.

### Device Files
It is easy to manipulate most devices on a Unix system because the kernel presents many of the device I/O interfaces to user processes as **files**. These device files are sometimes called `device nodes`. Not only can a programmer use regular file operations to work with a device, but some devices are also accessible to standard programs like `cat`. However, not all devices or device capabilities are accessible with standard file I/O.

Device files are in the `/dev` directory, and running `ls /dev` reveals more than a few files in `/dev`.

if run 
```
ls -l
brw-rw----     1 root disk 8, 1 Sep  6 08:37 sda1
crw-rw-rw-     1 root root 1, 3 Sep  6 08:37 null
prw-r--r--     1 root root    0 Mar  3 19:17 fdata
srw-rw-rw-     1 root root    0 Dec 18 07:43 log
```
if the first char in file mode is `b`, `c`, `p`, or `s`, the file is a device. These letters stand for block, character, pipe, and socket, respectively.

The numbers before the dates in the first two lines are the `major` and `minor` device numbers that help the kernel identify the device. Similar devices usually have the same major number.

#### Block device
Programs access data from a block device in fixed chunks. The sda1 in the preceding example is a disk device, a type of block device.

#### Character device
Character devices work with data streams. Printers directly attached to your computer are represented by character devices. It’s important to note that during character device interaction, the kernel cannot back up and reexamine the data stream after it has passed data to a device or process.

#### Pipe device
Named pipes are like character devices, with another process at the other end of the I/O stream instead of a kernel driver.

### Socket device
Sockets are special-purpose interfaces that are frequently used for `interprocess communication`. 

> Not all devices have device files because the block and character device I/O interfaces are not appropriate in all cases. For example, `network interfaces` don’t have device files. It is theoretically possible to interact with a network interface using a single character device, but because it would be exceptionally difficult, the kernel uses other I/O interfaces.

### The sysfs Device Path
To provide a uniform view for attached devices based on their actual hardware attributes, the Linux kernel offers the `sysfs` interface through a system of files and directories. The base path for devices is `/sys/devices` (this is a real directory!).
```
ls -ltr /sys/devices/

total 0
drwxr-xr-x 21 root root 0 Apr 25 23:18 virtual
drwxr-xr-x  3 root root 0 Apr 25 23:18 tracepoint
drwxr-xr-x 10 root root 0 Apr 25 23:18 system
drwxr-xr-x  3 root root 0 Apr 25 23:18 software
drwxr-xr-x  8 root root 0 Apr 25 23:18 pnp0
drwxr-xr-x  9 root root 0 Apr 25 23:18 platform
drwxr-xr-x 15 root root 0 Apr 25 23:18 pci0000:00
drwxr-xr-x  5 root root 0 Apr 25 23:18 msr
drwxr-xr-x  6 root root 0 Apr 25 23:18 LNXSYSTM:00
drwxr-xr-x  3 root root 0 Apr 25 23:18 breakpoint
```

The `/dev` file is there so that user processes can use the device, whereas the `/sys/devices` path is used to view information and manage the device. In `/dev` you can run:
```
udevadm info --query=all --name=/dev/null

P: /devices/virtual/mem/null
N: null
E: DEVMODE=0666
E: DEVNAME=/dev/null
E: DEVPATH=/devices/virtual/mem/null
E: MAJOR=1
E: MINOR=3
E: SUBSYSTEM=mem
```
this command will show the sysfs location `/devices/virtual/mem/null`

### dd and Devices
The program `dd` is extremely useful when working with block and character devices. This program’s sole function is to read from an input file or stream and write to an output file or stream, possibly doing some encoding conversion on the way.

I am not using it.

### Device Name Summary
Not necessarily as described below, may be some variations:

*  Hard Disks: /dev/sd*

Most hard disks attached to current Linux systems correspond to device names with an `sd` prefix, such as `/dev/sda`, `/dev/sdb`, and so on. These devices represent entire disks; the kernel makes separate device files, such as `/dev/sda1` and `/dev/sda2`, for the partitions on a disk.

> The `sd` portion of the name stands for SCSI disk.

Linux assigns devices to device files in the **order** in which its drivers encounter devices. This may cause problem when you remove one disk and insert another, because the device name changed for old disk. Most modern Linux systems use the Universally Unique Identifier (`UUID`) for persistent disk device access.

* CD and DVD Drives: /dev/sr*

Linux recognizes most optical storage drives as the SCSI devices /dev/sr0, /dev/sr1, and so on. 

* PATA Hard Disks: /dev/hd*

* Terminals: /dev/tty*, /dev/pts/*, and /dev/tty

Terminals are devices for moving characters between a user process and an I/O device, usually for text output to a terminal screen. 

`Pseudoterminal` devices are emulated terminals that understand the I/O features of real terminals. 

Two common terminal devices are `/dev/tty1` (the first virtual console) and `/dev/pts/0` (the first pseudoterminal device). The `/dev/tty` device is the controlling terminal of the current process.

> teletypewriter, `tty` in shorthand

I am always confused, at least you need to know **shell** is the command line interpreter!
[What is the difference between Terminal, Console, Shell, and Command Line?](https://askubuntu.com/questions/506510/what-is-the-difference-between-terminal-console-shell-and-command-line)

Linux has two primary display modes: `text mode` and an `X Window System server` (graphics mode, usually via a display manager). Although Linux systems traditionally booted in text mode, most distributions now use kernel parameters and interim graphical display mechanisms to completely hide text mode as the system is booting. In such cases, the system switches over to full graphics mode near the end of the boot process.

OK, skip rest of the content in Chapter 3.

## Chapter 4. Disks and Filesystems

Schematic of a typical Linux disk:

![](https://drive.google.com/uc?id=1e0ziORmoEx6zPk6J6qEid26aWl-zw21G)

`Partitions` are subdivisions of the whole disk. On Linux, they’re denoted with a number after the whole block device, and therefore have device names such as `/dev/sda1` and `/dev/sdb3`.

Partitions are defined on a small area of the disk called a `partition table`.

The next layer after the partition is the `filesystem`, the database of files and directories that you’re accustomed to interacting with in user space.

To access data on a disk, the Linux kernel uses the system of layers like this:

![](https://drive.google.com/uc?id=1Hy8vWUIBuo33oH976PNEkVzvAOzCuyKJ)

> Notice that you can work with the disk through the filesystem as well as directly through the disk devices.

### Partitioning Disk Devices
> You can view RedHat [Doc](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/ch-partitions) for more information about partition

Let's view the partition table:
```
parted -l

Model: ATA WDC WD3200AAJS-2 (scsi)
Disk /dev/sda: 320GB
Sector size (logical/physical): 512B/512B
Partition Table: msdos

Number   Start   End    Size   Type      File system    Flags
 1       1049kB  316GB  316GB  primary   ext4           boot
 2       316GB   320GB  4235MB extended
 5       316GB   320GB  4235MB logical   linux-swap(v1)

Model: FLASH Drive UT_USB20 (scsi)
Disk /dev/sdf: 4041MB
Sector size (logical/physical): 512B/512B
Partition Table: gpt

Number  Start   End     Size     File system  Name        Flags
 1      17.4kB  1000MB  1000MB                myfirst
 2      1000MB  4040MB  3040MB                mysecond
```
There are 2 different partition tables: MBR (msdos) and GPT (gpt). The MBR table in this example contains primary, extended, and logical partitions. 

### Changing Partition Tables
You can use `parted` command to change partition. Check `/proc/partitions` can get full partition information.
```
cat /proc/partitions

major minor  #blocks  name
 252        0  262144000 vda
 252        1    1048576 vda1
 252        2  260976640 vda2
 253        0  252706816 dm-0
 253        1    8257536 dm-1
```

### Filesystems
The last link between the kernel and user space for disks is typically the file-system; this is what you’re accustomed to interacting with when you run commands such as `ls` and `cd`. As previously mentioned, the **filesystem is a form of database**; it supplies the structure to transform a simple block device into the sophisticated hierarchy of files and subdirectories that users can understand.

#### Filesystem Types

* The `Fourth Extended filesystem (ext4)` is the current iteration of a line of filesystems native to Linux. The `Second Extended filesystem (ext2)` was a longtime default for Linux systems inspired by traditional Unix filesystems such as the Unix File System (UFS) and the Fast File System (FFS). The `Third Extended filesystem (ext3)` added a journal feature (a small cache outside the normal filesystem data structure) to enhance data integrity and hasten booting. The ext4 filesystem is an incremental improvement with support for larger files than ext2 or ext3 support and a greater number of subdirectories.

#### Create a Filesystems
Once you’re done with the partitioning process, you’re ready to create filesystems. As with partitioning, you’ll do this in user space because a user-space process can directly access and manipulate a block device. 

For example, you can create an ext4 partition on /dev/sdf2
```
mkfs -t ext4 /dev/sdf2
```

Filesystem creation is a task that you should only need to perform after adding a new disk or repartitioning an old one. You should create a filesystem just once for each new partition that has no preexisting data (or that has data that you want to remove). Creating a new filesystem on top of an existing filesystem will effectively destroy the old data.

It turns out that mkfs is only a frontend for a series of filesystem creation programs:
```
ls -l /sbin/mkfs.*

-rwxr-xr-x. 1 root root 375240 Mar  7  2017 /sbin/mkfs.btrfs
-rwxr-xr-x  1 root root  37080 Jul 12  2018 /sbin/mkfs.cramfs
-rwxr-xr-x  4 root root  96384 Apr 10  2018 /sbin/mkfs.ext2
-rwxr-xr-x  4 root root  96384 Apr 10  2018 /sbin/mkfs.ext3
-rwxr-xr-x  4 root root  96384 Apr 10  2018 /sbin/mkfs.ext4
-rwxr-xr-x  1 root root  37184 Jul 12  2018 /sbin/mkfs.minix
-rwxr-xr-x. 1 root root 368504 Feb 27  2018 /sbin/mkfs.xfs
```

#### Mounting a Filesystem
On Unix, the process of attaching a filesystem is called `mounting`. When the system boots, the kernel reads some configuration data and mounts root (/) based on the configuration data.

When mounting a filesystem, the common terminology is *mount a device on a mount point.*

To see current system mount status:
```
mount 
...
cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)
/dev/mapper/rhel-root on / type xfs (rw,relatime,attr2,inode64,noquota)
mqueue on /dev/mqueue type mqueue (rw,relatime)
hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime)
/dev/vda1 on /boot type xfs (rw,relatime,attr2,inode64,noquota)
tmpfs on /run/user/0 type tmpfs (rw,nosuid,nodev,relatime,size=800956k,mode=700)
...
```
There are 3 key fields:
* The filesystem's device, such as a disk partition; where the actual file-system data resides
* The filesystem type
* The mount point—that is, the place in the current system’s directory hierarchy where the filesystem will be attached. 

For example, to mount the Fourth Extended filesystem /dev/sdf2 on /home/extra, use this command:
```
mount -t ext4 /dev/sdf2 /home/extra
```
To unmount (detach) a filesystem, use the umount command:
```
umount mountpoint
```
#### Filesystem UUID
You can identify and mount filesystems by their `Universally Unique Identifier (UUID)`, a software standard. The UUID is a type of serial number, and each one should be different.

For example, if you know the UUID of /dev/sdf2 is a9011c2b-1c03-4288-b3fe-8ba961ab0898, so you can mount it as:
```
mount UUID=a9011c2b-1c03-4288-b3fe-8ba961ab0898 /home/extra
```
Here no `-t  ext4` option, because mount know that.

To view a list of devices and the corresponding filesystems and UUIDs on your system, use the blkid (block ID) program:
```
blkid

/dev/sdf2: UUID="a9011c2b-1c03-4288-b3fe-8ba961ab0898" TYPE="ext4"
/dev/sda1: UUID="70ccd6e7-6ae6-44f6-812c-51aab8036d29" TYPE="ext4"
/dev/sda5: UUID="592dcfd1-58da-4769-9ea8-5f412a896980" TYPE="swap"
/dev/sde1: SEC_TYPE="msdos" UUID="3762-6138" TYPE="vfat"
```

For one thing, they’re the preferred way to automatically mount filesystems in `/etc/fstab` at boot time.

#### Disk Buffering, Caching, and Filesystems
Linux, like other versions of Unix, buffers writes to the disk. This means that the kernel usually doesn’t immediately write changes to filesystems when processes request changes. **Instead it stores the changes in RAM until the kernel can conveniently make the actual change to the disk**. This buffering system is transparent to the user and improves performance.

> This is the reason why before we remove the USB, we need to unmount it in case of data lose.

When you unmount a filesystem with umount, the kernel automatically synchronizes with the disk. At any other time, you can force the kernel to write the changes in its buffer to the disk by running the `sync` command.


#### The /etc/fstab Filesystem Table
I encounter this when write `/etc/fstab` file with NFS when developing k8s.
```
/dev/mapper/rhel-root   /                       xfs     defaults        0 0
UUID=a44461e9-e1d7-45fd-a387-255fafd14746 /boot                   xfs     defaults        0 0
/dev/mapper/rhel-swap   swap                    swap    defaults        0 0
halos1.fyre.ibm.com:/data /mnt nfs defaults,timeo=10,retrans=3,rsize=1048576,wsize=1048576 0 0

```
To mount filesystems at boot time and take the drudgery out of the mount command, Linux systems keep a permanent list of filesystems and options in `/etc/fstab`.

* `The device or UUID`. Most current Linux systems no longer use the device in /etc/fstab, preferring the UUID. 

* `The mount point`. Indicates where to attach the filesystem.

* `The filesystem type`.

* `Options`. Use long mount options separated by commas.

* `Backup information for use by the dump command`. You should always use a 0 in this field.

* `The filesystem integrity test order.` To ensure that fsck always runs on the root first, always set this to 1 for the root filesystem and 2 for any other filesystems on a hard disk. Use `0` to disable the bootup check for everything else, including CD-ROM drives, swap, and the /proc file-system

You can also try to mount all entries at once in /etc/fstab that do not contain the noauto option with this command:
```
mount -a
```

Let's see some commonly use options:

* `defaults`. This uses the mount defaults: read-write mode, enable device files, executables, the setuid bit, and so on. Use this when you don’t want to give the filesystem any special options but you do want to fill all fields in /etc/fstab.

* `noauto`. This option tells a mount -a command to ignore the entry. 

#### Filesystem Capacity
To view the size and utilization of your currently mounted filesystems, use the `df` command. 
```
df -BM

Filesystem                1M-blocks   Used Available Use% Mounted on
/dev/mapper/rhel-root       245640M 75636M   170005M  31% /
devtmpfs                      7931M     0M     7931M   0% /dev
tmpfs                         7943M     0M     7943M   0% /dev/shm
tmpfs                         7943M   835M     7109M  11% /run
tmpfs                         7943M     0M     7943M   0% /sys/fs/cgroup
/dev/vda1                     1014M   183M      832M  19% /boot
...
```

#### Checking and Repairing Filesystems
Filesystem errors are usually due to a user shutting down the system in a rude way (for example, by pulling out the power cord). In such cases, the filesystem cache in memory may not match the data on the disk, and the system also may be in the process of altering the filesystem when you happen to give the computer a kick. Although a new generation of filesystems supports journals to make filesystem corruption far less common, you should always shut the system down properly. And regardless of the filesystem in use, filesystem checks are still necessary every now and to maintain sanity.

The tool to check a filesystem is `fsck`.

In the worst cases, you can try:
* You can try to extract the entire filesystem image from the disk with `dd` and transfer it to a partition on another disk of the same size.

* You can try to patch the filesystem as much as possible, mount it in read-only mode, and salvage what you can.

* You can try `debugfs`.

#### Special-Purpose Filesystems
Not all filesystems represent storage on physical media. Specifically, most versions of Unix have filesystems that serve as system interfaces. That is, rather than serving only as a means to store data on a device, a filesystem can represent system information such as process IDs and kernel diagnostics.

The special filesystem types in common use on Linux include the following:
* `proc`. Mounted on /proc. The name proc is actually an abbreviation for process. Each numbered directory inside /proc is actually the process ID of a current process on the system; the files in those directories represent various aspects of the processes. The file /proc/self represents the current process. 

* `sysfs`. Mounted on /sys.

* `tmpfs`. Mounted on /run and other locations. With tmpfs, you can use your physical memory and swap space as temporary storage, stored in volatile memory instead of a persistent storage device.

### Swap Space
Not every partition on a disk contains a filesystem. It’s also possible to augment the RAM on a machine with disk space. The disk area used to store memory pages is called `swap space` (or just swap for short).

you can use `free` command to see the swap usage:
```
free -m

              total        used        free      shared  buff/cache   available
Mem:          32010       10894        3992        1605       17123       18811
Swap:          8063          64        7999
```

you can use a disk partition and a regular file as swap space, for disk:
1. Ensure partition is empty
2. Run `mkswap dev`, dev is the partition device
3. Execute `swapon dev` to register the space with the kernel.
4. Register in `/etc/fstab` file

Use these commands to create an empty file, initialize it as swap, and add it to the swap pool:
```
dd if=/dev/zero of=swap_file bs=1024k count=num_mb
mkswap swap_file
swapon swap_file
```
Here, `swap_file` is the name of the new swap file, and `num_mb` is the desired size, in megabytes.

To remove a swap partition or file from the kernel’s active pool, use the `swapoff` command.

**Note** some administrators configure certain systems with **no** swap space at all. For example, high-performance network servers should never dip into swap space and should avoid disk access if at all possible.

It’s dangerous to do this on a general-purpose machine. If a machine completely runs out of both real memory and swap space, the Linux kernel invokes the `out-of-memory (OOM)` killer to kill a process in order to free up some memory. You obviously don’t want this to happen to your desktop applications. On the other hand, high-performance servers include sophisticated monitoring and load-balancing systems to ensure that they never reach the danger zone.

### Looking Forward: Disks and User Space

In disk-related components on a Unix system, the boundaries between user space and the kernel can be difficult to characterize. As you’ve seen, the kernel handles raw block I/O from the devices, and user-space tools can use the block I/O through device files. However, user space typically uses the block I/O only for initializing operations such as partitioning, file-system creation, and swap space creation.

In normal use, user space uses only the filesystem support that the kernel provides on top of the block I/O.

## Chapter 5. How the Linux Kernel Boots
You’ll learn how the kernel moves into memory up to the point where the first user process starts.

**A simplified view of the boot process looks like this:**
1. The machine’s BIOS or boot firmware loads and runs a boot loader.
2. The boot loader finds the kernel image on disk, loads it into memory, and starts it.
3. The kernel initializes the devices and its drivers.
4. The kernel mounts the root filesystem.
5. The kernel starts a program called init with a process ID of 1. This point is the user space start.
6. init sets the rest of the system processes in motion.
7. At some point, init starts a process allowing you to log in, usually at the end or near the end of the boot.

### Startup Messages
There are two ways to view the kernel’s boot and runtime diagnostic messages:

* Look at the kernel system log file. You’ll often find this in `/var/log/ kern.log`, but depending on how your system is configured, it might also be lumped together with a lot of other system logs in `/var/log/messages` or elsewhere.

* Use the `dmesg` command, but be sure to pipe the output to less because there will be much more than a screen’s worth. The `dmesg` command uses the kernel ring buffer, which is of limited size, but most newer kernels have a large enough buffer to hold boot messages for a long time.

### Kernel Initialization and Boot Options
Upon startup, the Linux kernel initializes in this general order:
1. CPU inspection
2. Memory inspection
3. Device bus discovery
4. Device discovery
5. Auxiliary kernel subsystem setup (networking, and so on)
6. Root filesystem mount
7. User space start

The following memory management messages are a good indication that the user-space handoff is about to happen because this is where the kernel protects its own memory from user-space processes:
```
[    0.972934] Freeing unused kernel memory: 1844k freed
[    0.973411] Write protecting the kernel read-only data: 12288k
[    0.975623] Freeing unused kernel memory: 832k freed
[    0.977405] Freeing unused kernel memory: 676k freed
```

### Kernel Parameters
I just encountered an issue about kernel parameters for Db2... Let's see.

When running the Linux kernel, the boot loader passes in a set of text-based **kernel parameters** that tell the kernel how it should start. The parameters specify many different types of behavior, such as the amount of diagnostic output the kernel should produce and device driver–specific options.

You can view the kernel parameters from your system’s boot by looking at the `/proc/cmdline` file:
```
BOOT_IMAGE=/vmlinuz-3.10.0-862.14.4.el7.x86_64 root=/dev/mapper/rhel-root ro crashkernel=auto rd.lvm.lv=rhel/root rd.lvm.lv=rhel/swap rhgb quiet elevator=noop LANG=en_US.UTF-8

```
The `root=/dev/mapper/rhel-root` is where root filesystem resides.

### Boot Loader
[Other boot loader intro](https://searchdatacenter.techtarget.com/definition/boot-loader-boot-manager)

At the start of the boot process, before the kernel and init start, a boot loader starts the kernel. The task of a boot loader sounds simple: It loads the kernel into memory, and then starts the kernel with a set of kernel parameters.

Kernel and its parameters are usually somewhere on the root filesystem.

On PCs, boot loaders use the `Basic Input/Output System (BIOS)` or `Unified Extensible Firmware Interface (UEFI)` to access disks. Nearly all disk hardware has firmware that allows the BIOS to access attached storage hardware with `Linear Block Addressing (LBA)`. Although it exhibits poor performance, this mode of access does allow universal access to disks. Boot loaders are often the only programs to use the BIOS for disk access; the kernel uses its own high-performance drivers.

Most modern boot loaders can read partition tables and have built-in support for read-only access to filesystems. 

#### Boot loader tasks
1. Select among multiple kernels.
2. Switch between sets of kernel parameters.
3. Allow the user to manually override and edit kernel image names and parameters 
4. Provide support for booting other operating systems.

#### Boot loader typres

* GRUB. A near-universal standard on Linux systems (mainly talks about this)
* LILO. One of the first Linux boot loaders.
* LOADLIN. Boots a kernel from MS-DOS

### GRUB Introduction
GRUB stands for Grand Unified Boot Loader. We’ll cover GRUB 2.

This section talks about GRUB menu and look into some boot options, actually, if you check `/boot` directory, you will see kernel image file and initial RAM filesystem:
```
...
-rwxr-xr-x. 1 root root  6381872 Mar 21  2018 vmlinuz-3.10.0-862.el7.x86_64
-rw-r--r--. 1 root root   304926 Mar 21  2018 symvers-3.10.0-862.el7.x86_64.gz
drwx------. 5 root root       97 Oct  1  2018 grub2
-rw-------  1 root root 21096334 Oct  1  2018 initramfs-3.10.0-862.9.1.el7.x86_64.img
...

```

Not interested in the rest of the content in this chapter.

## Chapter 6. How User Space Starts
The point where the kernel starts its first user-space process, init, is significant—not just because that’s where the memory and CPU are finally ready for normal system operation, but because that’s where you can see how the rest of the system builds up as a whole. 

User space is far more modular. It’s much easier to see what goes into the user space startup and operation.

User space starts in roughly this order:

1. init
2. Essential low-level services such as udevd and syslogd
3. Network configuration
4. Mid- and high-level services (cron, printing, and so on)
5. Login prompts, GUIs, and other high-level applications

### Introduction to init
[wiki init](https://en.wikipedia.org/wiki/Init)

The init program is a **user-space program** like any other program on the Linux system, and you’ll find it in `/sbin` along with many of the other system binaries. Its main purpose is to start and stop the essential service processes on the system, but newer versions have more responsibilities.

In my vm `/sbin` directory:
```
lrwxrwxrwx  1 root root          22 Oct  1  2018 init -> ../lib/systemd/systemd
```

There are three major implementations of init in Linux distributions:

* `System V` init. A traditional sequenced init (Sys V, usually *pronounced "sys-five"*). Red Hat Enterprise Linux and several other distributions use this version.
* `systemd`. The emerging standard for init. Many distributions have moved to systemd, and most that have not yet done so are planning to move to it.
* `Upstart`. The init on Ubuntu installations. However, as of this writing, Ubuntu has also planned to migrate to systemd.

There are many different implementations of init because `System V` init and other older versions relied on a sequence that performed only one startup task at a time. `systemd` and `Upstart` attempt to remedy the performance issue by allowing many services to start in parallel thereby speeding up the boot process. 

### System V Runlevels
[wiki Runlevel](https://en.wikipedia.org/wiki/Runlevel)

At any given time on a Linux system, a certain base set of processes is running. In System V init, this state of the machine is called its `runlevel`, which is denoted by a number from 0 through 6. A system spends most of its time in a single runlevel, but when you shut the machine down, init switches to a different runlevel in order to terminate the system services in an orderly fashion and to tell the kernel to stop.

You can check your system’s runlevel with the `who -r` command:
```
who -r

run-level 3  2019-04-17 13:49
```
Runlevels serve various purposes, but the most common one is to distinguish between system startup, shutdown, single-user mode, and console mode states.

But runlevels are becoming a thing of the past. Even though all three init versions in this book support them, systemd and Upstart consider runlevels obsolete as end states for the system. 

### Identifying Your init
* If your system has /usr/lib/systemd and /etc/systemd directories, you have systemd.

* If you have an /etc/init directory that contains several .conf files, you’re probably running Upstart 

* If neither of the above is true, but you have an /etc/inittab file, you’re probably running System V init.

**Here I focus on systemd**

### systemd
The systemd init is one of the newest init implementations on Linux. In addition to handling the regular boot process, systemd aims to incorporate a number of standard Unix services such as cron and inetd. One of its most significant features is its ability to defer the start of services and operating system features until they are necessary.

Let’s outline what happens when systemd runs at boot time:
1. systemd loads its configuration.
2. systemd determines its boot goal, which is usually named default.target.
3. systemd determines all of the dependencies of the default boot goal, dependencies of these dependencies, and so on.
4. systemd activates the dependencies and the boot goal.
5. After boot, systemd can react to system events (such as uevents) and activate additional components.

#### Units and Unit Types
One of the most interesting things about `systemd` is that it does not just operate processes and services; it can also mount filesystems, monitor network sockets, run timers, and more. Each type of capability is called a `unit type`, and each specific capability is called a `unit`. When you turn on a unit, you activate it.

The default boot goal is usually a `target unit` that groups together a number of `service` and `mount` units as dependencies. 

[understand systemd unit and unit files](https://www.digitalocean.com/community/tutorials/understanding-systemd-units-and-unit-files)


#### systemd Dependencies
To accommodate the need for flexibility and fault tolerance, systemd offers a myriad of dependency types and styles:
* `Requires` Strict dependencies. When activating a unit with a Requires dependency unit, systemd attempts to activate the dependency unit. If the dependency unit fails, systemd deactivates the dependent unit.

* `Wants`. Dependencies for activation only. Upon activating a unit, systemd activates the unit’s Wants dependencies, but it doesn’t care if those dependencies fail.

* `Requisite`. Units that must already be active.

* `Conflicts`. Negative dependencies. When activating a unit with a Conflict dependency, systemd automatically deactivates the dependency if it is active.

There are many other dependency syntax, like ordering, conditional, etc...

#### systemd Configuration
The systemd configuration files are spread among many directories across the system, so you typically won’t find the files for all of the units on a system in one place.

That said, there are two main directories for systemd configuration: the system unit directory (globally configured, usually `/usr/lib/systemd/system`) and a system configuration directory (local definitions, usually `/etc/systemd/system`).

> Note: Avoid making changes to the system unit directory because your distribution will maintain it for you. Make your local changes to the system configuration directory.

To see the system unit and configuration directories on your system, use the following commands:
```
pkg-config systemd --variable=systemdsystemunitdir
```

Let's see Unit files in `/usr/lib/systemd/system`, there is a sshd.service file:
```
[Unit]
Description=OpenSSH server daemon
Documentation=man:sshd(8) man:sshd_config(5)
After=network.target sshd-keygen.service
Wants=sshd-keygen.service

[Service]
Type=notify
EnvironmentFile=/etc/sysconfig/sshd
ExecStart=/usr/sbin/sshd -D $OPTIONS
ExecReload=/bin/kill -HUP $MAINPID
KillMode=process
Restart=on-failure
RestartSec=42s

[Install]
WantedBy=multi-user.target
```
The [Unit] section gives some details about the unit and contains description and dependency information. 

You’ll find the details about the service in the [Service] section, including how to prepare, start, and reload the service. 

During normal operation, systemd ignores the [Install] section. However, consider the case when sshd.service is disabled on your system and you would like to turn it on. When you enable a unit, systemd reads the [Install] section.

The [Install] section is usually responsible for the the .wants and .requires directories in the system configuration directory (`/etc/systemd/system`), see:
```
basic.target.wants                                       getty.target.wants           remote-fs.target.wants
default.target                                           local-fs.target.wants        sockets.target.wants
default.target.wants                                     multi-user.target.wants      sysinit.target.wants
dev-virtio\x2dports-org.qemu.guest_agent.0.device.wants  network-online.target.wants  system-update.target.wants
```

the $OPTIONS in unit file is the variable, also specifier is another variable-like feature often found in unit files, like %n and %H.

### systemd Operation
You’ll interact with systemd primarily through the `systemctl` command, which allows you to activate and deactivate services, list status, reload the configuration, and much more.

List of active units:
```
systemctl

  UNIT                                           LOAD   ACTIVE SUB       DESCRIPTION
...
  sys-kernel-debug.mount                         loaded active mounted   Debug File System
  var-lib-nfs-rpc_pipefs.mount                   loaded active mounted   RPC Pipe File System
  brandbot.path                                  loaded active waiting   Flexible branding
...
```
List all units, includes inactives:
```
systemctl --all
```
Get status of a unit:
```
systemctl status sshd.service
```

To activate, deactivate, and restart units, use the systemd `start`, `stop`, and `restart` commands. However, if you’ve changed a unit configuration file, you can tell systemd to reload the file in one of two ways:
```
systemctl reload unit #Reloads just the configuration for unit.
```
```
systemctl daemon-reload #Reloads all unit configurations.
```

#### systemd Process Tracking and Synchronization
systemd wants a reasonable amount of information and control over every process that it starts. The main problem that it faces is that a service can start in different ways; it may fork new instances of itself or even daemonize and detach itself from the original process.

To minimize the work that a package developer or administrator needs to do in order to create a working unit file, systemd uses `control groups (cgroups)`, an optional Linux kernel feature that allows for finer tracking of a process hierarchy.

#### systemd On-Demand and Resource-Parallelized Startup
One of systemd’s most significant features is its ability to delay a unit startup until it is absolutely needed. 

#### systemd Auxiliary Programs
When starting out with systemd, you may notice the exceptionally large number of programs in `/lib/systemd`. These are primarily support programs for units. For example, `udevd` is part of systemd, and you’ll find it there as `systemd-udevd`. Another, the `systemd-fsck` program, works as a middleman between systemd and fsck.

### Shutting Down Your System
init controls how the system shuts down and reboots. The commands to shut down the system are the same regardless of which version of init you run. The proper way to shut down a Linux machine is to use the `shutdown` command.

to shutdown machine immediately:
```
shutdown -h now
```
to reboot the machine now:
```
shutdown -r now
```

When system shutdown time finally arrives, shutdown tells init to begin the shutdown process. On systemd, it means activating the shutdown units; and on System V init, it means changing the runlevel to 0 or 6.

### The Initial RAM Filesystem
The `initramfs` is in `/boot` directory. 
```
ls -ltr | grep init

-rw-------. 1 root root 55376391 Apr 13  2018 initramfs-0-rescue-e57cfe9136e9430587366e04f14195e1.img
-rw-------. 1 root root 13131435 Apr 13  2018 initramfs-3.10.0-862.el7.x86_64kdump.img
-rw-------  1 root root 21098233 Jul 23  2018 initramfs-3.10.0-862.el7.x86_64.img
-rw-------  1 root root 21134858 Oct  1  2018 initramfs-3.10.0-862.14.4.el7.x86_64.img
-rw-------  1 root root 21096334 Oct  1  2018 initramfs-3.10.0-862.9.1.el7.x86_64.img

```
The problem stems from the availability of many different kinds of storage hardware. Remember, the Linux kernel does not talk to the PC BIOS or EFI interfaces to get data from disks, so in order to mount its root file-system, it needs driver support for the underlying storage mechanism.

The workaround is to gather a small collection of kernel driver modules along with a few other utilities into an archive. The boot loader loads this archive into memory before running the kernel.

## Chapter 7. System Configuration
When you first look in the `/etc` directory, you might feel a bit overwhelmed. Although most of the files that you see affect a system’s operations to some extent, a few are fundamental.

### The Structure of /etc
Most system configuration files on a Linux system are found in `/etc`. Historically, each program had one or more configuration files there, and because there are so many packages on a Unix system, /etc would accumulate files quickly.

The trend for many years now has been to place system configuration files into subdirectories under `/etc`. There are still a few individual configuration files in /etc, but for the most part, if you run `ls -F /etc`, you’ll see that most of the items there are now subdirectories.

What kind of configuration files are found in `/etc`? The basic guideline is that customizable configurations for a single machine. And you’ll often find that noncustomizable system configuration files may be found elsewhere, as with the prepackaged systemd unit files in `/usr/lib/systemd`.
 
### System Logging
Most system programs write their diagnostic output to the `syslog` service. The traditional syslogd daemon waits for messages and, depending on the type of message received, funnels the output to a file, the screen, users, or some combination of these, or just ignores it.

#### The System Logger
Most Linux distributions run a new version of syslogd called `rsyslogd` that does much more than simply write log messages to files. For example, in my vm:
```
systemctl status rsyslog

   rsyslog.service - System Logging Service
   Loaded: loaded (/usr/lib/systemd/system/rsyslog.service; enabled; vendor preset: enabled)
   Active: active (running) since Wed 2019-04-17 13:49:14 PDT; 2 weeks 6 days ago
...
```
Many of the files in `/var/log` aren’t maintained by the system logger. The only way to know for sure which ones belong to rsyslogd is to look at its configuration file.

#### Configuration Files
The base rsyslogd configuration file is `/etc/rsyslog.conf`, but you’ll find certain configurations in other directories, such as `/etc/rsyslog.d`.

It talks about the syntax in the configuration file:
The configuration format is a blend of traditional rules and `rsyslog-specific` extensions. One rule of thumb is that anything beginning with a dollar sign ($) is an extension.

### User Management Files
Unix systems allow for multiple independent users. At the kernel level, users are simply numbers (user IDs).

#### The /etc/passwd File
The plaintext file /etc/passwd maps usernames to user IDs.
```
root:x:0:0:Superuser:/root:/bin/sh
######
daemon:*:1:1:daemon:/usr/sbin:/bin/sh
#or
daemon:x:2:2:daemon:/sbin:/sbin/nologin
######
bin:*:2:2:bin:/bin:/bin/sh
sys:*:3:3:sys:/dev:/bin/sh
nobody:*:65534:65534:nobody:/home:/bin/false
juser:x:3119:1000:J. Random User:/home/juser:/bin/bash
beazley:x:143:1000:David Beazley:/home/beazley:/bin/bash
```
The fields are as follows:
* The username.

* The user’s encrypted password. On most Linux systems, the password is not actually stored in the `passwd` file, but rather, in the `shadow` file . Normal users do not have read permission for shadow. The second field in `passwd` or `shadow` is the encrypted password, Unix passwords are never stored as clear text.

* An `x` in the second passwd file field indicates that the encrypted password is stored in the `shadow` file. A `*` indicates that the user cannot log in, and if the field is blank (that is, you see two colons in a row, like ::), no password is required to log in. (Beware of blank passwords. You should never have a user without a password.)

* The user ID (UID), which is the user’s representation in the kernel.

* The group ID (GID). This should be one of the numbered entries in the `/etc/group` file. Groups determine file permissions and little else. This group is also called the user’s `primary` group.

* The user’s real name. You’ll sometimes find commas in this field, denoting room and telephone numbers.

* The user’s home directory.

* The user’s shell (the program that runs when the user runs a terminal session).

#### Special Users
The superuser (root) always has UID 0 and GID 0. Some users, such as daemon, have no login privileges. The nobody user is an underprivileged user. Some processes run as nobody because the nobody user cannot write to anything on the system.

The users that cannot log in are called `pseudo-users`. Although they can’t log in, the system can start processes with their user IDs. Pseudo-users such as nobody are usually created for security reasons.

#### The /etc/shadow File
The shadow password file `/etc/shadow` on a Linux system normally contains user authentication information, including the encrypted passwords and password expiration information that correspond to the users in `/etc/passwd`.

Regular users interact with `/etc/passwd` using the `passwd` command. By default, `passwd` changes the user’s password. The `passwd` command is an `suid-root` program, because only the superuser can change the `/etc/passwd` file.
```
-rwsr-xr-x. 1 root root 27832 Jan 29  2014 /usr/bin/passwd
```

> in `/etc/shells` file have multiple shell types:
> ```
> /bin/sh
> /bin/bash
> /sbin/nologin
> /usr/bin/sh
> /usr/bin/bash
> /usr/sbin/nologin
> /bin/ksh
> /bin/rksh
> ```

Because /etc/passwd is plaintext, the superuser may use any text editor to make changes. To add a user, simply add an appropriate line and create a home directory for the user; to delete, do the opposite. However, to edit the file, you’ll most likely want to use the `vipw` program

Use `adduser` and `userde`l to add and remove users. Run `passwd user` as the superuser.

#### Working with Groups
Groups in Unix offer a way to **share** files with certain users but deny access to all others. The idea is that you can set read or write permission bits for a particular group, excluding everyone else.

The `/etc/group` file defines the group IDs:
```
root:*:0:juser
daemon:*:1:
bin:*:2:
disk:*:6:juser,beazley
nogroup:*:65534:
user:*:1000:
```
* The group name.

* The group password. This is hardly ever used, nor should you use it. Use * or any other default value.

* The group ID (a number). The GID must be unique within the group file. This number goes into a user’s group field in that user’s `/etc/passwd` entry.

* An optional list of users that belong to the group. In addition to the users listed here, users with the corresponding group ID in their passwd file entries also belong to the group.

> Linux distributions often create a new group for each new user added, with the same name as the user.

### Setting the Time
Unix machines depend on accurate timekeeping. The kernel maintains the system clock, which is the clock that is consulted when you run commands like date.

PC hardware has a battery-backed `real-time clock (RTC)`. The RTC isn’t the best clock in the world, but it’s better than nothing. The kernel usually sets its time based on the RTC at boot time, and you can reset the system clock to the current hardware time with `hwclock`.

You should not try to fix the time drift with `hwclock` because time-based system events can get lost or mangled. Usually it’s best to keep your system time correct with a network time daemon.

#### Network Time
If your machine is permanently connected to the Internet, you can run a `Network Time Protocol (NTP)` daemon to maintain the time using a remote server. Many distributions have built-in support for an NTP daemon, but it may not be enabled by default. You might need to install an ntpd package to get it to work.

### Scheduling Recurring Tasks with cron
The Unix cron service runs programs repeatedly on a fixed schedule. Most experienced administrators consider cron to be **vital** to the system because it can perform automatic system maintenance. For example, cron runs log file rotation utilities to ensure that your hard drive doesn’t fill up with old log files. You should know how to use cron because it’s just plain useful.

> Also see `cronjob` in k8s [doc](https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/).

You can run any program with `cron` at whatever times suit you. The program running through cron is called a `cron job`. To install a cron job, you’ll create an entry line in your `crontab` file, usually by running the `crontab` command.

for example:
```
15 09 * * * /home/juser/bin/spmake
```
* Minute (0 through 59). The cron job above is set for minute 15.

* Hour (0 through 23). The job above is set for the ninth hour.

* Day of month (1 through 31).

* Month (1 through 12).

* Day of week (0 through 7). The numbers 0 and 7 are Sunday.

A `*` in any field means to match every value. The preceding example runs spmake daily because the day of month, month, and day of week fields are all filled with stars, which cron reads as “run this job every day, of every month, of every week.”

also can be 5th and the 14th day of each month:
```
15 09 5,14 * * /home/juser/bin/spmake
```

#### Installing Crontab Files
Each user can have his or her own crontab file, which means that every system may have multiple crontabs, usually found in `/var/spool/cron/` folder. the `crontab` command installs, lists, edits, and removes a user’s crontab.

The easiest way to install a crontab is to put your crontab entries into a file and then use `crontab file` to install file as your current crontab.

Actually, there is a default place for every user crontab file includes root. Once you create a crontab file for the user, the corresponding folder is put under /var/spool/cron/`.

For example, run as root, I want to set a recurring task for user `dsadm`:
```
crontab -u dsadm -e
```
Then edit like this:
```
00 21 * * * /home/dsadm/test.sh > /tmp/cron-log 2>&1
```
after run the job, go to `/tmp` folder you will see the log file.

to list the `dsadm` cron job:
```
crontab -l -u dsadm
```
to remove cron job for `dsadm`
```
crontab -r -u dsadm
```

#### System Crontab Files
Linux distributions normally have an `/etc/crontab` file. You can also edit here, but the format is a little bit difference:
```
# Example of job definition:
# .---------------- minute (0 - 59)
# |  .------------- hour (0 - 23)
# |  |  .---------- day of month (1 - 31)
# |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...
# |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
# |  |  |  |  |
# *  *  *  *  * user-name  command to be executed
```

### Understanding User IDs and User Switching
We’ve discussed how `setuid` programs such as `sudo` and `su` allow you to change users:
```
---s--x--x 1 root root 143248 May 28  2018 /usr/bin/sudo
-rwsr-xr-x 1 root root 32184 Jul 12  2018 /usr/bin/su
```

In reality, every process has more than one user ID. When you run a setuid program, Linux sets the effective user ID to the program’s owner during execution, but it keeps your original user ID in the real user ID.

Think of the effective user ID as the actor and the real user ID as the owner. The real user ID defines the user that can interact with the running process—most significantly, which user can kill and send signals to a process. For example, if user A starts a new process that runs as user B (based on setuid permissions), user A still owns the process and can kill it.

On normal Linux systems, most processes have the **same** `effective user ID` and `real user ID`. I verify this by a test.sh script run as` dsadm`, the euser and ruser are the same:
```
-rwsr-xr-x 1 root  root        56 May 11 22:17 test.sh
```

By default, `ps` and other system diagnostic programs show the `effective user ID`.

In conductor container, I run many su commands, you can see this, the euser and ruser are different:
```
ps -eo pid,euser,ruser,comm

1574 root     dsadm    su
2735 root     dsadm    su
4535 root     dsadm    su
```

### PAM
In 1995 Sun Microsystems proposed a new standard called `Pluggable Authentication Modules (PAM)`, a system of shared libraries for authentication. To authenticate a user, an application hands the user to PAM to determine whether the user can successfully identify itself. 

Because there are many kinds of authentication scenarios, PAM employs a number of dynamically `loadable authentication modules`. Each module performs a specific task; for example, the `pam_unix.so` module can check a user’s password.

#### PAM Configuration
You’ll normally find PAM’s application configuration files in the `/etc/pam.d` directory (older systems may use a single `/etc/pam.conf` file). 

Let's see an example:
```
auth       requisite     pam_shells.so
```
Each configuration line has three fields: `function type`, `control argument`, and `module`:
* Function type. The function that a user application asks PAM to perform. Here, it’s `auth`, the task of authenticating the user.

* Control argument. This setting controls what PAM does after success or failure of its action for the current line (`requisite` in this example).

* Module. The authentication module that runs for this line, determining what the line actually does. Here, the `pam_shells.so` module checks to see whether the user’s current shell is listed in `/etc/shells`.

PAM configuration is detailed on the pam.conf(5) manual page:
```
man 5 pam.conf
```

## Chapter 8. A Closer Look at Processes and Resource Utilization
This chapter takes you deeper into the relationships between processes, the kernel, and system resources.

Many of the tools that you see in this chapter are often thought of as performance-monitoring tools. They’re particularly helpful if your system is slowing to a crawl and you’re trying to figure out why.

### Tracking Processes
The `top` program is often more useful than `ps` because it displays the current system status as well as many of the fields in a `ps` listing, and it updates the display every second.

You can send commands to `top` with keystrokes. When you enter `top` command:
```js
Tasks: 382 total,   2 running, 380 sleeping,   0 stopped,   0 zombie
%Cpu(s):  1.2 us,  0.5 sy,  0.0 ni, 96.4 id,  0.4 wa,  0.0 hi,  0.2 si,  1.3 st
KiB Mem :  8009536 total,   441360 free,   930236 used,  6637940 buff/cache
KiB Swap:        0 total,        0 free,        0 used.  6284448 avail Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
10438 root      20   0  519148 282140  19392 S   4.6  3.5 586:19.28 kube-apiserver
10449 root      20   0  275308  80184  14584 S   4.3  1.0 516:46.86 kube-controller
 9691 root      20   0 1648968 159104  30708 S   2.6  2.0 492:06.83 kubelet
10206 root      20   0   10.1g  56568   7760 S   2.0  0.7 262:08.45 etcd
19459 root      20   0   82100  53780   9392 S   1.7  0.7 209:35.52 calico-node
...
```
If you see task %CPU is larger than 100, it must be a multi-thread and leverages multi-core. You can use `H` to toggle thread display rather than task, you will see multi-thead and each of them %CPU.

> Note: if you want to see memory in MB, GB... Typing `shift + e/E` cycle through.

then type followings:
```yaml
Spacebar: Updates the display immediately.
H: show threads in stead of tasks.
M: Sorts by current resident memory usage.
T: Sorts by total (cumulative) CPU usage.
P: Sorts by current CPU usage (the default).
u: Displays only one user’s processes.
f: Selects different statistics to display and sort. (use arrow to move and space to select)
?: Displays a usage summary for all top commands.
```

### Finding Open Files by lsof
One use for this command is when a disk cannot be unmounted because (unspecified) files are in use. The listing of open files can be consulted (suitably filtered if necessary) to identify the process that is using the files.

The `lsof` command lists open files and the processes using them. `lsof` doesn’t stop at regular files, it can list network resources, dynamic libraries, pipes, and more.

For example:
Display entries for open files in `/usr` directory and successors.
```
lsof /usr/*
```
List open files for a particular PID:
```
lsof -p 1623
```

### Tracing Program Execution and System Calls
The most common use is to start a program using `strace`, which prints a list of `system calls` made by the program. This is useful if the program continually crashes, or does not behave as expected; for example using strace may reveal that the program is attempting to access a file which does not exist or cannot be read.

The `strace` (system call trace) and `ltrace` (library trace) commands can help you discover what a program attempts to do. These tools produce extraordinarily large amounts of output, but once you know what to look for, you’ll have more tools at your disposal for tracking down problems.

For example:
```
strace cat not_a_file
```
you get errors in `open("not_a_file", O_RDONLY)` line:
```
execve("/usr/bin/cat", ["cat", "not_a_file"], [/* 23 vars */]) = 0
brk(NULL)                               = 0x81f000
mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fc85159d000
access("/etc/ld.so.preload", R_OK)      = -1 ENOENT (No such file or directory)
...
...
close(3)                                = 0
fstat(1, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 0), ...}) = 0
open("not_a_file", O_RDONLY)            = -1 ENOENT (No such file or directory)
write(2, "cat: ", 5cat: )                    = 5
write(2, "not_a_file", 10not_a_file) 
...
```

### Threads
In Linux, some processes are divided into pieces called threads.

To display the thread information in `ps`, add the m option.
For example:
```
ps axm -o pid,tid,command
```
```
PID   TID   COMMAND
1891     - db2ckpwd 0
    -  1891 -
1892     - db2ckpwd 0
    -  1892 -
3501     - db2fmp ( ,1,0,0,0,0,0,00000000,0,0,0,0000000000000000,0000000000000000,00000000,00000000,00000000,0000000
    -  3501 -
    -  3502 -
    -  3503 -
...
```
The main thread ID is the as the process ID

### Introduction to Resource Monitoring
To monitor one or more specific processes over time, use the `-p` option to `top`, with this syntax:
```
top -p <pid>
```

#### Adjusting Process Priorities
You can change the way the kernel **schedules** a process in order to give the process more or less CPU time than other processes. 

The kernel runs each process according to its scheduling priority, which is a number between `–20` and `20`, with `–20` being the **foremost** priority.
```
ps axl

F   UID   PID  PPID PRI  NI    VSZ   RSS WCHAN  STAT TTY        TIME COMMAND
4  1000     1     0  20   0  15120  1596 do_wai Ss   ?          0:00 /bin/bash /opt/IBM/InformationServer/initScripts
4     0  1882     1  20   0 1225076 48936 futex_ Sl  ?          0:00 db2wdog 0 [db2inst1]
4  1000  1884  1882  20   0 10302284 1699292 futex_ Sl ?       58:25 db2sysc 0
5     0  1890  1882  20   0 1227236 18292 do_msg S   ?          0:13 db2ckpwd 0

```
```
top

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 1884 db2inst1  20   0    9.8g   1.6g   1.6g S   1.3 10.4  58:25.82 db2sysc
    1 db2inst1  20   0   15120   1596   1360 S   0.0  0.0   0:00.01 startcontainer.
 1882 root      20   0 1225076  48936  33072 S   0.0  0.3   0:00.13 db2syscr
```

`PR` is the priority value. `NI` (nice value), high nice value means nicer, more likely to give up CPU time.

Alter the nice value:
```
renice <value> <pid>
```

#### Load Averages
The `load average` is the average number of processes currently ready to run. Keep in mind that most processes on your system are usually waiting for input (from the keyboard, mouse, or network, for example), meaning that most processes are not ready to run and should contribute nothing to the load average. Only processes that are actually doing something affect the load average.

```bash
# uptime
... up 91 days, ... load average: 0.08, 0.03, 0.01
```

The three numbers are the load averages for the past 1 minute, 5 minutes, and 15 minutes, respectively. An average of only 0.01 processes have been running across all processors for the past 15 minutes. 

If a load average goes up to around 1, a single process is probably using the CPU nearly all of the time. To identify that process, use the top command; the process will usually rise to the the top of the display.

If you have two cores, a load average of 1 means that only one of the cores is likely active at any given time, and a load average of 2 means that both cores have just enough to do all of the time.

A high load average does not necessarily mean that your system is having trouble. A system with enough memory and I/O resources can easily handle many running processes. If your load average is high and your system still responds well, don’t panic

However, if you sense that the system is slow and the load average is high, you might be running into memory performance problems. 

### Memory
CPU has a `memory management unit (MMU)` that translates the virtual memory addresses used by processes into real ones. The kernel assists the MMU by breaking the memory used by processes into smaller chunks called `pages`.

The kernel maintains a data structure, called a `page table`, that contains a mapping of a processes’ virtual page addresses to real page addresses in memory. As a process accesses memory, the MMU translates the virtual addresses used by the process into real addresses based on the kernel’s page table.

A user process does not actually need all of its pages to be immediately available in order to run. The kernel generally loads and allocates pages as a process needs them; this system is known as `on-demand paging` or just `demand paging`.

#### Page Faults
If a memory page is not ready when a process wants to use it, the process triggers a `page fault`.

* MINOR PAGE FAULTS
  A minor page fault occurs when the desired page is actually in main memory but the MMU doesn’t know where it is. This can happen when the process requests more memory or when the MMU doesn’t have enough space to store all of the page locations for a process. In this case, the kernel tells the MMU about the page and permits the process to continue. Minor page faults aren’t such a big deal, and many occur as a process runs. Unless you need maximum performance from some memory-intensive program, you probably shouldn’t worry about them.

* MAJOR PAGE FAULTS
  A major page fault occurs when the desired memory page isn’t in main memory at all, which means that the **kernel must load it from the disk or some other slow storage mechanism**. Some major page faults are unavoidable, such as those that occur when you load the code from disk when running a program for the first time. 

Let's see the page faults:
```
# /usr/bin/time netstat > /dev/null

0.05user 0.02system 0:01.74elapsed 4%CPU (0avgtext+0avgdata 2556maxresident)k
1752inputs+0outputs (3major+781minor)pagefaults 0swaps
```
There are 3 major page faults and 781 minor page faults when running `netstat` program. The major page faults occurred when the kernel needed to load the program from the disk for the first time. If you ran the command again, you probably wouldn’t get any major page faults because the kernel would have cached the pages from the disk: 
```
# /usr/bin/time netstat > /dev/null

0.04user 0.02system 0:01.61elapsed 3%CPU (0avgtext+0avgdata 2552maxresident)k
0inputs+0outputs (0major+783minor)pagefaults 0swaps
```

> Note that `time` here is not the shell built-in `time` command! If you run
> ```
> type -a time
> ```
> you will see
> ```
> time is a shell keyword
> time is /usr/bin/time
> ```
> see this [doc](https://www.cyberciti.biz/faq/linux-command-to-see-major-minor-pagefaults/)

If you’d rather see the **number** of page faults of processes as they’re running, use `top` or `ps`. When running `top`, use `f` to add the displayed fields and space to display the `nMaj` and `nMin`.
```
# top

  PID USER      %CPU  PR  NI    VIRT    RES    SHR S %MEM     TIME+ COMMAND                                nMaj nMin
 1303 dsadm      1.7  20   0 4753196  82704  13412 S  0.5 158:31.05 java                                      0  25k
 1929 dsadm      0.3  20   0  203344   2556   2116 S  0.0   2:16.80 ResTrackApp                               0 1028
```

When using `ps`, you can use a custom output format to view the page faults for a particular process:
```
# ps -o pid,min_flt,maj_flt 1

  PID  MINFL  MAJFL
    1   2059      6
```

### Monitoring CPU and Memory Performance
Among the many tools available to monitor system performance, the `vmstat` command is one of the oldest, with minimal overhead. You’ll find it handy for getting a high-level view of how often the kernel is swapping pages in and out, how busy the CPU is, and IO utilization.

```
vmstat 2

procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 2  0      0 174452   1064 6874152    0    0    17    55    4    0  2  2 95  0  1
 2  0      0 173900   1064 6874168    0    0     0    16 10362 12869  5  3 90  0  2
 3  0      0 173932   1064 6874180    0    0     0   291 9110 10761  2  1 95  1  1
 0  0      0 174056   1064 6874180    0    0     0    59 9126 12447  3  3 92  0  1
 0  0      0 174228   1064 6874184    0    0     0   167 7100 9601  1  1 97  0  1
```

Not easy to understand, can dig deeper into it by reading vmstat(8) manual page. 

### I/O Monitoring
Like `vmstat` and `netstat` (talk later), we have `iostat`.
```
iostat 2 -d -p ALL

Linux 3.10.0-862.14.4.el7.x86_64 (dstest1.fyre.ibm.com)         05/21/2019      _x86_64_        (8 CPU)

Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
vda              18.12        87.72       201.61  270042131  620650624
vda1              0.00         0.00         0.00      11165       2270
vda2             13.59        87.71       201.61  270022634  620648353
dm-0             17.99        86.13       201.61  265157850  620648353
dm-1              0.06         1.58         0.00    4860836          0
```
This means update every 2 seconds, show device only and show all partitions.

If you need to dig even deeper to see I/O resources used by individual processes, the `iotop` tool can help. Using `iotop` is similar to using top.
```
iotop

Total DISK READ:         4.76 K/s | Total DISK WRITE:     333.31 K/s
  TID  PRIO  USER       DISK READ DISK  WRITE  SWAPIN     IO> COMMAND
  260 be/3 root          0.00 B/s  38.09 K/s  0.00 %  6.98 % [jbd2/sda1-8]
 2611 be/4 juser         4.76 K/s  10.32 K/s  0.00 %  0.21 % zeitgeist-daemon
 ...
```
It shows TID (thread ID) instead of PID, PRIO (priority) indicates the IO priority, `be/3` is more important than `be/4`. The kernel uses the `scheduling class` to add more control for I/O scheduling. You’ll see three scheduling classes from `iotop`:

* **be** Best-effort. The kernel does its best to fairly schedule I/O for this class. Most processes run under this I/O scheduling class.

* **rt** Real-time. The kernel schedules any real-time I/O before any other class of I/O, no matter what.

* **idle** Idle. The kernel performs I/O for this class only when there is no other I/O to be done. There is no priority level for the idle scheduling class.

### Per-Process Monitoring
The `pidstat` utility allows you to see the resource consumption of a process over time in the style of `vmstat`. 
```
# pidstat -p 27946 1

Linux 3.10.0-862.14.4.el7.x86_64 (myk8s1.fyre.ibm.com)  05/21/2019      _x86_64_        (4 CPU)

08:16:27 PM   UID       PID    %usr %system  %guest    %CPU   CPU  Command
08:16:28 PM  1002     27946    0.00    0.00    0.00    0.00     3  tail
08:16:29 PM  1002     27946    0.00    0.00    0.00    0.00     3  tail
08:16:30 PM  1002     27946    0.00    0.00    0.00    0.00     3  tail
```
The `CPU` column tells you about this process is running on which CPU.

## Chapter 9. Network and Configuration
> Note that the `ifconfig` command, as well some of the others you’ll see later in this chapter (such as `route` and `arp`), has been technically supplanted with the newer `ip` command. The ip command can do more than the old commands, and it is preferable when writing scripts. However, most people still use the old commands when manually working with the network, and these commands can also be used on other versions of Unix. For this reason, we’ll use the old-style commands.

### Routes and the Kernel Routing Table
Let's see the routing table by `route` command, `-n` means show numerical address instead of hostname:
```
# route -n

Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         9.30.94.1       0.0.0.0         UG    0      0        0 eth1
9.30.94.0       0.0.0.0         255.255.254.0   U     0      0        0 eth1
169.254.0.0     0.0.0.0         255.255.0.0     U     1002   0        0 eth0
169.254.0.0     0.0.0.0         255.255.0.0     U     1003   0        0 eth1
172.16.0.0      0.0.0.0         255.255.0.0     U     0      0        0 eth0
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
192.168.0.0     0.0.0.0         255.255.255.0   U     0      0        0 *
192.168.0.2     0.0.0.0         255.255.255.255 UH    0      0        0 calib4daf4f1db0
192.168.0.3     0.0.0.0         255.255.255.255 UH    0      0        0 cali987b4d0c33f
192.168.1.0     172.16.182.156  255.255.255.0   UG    0      0        0 eth0
192.168.2.0     172.16.182.187  255.255.255.0   UG    0      0        0 eth0
```
The `Destination` column tells you a network prefix (outside network), and the `Genmask` column is the netmask corresponding to that network. Each network has a `U` under its `Flags` column, indicating that the route is active (“up”).

There is a `G` in the `Flags` column, meaning that communication for this network must be sent through the gateway in the Gateway column, for example, for networl `0.0.0.0/0` send through it's gateway `9.30.94.1`. If no `G` in `Flags`, indicating that the network is directly connected in some way.

An entry for `0.0.0.0/0` in the routing table has special significance because it matches any address on the Internet. This is the default route, and the address configured under the Gateway column (in the `route -n` output) in the default route is the `default gateway`.

### Basic ICMP and DNS Tools
#### ping
```
# ping baidu.com

PING baidu.com (123.125.114.144) 56(84) bytes of data.
64 bytes from 123.125.114.144 (123.125.114.144): icmp_seq=1 ttl=40 time=212 ms
64 bytes from 123.125.114.144 (123.125.114.144): icmp_seq=2 ttl=40 time=212 ms
64 bytes from 123.125.114.144 (123.125.114.144): icmp_seq=3 ttl=40 time=212 ms
^C
--- baidu.com ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2002ms
rtt min/avg/max/mdev = 212.335/212.495/212.578/0.393 ms
```
`56(84) bytes of data` means send a 56 bytes packet (84 bytes when include header).
`icmp_seq` is sequence number, sometimes you will have gap, usually means there’s some kind of connectivity problem.
`time` is round-trip time.

#### traceroute
One of the best things about `traceroute` is that it reports return trip times at each step in the route:
```
## -n will not do hostname lookup for IP in output
# traceroute -n google.com

traceroute to google.com (172.217.1.206), 30 hops max, 60 byte packets
 1  9.30.94.3  0.626 ms  0.742 ms  0.845 ms
 2  9.30.156.13  0.529 ms  0.801 ms  0.918 ms
 3  9.55.129.109  0.668 ms  0.852 ms 9.55.129.105  0.515 ms
 4  9.55.187.13  0.476 ms  0.255 ms  0.425 ms
 5  9.55.128.6  0.326 ms  0.433 ms  0.294 ms
 6  9.64.38.194  0.941 ms  0.898 ms  0.843 ms
 7  9.64.3.86  18.220 ms  18.230 ms  18.229 ms
 8  9.64.3.85  31.521 ms  31.527 ms  31.563 ms
 9  9.17.3.35  31.803 ms  31.613 ms  31.875 ms
 ...
```

#### DNS and host
To find the IP address behind a domain name, use the `host` command:
```
# host www.google.com

www.google.com has address 172.217.11.228
www.google.com has IPv6 address 2607:f8b0:400f:801::2004
```

You can also use `host` in reverse: Enter an IP address instead of a hostname to try to discover the hostname behind the IP address. But don’t expect this to work reliably. Many hostnames can represent a single IP address, and DNS doesn’t know how to determine which hostname should correspond to an IP address. 

### Kernel Network Interfaces
Network interfaces have names that usually indicate the kind of hardware underneath, such as `eth0` (the first Ethernet card in the computer) and `wlan0` (a wireless interface).

```
eth1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 9.30.94.85  netmask 255.255.254.0  broadcast 9.30.95.255
        ether 00:20:09:1e:5e:55  txqueuelen 1000  (Ethernet)
        RX packets 17164860  bytes 9046289828 (8.4 GiB)
        RX errors 0  dropped 6  overruns 0  frame 0
        TX packets 11669220  bytes 9566003426 (8.9 GiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
```
`UP,RUNNING` means this interface is active.

### Resolving Hostnames
On most systems, you can **override** hostname lookups with the `/etc/hosts` file.
Usually resolution will first check this file before resort to DNS server.

The traditional configuration file for DNS servers is `/etc/resolv.conf`:
```
## this is the search pattern:
search fyre.ibm.com. svl.ibm.com.
nameserver 172.16.200.52
nameserver 172.16.200.50
```
`172.16.200.52` and `172.16.200.50` are the DNS server IP.

### netstat command
This `netstat` command is extremely important and common in use. Ususally I use `netstat -tunlp`, let's dig deeper into it:

* `-t`: show TCP connection.
* `-u`: show UDP connection.
* `-n`: show numerical addresses.
* `-l`: show only listening sockets.
* `-p`: show PID belongs to.

Instead of `ifconfig` to see the interface, you can use:
```
# netstat -i

Kernel Interface table
Iface      MTU    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
cali9550  1440 60880456      0      6 0      37444050      0      0      0 BMRU
cali986f  1440        0      0      0 0             0      0      0      0 BMRU
docker0   1500        0      0      0 0             0      0      0      0 BMU
eth0      1500 5606028968      0      0 0      33647018      0      0      0 BMRU
eth1      1500 60880456      0      6 0      37444050      0      0      0 BMRU
lo       65536 431426103      0      0 0      431426103      0      0      0 LRU
```

Instead of `route -n` to see route table, you can use:
```
# netstat -rn
```

Show TCP connections (not include listening sockets):
```
# netstat -tn
```

To see well-known ports translate into names. check `/etc/services` file:
```
...
http            80/tcp          www www-http    # WorldWideWeb HTTP
http            80/udp          www www-http    # HyperText Transfer Protocol
http            80/sctp                         # HyperText Transfer Protoco
...
```
On Linux, only processes running as the superuser can use ports 1 through 1023. All user processes may listen on and create connections from ports 1024 and up.

**I skip the rest of this chapter, majority is concept**

## Chapter 10. Network Applications and Services
Let's mainly focus on some important commands here:

### curl command
`curl` is a command line tool to transfer data to or from a server, using any of the supported protocols (HTTP, FTP, IMAP, POP3, SCP, SFTP, SMTP, TFTP, TELNET, LDAP or FILE). `curl` is powered by `Libcurl`. This tool is preferred for automation, since it is designed to work without user interaction. curl can transfer multiple file at once.

you can refer this [article](https://www.geeksforgeeks.org/curl-command-in-linux-with-examples/).

### Diagnostic Tools
`lsof`(list open files) can track open files, but it can also list the programs currently using or listening to ports. Please read more when you need this tool.

`tcpdump`, a command tool version of wireshark.

`netcat`(or `nc`) I used it before for developing PXEngine, we use TCP to replace ssh connection between conductor and compute containers. `netcat` can connect to remote TCP/UDP ports, specify a local port, listen on ports, scan ports, redirect standard I/O to and from network connections, and more.

I remember I use `nc` to listen on a port and on other side connect to that port and transfer data.
```bash
## install
yum install -y nc
apt install -y netcat

## -l: listening mode
## -p: port
nc -l -p 1234
## client
nc localhost 1234
```
`netcat` can be used for TCP, UDP, Unix-domain sockets.


`nmap` scans all ports on a machine or network of machines looking for open ports, and it lists the ports it finds. 
```
# nmap myk8s1.fyre.ibm.com

Starting Nmap 6.40 ( http://nmap.org ) at 2019-05-22 23:33 PDT
Nmap scan report for myk8s1.fyre.ibm.com (9.30.94.85)
Host is up (0.00024s latency).
Not shown: 995 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
111/tcp  open  rpcbind
179/tcp  open  bgp
2049/tcp open  nfs
5000/tcp open  upnp

Nmap done: 1 IP address (1 host up) scanned in 0.20 seconds
```

## Chapter 11. Introduction to Shell Scripts
A shell script is a series of commands written in a file.
The `#!` part is called a `shebang`.

When writing scripts and working on the command line, just remember what happens whenever the shell runs a command:
1. Before running the command, the shell looks for variables, globs, and other substitutions and performs the substitutions if they appear.
2. The shell passes the results of the substitutions to the command.

if you use single quote:
```
grep 'r.*t' /etc/passwd
```
This will prevent sheel from expanding the `*` in current directory.
```
grep 'r.*t /etc/passwd'
```
This will fail, because things wrapped by single/double quote treat as one parameter.

Double quotes (") work just like single quotes, except that the shell expands **variables** that appear within double quotes. It will not expand globs like `*` in double quotes!

Just like I saw, use `shift` to forward arguments passed in:
```
#!/bin/sh
echo $1
shift
echo $1
```

`$#` is the number of arguments passed in, used in loop to pick up parameters.
`$@` represents all of the script arguments.
`$$` holds the PID of current shell.

bad message should go to standard error, just like redriect standard error to standard output:
```
echo $0: bad option ... 1>&2
```
`$?` exit code: If you intend to use the exit code of a command, you **must** use or store the code immediately after running the command. 

### if condition
Let's see an example, these 2 are good:
```
if [ "$1" = hi ]; then
if [ x"$1" = x"hi" ]; then
```
Here, `""` is vital, since user may not input `$1`, if no double quotes, it could be:
```
if [ = hi ]; then
```
the test (`[`) command aborts immediately.

> Note that the stuff follows `if` is a command! so we have `;` before `then`.

So you can use other commands instead of `[` command, cool!
```
#!/bin/sh
if grep -q daemon /etc/passwd; then
    echo The daemon user is in the passwd file.
else
    echo There is a big problem. daemon is not in the passwd file.
fi
```

Let's see `&&` and `||` and test condition:
```
#!/bin/sh
if [ "$1" = hi ] || [ "$1" = bye ]; then
    echo 'The first argument was "'$1'"'
fi
```
The `-a` and `-o` flags are the logical `and` and `or` operators in test:
```
[ "$1" = hi -o "$1" = ho ]
```

### test command
There are dozens of test operations, all of which fall into three general categories: file tests, string tests, and arithmetic tests.

#### file filter
`-f`: regular file return 0
`-e`: file exist return 0
`-s`: not empty file return 0
`-d`: directory return 0
`-h`: softlink return 0

File permission:
`-r`: readable
`-w`: writable
`-x`: executable
`-u`: setuid
`-g`: setgid
`-k`: sticky

> The test command follows symbolic links (except for the `-h` test). That is, if link is a symbolic link to a regular file, [ `-f` link ] returns an exit code of true (0).

Finally, three binary operators (tests that need two files as arguments) are used in file tests, but they’re not terribly common.
`[ file1 -nt file2 ]`: if file1 has a newer modification date than file2 return 0
`[ file1 -ot file2 ]`: if file1 has a older modification date than file2 return 0
`[ file1 -ef file2 ]`: compares two files and returns true if they share inode numbers and devices.

#### string test
`=`: equal
`!=`: not equal
`-z`: empty string return 0
`-n`: not empty return 0

#### arithmetic test
`-eq`: equal to
`-ne`: not equal to
`-lt`: less than
`-gt`: greater than
`-le`: less than or equal to
`-ge`: greater than or equal to

### case condition
The case keyword forms another conditional construct that is exceptionally useful for matching strings, it can do pattern matching:
```
#!/bin/sh
case $1 in
    bye)
        echo Fine, bye.
        ;;
    hi|hello)
        echo Nice to see you.
        ;;
    what*)
        echo Whatever.
        ;;
    *)
        echo 'Huh?'
        ;;
esac
```
> Each case must end with a double semicolon (;;) or you risk a syntax error.

### loop
for loop:
```
#!/bin/sh
for str in one two three four; do
    echo $str
done
```

while loop:
```
#!/bin/sh
FILE=/tmp/whiletest.$$;
echo firstline > $FILE
while tail -10 $FILE | grep -q firstline; do
    # add lines to $FILE until tail -10 $FILE no longer prints "firstline"
    echo -n Number of lines in $FILE:' '
    wc -l $FILE | awk '{print $1}'
    echo newline >> $FILE
done

rm -f $FILE
```
In fact, if you find that you need to use while, you should probably be using a language like awk or Python instead.

### Command Substitution
You can use a command’s output as an argument to another command, or you can store the command output in a shell variable by enclosing a command in `$()`.

### Temporary File Management
Note the `mktemp` command:
```
#!/bin/sh
TMPFILE1=$(mktemp /tmp/im1.XXXXXX)
TMPFILE2=$(mktemp /tmp/im2.XXXXXX)

cat /proc/interrupts > $TMPFILE1
sleep 2
cat /proc/interrupts > $TMPFILE2
diff $TMPFILE1 $TMPFILE2
rm -f $TMPFILE1 $TMPFILE2
```

If the script is aborted, the temporary files could be left behind. In the preceding example, pressing `CTRL-C` before the second cat command leaves a temporary file in `/tmp`. Avoid this if possible. Instead, use the `trap` command to create a signal handler to catch the signal that `CTRL-C` generates and remove the temporary files, as in this handler:
```
#!/bin/sh
TMPFILE1=$(mktemp /tmp/im1.XXXXXX)
TMPFILE2=$(mktemp /tmp/im2.XXXXXX)
trap "rm -f $TMPFILE1 $TMPFILE2; exit 1" INT
```
You must use exit in the handler to explicitly end script execution, or the shell will continue running as usual after running the signal handler.

> Note that in `startcontainer.sh` we also have trap and we use shell function there, now I understand!

### Important Shell Script Utilities

#### basename
This one strip the extension of file name:
```
# basename example.html .html

example
```
This one git rid of directory in full path:
```
# basename /usr/local/bin/example

example
```

#### awk
The `awk` command is not a simple single-purpose command; it’s actually a powerful programming language. Unfortunately, awk usage is now something of a lost art, having been replaced by larger languages such as Python.

#### sed
The sed program (sed stands for stream editor) is an automatic **text editor** that takes an input stream (a file or the standard input), alters it according to some expression, and prints the results to standard output.

#### expr
The `expr` command is a clumsy, slow way of doing math. If you find yourself using it frequently, you should probably be using a language like Python instead of a shell script.

### Subshells
An entirely **new** shell process that you can create just to run a command or two. The new shell has a **copy** of the original shell’s environment, and when the new shell exits, any changes you made to its shell environment disappear, leaving the initial shell to run as normal.

Using a subshell to make a single-use alteration to an environment variable is such a common task:
```
# (PATH=/usr/confusing:$PATH; ./runprogram.sh)
```

## Chapter 12. Moving Files Across the Network

### Quick copy browser
Go to target directory run:
```
python -m SimpleHTTPServer
```
This usually open 8000 port on your machine, then go to another machine open:
```
# use ifconfig to check the source machine IP
192.168.1.29:8000
```
you can see the content there.

### rsync
Actually you can first enable Mac ssh access then use `rsync` to backup files:
System Preference -> Sharing -> check remote login

To get `rsync` working between two hosts, the rsync program must be installed on both the source and destination, and you’ll need a way to access one machine from the other.

Copy files to remote home:
```
rsync files remote:
rsync files user@remote:
```
If `rsync` isn’t in the remote path but is on the system, use `--rsync-path=path` to manually specify its location.

Unless you supply extra options, `rsync` copies only files. You will see:
```
skipping directory xxx
```
To transfer entire directory hierarchies, complete with symbolic links, permissions, modes, and devices—use the `-a` option. 
```
rsync -nv files -a dir user@remote:
```
`-n`: dry-run, this is vital when you are not sure.
`-vv`: verbose mode

To make an exact replica of the source directory, you must delete files in the destination directory that do not exist in the source directory:
```
rsync -v --delete -a dir user@remote:
```
Please use `-n` dry-run to see what will be deleted before performing command.

Be particular careful with tailing slash after dir:
```
rsync -a dir/ user@remote:dest
```
This will copy all files under dir to dest folder in remote instead of copy dir into dest.

You can also `--exclude=`, `--exclude-from=` and `--include=` in command.

To speed operation, `rsync` uses a quick check to determine whether any files on the transfer source are already on the destination. The quick check uses a combination of the file size and its last-modified date. 

When the files on the source side are not identical to the files on the destination side, `rsync` transfers the source files and overwrites any files that exist on the remote side. The default behavior may be inadequate, though, because you may need additional reassurance that files are indeed the same before skipping over them in transfers, or you may want to put in some extra safeguards.

* `--checksum `(abbreviation: `-c`) Compute checksums (mostly unique signatures) of the files to see if they’re the same. This consumes additional I/O and CPU resources during transfers, but if you’re dealing with sensitive data or files that often have uniform sizes, this option is a must. (This will focus on file content, not date stamp)

* `--ignore-existing` Doesn’t clobber files already on the target side.

* `--backup` (abbreviation: `-b`) Doesn’t clobber files already on the target but rather renames these existing files by adding a `~` suffix to their names before transferring the new files.

* `--suffix=s` Changes the suffix used with --backup from `~` to `s`.

* `--update` (abbreviation: `-u`) Doesn’t clobber any file on the target that has a later date than the corresponding file on the source.

You can also compress the dir when transfer:
```
rsync -az dir user@remote:
```

You can also reverse the process:
```
rsync -a user@remote:dir dest
```

The rest of this chapter talks `samba` for file sharing, I skip it.


## Chapter 13. User Environments
Startup files play an important role at this point, because they set defaults for the shell and other interactive programs. They determine how the system behaves when a user logs in.

I see vi theme config in `~/.bashrc` file.

### The Command Path
The most important part of any shell startup file is the command path. The path should cover the directories that contain every application of interest to a regular user. At the very least, the path should contain these components, in order:
```
/usr/local/bin
/usr/bin
/bin
```
If the application is on another directory, use symbolic link to `/usr/local/bin` or you defined `bin` folder.

### The prompt
I never use this so far, usually prompt shows hostname, username, current directory and sign (`$` or `#`). you can change the color and more.

### Alias
This is common use, sometimes I use shell functions too.

### Permission mask
It depends on your needs:
```
umask 022/077
```

### Startup file order
These startup files are used to create **environment**. Each script has a specific use and affects the login environment differently. Every subsequent script executed can override the values assigned by previous scripts.

The two main shell `instance types` are `interactive` and `noninteractive`, but of those, only interactive shells are of interest because noninteractive shells (such as those that run shell scripts) usually don’t read any startup files. 

Interactive shells are the ones that you use to **run commands from a terminal**, they can be classified as `login` or `non-login`.

I know there are lots of startup files under each user's home directory or in other system folder, how do they take effect? In what order:
**Reference Doc**
[Difference between Login shell and Non login shell](http://howtolamp.com/articles/difference-between-login-and-non-login-shell/)

Logging in remotely with `SSH` also gives you a login shell. 

You can tell if a shell is a login shell by running `echo $0`; if the first character is a `-`, the shell’s a login shell.

When Bash is invoked as a `Login` shell:
1. Login process calls `/etc/profile`
2. `/etc/profile` calls the scripts in `/etc/profile.d/`
3. Login process calls `~/.bash_profile`, `~/.bash_login` and `~/.profile`. running only the first one that it sees.

Login Shells created by explicitly telling to login.
examples: `# su - | # su -l | # su --login | # su USERNAME - | # su -l USERNAME | # su --login USERNAME | # sudo -i`

When bash is invoked as a `Non-login` shell;
1. Non-login process(shell) calls `/etc/bashrc`
2. then calls `~/.bashrc`

`Non-Login` shells created using the below command syntax:
examples: `# su | # su USERNAME`

Note that I can run `bash` or `sh` or `csh` in terminal, it will give me a new simple prompt without user profile or setting...

It seems if you use non-login like `su dsadm`, the export env vars are still there in `env` scope, I think the reason is it's not login! still use current environment. But if you run `su - dsadm`, it is gone.










]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>book</tag>
        <tag>linux</tag>
      </tags>
  </entry>
</search>
